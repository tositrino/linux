diff -Naur linux-2.6.25-org/Documentation/ABI/testing/sysfs-perfmon linux-2.6.25-id/Documentation/ABI/testing/sysfs-perfmon
--- linux-2.6.25-org/Documentation/ABI/testing/sysfs-perfmon	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/Documentation/ABI/testing/sysfs-perfmon	2008-04-23 10:40:24.000000000 +0200
@@ -0,0 +1,82 @@
+What:		/sys/kernel/perfmon
+Date:		Nov 2007
+KernelVersion:	2.6.24
+Contact:	eranian@hpl.hp.com
+
+Description:	provide the configuration interface for the perfmon2 subsystems.
+	        The tree contains information about the detected hardware, current
+		state of the subsystem as well as some configuration parameters.
+	
+		The tree consists of the following entries:
+
+	/sys/kernel/perfmon/debug (read-write):
+
+		Enable perfmon2 debugging output via klogd. Debug messages produced during
+		PMU interrupt handling are not controlled by this entry. The traces a rate-limited
+		to avoid flooding of the console. It is possible to change the throttling
+	        via /proc/sys/kernel/printk_ratelimit
+
+	/sys/kernel/perfmon/debug_ovfl (read-write):
+	
+		Enable perfmon2 overflow interrupt debugging. It is complementary to the
+		previous entry. It is also rate-limited and can be controlled using the
+		same /proc/sys/kernel/printk_ratelimit entry.
+
+
+	/sys/kernel/perfmon/pmc_max_fast_arg (read-only):
+	
+		Number of perfmon2 syscall arguments copied directly onto the
+   		stack (copy_from_user) for pfm_write_pmcs(). Copying to the stack avoids
+		having to allocate a buffer. The unit is the number of pfarg_pmc_t
+		structures.
+
+	/sys/kernel/perfmon/pmd_max_fast_arg (read-only):
+
+		Number of perfmon2 syscall arguments copied directly onto the
+   		stack (copy_from_user) for pfm_write_pmds()/pfm_read_pmds(). Copying
+		to the stack avoids having to allocate a buffer. The unit is the number
+		of pfarg_pmd_t structures.
+
+
+	/sys/kernel/perfmon/reset_stats (write-only):
+
+		Reset the statistics collected by perfmon2. Stats are available
+		per-cpu via debugfs.
+   	
+	/sys/kernel/perfmon/smpl_buffer_mem_cur (read-only):
+	
+		Reports the amount of memory currently dedicated to sampling
+   		buffers by the kernel. The unit is byte.
+
+   	/sys/kernel/perfmon/smpl_buffer_mem_max (read-write):
+
+		Maximum amount of kernel memory usable for sampling buffers. -1 means
+		everything that is available. Unit is byte.
+
+   	/sys/kernel/perfmon/smpl_buffer_mem_cur (read-only):
+
+		Current utilization of kernel memory in bytes.
+
+   	/sys/kernel/perfmon/sys_group (read-write):
+	
+		Users group allowed to create a system-wide perfmon2 context (session).
+   		-1 means any group. This control will be kept until we find a package
+		able to control capabilities via PAM.
+
+	/sys/kernel/perfmon/task_group (read-write):
+	
+		Users group allowed to create a per-thread context (session).
+   		-1 means any group. This control will be kept until we find a
+		package able to control capabilities via PAM.
+
+	/sys/kernel/perfmon/sys_sessions_count (read-only):
+
+		Number of system-wide contexts currently attached to CPUs.
+
+	/sys/kernel/perfmon/task_sessions_count (read-only):
+
+		Number of per-thread contexts currently attached to threads.
+
+   	/sys/kernel/perfmon/version (read-only):
+	
+		Perfmon2 interface revision number.
diff -Naur linux-2.6.25-org/Documentation/ABI/testing/sysfs-perfmon-fmt linux-2.6.25-id/Documentation/ABI/testing/sysfs-perfmon-fmt
--- linux-2.6.25-org/Documentation/ABI/testing/sysfs-perfmon-fmt	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/Documentation/ABI/testing/sysfs-perfmon-fmt	2008-04-23 10:40:24.000000000 +0200
@@ -0,0 +1,18 @@
+What:		/sys/kernel/perfmon/formats
+Date:		2007
+KernelVersion:	2.6.24
+Contact:	eranian@hpl.hp.com
+
+Description:	provide description of available perfmon2 custom sampling buffer formats
+		which are implemented as independent kernel modules. Each formats gets
+		a subdir which a few entries.
+
+		The name of the subdir is the name of the sampling format. The same name
+		must be passed to pfm_create_context() to use the format.
+
+		Each subdir XX contains the following entries:
+
+	/sys/kernel/perfmon/formats/XX/version (read-only):
+
+		Version number of the format in clear text and null terminated.
+
diff -Naur linux-2.6.25-org/Documentation/ABI/testing/sysfs-perfmon-pmu linux-2.6.25-id/Documentation/ABI/testing/sysfs-perfmon-pmu
--- linux-2.6.25-org/Documentation/ABI/testing/sysfs-perfmon-pmu	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/Documentation/ABI/testing/sysfs-perfmon-pmu	2008-04-23 10:40:24.000000000 +0200
@@ -0,0 +1,46 @@
+What:		/sys/kernel/perfmon/pmu
+Date:		Nov 2007
+KernelVersion:	2.6.24
+Contact:	eranian@hpl.hp.com
+
+Description:	provide information about the currently loaded PMU description module.
+		The module contains the mapping of the actual performance counter registers
+		onto the logical PMU exposed by perfmon.  There is at most one PMU description
+		module loaded at any time.
+
+		The sysfs PMU tree provides a description of the mapping for each register.
+		There is one subdir per config and data registers along an entry for the
+		name of the PMU model.
+	
+		The model entry is as follows:
+
+	/sys/kernel/perfmon/pmu_desc/model (read-only):
+
+		Name of the PMU model is clear text and zero terminated.
+   	
+		Then for each logical PMU register, XX, gets a subtree with the following entries:
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/addr (read-only):
+	
+		The physical address or index of the actual underlying hardware register.
+		On Itanium, it corresponds to the index. But on X86 processor, this is
+		the actual MSR address.
+		
+	/sys/kernel/perfmon/pmu_desc/pm*XX/dfl_val (read-only):
+
+		The default value of the register in hexadecimal.
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/name (read-only):
+
+		The name of the hardware register.
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/rsvd_msk (read-only):
+
+		The bitmask of reserved bits, i.e., bits which cannot be changed by
+		applications. When a bit is set, it means the corresponding bit in the
+		actual register is reserved.
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/width (read-only):
+
+		the width in bits of the registers. This field is only relevant for counter
+		registers.
diff -Naur linux-2.6.25-org/Documentation/ABI/testing/sysfs-perfmon-stats linux-2.6.25-id/Documentation/ABI/testing/sysfs-perfmon-stats
--- linux-2.6.25-org/Documentation/ABI/testing/sysfs-perfmon-stats	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/Documentation/ABI/testing/sysfs-perfmon-stats	2008-04-23 10:40:24.000000000 +0200
@@ -0,0 +1,100 @@
+What:		/sys/devices/system/cpu/cpuXX/perfmon
+Date:		Nov 2007
+KernelVersion:	2.6.24
+Contact:	eranian@hpl.hp.com
+
+Description:	exports internal perfmon2 statistics to user. Mostly used for debugging
+		and performance analysis of the perfmon2 subsystem. Statistics can be
+		reset using the /sys/kernel/perfmon/reset_stats entry.
+	
+		There is one subdir per online CPU. Each subdir contains the following
+		entries:
+
+	/sys/devices/system/cpu/cpuXX/perfmon/ctxsw_count (read-only):
+	
+		Number of PMU context switches (switch-out, switch-in or both).
+
+	/sys/devices/system/cpu/cpuXX/perfmon/ctxsw_ns (read-only):
+
+		Number of nanoseconds spent in the PMU context switch routine.
+		Dividing this number by the value of ctxsw_count, yields average
+		cost of the PMU context switch.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/fmt_handler_calls (read-only):
+
+		Number of calls to the sampling format routine that handles
+		PMU interrupts, i.e., typically the routine that records a sample.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/fmt_handler_ns (read-only):
+
+		Number of nanoseconds spent in the routine that handle PMU
+		interrupt in the sampling format. Dividing this number by
+		the number of calls provided by fmt_handler_calls, yields
+		average time spent in this routine.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_all_count (read-only):
+
+		Number of PMU interrupts received by the kernel.
+
+
+	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_nmi_count (read-only):
+
+		Number of Non Maskeable Interrupts (NMI) received by the kernel
+		for perfmon. This is relevant only on X86 hardware.
+	
+	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_ns (read-only):
+
+		Number of nanoseconds spent in the perfmon2 PMU interrupt
+		handler routine. Dividing this number of ovfl_intr_all_count
+		yields the average time to handle one PMU interrupt.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_regular_count (read-only):
+
+		Number of PMU interrupts which are actually processed by 
+		the perfmon interrupt handler. There may be spurious or replay
+		interrupts.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_replay_count (read-only):
+
+		Number of PMU interrupts which were replayed on context switch in or
+		on event set switching. Interrupts get replayed when they were in
+		flight at the time monitoring had to be stopped.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_spurious_count (read-only):
+
+		Number of PMU interrupts which were dropped because there was no
+		active context (session).
+
+	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_notify_count (read-only):
+
+		Number of user level notification sent. Notification are appended
+		as messages to the context queue. Notifications may be sent on
+		PMU interrupts.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/pfm_restart_count (read-only):
+
+		Number of times pfm_restart() is called.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/reset_pmds_count (read-only):
+
+		Number of times pfm_reset_pmds() is called.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/set_switch_count (read-only):
+
+		Number of event set switches.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/set_switch_ns (read-only):
+
+		Number of nanoseconds spent in the set switching routine.
+		Dividing this number by set_switch_count yields the average
+		cost of switching sets.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/handle_timeout_count (read-only):
+
+		Number of times the pfm_handle_timeout() routine is called. It is
+		used for timeout-based set switching.
+
+	/sys/devices/system/cpu/cpuXX/perfmon/handle_work_count (read-only):
+
+		Number of times pfm_handle_work() is called. The routine handles
+		asynchronous perfmon2 work for per-thread contexts (sessions).
diff -Naur linux-2.6.25-org/Documentation/perfmon2.txt linux-2.6.25-id/Documentation/perfmon2.txt
--- linux-2.6.25-org/Documentation/perfmon2.txt	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/Documentation/perfmon2.txt	2008-04-23 10:40:24.000000000 +0200
@@ -0,0 +1,208 @@
+              The perfmon2 hardware monitoring interface
+              ------------------------------------------
+			     Stephane Eranian
+			<stephane.eranian@hp.com>
+
+I/ Introduction
+
+   The perfmon2 interface provides access to the hardware performance counters of
+   major processors. Nowadays, all processors implement some flavors of performance
+   counters which capture micro-architectural level information such as the number
+   of elapsed cycles, number of cache misses, and so on.
+
+   The interface is implemented as a set of new system calls and a set of config files
+   in /sys.
+
+   It is possible to monitoring a single thread or a CPU. In either mode, applications
+   can count or collect samples. System-wide monitoring is supported by running a
+   monitoring session on each CPU. The interface support event-based sampling where the
+   sampling period is expressed as the number of occurrences of event, instead of just a
+   timeout.  This approach provides a much better granularity and flexibility.
+
+   For performance reason, it is possible to use a kernel-level sampling buffer to minimize
+   the overhead incurred by sampling. The format of the buffer, i.e., what is recorded, how
+   it is recorded, and how it is exported to user-land is controlled by a kernel module called
+   a custom sampling format. The current implementation comes with a default format but
+   it is possible to create additional formats. There is an in-kernel registration
+   interface for formats. Each format is identified by a simple string which a tool
+   can pass when a monitoring session is created.
+
+   The interface also provides support for event set and multiplexing to work around
+   hardware limitations in the number of available counters or in how events can be 
+   combined. Each set defines as many counters as the hardware can support. The kernel
+   then multiplexes the sets. The interface supports time-base switching but also
+   overflow based switching, i.e., after n overflows of designated counters.
+
+   Applications never manipulates the actual performance counter registers. Instead they see
+   a logical Performance Monitoring Unit (PMU) composed of a set of config register (PMC)
+   and a set of data registers (PMD). Note that PMD are not necessarily counters, they
+   can be buffers. The logical PMU is then mapped onto the actual PMU using a mapping
+   table which is implemented as a kernel module. The mapping is chosen once for each
+   new processor. It is visible in /sys/kernel/perfmon/pmu_desc. The kernel module
+   is automatically loaded on first use.
+
+   A monitoring session, or context, is uniquely identified by a file descriptor
+   obtained when the context is created. File sharing semantics apply to access
+   the context inside a process. A context is never inherited across fork. The file
+   descriptor can be used to received counter overflow notifications or when the
+   sampling buffer is full. It is possible to use poll/select on the descriptor 
+   to wait for notifications from multiplex contexts. Similarly, the descriptor
+   supports asynchronous notification via SIGIO.
+
+   Counters are always exported as being 64-bit wide regardless of what the underlying
+   hardware implements.
+
+II/ Kernel compilation
+
+    To enable perfmon2, you need to enable CONFIG_PERFMON
+
+III/ OProfile interactions
+
+    The set of features offered by perfmon2 is rich enough to support migrating
+    Oprofile on top of it. That means that PMU programming and low-level interrupt
+    handling could be done by perfmon2. The Oprofile sampling buffer management code
+    in the kernel as well as how samples are exported to users could remain through
+    the use of a custom sampling buffer format. This is how Oprofile work on Itanium.
+
+    The current interactions with Oprofile are:
+	- on X86: Both subsystems can be compiled into the same kernel. There is enforced
+	          mutual exclusion between the two subsystems. When there is an Oprofile
+		  session, no perfmon2 session can exist and vice-versa. Perfmon2 session
+		  encapsulates both per-thread and system-wide sessions here.
+
+	- On IA-64: Oprofile works on top of perfmon2. Oprofile being a system-wide monitoring
+		    tool, the regular per-thread vs. system-wide session restrictions apply.
+
+	- on PPC: no integration yet. You need to enable/disble one of the two subsystems
+	- on MIPS: no integration yet. You need to enable/disble one of the two subsystems
+
+IV/ User tools
+
+    We have released a simple monitoring tool to demonstrate the feature of the
+    interface. The tool is called pfmon and it comes with a simple helper library
+    called libpfm. The library comes with a set of examples to show how to use the
+    kernel perfmon2 interface. Visit http://perfmon2.sf.net for details.
+
+    There maybe other tools available for perfmon2.
+
+V/ How to program?
+
+   The best way to learn how to program perfmon2, is to take a look at the source
+   code for the examples in libpfm. The source code is available from:
+		http://perfmon2.sf.net
+
+VI/ System calls overview
+
+   The interface is implemented by the following system calls:
+
+   * int pfm_create_context(pfarg_ctx_t *ctx, char *fmt, void *arg, size_t arg_size)
+
+      This function create a perfmon2 context. The type of context is per-thread by
+      default unless PFM_FL_SYSTEM_WIDE is passed in ctx. The sampling format name
+      is passed in fmt. Arguments to the format are passed in arg which is of size
+      arg_size. Upon successful return, the file descriptor identifying the context
+      is returned.
+
+   * int pfm_write_pmds(int fd, pfarg_pmd_t *pmds, int n)
+   
+      This function is used to program the PMD registers. It is possible to pass
+      vectors of PMDs.
+
+   * int pfm_write_pmcs(int fd, pfarg_pmc_t *pmds, int n)
+   
+      This function is used to program the PMC registers. It is possible to pass
+      vectors of PMDs.
+
+   * int pfm_read_pmds(int fd, pfarg_pmd_t *pmds, int n)
+   
+      This function is used to read the PMD registers. It is possible to pass
+      vectors of PMDs.
+
+   * int pfm_load_context(int fd, pfarg_load_t *load)
+   
+      This function is used to attach the context to a thread or CPU.
+      Thread means kernel-visible thread (NPTL). The thread identification
+      as obtained by gettid must be passed to load->load_target.
+
+      To operate on another thread (not self), it is mandatory that the thread
+      be stopped via ptrace().
+
+      To attach to a CPU, the CPU number must be specified in load->load_target
+      AND the call must be issued on that CPU. To monitor a CPU, a thread MUST
+      be pinned on that CPU.
+
+      Until the context is attached, the actual counters are not accessed.
+
+   * int pfm_unload_context(int fd)
+
+     The context is detached for the thread or CPU is was attached to.
+     As a consequence monitoring is stopped.
+
+     When monitoring another thread, the thread MUST be stopped via ptrace()
+     for this function to succeed.
+
+   * int pfm_start(int fd, pfarg_start_t *st)
+
+     Start monitoring. The context must be attached for this function to succeed.
+     Optionally, it is possible to specify the event set on which to start using the
+     st argument, otherwise just pass NULL.
+
+     When monitoring another thread, the thread MUST be stopped via ptrace()
+     for this function to succeed.
+
+   * int pfm_stop(int fd)
+
+     Stop monitoring. The context must be attached for this function to succeed.
+
+     When monitoring another thread, the thread MUST be stopped via ptrace()
+     for this function to succeed.
+
+
+   * int pfm_create_evtsets(int fd, pfarg_setdesc_t *sets, int n)
+
+     This function is used to create or change event sets. By default set 0 exists.
+     It is possible to create/change multiple sets in one call.
+
+     The context must be detached for this call to succeed.
+
+     Sets are identified by a 16-bit integer. They are sorted based on this
+     set and switching occurs in a round-robin fashion.
+
+   * int pfm_delete_evtsets(int fd, pfarg_setdesc_t *sets, int n)
+   
+     Delete event sets. The context must be detached for this call to succeed.
+
+
+   * int pfm_getinfo_evtsets(int fd, pfarg_setinfo_t *sets, int n)
+
+     Retrieve information about event sets. In particular it is possible
+     to get the number of activation of a set. It is possible to retrieve
+     information about multiple sets in one call.
+
+
+   * int pfm_restart(int fd)
+
+     Indicate to the kernel that the application is done processing an overflow
+     notification. A consequence of this call could be that monitoring resumes.
+
+   * int read(fd, pfm_msg_t *msg, sizeof(pfm_msg_t))
+
+   the regular read() system call can be used with the context file descriptor to
+   receive overflow notification messages. Non-blocking read() is supported.
+
+   Each message carry information about the overflow such as which counter overflowed
+   and where the program was (interrupted instruction pointer).
+
+   * int close(int fd)
+
+   To destroy a context, the regular close() system call is used.
+
+
+VII/ /sys interface overview
+
+   Refer to Documentation/ABI/testing/sysfs-perfmon-* for a detailed description
+   of the sysfs interface of perfmon2.
+
+VIII/ Documentation
+
+   Visit http://perfmon2.sf.net
diff -Naur linux-2.6.25-org/MAINTAINERS linux-2.6.25-id/MAINTAINERS
--- linux-2.6.25-org/MAINTAINERS	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/MAINTAINERS	2008-04-23 10:40:24.000000000 +0200
@@ -3098,6 +3098,14 @@
 L:	linux-kernel@vger.kernel.org
 S:	Maintained
 
+PERFMON SUBSYSTEM
+P:	Stephane Eranian
+M:	eranian@hpl.hp.com
+L:	perfmon@linux.hpl.hp.com
+W:	http://perfmon2.sf.net
+T:	git kernel.org:/pub/scm/linux/kernel/git/eranian/linux-2.6
+S:	Maintained
+
 PERSONALITY HANDLING
 P:	Christoph Hellwig
 M:	hch@infradead.org
diff -Naur linux-2.6.25-org/Makefile linux-2.6.25-id/Makefile
--- linux-2.6.25-org/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/Makefile	2008-04-23 10:40:24.000000000 +0200
@@ -603,7 +603,7 @@
 
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/
+core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/ perfmon/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
diff -Naur linux-2.6.25-org/arch/ia64/Kconfig linux-2.6.25-id/arch/ia64/Kconfig
--- linux-2.6.25-org/arch/ia64/Kconfig	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/Kconfig	2008-04-23 10:40:24.000000000 +0200
@@ -451,14 +451,6 @@
 config IA64_MCA_RECOVERY
 	tristate "MCA recovery from errors other than TLB."
 
-config PERFMON
-	bool "Performance monitor support"
-	help
-	  Selects whether support for the IA-64 performance monitor hardware
-	  is included in the kernel.  This makes some kernel data-structures a
-	  little bigger and slows down execution a bit, but it is generally
-	  a good idea to turn this on.  If you're unsure, say Y.
-
 config IA64_PALINFO
 	tristate "/proc/pal support"
 	help
@@ -530,6 +522,8 @@
 
 source "fs/Kconfig.binfmt"
 
+source "arch/ia64/perfmon/Kconfig"
+
 endmenu
 
 menu "Power management and ACPI"
diff -Naur linux-2.6.25-org/arch/ia64/Makefile linux-2.6.25-id/arch/ia64/Makefile
--- linux-2.6.25-org/arch/ia64/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/Makefile	2008-04-23 10:40:24.000000000 +0200
@@ -57,6 +57,7 @@
 core-$(CONFIG_IA64_HP_ZX1)	+= arch/ia64/dig/
 core-$(CONFIG_IA64_HP_ZX1_SWIOTLB) += arch/ia64/dig/
 core-$(CONFIG_IA64_SGI_SN2)	+= arch/ia64/sn/
+core-$(CONFIG_PERFMON)		+= arch/ia64/perfmon/
 
 drivers-$(CONFIG_PCI)		+= arch/ia64/pci/
 drivers-$(CONFIG_IA64_HP_SIM)	+= arch/ia64/hp/sim/
diff -Naur linux-2.6.25-org/arch/ia64/kernel/Makefile linux-2.6.25-id/arch/ia64/kernel/Makefile
--- linux-2.6.25-org/arch/ia64/kernel/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/Makefile	2008-04-23 11:19:25.000000000 +0200
@@ -5,7 +5,7 @@
 extra-y	:= head.o init_task.o vmlinux.lds
 
 obj-y := acpi.o entry.o efi.o efi_stub.o gate-data.o fsys.o ia64_ksyms.o irq.o irq_ia64.o	\
-	 irq_lsapic.o ivt.o machvec.o pal.o patch.o process.o perfmon.o ptrace.o sal.o		\
+	 irq_lsapic.o ivt.o machvec.o pal.o patch.o process.o ptrace.o sal.o		\
 	 salinfo.o semaphore.o setup.o signal.o sys_ia64.o time.o traps.o unaligned.o \
 	 unwind.o mca.o mca_asm.o topology.o
 
@@ -23,7 +23,6 @@
 obj-$(CONFIG_MODULES)		+= module.o
 obj-$(CONFIG_SMP)		+= smp.o smpboot.o
 obj-$(CONFIG_NUMA)		+= numa.o
-obj-$(CONFIG_PERFMON)		+= perfmon_default_smpl.o
 obj-$(CONFIG_IA64_CYCLONE)	+= cyclone.o
 obj-$(CONFIG_CPU_FREQ)		+= cpufreq/
 obj-$(CONFIG_IA64_MCA_RECOVERY)	+= mca_recovery.o
diff -Naur linux-2.6.25-org/arch/ia64/kernel/irq_ia64.c linux-2.6.25-id/arch/ia64/kernel/irq_ia64.c
--- linux-2.6.25-org/arch/ia64/kernel/irq_ia64.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/irq_ia64.c	2008-04-23 11:19:25.000000000 +0200
@@ -40,10 +40,6 @@
 #include <asm/system.h>
 #include <asm/tlbflush.h>
 
-#ifdef CONFIG_PERFMON
-# include <asm/perfmon.h>
-#endif
-
 #define IRQ_DEBUG	0
 
 #define IRQ_VECTOR_UNASSIGNED	(0)
@@ -653,9 +649,6 @@
 	}
 #endif
 #endif
-#ifdef CONFIG_PERFMON
-	pfm_init_percpu();
-#endif
 	platform_irq_init();
 }
 
diff -Naur linux-2.6.25-org/arch/ia64/kernel/perfmon_generic.h linux-2.6.25-id/arch/ia64/kernel/perfmon_generic.h
--- linux-2.6.25-org/arch/ia64/kernel/perfmon_generic.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/perfmon_generic.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,45 +0,0 @@
-/*
- * This file contains the generic PMU register description tables
- * and pmc checker used by perfmon.c.
- *
- * Copyright (C) 2002-2003  Hewlett Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- */
-
-static pfm_reg_desc_t pfm_gen_pmc_desc[PMU_MAX_PMCS]={
-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static pfm_reg_desc_t pfm_gen_pmd_desc[PMU_MAX_PMDS]={
-/* pmd0  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
-/* pmd1  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
-/* pmd2  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
-/* pmd3  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-/*
- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
- */
-static pmu_config_t pmu_conf_gen={
-	.pmu_name   = "Generic",
-	.pmu_family = 0xff, /* any */
-	.ovfl_val   = (1UL << 32) - 1,
-	.num_ibrs   = 0, /* does not use */
-	.num_dbrs   = 0, /* does not use */
-	.pmd_desc   = pfm_gen_pmd_desc,
-	.pmc_desc   = pfm_gen_pmc_desc
-};
-
diff -Naur linux-2.6.25-org/arch/ia64/kernel/perfmon_itanium.h linux-2.6.25-id/arch/ia64/kernel/perfmon_itanium.h
--- linux-2.6.25-org/arch/ia64/kernel/perfmon_itanium.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/perfmon_itanium.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,115 +0,0 @@
-/*
- * This file contains the Itanium PMU register description tables
- * and pmc checker used by perfmon.c.
- *
- * Copyright (C) 2002-2003  Hewlett Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- */
-static int pfm_ita_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
-
-static pfm_reg_desc_t pfm_ita_pmc_desc[PMU_MAX_PMCS]={
-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc8  */ { PFM_REG_CONFIG  , 0, 0xf00000003ffffff8UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc9  */ { PFM_REG_CONFIG  , 0, 0xf00000003ffffff8UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc10 */ { PFM_REG_MONITOR , 6, 0x0UL, -1UL, NULL, NULL, {RDEP(0)|RDEP(1),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc11 */ { PFM_REG_MONITOR , 6, 0x0000000010000000UL, -1UL, NULL, pfm_ita_pmc_check, {RDEP(2)|RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc12 */ { PFM_REG_MONITOR , 6, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc13 */ { PFM_REG_CONFIG  , 0, 0x0003ffff00000001UL, -1UL, NULL, pfm_ita_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static pfm_reg_desc_t pfm_ita_pmd_desc[PMU_MAX_PMDS]={
-/* pmd0  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(1),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
-/* pmd1  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(0),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
-/* pmd2  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-/* pmd3  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-/* pmd4  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
-/* pmd5  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
-/* pmd6  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
-/* pmd7  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
-/* pmd8  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd9  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd10 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd11 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd12 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd13 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd14 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd15 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd16 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd17 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(3),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static int
-pfm_ita_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	int ret;
-	int is_loaded;
-
-	/* sanitfy check */
-	if (ctx == NULL) return -EINVAL;
-
-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
-
-	/*
-	 * we must clear the (instruction) debug registers if pmc13.ta bit is cleared
-	 * before they are written (fl_using_dbreg==0) to avoid picking up stale information.
-	 */
-	if (cnum == 13 && is_loaded && ((*val & 0x1) == 0UL) && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc13.ta cleared, clearing ibr\n", cnum, *val));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(1, ctx, NULL, 0, regs);
-		if (ret) return ret;
-	}
-
-	/*
-	 * we must clear the (data) debug registers if pmc11.pt bit is cleared
-	 * before they are written (fl_using_dbreg==0) to avoid picking up stale information.
-	 */
-	if (cnum == 11 && is_loaded && ((*val >> 28)& 0x1) == 0 && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc11.pt cleared, clearing dbr\n", cnum, *val));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(0, ctx, NULL, 0, regs);
-		if (ret) return ret;
-	}
-	return 0;
-}
-
-/*
- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
- */
-static pmu_config_t pmu_conf_ita={
-	.pmu_name      = "Itanium",
-	.pmu_family    = 0x7,
-	.ovfl_val      = (1UL << 32) - 1,
-	.pmd_desc      = pfm_ita_pmd_desc,
-	.pmc_desc      = pfm_ita_pmc_desc,
-	.num_ibrs      = 8,
-	.num_dbrs      = 8,
-	.use_rr_dbregs = 1, /* debug register are use for range retrictions */
-};
-
-
diff -Naur linux-2.6.25-org/arch/ia64/kernel/perfmon_mckinley.h linux-2.6.25-id/arch/ia64/kernel/perfmon_mckinley.h
--- linux-2.6.25-org/arch/ia64/kernel/perfmon_mckinley.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/perfmon_mckinley.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,187 +0,0 @@
-/*
- * This file contains the McKinley PMU register description tables
- * and pmc checker used by perfmon.c.
- *
- * Copyright (C) 2002-2003  Hewlett Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- */
-static int pfm_mck_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
-
-static pfm_reg_desc_t pfm_mck_pmc_desc[PMU_MAX_PMCS]={
-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x0000000000800000UL, 0xfffff7fUL, NULL, pfm_mck_pmc_check, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc8  */ { PFM_REG_CONFIG  , 0, 0xffffffff3fffffffUL, 0xffffffff3ffffffbUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc9  */ { PFM_REG_CONFIG  , 0, 0xffffffff3ffffffcUL, 0xffffffff3ffffffbUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc10 */ { PFM_REG_MONITOR , 4, 0x0UL, 0xffffUL, NULL, pfm_mck_pmc_check, {RDEP(0)|RDEP(1),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc11 */ { PFM_REG_MONITOR , 6, 0x0UL, 0x30f01cf, NULL,  pfm_mck_pmc_check, {RDEP(2)|RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc12 */ { PFM_REG_MONITOR , 6, 0x0UL, 0xffffUL, NULL,  pfm_mck_pmc_check, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc13 */ { PFM_REG_CONFIG  , 0, 0x00002078fefefefeUL, 0x1e00018181818UL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc14 */ { PFM_REG_CONFIG  , 0, 0x0db60db60db60db6UL, 0x2492UL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc15 */ { PFM_REG_CONFIG  , 0, 0x00000000fffffff0UL, 0xfUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static pfm_reg_desc_t pfm_mck_pmd_desc[PMU_MAX_PMDS]={
-/* pmd0  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(1),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
-/* pmd1  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(0),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
-/* pmd2  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-/* pmd3  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
-/* pmd8  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd9  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd10 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd11 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd12 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd13 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd14 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd15 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd16 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd17 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(3),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-/*
- * PMC reserved fields must have their power-up values preserved
- */
-static int
-pfm_mck_reserved(unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	unsigned long tmp1, tmp2, ival = *val;
-
-	/* remove reserved areas from user value */
-	tmp1 = ival & PMC_RSVD_MASK(cnum);
-
-	/* get reserved fields values */
-	tmp2 = PMC_DFL_VAL(cnum) & ~PMC_RSVD_MASK(cnum);
-
-	*val = tmp1 | tmp2;
-
-	DPRINT(("pmc[%d]=0x%lx, mask=0x%lx, reset=0x%lx, val=0x%lx\n",
-		  cnum, ival, PMC_RSVD_MASK(cnum), PMC_DFL_VAL(cnum), *val));
-	return 0;
-}
-
-/*
- * task can be NULL if the context is unloaded
- */
-static int
-pfm_mck_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	int ret = 0, check_case1 = 0;
-	unsigned long val8 = 0, val14 = 0, val13 = 0;
-	int is_loaded;
-
-	/* first preserve the reserved fields */
-	pfm_mck_reserved(cnum, val, regs);
-
-	/* sanitfy check */
-	if (ctx == NULL) return -EINVAL;
-
-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
-
-	/*
-	 * we must clear the debug registers if pmc13 has a value which enable
-	 * memory pipeline event constraints. In this case we need to clear the
-	 * the debug registers if they have not yet been accessed. This is required
-	 * to avoid picking stale state.
-	 * PMC13 is "active" if:
-	 * 	one of the pmc13.cfg_dbrpXX field is different from 0x3
-	 * AND
-	 * 	at the corresponding pmc13.ena_dbrpXX is set.
-	 */
-	DPRINT(("cnum=%u val=0x%lx, using_dbreg=%d loaded=%d\n", cnum, *val, ctx->ctx_fl_using_dbreg, is_loaded));
-
-	if (cnum == 13 && is_loaded
-	    && (*val & 0x1e00000000000UL) && (*val & 0x18181818UL) != 0x18181818UL && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc13 settings, clearing dbr\n", cnum, *val));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(PFM_DATA_RR, ctx, NULL, 0, regs);
-		if (ret) return ret;
-	}
-	/*
-	 * we must clear the (instruction) debug registers if any pmc14.ibrpX bit is enabled
-	 * before they are (fl_using_dbreg==0) to avoid picking up stale information.
-	 */
-	if (cnum == 14 && is_loaded && ((*val & 0x2222UL) != 0x2222UL) && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc14 settings, clearing ibr\n", cnum, *val));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(PFM_CODE_RR, ctx, NULL, 0, regs);
-		if (ret) return ret;
-
-	}
-
-	switch(cnum) {
-		case  4: *val |= 1UL << 23; /* force power enable bit */
-			 break;
-		case  8: val8 = *val;
-			 val13 = ctx->ctx_pmcs[13];
-			 val14 = ctx->ctx_pmcs[14];
-			 check_case1 = 1;
-			 break;
-		case 13: val8  = ctx->ctx_pmcs[8];
-			 val13 = *val;
-			 val14 = ctx->ctx_pmcs[14];
-			 check_case1 = 1;
-			 break;
-		case 14: val8  = ctx->ctx_pmcs[8];
-			 val13 = ctx->ctx_pmcs[13];
-			 val14 = *val;
-			 check_case1 = 1;
-			 break;
-	}
-	/* check illegal configuration which can produce inconsistencies in tagging
-	 * i-side events in L1D and L2 caches
-	 */
-	if (check_case1) {
-		ret =   ((val13 >> 45) & 0xf) == 0
-		   && ((val8 & 0x1) == 0)
-		   && ((((val14>>1) & 0x3) == 0x2 || ((val14>>1) & 0x3) == 0x0)
-		       ||(((val14>>4) & 0x3) == 0x2 || ((val14>>4) & 0x3) == 0x0));
-
-		if (ret) DPRINT((KERN_DEBUG "perfmon: failure check_case1\n"));
-	}
-
-	return ret ? -EINVAL : 0;
-}
-
-/*
- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
- */
-static pmu_config_t pmu_conf_mck={
-	.pmu_name      = "Itanium 2",
-	.pmu_family    = 0x1f,
-	.flags	       = PFM_PMU_IRQ_RESEND,
-	.ovfl_val      = (1UL << 47) - 1,
-	.pmd_desc      = pfm_mck_pmd_desc,
-	.pmc_desc      = pfm_mck_pmc_desc,
-	.num_ibrs       = 8,
-	.num_dbrs       = 8,
-	.use_rr_dbregs = 1 /* debug register are use for range restrictions */
-};
-
-
diff -Naur linux-2.6.25-org/arch/ia64/kernel/perfmon_montecito.h linux-2.6.25-id/arch/ia64/kernel/perfmon_montecito.h
--- linux-2.6.25-org/arch/ia64/kernel/perfmon_montecito.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/perfmon_montecito.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,269 +0,0 @@
-/*
- * This file contains the Montecito PMU register description tables
- * and pmc checker used by perfmon.c.
- *
- * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
- *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
- */
-static int pfm_mont_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
-
-#define RDEP_MONT_ETB	(RDEP(38)|RDEP(39)|RDEP(48)|RDEP(49)|RDEP(50)|RDEP(51)|RDEP(52)|RDEP(53)|RDEP(54)|\
-			 RDEP(55)|RDEP(56)|RDEP(57)|RDEP(58)|RDEP(59)|RDEP(60)|RDEP(61)|RDEP(62)|RDEP(63))
-#define RDEP_MONT_DEAR  (RDEP(32)|RDEP(33)|RDEP(36))
-#define RDEP_MONT_IEAR  (RDEP(34)|RDEP(35))
-
-static pfm_reg_desc_t pfm_mont_pmc_desc[PMU_MAX_PMCS]={
-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(4),0, 0, 0}, {0,0, 0, 0}},
-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(5),0, 0, 0}, {0,0, 0, 0}},
-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(6),0, 0, 0}, {0,0, 0, 0}},
-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(7),0, 0, 0}, {0,0, 0, 0}},
-/* pmc8  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(8),0, 0, 0}, {0,0, 0, 0}},
-/* pmc9  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(9),0, 0, 0}, {0,0, 0, 0}},
-/* pmc10 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(10),0, 0, 0}, {0,0, 0, 0}},
-/* pmc11 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(11),0, 0, 0}, {0,0, 0, 0}},
-/* pmc12 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(12),0, 0, 0}, {0,0, 0, 0}},
-/* pmc13 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(13),0, 0, 0}, {0,0, 0, 0}},
-/* pmc14 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(14),0, 0, 0}, {0,0, 0, 0}},
-/* pmc15 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(15),0, 0, 0}, {0,0, 0, 0}},
-/* pmc16 */ { PFM_REG_NOTIMPL, },
-/* pmc17 */ { PFM_REG_NOTIMPL, },
-/* pmc18 */ { PFM_REG_NOTIMPL, },
-/* pmc19 */ { PFM_REG_NOTIMPL, },
-/* pmc20 */ { PFM_REG_NOTIMPL, },
-/* pmc21 */ { PFM_REG_NOTIMPL, },
-/* pmc22 */ { PFM_REG_NOTIMPL, },
-/* pmc23 */ { PFM_REG_NOTIMPL, },
-/* pmc24 */ { PFM_REG_NOTIMPL, },
-/* pmc25 */ { PFM_REG_NOTIMPL, },
-/* pmc26 */ { PFM_REG_NOTIMPL, },
-/* pmc27 */ { PFM_REG_NOTIMPL, },
-/* pmc28 */ { PFM_REG_NOTIMPL, },
-/* pmc29 */ { PFM_REG_NOTIMPL, },
-/* pmc30 */ { PFM_REG_NOTIMPL, },
-/* pmc31 */ { PFM_REG_NOTIMPL, },
-/* pmc32 */ { PFM_REG_CONFIG,  0, 0x30f01ffffffffffUL, 0x30f01ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc33 */ { PFM_REG_CONFIG,  0, 0x0,  0x1ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc34 */ { PFM_REG_CONFIG,  0, 0xf01ffffffffffUL, 0xf01ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc35 */ { PFM_REG_CONFIG,  0, 0x0,  0x1ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc36 */ { PFM_REG_CONFIG,  0, 0xfffffff0, 0xf, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc37 */ { PFM_REG_MONITOR, 4, 0x0, 0x3fff, NULL, pfm_mont_pmc_check, {RDEP_MONT_IEAR, 0, 0, 0}, {0, 0, 0, 0}},
-/* pmc38 */ { PFM_REG_CONFIG,  0, 0xdb6, 0x2492, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc39 */ { PFM_REG_MONITOR, 6, 0x0, 0xffcf, NULL, pfm_mont_pmc_check, {RDEP_MONT_ETB,0, 0, 0}, {0,0, 0, 0}},
-/* pmc40 */ { PFM_REG_MONITOR, 6, 0x2000000, 0xf01cf, NULL, pfm_mont_pmc_check, {RDEP_MONT_DEAR,0, 0, 0}, {0,0, 0, 0}},
-/* pmc41 */ { PFM_REG_CONFIG,  0, 0x00002078fefefefeUL, 0x1e00018181818UL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc42 */ { PFM_REG_MONITOR, 6, 0x0, 0x7ff4f, NULL, pfm_mont_pmc_check, {RDEP_MONT_ETB,0, 0, 0}, {0,0, 0, 0}},
-	    { PFM_REG_END    , 0, 0x0, -1, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static pfm_reg_desc_t pfm_mont_pmd_desc[PMU_MAX_PMDS]={
-/* pmd0  */ { PFM_REG_NOTIMPL, }, 
-/* pmd1  */ { PFM_REG_NOTIMPL, },
-/* pmd2  */ { PFM_REG_NOTIMPL, },
-/* pmd3  */ { PFM_REG_NOTIMPL, },
-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(4),0, 0, 0}},
-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(5),0, 0, 0}},
-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(6),0, 0, 0}},
-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(7),0, 0, 0}},
-/* pmd8  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(8),0, 0, 0}}, 
-/* pmd9  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(9),0, 0, 0}},
-/* pmd10 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(10),0, 0, 0}},
-/* pmd11 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(11),0, 0, 0}},
-/* pmd12 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(12),0, 0, 0}},
-/* pmd13 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(13),0, 0, 0}},
-/* pmd14 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(14),0, 0, 0}},
-/* pmd15 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(15),0, 0, 0}},
-/* pmd16 */ { PFM_REG_NOTIMPL, },
-/* pmd17 */ { PFM_REG_NOTIMPL, },
-/* pmd18 */ { PFM_REG_NOTIMPL, },
-/* pmd19 */ { PFM_REG_NOTIMPL, },
-/* pmd20 */ { PFM_REG_NOTIMPL, },
-/* pmd21 */ { PFM_REG_NOTIMPL, },
-/* pmd22 */ { PFM_REG_NOTIMPL, },
-/* pmd23 */ { PFM_REG_NOTIMPL, },
-/* pmd24 */ { PFM_REG_NOTIMPL, },
-/* pmd25 */ { PFM_REG_NOTIMPL, },
-/* pmd26 */ { PFM_REG_NOTIMPL, },
-/* pmd27 */ { PFM_REG_NOTIMPL, },
-/* pmd28 */ { PFM_REG_NOTIMPL, },
-/* pmd29 */ { PFM_REG_NOTIMPL, },
-/* pmd30 */ { PFM_REG_NOTIMPL, },
-/* pmd31 */ { PFM_REG_NOTIMPL, },
-/* pmd32 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(33)|RDEP(36),0, 0, 0}, {RDEP(40),0, 0, 0}},
-/* pmd33 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(32)|RDEP(36),0, 0, 0}, {RDEP(40),0, 0, 0}},
-/* pmd34 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(35),0, 0, 0}, {RDEP(37),0, 0, 0}},
-/* pmd35 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(34),0, 0, 0}, {RDEP(37),0, 0, 0}},
-/* pmd36 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(32)|RDEP(33),0, 0, 0}, {RDEP(40),0, 0, 0}},
-/* pmd37 */ { PFM_REG_NOTIMPL, },
-/* pmd38 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd39 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd40 */ { PFM_REG_NOTIMPL, },
-/* pmd41 */ { PFM_REG_NOTIMPL, },
-/* pmd42 */ { PFM_REG_NOTIMPL, },
-/* pmd43 */ { PFM_REG_NOTIMPL, },
-/* pmd44 */ { PFM_REG_NOTIMPL, },
-/* pmd45 */ { PFM_REG_NOTIMPL, },
-/* pmd46 */ { PFM_REG_NOTIMPL, },
-/* pmd47 */ { PFM_REG_NOTIMPL, },
-/* pmd48 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd49 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd50 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd51 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd52 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd53 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd54 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd55 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd56 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd57 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd58 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd59 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd60 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd61 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd62 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd63 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-	    { PFM_REG_END   , 0, 0x0, -1, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-/*
- * PMC reserved fields must have their power-up values preserved
- */
-static int
-pfm_mont_reserved(unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	unsigned long tmp1, tmp2, ival = *val;
-
-	/* remove reserved areas from user value */
-	tmp1 = ival & PMC_RSVD_MASK(cnum);
-
-	/* get reserved fields values */
-	tmp2 = PMC_DFL_VAL(cnum) & ~PMC_RSVD_MASK(cnum);
-
-	*val = tmp1 | tmp2;
-
-	DPRINT(("pmc[%d]=0x%lx, mask=0x%lx, reset=0x%lx, val=0x%lx\n",
-		  cnum, ival, PMC_RSVD_MASK(cnum), PMC_DFL_VAL(cnum), *val));
-	return 0;
-}
-
-/*
- * task can be NULL if the context is unloaded
- */
-static int
-pfm_mont_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	int ret = 0;
-	unsigned long val32 = 0, val38 = 0, val41 = 0;
-	unsigned long tmpval;
-	int check_case1 = 0;
-	int is_loaded;
-
-	/* first preserve the reserved fields */
-	pfm_mont_reserved(cnum, val, regs);
-
-	tmpval = *val;
-
-	/* sanity check */
-	if (ctx == NULL) return -EINVAL;
-
-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
-
-	/*
-	 * we must clear the debug registers if pmc41 has a value which enable
-	 * memory pipeline event constraints. In this case we need to clear the
-	 * the debug registers if they have not yet been accessed. This is required
-	 * to avoid picking stale state.
-	 * PMC41 is "active" if:
-	 * 	one of the pmc41.cfg_dtagXX field is different from 0x3
-	 * AND
-	 * 	at the corresponding pmc41.en_dbrpXX is set.
-	 * AND
-	 *	ctx_fl_using_dbreg == 0  (i.e., dbr not yet used)
-	 */
-	DPRINT(("cnum=%u val=0x%lx, using_dbreg=%d loaded=%d\n", cnum, tmpval, ctx->ctx_fl_using_dbreg, is_loaded));
-
-	if (cnum == 41 && is_loaded 
-	    && (tmpval & 0x1e00000000000UL) && (tmpval & 0x18181818UL) != 0x18181818UL && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc41 settings, clearing dbr\n", cnum, tmpval));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers if:
-		 * AND
-		 */
-		ret = pfm_write_ibr_dbr(PFM_DATA_RR, ctx, NULL, 0, regs);
-		if (ret) return ret;
-	}
-	/*
-	 * we must clear the (instruction) debug registers if:
-	 * 	pmc38.ig_ibrpX is 0 (enabled)
-	 * AND
-	 *	ctx_fl_using_dbreg == 0  (i.e., dbr not yet used)
-	 */
-	if (cnum == 38 && is_loaded && ((tmpval & 0x492UL) != 0x492UL) && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc38=0x%lx has active pmc38 settings, clearing ibr\n", tmpval));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(PFM_CODE_RR, ctx, NULL, 0, regs);
-		if (ret) return ret;
-
-	}
-	switch(cnum) {
-		case  32: val32 = *val;
-			  val38 = ctx->ctx_pmcs[38];
-			  val41 = ctx->ctx_pmcs[41];
-			  check_case1 = 1;
-			  break;
-		case  38: val38 = *val;
-			  val32 = ctx->ctx_pmcs[32];
-			  val41 = ctx->ctx_pmcs[41];
-			  check_case1 = 1;
-			  break;
-		case  41: val41 = *val;
-			  val32 = ctx->ctx_pmcs[32];
-			  val38 = ctx->ctx_pmcs[38];
-			  check_case1 = 1;
-			  break;
-	}
-	/* check illegal configuration which can produce inconsistencies in tagging
-	 * i-side events in L1D and L2 caches
-	 */
-	if (check_case1) {
-		ret =   (((val41 >> 45) & 0xf) == 0 && ((val32>>57) & 0x1) == 0)
-		     && ((((val38>>1) & 0x3) == 0x2 || ((val38>>1) & 0x3) == 0)
-		     ||  (((val38>>4) & 0x3) == 0x2 || ((val38>>4) & 0x3) == 0));
-		if (ret) {
-			DPRINT(("invalid config pmc38=0x%lx pmc41=0x%lx pmc32=0x%lx\n", val38, val41, val32));
-			return -EINVAL;
-		}
-	}
-	*val = tmpval;
-	return 0;
-}
-
-/*
- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
- */
-static pmu_config_t pmu_conf_mont={
-	.pmu_name        = "Montecito",
-	.pmu_family      = 0x20,
-	.flags           = PFM_PMU_IRQ_RESEND,
-	.ovfl_val        = (1UL << 47) - 1,
-	.pmd_desc        = pfm_mont_pmd_desc,
-	.pmc_desc        = pfm_mont_pmc_desc,
-	.num_ibrs        = 8,
-	.num_dbrs        = 8,
-	.use_rr_dbregs   = 1 /* debug register are use for range retrictions */
-};
diff -Naur linux-2.6.25-org/arch/ia64/kernel/process.c linux-2.6.25-id/arch/ia64/kernel/process.c
--- linux-2.6.25-org/arch/ia64/kernel/process.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/process.c	2008-04-23 11:19:36.000000000 +0200
@@ -42,13 +42,10 @@
 #include <asm/uaccess.h>
 #include <asm/unwind.h>
 #include <asm/user.h>
+#include <linux/perfmon.h>
 
 #include "entry.h"
 
-#ifdef CONFIG_PERFMON
-# include <asm/perfmon.h>
-#endif
-
 #include "sigframe.h"
 
 void (*ia64_mark_idle)(int);
@@ -215,6 +212,22 @@
 {
 	local_irq_enable();
 	while (!need_resched()) {
+#ifdef CONFIG_PERFMON
+		u64 psr = 0;
+		/*
+		 * If requested, we stop the PMU to avoid
+		 * measuring across the core idle loop.
+		 *
+		 * dcr.pp is not modified on purpose
+		 * it is used when coming out of
+		 * safe_halt() via interrupt
+		 */
+		if ((__get_cpu_var(pfm_syst_info) & PFM_ITA_CPUINFO_IDLE_EXCL)) {
+			psr = ia64_getreg(_IA64_REG_PSR);
+			if (psr & IA64_PSR_PP)
+				ia64_rsm(IA64_PSR_PP);
+		}
+#endif
 		if (can_do_pal_halt) {
 			local_irq_disable();
 			if (!need_resched()) {
@@ -223,6 +236,12 @@
 			local_irq_enable();
 		} else
 			cpu_relax();
+#ifdef CONFIG_PERFMON
+		if ((__get_cpu_var(pfm_syst_info) & PFM_ITA_CPUINFO_IDLE_EXCL)) {
+			if (psr & IA64_PSR_PP)
+				ia64_ssm(IA64_PSR_PP);
+		}
+#endif
 	}
 }
 
@@ -323,22 +342,9 @@
 void
 ia64_save_extra (struct task_struct *task)
 {
-#ifdef CONFIG_PERFMON
-	unsigned long info;
-#endif
-
 	if ((task->thread.flags & IA64_THREAD_DBG_VALID) != 0)
 		ia64_save_debug_regs(&task->thread.dbr[0]);
 
-#ifdef CONFIG_PERFMON
-	if ((task->thread.flags & IA64_THREAD_PM_VALID) != 0)
-		pfm_save_regs(task);
-
-	info = __get_cpu_var(pfm_syst_info);
-	if (info & PFM_CPUINFO_SYST_WIDE)
-		pfm_syst_wide_update_task(task, info, 0);
-#endif
-
 #ifdef CONFIG_IA32_SUPPORT
 	if (IS_IA32_PROCESS(task_pt_regs(task)))
 		ia32_save_state(task);
@@ -348,22 +354,9 @@
 void
 ia64_load_extra (struct task_struct *task)
 {
-#ifdef CONFIG_PERFMON
-	unsigned long info;
-#endif
-
 	if ((task->thread.flags & IA64_THREAD_DBG_VALID) != 0)
 		ia64_load_debug_regs(&task->thread.dbr[0]);
 
-#ifdef CONFIG_PERFMON
-	if ((task->thread.flags & IA64_THREAD_PM_VALID) != 0)
-		pfm_load_regs(task);
-
-	info = __get_cpu_var(pfm_syst_info);
-	if (info & PFM_CPUINFO_SYST_WIDE) 
-		pfm_syst_wide_update_task(task, info, 1);
-#endif
-
 #ifdef CONFIG_IA32_SUPPORT
 	if (IS_IA32_PROCESS(task_pt_regs(task)))
 		ia32_load_state(task);
@@ -489,8 +482,7 @@
 	 * call behavior where scratch registers are preserved across
 	 * system calls (unless used by the system call itself).
 	 */
-#	define THREAD_FLAGS_TO_CLEAR	(IA64_THREAD_FPH_VALID | IA64_THREAD_DBG_VALID \
-					 | IA64_THREAD_PM_VALID)
+#	define THREAD_FLAGS_TO_CLEAR	(IA64_THREAD_FPH_VALID | IA64_THREAD_DBG_VALID)
 #	define THREAD_FLAGS_TO_SET	0
 	p->thread.flags = ((current->thread.flags & ~THREAD_FLAGS_TO_CLEAR)
 			   | THREAD_FLAGS_TO_SET);
@@ -512,10 +504,8 @@
 	}
 #endif
 
-#ifdef CONFIG_PERFMON
-	if (current->thread.pfm_context)
-		pfm_inherit(p, child_ptregs);
-#endif
+	pfm_copy_thread(p);
+
 	return retval;
 }
 
@@ -754,15 +744,13 @@
 {
 
 	ia64_drop_fpu(current);
-#ifdef CONFIG_PERFMON
-       /* if needed, stop monitoring and flush state to perfmon context */
-	if (current->thread.pfm_context)
-		pfm_exit_thread(current);
+
+        /* if needed, stop monitoring and flush state to perfmon context */
+	pfm_exit_thread(current);
 
 	/* free debug register resources */
-	if (current->thread.flags & IA64_THREAD_DBG_VALID)
-		pfm_release_debug_registers(current);
-#endif
+	pfm_release_dbregs(current);
+
 	if (IS_IA32_PROCESS(task_pt_regs(current)))
 		ia32_drop_ia64_partial_page_list(current);
 }
diff -Naur linux-2.6.25-org/arch/ia64/kernel/ptrace.c linux-2.6.25-id/arch/ia64/kernel/ptrace.c
--- linux-2.6.25-org/arch/ia64/kernel/ptrace.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/ptrace.c	2008-04-23 11:19:36.000000000 +0200
@@ -17,6 +17,7 @@
 #include <linux/security.h>
 #include <linux/audit.h>
 #include <linux/signal.h>
+#include <linux/perfmon.h>
 
 #include <asm/pgtable.h>
 #include <asm/processor.h>
@@ -25,9 +26,6 @@
 #include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/unwind.h>
-#ifdef CONFIG_PERFMON
-#include <asm/perfmon.h>
-#endif
 
 #include "entry.h"
 
@@ -1120,7 +1118,6 @@
 				"address 0x%lx\n", addr);
 			return -1;
 		}
-#ifdef CONFIG_PERFMON
 		/*
 		 * Check if debug registers are used by perfmon. This
 		 * test must be done once we know that we can do the
@@ -1138,8 +1135,8 @@
 		 * IA64_THREAD_DBG_VALID. The registers are restored
 		 * by the PMU context switch code.
 		 */
-		if (pfm_use_debug_registers(child)) return -1;
-#endif
+		if (pfm_use_dbregs(child))
+			return -1;
 
 		if (!(child->thread.flags & IA64_THREAD_DBG_VALID)) {
 			child->thread.flags |= IA64_THREAD_DBG_VALID;
diff -Naur linux-2.6.25-org/arch/ia64/kernel/setup.c linux-2.6.25-id/arch/ia64/kernel/setup.c
--- linux-2.6.25-org/arch/ia64/kernel/setup.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/setup.c	2008-04-23 11:19:37.000000000 +0200
@@ -45,6 +45,7 @@
 #include <linux/cpufreq.h>
 #include <linux/kexec.h>
 #include <linux/crash_dump.h>
+#include <linux/perfmon.h>
 
 #include <asm/ia32.h>
 #include <asm/machvec.h>
@@ -970,6 +971,8 @@
 	}
 	platform_cpu_init();
 	pm_idle = default_idle;
+
+	pfm_init_percpu();
 }
 
 void __init
diff -Naur linux-2.6.25-org/arch/ia64/kernel/smpboot.c linux-2.6.25-id/arch/ia64/kernel/smpboot.c
--- linux-2.6.25-org/arch/ia64/kernel/smpboot.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/smpboot.c	2008-04-23 11:19:37.000000000 +0200
@@ -39,6 +39,7 @@
 #include <linux/efi.h>
 #include <linux/percpu.h>
 #include <linux/bitops.h>
+#include <linux/perfmon.h>
 
 #include <asm/atomic.h>
 #include <asm/cache.h>
@@ -379,10 +380,6 @@
 	extern void ia64_init_itm(void);
 	extern volatile int time_keeper_id;
 
-#ifdef CONFIG_PERFMON
-	extern void pfm_init_percpu(void);
-#endif
-
 	cpuid = smp_processor_id();
 	phys_id = hard_smp_processor_id();
 	itc_master = time_keeper_id;
@@ -408,10 +405,6 @@
 
 	ia64_mca_cmc_vector_setup();	/* Setup vector on AP */
 
-#ifdef CONFIG_PERFMON
-	pfm_init_percpu();
-#endif
-
 	local_irq_enable();
 
 	if (!(sal_platform_features & IA64_SAL_PLATFORM_FEATURE_ITC_DRIFT)) {
@@ -748,6 +741,7 @@
 	fixup_irqs();
 	local_flush_tlb_all();
 	cpu_clear(cpu, cpu_callin_map);
+	pfm_cpu_disable();
 	return 0;
 }
 
diff -Naur linux-2.6.25-org/arch/ia64/kernel/sys_ia64.c linux-2.6.25-id/arch/ia64/kernel/sys_ia64.c
--- linux-2.6.25-org/arch/ia64/kernel/sys_ia64.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/kernel/sys_ia64.c	2008-04-23 11:19:37.000000000 +0200
@@ -284,3 +284,10 @@
 }
 
 #endif /* CONFIG_PCI */
+
+#ifndef CONFIG_PERFMON
+asmlinkage long sys_perfmonctl (int fd, int cmd, void __user *arg, int count)
+{
+	return -ENOSYS;
+}
+#endif
diff -Naur linux-2.6.25-org/arch/ia64/lib/Makefile linux-2.6.25-id/arch/ia64/lib/Makefile
--- linux-2.6.25-org/arch/ia64/lib/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/lib/Makefile	2008-04-23 11:19:37.000000000 +0200
@@ -13,7 +13,6 @@
 
 obj-$(CONFIG_ITANIUM)	+= copy_page.o copy_user.o memcpy.o
 obj-$(CONFIG_MCKINLEY)	+= copy_page_mck.o memcpy_mck.o
-lib-$(CONFIG_PERFMON)	+= carta_random.o
 
 AFLAGS___divdi3.o	=
 AFLAGS___udivdi3.o	= -DUNSIGNED
diff -Naur linux-2.6.25-org/arch/ia64/oprofile/init.c linux-2.6.25-id/arch/ia64/oprofile/init.c
--- linux-2.6.25-org/arch/ia64/oprofile/init.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/oprofile/init.c	2008-04-23 11:19:37.000000000 +0200
@@ -12,8 +12,8 @@
 #include <linux/init.h>
 #include <linux/errno.h>
  
-extern int perfmon_init(struct oprofile_operations * ops);
-extern void perfmon_exit(void);
+extern int op_perfmon_init(struct oprofile_operations * ops);
+extern void op_perfmon_exit(void);
 extern void ia64_backtrace(struct pt_regs * const regs, unsigned int depth);
 
 int __init oprofile_arch_init(struct oprofile_operations * ops)
@@ -22,7 +22,7 @@
 
 #ifdef CONFIG_PERFMON
 	/* perfmon_init() can fail, but we have no way to report it */
-	ret = perfmon_init(ops);
+	ret = op_perfmon_init(ops);
 #endif
 	ops->backtrace = ia64_backtrace;
 
@@ -33,6 +33,6 @@
 void oprofile_arch_exit(void)
 {
 #ifdef CONFIG_PERFMON
-	perfmon_exit();
+	op_perfmon_exit();
 #endif
 }
diff -Naur linux-2.6.25-org/arch/ia64/oprofile/perfmon.c linux-2.6.25-id/arch/ia64/oprofile/perfmon.c
--- linux-2.6.25-org/arch/ia64/oprofile/perfmon.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/ia64/oprofile/perfmon.c	2008-04-23 11:19:37.000000000 +0200
@@ -10,19 +10,21 @@
 #include <linux/kernel.h>
 #include <linux/oprofile.h>
 #include <linux/sched.h>
-#include <asm/perfmon.h>
+#include <linux/module.h>
+#include <linux/perfmon.h>
 #include <asm/ptrace.h>
 #include <asm/errno.h>
 
 static int allow_ints;
 
 static int
-perfmon_handler(struct task_struct *task, void *buf, pfm_ovfl_arg_t *arg,
-                struct pt_regs *regs, unsigned long stamp)
+perfmon_handler(void *buf, struct pfm_ovfl_arg *arg,
+                unsigned long ip, u64 stamp, void *data)
 {
+	struct pt_regs *regs = data;
 	int event = arg->pmd_eventid;
  
-	arg->ovfl_ctrl.bits.reset_ovfl_pmds = 1;
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
 
 	/* the owner of the oprofile event buffer may have exited
 	 * without perfmon being shutdown (e.g. SIGSEGV)
@@ -45,17 +47,13 @@
 	allow_ints = 0;
 }
 
-
-#define OPROFILE_FMT_UUID { \
-	0x77, 0x7a, 0x6e, 0x61, 0x20, 0x65, 0x73, 0x69, 0x74, 0x6e, 0x72, 0x20, 0x61, 0x65, 0x0a, 0x6c }
-
-static pfm_buffer_fmt_t oprofile_fmt = {
- 	.fmt_name 	    = "oprofile_format",
- 	.fmt_uuid	    = OPROFILE_FMT_UUID,
- 	.fmt_handler	    = perfmon_handler,
+static struct pfm_smpl_fmt oprofile_fmt = {
+	.fmt_name = "OProfile",
+	.fmt_handler = perfmon_handler,
+	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
+	.owner = THIS_MODULE
 };
 
-
 static char * get_cpu_type(void)
 {
 	__u8 family = local_cpu_data->family;
@@ -75,9 +73,9 @@
 
 static int using_perfmon;
 
-int perfmon_init(struct oprofile_operations * ops)
+int __init op_perfmon_init(struct oprofile_operations * ops)
 {
-	int ret = pfm_register_buffer_fmt(&oprofile_fmt);
+	int ret = pfm_fmt_register(&oprofile_fmt);
 	if (ret)
 		return -ENODEV;
 
@@ -90,10 +88,10 @@
 }
 
 
-void perfmon_exit(void)
+void __exit op_perfmon_exit(void)
 {
 	if (!using_perfmon)
 		return;
 
-	pfm_unregister_buffer_fmt(oprofile_fmt.fmt_uuid);
+	pfm_fmt_unregister(&oprofile_fmt);
 }
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/Kconfig linux-2.6.25-id/arch/ia64/perfmon/Kconfig
--- linux-2.6.25-org/arch/ia64/perfmon/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/Kconfig	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,58 @@
+menu "Hardware Performance Monitoring support"
+config PERFMON
+	bool "Perfmon2 performance monitoring interface"
+	default n
+	help
+	Enables the perfmon2 interface to access the hardware
+	performance counters. See <http://perfmon2.sf.net/> for
+	more details.
+
+config PERFMON_DEBUG
+	bool "Perfmon debugging"
+	default n
+	depends on PERFMON
+	help
+	Enables perfmon debugging support
+
+config IA64_PERFMON_COMPAT
+	bool "Enable old perfmon-2 compatbility mode"
+	default n
+	depends on PERFMON
+	help
+	Enable this option to allow performance tools which used the old
+	perfmon-2 interface to continue to work. Old tools are those using
+	the obsolete commands and arguments. Check your programs and look
+	in include/asm-ia64/perfmon_compat.h for more information.
+
+config IA64_PERFMON_GENERIC
+	tristate "Generic IA-64 PMU support"
+	depends on PERFMON
+	default n
+	help
+	Enables generic IA-64 PMU support.
+	The generic PMU is defined by the IA-64 architecture document.
+	This option should only be necessary when running with a PMU that
+	is not yet explicitely supported. Even then, there is no guarantee
+	that this support will work.
+
+config IA64_PERFMON_ITANIUM
+	tristate "Itanium (Merced) Performance Monitoring support"
+	depends on PERFMON
+	default n
+	help
+	Enables Itanium (Merced) PMU support.
+
+config IA64_PERFMON_MCKINLEY
+	tristate "Itanium 2 (McKinley) Performance Monitoring  support"
+	depends on PERFMON
+	default n
+	help
+	Enables Itanium 2 (McKinley, Madison, Deerfield) PMU support.
+
+config IA64_PERFMON_MONTECITO
+	tristate "Itanium 2 9000 (Montecito) Performance Monitoring  support"
+	depends on PERFMON
+	default n
+	help
+	Enables support for Itanium 2 9000 (Montecito) PMU.
+endmenu
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/Makefile linux-2.6.25-id/arch/ia64/perfmon/Makefile
--- linux-2.6.25-org/arch/ia64/perfmon/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/Makefile	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,11 @@
+#
+# Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+# Contributed by Stephane Eranian <eranian@hpl.hp.com>
+#
+obj-$(CONFIG_PERFMON)			+= perfmon.o
+obj-$(CONFIG_IA64_PERFMON_COMPAT)	+= perfmon_default_smpl.o \
+					   perfmon_compat.o
+obj-$(CONFIG_IA64_PERFMON_GENERIC)	+= perfmon_generic.o
+obj-$(CONFIG_IA64_PERFMON_ITANIUM)	+= perfmon_itanium.o
+obj-$(CONFIG_IA64_PERFMON_MCKINLEY)	+= perfmon_mckinley.o
+obj-$(CONFIG_IA64_PERFMON_MONTECITO)	+= perfmon_montecito.o
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/perfmon.c linux-2.6.25-id/arch/ia64/perfmon/perfmon.c
--- linux-2.6.25-org/arch/ia64/perfmon/perfmon.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/perfmon.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,949 @@
+/*
+ * This file implements the IA-64 specific
+ * support for the perfmon2 interface
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+struct pfm_arch_session {
+	u32	pfs_sys_use_dbr;    /* syswide session uses dbr */
+	u32	pfs_ptrace_use_dbr; /* a thread uses dbr via ptrace()*/
+};
+
+DEFINE_PER_CPU(u32, pfm_syst_info);
+
+static struct pfm_arch_session pfm_arch_sessions;
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_arch_sessions_lock);
+
+static inline void pfm_clear_psr_pp(void)
+{
+	ia64_rsm(IA64_PSR_PP);
+}
+
+static inline void pfm_set_psr_pp(void)
+{
+	ia64_ssm(IA64_PSR_PP);
+}
+
+static inline void pfm_clear_psr_up(void)
+{
+	ia64_rsm(IA64_PSR_UP);
+}
+
+static inline void pfm_set_psr_up(void)
+{
+	ia64_ssm(IA64_PSR_UP);
+}
+
+static inline void pfm_set_psr_l(u64 val)
+{
+	ia64_setreg(_IA64_REG_PSR_L, val);
+}
+
+static inline void pfm_restore_ibrs(u64 *ibrs, unsigned int nibrs)
+{
+	unsigned int i;
+
+	for (i = 0; i < nibrs; i++) {
+		ia64_set_ibr(i, ibrs[i]);
+		ia64_dv_serialize_instruction();
+	}
+	ia64_srlz_i();
+}
+
+static inline void pfm_restore_dbrs(u64 *dbrs, unsigned int ndbrs)
+{
+	unsigned int i;
+
+	for (i = 0; i < ndbrs; i++) {
+		ia64_set_dbr(i, dbrs[i]);
+		ia64_dv_serialize_data();
+	}
+	ia64_srlz_d();
+}
+
+irqreturn_t pmu_interrupt_handler(int irq, void *arg)
+{
+	struct pt_regs *regs;
+	regs = get_irq_regs();
+	irq_enter();
+	pfm_interrupt_handler(instruction_pointer(regs), regs);
+	irq_exit();
+	return IRQ_HANDLED;
+}
+static struct irqaction perfmon_irqaction = {
+	.handler = pmu_interrupt_handler,
+	.flags = IRQF_DISABLED, /* means keep interrupts masked */
+	.name = "perfmon"
+};
+
+void pfm_arch_quiesce_pmu_percpu(void)
+{
+	u64 dcr;
+	/*
+	 * make sure no measurement is active
+	 * (may inherit programmed PMCs from EFI).
+	 */
+	pfm_clear_psr_pp();
+	pfm_clear_psr_up();
+
+	/*
+	 * ensure dcr.pp is cleared
+	 */
+	dcr = ia64_getreg(_IA64_REG_CR_DCR);
+	ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
+
+	/*
+	 * we run with the PMU not frozen at all times
+	 */
+	ia64_set_pmc(0, 0);
+	ia64_srlz_d();
+}
+
+void pfm_arch_init_percpu(void)
+{
+	pfm_arch_quiesce_pmu_percpu();
+	/*
+	 * program PMU interrupt vector
+	 */
+	ia64_setreg(_IA64_REG_CR_PMV, IA64_PERFMON_VECTOR);
+	ia64_srlz_d();
+}
+
+int pfm_arch_context_create(struct pfm_context *ctx, u32 ctx_flags)
+{
+	struct pfm_arch_context *ctx_arch;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	ctx_arch->flags.use_dbr = 0;
+	ctx_arch->flags.insecure = (ctx_flags & PFM_ITA_FL_INSECURE) ? 1: 0;
+
+	PFM_DBG("insecure=%d", ctx_arch->flags.insecure);
+
+	return 0;
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * Context is locked. Interrupts are masked. Monitoring may be active.
+ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
+ *
+ * Return:
+ * 	non-zero : did not save PMDs (as part of stopping the PMU)
+ * 	       0 : saved PMDs (no need to save them in caller)
+ */
+int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
+		       struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+	u64 psr, tmp;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * save current PSR: needed because we modify it
+	 */
+	ia64_srlz_d();
+	psr = ia64_getreg(_IA64_REG_PSR);
+
+	/*
+	 * stop monitoring:
+	 * This is the last instruction which may generate an overflow
+	 *
+	 * we do not clear ipsr.up
+	 */
+	pfm_clear_psr_up();
+	ia64_srlz_d();
+
+	/*
+	 * extract overflow status bits
+	 */
+	tmp =  ia64_get_pmc(0) & ~0xf;
+
+	/*
+	 * keep a copy of psr.up (for reload)
+	 */
+	ctx_arch->ctx_saved_psr_up = psr & IA64_PSR_UP;
+
+	/*
+	 * save overflow status bits
+	 */
+	set->povfl_pmds[0] = tmp;
+
+	/*
+	 * record how many pending overflows
+	 * XXX: assume identity mapping for counters
+	 */
+	set->npend_ovfls = ia64_popcnt(tmp);
+
+	/*
+	 * make sure the PMU is unfrozen for the next task
+	 */
+	if (set->npend_ovfls) {
+		ia64_set_pmc(0, 0);
+		ia64_srlz_d();
+	}
+	return 1;
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * set cannot be NULL. Context is locked. Interrupts are masked.
+ * Caller has already restored all PMD and PMC registers.
+ *
+ * must reactivate monitoring
+ */
+void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx,
+		      struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * when monitoring is not explicitly started
+	 * then psr_up = 0, in which case we do not
+	 * need to restore
+	 */
+	if (likely(ctx_arch->ctx_saved_psr_up)) {
+		pfm_set_psr_up();
+		ia64_srlz_d();
+	}
+}
+
+int pfm_arch_reserve_session(struct pfm_context *ctx, u32 cpu)
+{
+	struct pfm_arch_context *ctx_arch;
+	int is_system;
+	int ret = 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+	spin_lock(&pfm_arch_sessions_lock);
+
+	if (is_system && ctx_arch->flags.use_dbr) {
+		PFM_DBG("syswide context uses dbregs");
+
+		if (pfm_arch_sessions.pfs_ptrace_use_dbr) {
+			PFM_DBG("cannot reserve syswide context: "
+				  "dbregs in use by ptrace");
+			ret = -EBUSY;
+		} else {
+			pfm_arch_sessions.pfs_sys_use_dbr++;
+		}
+	}
+	spin_unlock(&pfm_arch_sessions_lock);
+
+	return ret;
+}
+
+void pfm_arch_release_session(struct pfm_context *ctx, u32 cpu)
+{
+	struct pfm_arch_context *ctx_arch;
+	int is_system;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+	spin_lock(&pfm_arch_sessions_lock);
+
+	if (is_system && ctx_arch->flags.use_dbr) {
+		pfm_arch_sessions.pfs_sys_use_dbr--;
+	}
+	spin_unlock(&pfm_arch_sessions_lock);
+}
+
+/*
+ * function called from pfm_load_context_*(). Task is not guaranteed to be
+ * current task. If not then other task is guaranteed stopped and off any CPU.
+ * context is locked and interrupts are masked.
+ *
+ * On PFM_LOAD_CONTEXT, the interface guarantees monitoring is stopped.
+ *
+ * For system-wide task is NULL
+ */
+int pfm_arch_load_context(struct pfm_context *ctx, struct pfm_event_set *set,
+			  struct task_struct *task)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pt_regs *regs;
+	int ret = 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * cannot load a context which is using range restrictions,
+	 * into a thread that is being debugged.
+	 *
+	 * if one set out of several is using the debug registers, then
+	 * we assume the context as whole is using them.
+	 */
+	if (ctx_arch->flags.use_dbr) {
+		if (ctx->flags.system) {
+			spin_lock(&pfm_arch_sessions_lock);
+
+			if (pfm_arch_sessions.pfs_ptrace_use_dbr) {
+				PFM_DBG("cannot reserve syswide context: "
+					"dbregs in use by ptrace");
+				ret = -EBUSY;
+			} else {
+				pfm_arch_sessions.pfs_sys_use_dbr++;
+				PFM_DBG("pfs_sys_use_dbr=%u", pfm_arch_sessions.pfs_sys_use_dbr);
+			}
+			spin_unlock(&pfm_arch_sessions_lock);
+
+		} else if (task->thread.flags & IA64_THREAD_DBG_VALID) {
+			PFM_DBG("load_pid [%d] thread is debugged, cannot "
+				  "use range restrictions", task->pid);
+			ret = -EBUSY;
+		}
+		if (ret)
+			return ret;
+	}
+
+	/*
+	 * We need to intervene on context switch to toggle the
+	 * psr.pp bit in system-wide. As such, we set the TIF
+	 * flag so that pfm_arch_ctxswout_sys() and the
+	 * pfm_arch_ctxswin_sys() functions get called
+	 * from pfm_ctxsw_sys();
+	 */
+	if (ctx->flags.system) {
+		set_thread_flag(TIF_PERFMON_CTXSW);
+		PFM_DBG("[%d] set TIF", current->pid);
+		return 0;
+	}
+
+	regs = task_pt_regs(task);
+
+	/*
+	 * self-monitoring systematically allows user level control
+	 */
+	if (task != current) {
+		/*
+		 * when not current, task is stopped, so this is safe
+		 */
+		ctx_arch->ctx_saved_psr_up = 0;
+		ia64_psr(regs)->up = ia64_psr(regs)->pp = 0;
+	} else
+		ctx_arch->flags.insecure = 1;
+
+	/*
+	 * allow user level control (start/stop/read pmd) if:
+	 * 	- self-monitoring
+	 * 	- requested at context creation (PFM_IA64_FL_INSECURE)
+	 *
+	 * There is not security hole with PFM_IA64_FL_INSECURE because
+	 * when not self-monitored, the caller must have permissions to
+	 * attached to the task.
+	 */
+	if (ctx_arch->flags.insecure) {
+		ia64_psr(regs)->sp = 0;
+		PFM_DBG("clearing psr.sp for [%d]", task->pid);
+	}
+	return 0;
+}
+
+int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
+{
+#define PFM_SETFL_BOTH_SWITCH	(PFM_SETFL_OVFL_SWITCH|PFM_SETFL_TIME_SWITCH)
+#define PFM_ITA_SETFL_BOTH_INTR	(PFM_ITA_SETFL_INTR_ONLY|\
+				 PFM_ITA_SETFL_EXCL_INTR)
+
+/* exclude return value field */
+#define PFM_SETFL_ALL_MASK	( PFM_ITA_SETFL_BOTH_INTR \
+				| PFM_SETFL_BOTH_SWITCH \
+				| PFM_ITA_SETFL_IDLE_EXCL)
+
+	if ((flags & ~PFM_SETFL_ALL_MASK)) {
+		PFM_DBG("invalid flags=0x%x", flags);
+		return -EINVAL;
+	}
+
+	if ((flags & PFM_ITA_SETFL_BOTH_INTR) == PFM_ITA_SETFL_BOTH_INTR) {
+		PFM_DBG("both excl intr and ontr only are set");
+		return -EINVAL;
+	}
+
+	if ((flags & PFM_ITA_SETFL_IDLE_EXCL) && !ctx->flags.system) {
+		PFM_DBG("idle exclude flag only for system-wide context");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * function called from pfm_unload_context_*(). Context is locked.
+ * interrupts are masked. task is not guaranteed to be current task.
+ * Access to PMU is not guaranteed.
+ *
+ * function must do whatever arch-specific action is required on unload
+ * of a context.
+ *
+ * called for both system-wide and per-thread. task is NULL for ssytem-wide
+ */
+int pfm_arch_unload_context(struct pfm_context *ctx, struct task_struct *task)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pt_regs *regs;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	if (ctx->flags.system) {
+		/*
+		 * disable context switch hook
+		 */
+		clear_thread_flag(TIF_PERFMON_CTXSW);
+
+		if (ctx_arch->flags.use_dbr) {
+			spin_lock(&pfm_arch_sessions_lock);
+			pfm_arch_sessions.pfs_sys_use_dbr--;
+			PFM_DBG("sys_use_dbr=%u", pfm_arch_sessions.pfs_sys_use_dbr);
+			spin_unlock(&pfm_arch_sessions_lock);
+		}
+		return 0;
+	}
+
+	regs = task_pt_regs(task);
+
+	/*
+	 * cancel user level control for per-task context
+	 */
+	ia64_psr(regs)->sp = 1;
+	PFM_DBG("setting psr.sp for [%d]", task->pid);
+	return 0;
+}
+
+/*
+ * mask monitoring by setting the privilege level to 0
+ * we cannot use psr.pp/psr.up for this, it is controlled by
+ * the user
+ */
+void pfm_arch_mask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	unsigned long mask;
+	unsigned int i;
+
+	/*
+	 * as an optimization we look at the first 64 PMC
+	 * registers only starting at PMC4.
+	 */
+	mask = arch_info->mask_pmcs[0] >> PFM_ITA_FCNTR;
+	for(i = PFM_ITA_FCNTR; mask; i++, mask >>=1) {
+		if (likely(mask & 0x1))
+			ia64_set_pmc(i, set->pmcs[i] & ~0xfUL);
+	}
+	/*
+	 * make changes visisble
+	 */
+	ia64_srlz_d();
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
+ * context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMD registers from set.
+ */
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+	unsigned long *mask;
+	u16 i, num;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	if (ctx_arch->flags.insecure) {
+		num = pfm_pmu_conf->regs.num_rw_pmd;
+		mask = pfm_pmu_conf->regs.rw_pmds;
+	} else {
+		num = set->nused_pmds;
+		mask = set->used_pmds;
+	}
+	/*
+	 * must restore all implemented read-write PMDS to avoid leaking
+	 * information especially when PFM_IA64_FL_INSECURE is set.
+	 *
+	 * XXX: should check PFM_IA64_FL_INSECURE==0 and use used_pmd instead
+	 */
+	for (i = 0; num; i++) {
+		if (likely(test_bit(i, mask))) {
+			pfm_arch_write_pmd(ctx, i, set->pmds[i].value);
+			num--;
+		}
+	}
+	ia64_srlz_d();
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
+ * context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMC registers from set if needed
+ */
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	u64 mask2 = 0, val, plm;
+	unsigned long impl_mask, mask_pmcs;
+	unsigned int i;
+
+	/*
+	 * as an optimization we only look at the first 64
+	 * PMC registers. In fact, we should never scan the
+	 * entire impl_pmcs because ibr/dbr are implemented
+	 * separately.
+	 *
+	 * always skip PMC0-PMC3. PMC0 taken care of when saving
+	 * state. PMC1-PMC3 not used until we get counters in
+	 * the 60 and above index range.
+	 */
+	impl_mask = pfm_pmu_conf->regs.pmcs[0] >> PFM_ITA_FCNTR;
+	mask_pmcs = arch_info->mask_pmcs[0] >> PFM_ITA_FCNTR;
+	plm = ctx->state == PFM_CTX_MASKED ? ~0xf : ~0x0;
+
+	for (i = PFM_ITA_FCNTR;
+	     impl_mask;
+	     i++, impl_mask >>=1, mask_pmcs >>=1) {
+		if (likely(impl_mask & 0x1)) {
+			mask2 = mask_pmcs & 0x1 ? plm : ~0;
+			val = set->pmcs[i] & mask2;
+			ia64_set_pmc(i, val);
+			PFM_DBG_ovfl("pmc%u=0x%lx", i, val);
+		}
+	}
+	/*
+	 * restore DBR/IBR
+	 */
+	if (set->priv_flags & PFM_ITA_SETFL_USE_DBR) {
+		pfm_restore_ibrs(set->pmcs+256, 8);
+		pfm_restore_dbrs(set->pmcs+264, 8);
+	}
+	ia64_srlz_d();
+}
+
+void pfm_arch_unmask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 psr;
+	int is_system;
+
+	is_system = ctx->flags.system;
+
+	psr = ia64_getreg(_IA64_REG_PSR);
+
+	/*
+	 * monitoring is masked via the PMC.plm
+	 *
+	 * As we restore their value, we do not want each counter to
+	 * restart right away. We stop monitoring using the PSR,
+	 * restore the PMC (and PMD) and then re-establish the psr
+	 * as it was. Note that there can be no pending overflow at
+	 * this point, because monitoring is still MASKED.
+	 *
+	 * Because interrupts are masked we can avoid changing
+	 * DCR.pp.
+	 */
+	if (is_system)
+		pfm_clear_psr_pp();
+	else
+		pfm_clear_psr_up();
+
+	ia64_srlz_d();
+
+	pfm_arch_restore_pmcs(ctx, set);
+
+	/*
+	 * restore psr
+	 *
+	 * monitoring may start right now but interrupts
+	 * are still masked
+	 */
+	pfm_set_psr_l(psr);
+	ia64_srlz_d();
+}
+
+/*
+ * Called from pfm_stop()
+ *
+ * For per-thread:
+ *   task is not necessarily current. If not current task, then
+ *   task is guaranteed stopped and off any cpu. Access to PMU
+ *   is not guaranteed. Interrupts are masked. Context is locked.
+ *   Set is the active set.
+ *
+ * must disable active monitoring. ctx cannot be NULL
+ */
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
+		   struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pt_regs *regs;
+	u64 dcr, psr;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	regs = task_pt_regs(task);
+
+	if (!ctx->flags.system) {
+		/*
+		 * in ZOMBIE state we always have task == current due to
+		 * pfm_exit_thread()
+		 */
+		ia64_psr(regs)->up = 0;
+		ctx_arch->ctx_saved_psr_up = 0;
+
+		/*
+		 * in case of ZOMBIE state, there is no unload to clear
+		 * insecure monitoring, so we do it in stop instead.
+		 */
+		if (ctx->state == PFM_CTX_ZOMBIE)
+			ia64_psr(regs)->sp = 1;
+
+		if (task == current) {
+			pfm_clear_psr_up();
+			ia64_srlz_d();
+		}
+	} else if (ctx->flags.started) { /* do not stop twice */
+		dcr = ia64_getreg(_IA64_REG_CR_DCR);
+		psr = ia64_getreg(_IA64_REG_PSR);
+
+		ia64_psr(regs)->pp = 0;
+		ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
+		pfm_clear_psr_pp();
+		ia64_srlz_d();
+
+		if (set->flags & PFM_ITA_SETFL_IDLE_EXCL) {
+			PFM_DBG("disabling idle exclude");
+			__get_cpu_var(pfm_syst_info) &= ~PFM_ITA_CPUINFO_IDLE_EXCL;
+		}
+	}
+}
+
+/*
+ * called from pfm_start()
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-thread:
+ * 	Task is not necessarily current. If not current task, then task
+ * 	is guaranteed stopped and off any cpu. No access to PMU is task
+ *	is not current.
+ *
+ * For system-wide:
+ * 	task is always current
+ *
+ * must enable active monitoring.
+ */
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
+		    struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pt_regs *regs;
+	u64 dcr, dcr_pp, psr_pp;
+	u32 flags;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	regs = task_pt_regs(task);
+	flags = set->flags;
+
+	/*
+	 * per-thread mode
+	 */
+	if (!ctx->flags.system) {
+
+		ia64_psr(regs)->up = 1;
+
+		if (task == current) {
+			pfm_set_psr_up();
+			ia64_srlz_d();
+		} else {
+			/*
+			 * activate monitoring at next ctxswin
+			 */
+			ctx_arch->ctx_saved_psr_up = IA64_PSR_UP;
+		}
+		return;
+	}
+
+	/*
+	 * system-wide mode
+	 */
+	dcr = ia64_getreg(_IA64_REG_CR_DCR);
+	if (flags & PFM_ITA_SETFL_INTR_ONLY) {
+		dcr_pp = 1;
+		psr_pp = 0;
+	} else if (flags & PFM_ITA_SETFL_EXCL_INTR) {
+		dcr_pp = 0;
+		psr_pp = 1;
+	} else {
+		dcr_pp = psr_pp = 1;
+	}
+	PFM_DBG("dcr_pp=%lu psr_pp=%lu", dcr_pp, psr_pp);
+
+	/*
+	 * update dcr_pp and psr_pp
+	 */
+	if (dcr_pp)
+		ia64_setreg(_IA64_REG_CR_DCR, dcr | IA64_DCR_PP);
+	else
+		ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
+
+	if (psr_pp) {
+		pfm_set_psr_pp();
+		ia64_psr(regs)->pp = 1;
+	} else {
+		pfm_clear_psr_pp();
+		ia64_psr(regs)->pp = 0;
+	}
+	ia64_srlz_d();
+
+	if (set->flags & PFM_ITA_SETFL_IDLE_EXCL) {
+		PFM_DBG("enable idle exclude");
+		__get_cpu_var(pfm_syst_info) |= PFM_ITA_CPUINFO_IDLE_EXCL;
+	}
+}
+
+/*
+ * Only call this function when a process is trying to
+ * write the debug registers (reading is always allowed)
+ * called from arch/ia64/kernel/ptrace.c:access_uarea()
+ */
+int __pfm_use_dbregs(struct task_struct *task)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_context *ctx;
+	unsigned long flags;
+	int ret = 0;
+
+	PFM_DBG("called for [%d]", task->pid);
+
+	ctx = task->pfm_context;
+
+	/*
+	 * do it only once
+	 */
+	if (task->thread.flags & IA64_THREAD_DBG_VALID) {
+		PFM_DBG("IA64_THREAD_DBG_VALID already set");
+		return 0;
+	}
+	if (ctx) {
+		spin_lock_irqsave(&ctx->lock, flags);
+		ctx_arch = pfm_ctx_arch(ctx);
+
+		if (ctx_arch->flags.use_dbr == 1) {
+			PFM_DBG("PMU using dbregs already, no ptrace access");
+			ret = -1;
+		}
+		spin_unlock_irqrestore(&ctx->lock, flags);
+		if (ret)
+			return ret;
+	}
+
+	spin_lock(&pfm_arch_sessions_lock);
+
+	/*
+	 * We cannot allow setting breakpoints when system wide monitoring
+	 * sessions are using the debug registers.
+	 */
+	if (!pfm_arch_sessions.pfs_sys_use_dbr)
+		pfm_arch_sessions.pfs_ptrace_use_dbr++;
+	else
+		ret = -1;
+
+	PFM_DBG("ptrace_use_dbr=%u  sys_use_dbr=%u by [%d] ret = %d",
+		  pfm_arch_sessions.pfs_ptrace_use_dbr,
+		  pfm_arch_sessions.pfs_sys_use_dbr,
+		  task->pid, ret);
+
+	spin_unlock(&pfm_arch_sessions_lock);
+	if (ret)
+		return ret;
+#ifndef CONFIG_SMP
+	/*
+	 * in UP, we need to check whether the current
+	 * owner of the PMU is not using the debug registers
+	 * for monitoring. Because we are using a lazy
+	 * save on ctxswout, we must force a save in this
+	 * case because the debug registers are being
+	 * modified by another task. We save the current
+	 * PMD registers, and clear ownership. In ctxswin,
+	 * full state will be reloaded.
+	 *
+	 * Note: we overwrite task.
+	 */
+	task = __get_cpu_var(pmu_owner);
+	ctx = __get_cpu_var(pmu_ctx);
+
+	if (task == NULL)
+		return 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	if (ctx_arch->flags.use_dbr)
+		pfm_save_pmds_release(ctx);
+#endif
+	return 0;
+}
+
+/*
+ * This function is called for every task that exits with the
+ * IA64_THREAD_DBG_VALID set. This indicates a task which was
+ * able to use the debug registers for debugging purposes via
+ * ptrace(). Therefore we know it was not using them for
+ * perfmormance monitoring, so we only decrement the number
+ * of "ptraced" debug register users to keep the count up to date
+ */
+int __pfm_release_dbregs(struct task_struct *task)
+{
+	int ret;
+
+	spin_lock(&pfm_arch_sessions_lock);
+
+	if (pfm_arch_sessions.pfs_ptrace_use_dbr == 0) {
+		PFM_ERR("invalid release for [%d] ptrace_use_dbr=0", task->pid);
+		ret = -1;
+	}  else {
+		pfm_arch_sessions.pfs_ptrace_use_dbr--;
+		ret = 0;
+	}
+	spin_unlock(&pfm_arch_sessions_lock);
+
+	return ret;
+}
+
+int pfm_ia64_mark_dbregs_used(struct pfm_context *ctx,
+			      struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct task_struct *task;
+	struct thread_struct *thread;
+	int ret = 0, state;
+	int i, can_access_pmu = 0;
+	int is_loaded, is_system;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	state = ctx->state;
+	task = ctx->task;
+	is_loaded = state == PFM_CTX_LOADED || state == PFM_CTX_MASKED;
+	is_system = ctx->flags.system;
+	can_access_pmu = __get_cpu_var(pmu_owner) == task || is_system;
+
+	if (is_loaded == 0)
+		goto done;
+
+	if (is_system == 0) {
+		thread = &(task->thread);
+
+		/*
+		 * cannot use debug registers for montioring if they are
+		 * already used for debugging
+		 */
+		if (thread->flags & IA64_THREAD_DBG_VALID) {
+			PFM_DBG("debug registers already in use for [%d]",
+				  task->pid);
+			return -EBUSY;
+		}
+	}
+
+	/*
+	 * check for debug registers in system wide mode
+	 */
+	spin_lock(&pfm_arch_sessions_lock);
+
+	if (is_system) {
+		if (pfm_arch_sessions.pfs_ptrace_use_dbr)
+			ret = -EBUSY;
+		else
+			pfm_arch_sessions.pfs_sys_use_dbr++;
+	}
+
+	spin_unlock(&pfm_arch_sessions_lock);
+
+	if (ret != 0)
+		return ret;
+
+	/*
+	 * clear hardware registers to make sure we don't
+	 * pick up stale state.
+	 */
+	if (can_access_pmu) {
+		PFM_DBG("clearing ibrs, dbrs");
+		for (i = 0; i < 8; i++) {
+			ia64_set_ibr(i, 0);
+			ia64_dv_serialize_instruction();
+		}
+		ia64_srlz_i();
+		for (i = 0; i < 8; i++) {
+			ia64_set_dbr(i, 0);
+			ia64_dv_serialize_data();
+		}
+		ia64_srlz_d();
+	}
+done:
+	/*
+	 * debug registers are now in use
+	 */
+	ctx_arch->flags.use_dbr = 1;
+	set->priv_flags |= PFM_ITA_SETFL_USE_DBR;
+	PFM_DBG("set%u use_dbr=1", set->id);
+	return 0;
+}
+EXPORT_SYMBOL(pfm_ia64_mark_dbregs_used);
+
+char *pfm_arch_get_pmu_module_name(void)
+{
+	switch(local_cpu_data->family) {
+	case 0x07:
+		return "perfmon_itanium";
+	case 0x1f:
+		return "perfmon_mckinley";
+	case 0x20:
+		return "perfmon_montecito";
+	default:
+		return "perfmon_generic";
+	}
+	return NULL;
+}
+
+/*
+ * global arch-specific intialization, called only once
+ */
+int __init pfm_arch_init(void)
+{
+	int ret;
+
+	spin_lock_init(&pfm_arch_sessions_lock);
+
+#ifdef CONFIG_IA64_PERFMON_COMPAT
+	ret = pfm_ia64_compat_init();
+	if (ret)
+		return ret;
+#endif
+	register_percpu_irq(IA64_PERFMON_VECTOR, &perfmon_irqaction);
+
+
+	return 0;
+}
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/perfmon_compat.c linux-2.6.25-id/arch/ia64/perfmon/perfmon_compat.c
--- linux-2.6.25-org/arch/ia64/perfmon/perfmon_compat.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/perfmon_compat.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,1247 @@
+/*
+ * This file implements the IA-64 specific
+ * support for the perfmon2 interface
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/file.h>
+#include <linux/vmalloc.h>
+#include <linux/proc_fs.h>
+#include <linux/perfmon.h>
+#include <asm/perfmon_compat.h>
+#include <asm/uaccess.h>
+
+asmlinkage long sys_pfm_stop(int fd);
+asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *st);
+asmlinkage long sys_pfm_unload_context(int fd);
+asmlinkage long sys_pfm_restart(int fd);
+asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ld);
+
+extern ssize_t __pfm_read(struct pfm_context *ctx,
+			  union pfarg_msg *msg_buf,
+			  int non_block);
+/*
+ * function providing some help for backward compatiblity with old IA-64
+ * applications. In the old model, certain attributes of a counter were
+ * passed via the PMC, now they are passed via the PMD.
+ */
+static int pfm_compat_update_pmd(struct pfm_context *ctx, u16 set_id, u16 cnum,
+		          u32 rflags,
+			  unsigned long *smpl_pmds,
+		          unsigned long *reset_pmds,
+			  u64 eventid)
+{
+	struct pfm_event_set *set;
+	int is_counting;
+	unsigned long *impl_pmds;
+	u32 flags = 0;
+	u16 max_pmd;
+
+	impl_pmds = pfm_pmu_conf->regs.pmds;
+	max_pmd	= pfm_pmu_conf->regs.max_pmd;
+
+	/*
+	 * given that we do not maintain PMC ->PMD dependencies
+	 * we cannot figure out what to do in case PMCxx != PMDxx
+	 */
+	if (cnum > max_pmd)
+		return 0;
+
+	/*
+	 * assumes PMCxx controls PMDxx which is always true for counters
+	 * on Itanium PMUs.
+	 */
+	is_counting = pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_C64;
+	set = pfm_find_set(ctx, set_id, 0);
+
+	/*
+	 * for v2.0, we only allowed counting PMD to generate
+	 * user-level notifications. Same thing with randomization.
+	 */
+	if (is_counting) {
+		if (rflags & PFM_REGFL_OVFL_NOTIFY)
+			flags |= PFM_REGFL_OVFL_NOTIFY;
+		if (rflags & PFM_REGFL_RANDOM)
+			flags |= PFM_REGFL_RANDOM;
+		/*
+		 * verify validity of smpl_pmds
+		 */
+		if (unlikely(bitmap_subset(smpl_pmds,
+					   impl_pmds, max_pmd) == 0)) {
+			PFM_DBG("invalid smpl_pmds=0x%llx for pmd%u",
+				  (unsigned long long)smpl_pmds[0], cnum);
+			return -EINVAL;
+		}
+		/*
+		 * verify validity of reset_pmds
+		 */
+		if (unlikely(bitmap_subset(reset_pmds,
+					   impl_pmds, max_pmd) == 0)) {
+			PFM_DBG("invalid reset_pmds=0x%lx for pmd%u",
+				  reset_pmds[0], cnum);
+			return -EINVAL;
+		}
+		/*
+		 * ensures that a PFM_READ_PMDS succeeds with a
+		 * corresponding PFM_WRITE_PMDS
+		 */
+		__set_bit(cnum, set->used_pmds);
+
+	} else if (rflags & (PFM_REGFL_OVFL_NOTIFY|PFM_REGFL_RANDOM)) {
+		PFM_DBG("cannot set ovfl_notify or random on pmd%u", cnum);
+		return -EINVAL;
+	}
+
+	set->pmds[cnum].flags = flags;
+
+	if (is_counting) {
+		bitmap_copy(set->pmds[cnum].reset_pmds,
+			    reset_pmds,
+			    max_pmd);
+
+		bitmap_copy(set->pmds[cnum].smpl_pmds,
+			    smpl_pmds,
+			    max_pmd);
+
+		set->pmds[cnum].eventid = eventid;
+
+		/*
+		 * update ovfl_notify
+		 */
+		if (rflags & PFM_REGFL_OVFL_NOTIFY)
+			__set_bit(cnum, set->ovfl_notify);
+		else
+			__clear_bit(cnum, set->ovfl_notify);
+
+	}
+	PFM_DBG("pmd%u flags=0x%x eventid=0x%lx r_pmds=0x%lx s_pmds=0x%lx",
+		  cnum, flags,
+		  eventid,
+		  reset_pmds[0],
+		  smpl_pmds[0]);
+
+	return 0;
+}
+
+
+int __pfm_write_ibrs_old(struct pfm_context *ctx, void *arg, int count)
+{
+	struct pfarg_dbreg *req = arg;
+	struct pfarg_pmc pmc;
+	int i, ret = 0;
+
+	memset(&pmc, 0, sizeof(pmc));
+
+	for (i = 0; i < count; i++, req++) {
+		pmc.reg_num   = 256+req->dbreg_num;
+		pmc.reg_value = req->dbreg_value;
+		pmc.reg_flags = 0;
+		pmc.reg_set   = req->dbreg_set;
+
+		ret = __pfm_write_pmcs(ctx, &pmc, 1);
+
+		req->dbreg_flags &= ~PFM_REG_RETFL_MASK;
+		req->dbreg_flags |= pmc.reg_flags;
+
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static long pfm_write_ibrs_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_dbreg *req = NULL;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (ret == 0)
+		ret = __pfm_write_ibrs_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+int __pfm_write_dbrs_old(struct pfm_context *ctx, void *arg, int count)
+{
+	struct pfarg_dbreg *req = arg;
+	struct pfarg_pmc pmc;
+	int i, ret = 0;
+
+	memset(&pmc, 0, sizeof(pmc));
+
+	for (i = 0; i < count; i++, req++) {
+		pmc.reg_num   = 264+req->dbreg_num;
+		pmc.reg_value = req->dbreg_value;
+		pmc.reg_flags = 0;
+		pmc.reg_set   = req->dbreg_set;
+
+		ret = __pfm_write_pmcs(ctx, &pmc, 1);
+
+		req->dbreg_flags &= ~PFM_REG_RETFL_MASK;
+		req->dbreg_flags |= pmc.reg_flags;
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static long pfm_write_dbrs_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_dbreg *req = NULL;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (ret == 0)
+		ret = __pfm_write_dbrs_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+int __pfm_write_pmcs_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
+			 int count)
+{
+	struct pfarg_pmc req;
+	unsigned int i;
+	int ret, error_code;
+
+	memset(&req, 0, sizeof(req));
+
+	for (i = 0; i < count; i++, req_old++) {
+		req.reg_num   = req_old->reg_num;
+		req.reg_set   = req_old->reg_set;
+		req.reg_flags = 0;
+		req.reg_value = req_old->reg_value;
+
+		ret = __pfm_write_pmcs(ctx, (void *)&req, 1);
+		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
+		req_old->reg_flags |= req.reg_flags;
+
+		if (ret)
+			return ret;
+
+		ret = pfm_compat_update_pmd(ctx, req_old->reg_set,
+				      req_old->reg_num,
+				      (u32)req_old->reg_flags,
+				      req_old->reg_smpl_pmds,
+				      req_old->reg_reset_pmds,
+				      req_old->reg_smpl_eventid);
+
+		error_code = ret ? PFM_REG_RETFL_EINVAL : 0;
+		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
+		req_old->reg_flags |= error_code;
+
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static long pfm_write_pmcs_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_reg *req = NULL;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (ret == 0)
+		ret = __pfm_write_pmcs_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+int __pfm_write_pmds_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
+			 int count)
+{
+	struct pfarg_pmd req;
+	int i, ret;
+
+	memset(&req, 0, sizeof(req));
+
+	for (i = 0; i < count; i++, req_old++) {
+		req.reg_num   = req_old->reg_num;
+		req.reg_set   = req_old->reg_set;
+		req.reg_value = req_old->reg_value;
+		req.reg_flags = req_old->reg_flags;
+
+		req.reg_long_reset  = req_old->reg_long_reset;
+		req.reg_short_reset = req_old->reg_short_reset;
+		req.reg_random_mask = req_old->reg_random_mask;
+		/*
+		 * reg_random_seed is ignored since v2.3
+		 */
+
+		/*
+		 * skip last_reset_val not used for writing
+		 * skip smpl_pmds, reset_pmds, eventid, ovfl_swtch_cnt
+		 * as set in pfm_write_pmcs_old.
+		 */
+		req.reg_ovfl_switch_cnt = req_old->reg_ovfl_switch_cnt;
+
+		ret = __pfm_write_pmds(ctx, (void *)&req, 1, 1);
+
+		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
+		req_old->reg_flags |= req.reg_flags;
+
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static long pfm_write_pmds_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_reg *req = NULL;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (ret == 0)
+		ret = __pfm_write_pmds_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+int __pfm_read_pmds_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
+			int count)
+{
+	struct pfarg_pmd req;
+	int i, ret;
+
+	memset(&req, 0, sizeof(req));
+
+	for (i = 0; i < count; i++, req_old++) {
+		req.reg_num   = req_old->reg_num;
+		req.reg_set   = req_old->reg_set;
+
+		/* skip value not used for reading */
+		req.reg_flags = req_old->reg_flags;
+
+		/* skip short/long_reset not used for reading */
+		/* skip last_reset_val not used for reading */
+		/* skip ovfl_switch_cnt not used for reading */
+
+		ret = __pfm_read_pmds(ctx, (void *)&req, 1);
+
+		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
+		req_old->reg_flags |= req.reg_flags;
+		if (ret)
+			return ret;
+
+		/* update fields */
+		req_old->reg_value = req.reg_value;
+
+		req_old->reg_last_reset_val  = req.reg_last_reset_val;
+		req_old->reg_ovfl_switch_cnt = req.reg_ovfl_switch_cnt;
+	}
+	return 0;
+}
+
+static long pfm_read_pmds_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_reg *req = NULL;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (ret == 0)
+		ret = __pfm_read_pmds_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+/*
+ * OBSOLETE: use /proc/perfmon_map instead
+ */
+static long pfm_get_default_pmcs_old(int fd, void __user *ureq, int count)
+{
+	struct pfarg_reg *req = NULL;
+	void *fptr;
+	size_t sz;
+	int ret, i;
+	unsigned int cnum;
+
+	if (count < 1)
+		return -EINVAL;
+
+	/*
+	 * ensure the pfm_pmu_conf does not disappear while
+	 * we use it
+	 */
+	ret = pfm_pmu_conf_get(1);
+	if (ret)
+		return ret;
+
+	sz = count*sizeof(*ureq);
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+
+	for (i = 0; i < count; i++, req++) {
+		cnum   = req->reg_num;
+
+		if (i >= PFM_MAX_PMCS ||
+		    (pfm_pmu_conf->pmc_desc[cnum].type & PFM_REG_I) == 0) {
+			req->reg_flags = PFM_REG_RETFL_EINVAL;
+			break;
+		}
+		req->reg_value = pfm_pmu_conf->pmc_desc[cnum].dfl_val;
+		req->reg_flags = 0;
+
+		PFM_DBG("pmc[%u]=0x%lx", cnum, req->reg_value);
+	}
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	pfm_pmu_conf_put();
+
+	return ret;
+}
+
+/*
+ * allocate a sampling buffer and remaps it into the user address space of
+ * the task. This is only in compatibility mode
+ *
+ * function called ONLY on current task
+ */
+int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx, size_t rsize,
+			      struct file *filp)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma = NULL;
+	struct pfm_arch_context *ctx_arch;
+	size_t size;
+	int ret;
+	extern struct vm_operations_struct pfm_buf_map_vm_ops;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * allocate buffer + map desc
+	 */
+	ret = pfm_smpl_buffer_alloc(ctx, rsize);
+	if (ret)
+		return ret;
+
+	size = ctx->smpl_size;
+
+
+	/* allocate vma */
+	vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+	if (!vma) {
+		PFM_DBG("Cannot allocate vma");
+		goto error_kmem;
+	}
+	memset(vma, 0, sizeof(*vma));
+
+	/*
+	 * partially initialize the vma for the sampling buffer
+	 */
+	vma->vm_mm	     = mm;
+	vma->vm_flags	     = VM_READ| VM_MAYREAD |VM_RESERVED;
+	vma->vm_page_prot    = PAGE_READONLY;
+	vma->vm_ops	     = &pfm_buf_map_vm_ops;
+	vma->vm_file	     = filp;
+	vma->vm_private_data = ctx;
+	vma->vm_pgoff        = 0;
+
+	/*
+	 * simulate effect of mmap()
+	 */
+	get_file(filp);
+
+	/*
+	 * Let's do the difficult operations next.
+	 *
+	 * now we atomically find some area in the address space and
+	 * remap the buffer into it.
+	 */
+	down_write(&current->mm->mmap_sem);
+
+	/* find some free area in address space, must have mmap sem held */
+	vma->vm_start = get_unmapped_area(NULL, 0, size, 0,
+					  MAP_PRIVATE|MAP_ANONYMOUS);
+	if (vma->vm_start == 0) {
+		PFM_DBG("cannot find unmapped area of size %zu", size);
+		up_write(&current->mm->mmap_sem);
+		goto error;
+	}
+	vma->vm_end = vma->vm_start + size;
+
+	PFM_DBG("aligned_size=%zu mapped @0x%lx", size, vma->vm_start);
+	/*
+	 * now insert the vma in the vm list for the process, must be
+	 * done with mmap lock held
+	 */
+	insert_vm_struct(mm, vma);
+
+	mm->total_vm  += size >> PAGE_SHIFT;
+
+	up_write(&current->mm->mmap_sem);
+
+	/*
+	 * IMPORTANT: we do not issue the fput()
+	 * because we want to increase the ref count
+	 * on the descriptor to simulate what mmap()
+	 * would do
+	 */
+
+	/*
+	 * used to propagate vaddr to syscall stub
+	 */
+	ctx_arch->ctx_smpl_vaddr = (void *)vma->vm_start;
+
+	return 0;
+error:
+	kmem_cache_free(vm_area_cachep, vma);
+error_kmem:
+	pfm_release_buf_space(ctx, ctx->smpl_size);
+	vfree(ctx->smpl_addr);
+	return -ENOMEM;
+}
+
+#define PFM_DEFAULT_SMPL_UUID { \
+		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82,\
+		0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
+
+static pfm_uuid_t old_default_uuid = PFM_DEFAULT_SMPL_UUID;
+static pfm_uuid_t null_uuid;
+
+/*
+ * function invoked in case, pfm_context_create fails
+ * at the last operation, copy_to_user. It needs to
+ * undo memory allocations and free the file descriptor
+ */
+static void pfm_undo_create_context_fd(int fd, struct pfm_context *ctx)
+{
+	struct files_struct *files = current->files;
+	struct file *file;
+	int fput_needed;
+
+	file = fget_light(fd, &fput_needed);
+	/*
+	 * there is no fd_uninstall(), so we do it
+	 * here. put_unused_fd() does not remove the
+	 * effect of fd_install().
+	 */
+
+	spin_lock(&files->file_lock);
+	files->fd_array[fd] = NULL;
+	spin_unlock(&files->file_lock);
+
+	fput_light(file, fput_needed);
+
+	/*
+	 * decrement ref count and kill file
+	 */
+	put_filp(file);
+
+	put_unused_fd(fd);
+
+	pfm_context_free(ctx);
+}
+
+static int pfm_get_smpl_arg_old(pfm_uuid_t uuid, void __user *fmt_uarg,
+				size_t usize, void **arg,
+				struct pfm_smpl_fmt **fmt)
+{
+	struct pfm_smpl_fmt *f;
+	void *addr = NULL;
+	size_t sz;
+	int ret;
+
+	if (!memcmp(uuid, null_uuid, sizeof(pfm_uuid_t)))
+		return 0;
+
+	if (memcmp(uuid, old_default_uuid, sizeof(pfm_uuid_t))) {
+		PFM_DBG("compatibility mode supports only default sampling format");
+		return -EINVAL;
+	}
+	/*
+	 * find fmt and increase refcount
+	 */
+	f = pfm_smpl_fmt_get("default-old");
+	if (f == NULL) {
+		PFM_DBG("default-old buffer format not found");
+		return -EINVAL;
+	}
+
+	/*
+	 * expected format argument size
+	 */
+	sz = f->fmt_arg_size;
+
+	/*
+	 * check user size matches expected size
+	 * usize = -1 is for IA-64 backward compatibility
+	 */
+	ret = -EINVAL;
+	if (sz != usize && usize != -1) {
+		PFM_DBG("invalid arg size %zu, format expects %zu",
+			usize, sz);
+		goto error;
+	}
+
+	ret = -ENOMEM;
+	addr = kmalloc(sz, GFP_KERNEL);
+	if (addr == NULL)
+		goto error;
+
+	ret = -EFAULT;
+	if (copy_from_user(addr, fmt_uarg, sz))
+		goto error;
+
+	*arg = addr;
+	*fmt = f;
+	return 0;
+
+error:
+	kfree(addr);
+	pfm_smpl_fmt_put(f);
+	return ret;
+}
+
+static long pfm_create_context_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *new_ctx;
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_smpl_fmt *fmt = NULL;
+	struct pfarg_context req_old;
+	void __user *usmpl_arg;
+	void *smpl_arg = NULL;
+	struct pfarg_ctx req;
+	int ret;
+
+	if (count != 1)
+		return -EINVAL;
+
+	if (copy_from_user(&req_old, ureq, sizeof(req_old)))
+		return -EFAULT;
+
+	memset(&req, 0, sizeof(req));
+
+	/*
+	 * sampling format args are following pfarg_context
+	 */
+	usmpl_arg = ureq+sizeof(req_old);
+
+	ret = pfm_get_smpl_arg_old(req_old.ctx_smpl_buf_id, usmpl_arg, -1,
+			           &smpl_arg, &fmt);
+	if (ret)
+		return ret;
+
+	req.ctx_flags = req_old.ctx_flags;
+
+	/*
+	 * returns file descriptor if >=0, or error code */
+	ret = __pfm_create_context(&req, fmt, smpl_arg, PFM_COMPAT, &new_ctx);
+	if (ret >= 0) {
+		ctx_arch = pfm_ctx_arch(new_ctx);
+		req_old.ctx_fd = ret;
+		req_old.ctx_smpl_vaddr = ctx_arch->ctx_smpl_vaddr;
+	}
+
+	if (copy_to_user(ureq, &req_old, sizeof(req_old))) {
+		pfm_undo_create_context_fd(req_old.ctx_fd, new_ctx);
+		ret = -EFAULT;
+	}
+
+	kfree(smpl_arg);
+
+	return ret;
+}
+
+/*
+ * obsolete call: use /proc/perfmon
+ */
+static long pfm_get_features_old(int fd, void __user *arg, int count)
+{
+	struct pfarg_features req;
+	int ret = 0;
+
+	if (count != 1)
+		return -EINVAL;
+
+	memset(&req, 0, sizeof(req));
+
+	req.ft_version = PFM_VERSION;
+
+	if (copy_to_user(arg, &req, sizeof(req)))
+		ret = -EFAULT;
+
+	return ret;
+}
+
+static long pfm_debug_old(int fd, void __user *arg, int count)
+{
+	int m;
+
+	if (count != 1)
+		return -EINVAL;
+
+	if (get_user(m, (int __user*)arg))
+		return -EFAULT;
+
+
+	pfm_controls.debug = m == 0 ? 0 : 1;
+
+	PFM_INFO("debugging %s (timing reset)",
+		 pfm_controls.debug ? "on" : "off");
+
+	if (m == 0)
+		for_each_online_cpu(m) {
+			memset(&per_cpu(pfm_stats,m), 0,
+			       sizeof(struct pfm_stats));
+		}
+	return 0;
+}
+
+static long pfm_unload_context_old(int fd, void __user *arg, int count)
+{
+	if (count)
+		return -EINVAL;
+
+	return sys_pfm_unload_context(fd);
+}
+
+static long pfm_restart_old(int fd, void __user *arg, int count)
+{
+	if (count)
+		return -EINVAL;
+
+	return sys_pfm_restart(fd);
+}
+
+static long pfm_stop_old(int fd, void __user *arg, int count)
+{
+	if (count)
+		return -EINVAL;
+
+	return sys_pfm_stop(fd);
+}
+
+static long pfm_start_old(int fd, void __user *arg, int count)
+{
+	if (count > 1)
+		return -EINVAL;
+
+	return sys_pfm_start(fd, arg);
+}
+
+static long pfm_load_context_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct file *filp;
+	unsigned long flags;
+	struct pfarg_load req;
+	int ret, fput_needed;
+
+	if (count != 1)
+		return -EINVAL;
+
+	if (copy_from_user(&req, ureq, sizeof(req)))
+		return -EFAULT;
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	task = NULL;
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	/*
+	 * in per-thread mode (not self-monitoring), get a reference
+	 * on task to monitor. This must be done with interrupts enabled
+	 * Upon succesful return, refcount on task is increased.
+	 *
+	 * fget_light() is protecting the context.
+	 */
+	if (!ctx->flags.system) {
+		if (req.load_pid != current->pid) {
+			ret = pfm_get_task(ctx, req.load_pid, &task);
+			if (ret)
+				goto error;
+		} else
+			task = current;
+	}
+	/*
+	 * irqsave is required to avoid race in case context is already
+	 * loaded or with switch timeout in the case of self-monitoring
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	/*
+	 * the new interface requires the desired CPU to be explicitely set
+	 * in this field. the kernel then checks you are on the right CPU
+	 */
+	if (ctx->flags.system)
+		req.load_pid = smp_processor_id();
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags);
+	if (!ret)
+		ret = __pfm_load_context(ctx, &req, task);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/*
+	 * in per-thread mode (not self-monitoring), we need
+	 * to decrease refcount on task to monitor:
+	 *   - load successful: we have a reference to the task in ctx->task
+	 *   - load failed    : undo the effect of pfm_get_task()
+	 */
+	if (task && task != current)
+		put_task_struct(task);
+
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+/*
+ * perfmon command descriptions
+ */
+struct pfm_cmd_desc {
+	long (*cmd_func)(int fd, void __user *arg, int count);
+};
+
+/*
+ * functions MUST be listed in the increasing order of
+ * their index (see permfon.h)
+ */
+#define PFM_CMD(name)  \
+	{ .cmd_func = name,  \
+	}
+#define PFM_CMD_NONE		\
+	{ .cmd_func = NULL   \
+	}
+
+static struct pfm_cmd_desc pfm_cmd_tab[]={
+/* 0  */PFM_CMD_NONE,
+/* 1  */PFM_CMD(pfm_write_pmcs_old),
+/* 2  */PFM_CMD(pfm_write_pmds_old),
+/* 3  */PFM_CMD(pfm_read_pmds_old),
+/* 4  */PFM_CMD(pfm_stop_old),
+/* 5  */PFM_CMD(pfm_start_old),
+/* 6  */PFM_CMD_NONE,
+/* 7  */PFM_CMD_NONE,
+/* 8  */PFM_CMD(pfm_create_context_old),
+/* 9  */PFM_CMD_NONE,
+/* 10 */PFM_CMD(pfm_restart_old),
+/* 11 */PFM_CMD_NONE,
+/* 12 */PFM_CMD(pfm_get_features_old),
+/* 13 */PFM_CMD(pfm_debug_old),
+/* 14 */PFM_CMD_NONE,
+/* 15 */PFM_CMD(pfm_get_default_pmcs_old),
+/* 16 */PFM_CMD(pfm_load_context_old),
+/* 17 */PFM_CMD(pfm_unload_context_old),
+/* 18 */PFM_CMD_NONE,
+/* 19 */PFM_CMD_NONE,
+/* 20 */PFM_CMD_NONE,
+/* 21 */PFM_CMD_NONE,
+/* 22 */PFM_CMD_NONE,
+/* 23 */PFM_CMD_NONE,
+/* 24 */PFM_CMD_NONE,
+/* 25 */PFM_CMD_NONE,
+/* 26 */PFM_CMD_NONE,
+/* 27 */PFM_CMD_NONE,
+/* 28 */PFM_CMD_NONE,
+/* 29 */PFM_CMD_NONE,
+/* 30 */PFM_CMD_NONE,
+/* 31 */PFM_CMD_NONE,
+/* 32 */PFM_CMD(pfm_write_ibrs_old),
+/* 33 */PFM_CMD(pfm_write_dbrs_old),
+};
+#define PFM_CMD_COUNT ARRAY_SIZE(pfm_cmd_tab)
+
+/*
+ * system-call entry point (must return long)
+ */
+asmlinkage long sys_perfmonctl (int fd, int cmd, void __user *arg, int count)
+{
+	if (perfmon_disabled)
+		return -ENOSYS;
+
+	if (unlikely(cmd < 0 || cmd >= PFM_CMD_COUNT
+		     || pfm_cmd_tab[cmd].cmd_func == NULL)) {
+		PFM_DBG("invalid cmd=%d", cmd);
+		return -EINVAL;
+	}
+	return (long)pfm_cmd_tab[cmd].cmd_func(fd, arg, count);
+}
+
+/*
+ * Called from pfm_read() for a perfmon v2.0 context.
+ *
+ * compatibility mode pfm_read() routine. We need a separate
+ * routine because the definition of the message has changed.
+ * The pfm_msg and pfarg_msg structures are different.
+ *
+ * return: sizeof(pfm_msg_t) on success, -errno otherwise
+ */
+ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size)
+{
+	union pfarg_msg msg_buf;
+	pfm_msg_t old_msg_buf;
+	pfm_ovfl_msg_t *o_msg;
+	struct pfarg_ovfl_msg *n_msg;
+	int ret;
+
+	PFM_DBG("msg=%p size=%zu", buf, size);
+
+	/*
+	 * cannot extract partial messages.
+	 * check even when there is no message
+	 *
+	 * cannot extract more than one message per call. Bytes
+	 * above sizeof(msg) are ignored.
+	 */
+	if (size < sizeof(old_msg_buf)) {
+		PFM_DBG("message is too small size=%zu must be >=%zu)",
+			size,
+			sizeof(old_msg_buf));
+		return -EINVAL;
+	}
+
+	ret =  __pfm_read(ctx, &msg_buf, non_block);
+	if (ret < 1)
+		return ret;
+
+	/*
+	 * force return value to old message size
+	 */
+	ret = sizeof(old_msg_buf);
+
+	o_msg = &old_msg_buf.pfm_ovfl_msg;
+	n_msg = &msg_buf.pfm_ovfl_msg;
+
+	switch(msg_buf.type) {
+		case PFM_MSG_OVFL:
+			o_msg->msg_type   = PFM_MSG_OVFL;
+			o_msg->msg_ctx_fd = 0;
+			o_msg->msg_active_set = n_msg->msg_active_set;
+			o_msg->msg_tstamp = 0;
+
+			o_msg->msg_ovfl_pmds[0] = n_msg->msg_ovfl_pmds[0];
+			o_msg->msg_ovfl_pmds[1] = n_msg->msg_ovfl_pmds[1];
+			o_msg->msg_ovfl_pmds[2] = n_msg->msg_ovfl_pmds[2];
+			o_msg->msg_ovfl_pmds[3] = n_msg->msg_ovfl_pmds[3];
+			break;
+		case PFM_MSG_END:
+			o_msg->msg_type = PFM_MSG_END;
+			o_msg->msg_ctx_fd = 0;
+			o_msg->msg_tstamp = 0;
+			break;
+		default:
+			PFM_DBG("unknown msg type=%d", msg_buf.type);
+	}
+	if(copy_to_user(buf, &old_msg_buf, sizeof(old_msg_buf)))
+		ret = -EFAULT;
+	PFM_DBG_ovfl("ret=%d", ret);
+	return ret;
+}
+
+/*
+ * legacy /proc/perfmon simplified interface (we only maintain the
+ * global information (no more per-cpu stats, use
+ * /sys/devices/system/cpu/cpuXX/perfmon
+ */
+static struct proc_dir_entry 	*perfmon_proc;
+
+static void *pfm_proc_start(struct seq_file *m, loff_t *pos)
+{
+	if (*pos == 0)
+		return (void *)1;
+
+	return NULL;
+}
+
+static void *pfm_proc_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	++*pos;
+	return pfm_proc_start(m, pos);
+}
+
+static void pfm_proc_stop(struct seq_file *m, void *v)
+{
+}
+
+/*
+ * this is a simplified version of the legacy /proc/perfmon.
+ * We have retained ONLY the key information that tools are actually
+ * using
+ */
+static void pfm_proc_show_header(struct seq_file *m)
+{
+	char buf[128];
+
+	pfm_sysfs_session_show(buf, sizeof(buf), 3);
+
+	seq_printf(m, "perfmon version            : %u.%u\n",
+		PFM_VERSION_MAJ, PFM_VERSION_MIN);
+
+	seq_printf(m, "model                      : %s", buf);
+}
+
+static int pfm_proc_show(struct seq_file *m, void *v)
+{
+	pfm_proc_show_header(m);
+	return 0;
+}
+
+struct seq_operations pfm_proc_seq_ops = {
+	.start = pfm_proc_start,
+	.next =	pfm_proc_next,
+	.stop =	pfm_proc_stop,
+	.show =	pfm_proc_show
+};
+
+static int pfm_proc_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &pfm_proc_seq_ops);
+}
+
+
+static struct file_operations pfm_proc_fops = {
+	.open = pfm_proc_open,
+	.read = seq_read,
+	.llseek	= seq_lseek,
+	.release = seq_release,
+};
+
+/*
+ * called from pfm_arch_init(), global initialization, called once
+ */
+int __init pfm_ia64_compat_init(void)
+{
+	/*
+	 * create /proc/perfmon
+	 */
+	perfmon_proc = create_proc_entry("perfmon", S_IRUGO, NULL);
+	if (perfmon_proc == NULL) {
+		PFM_ERR("cannot create /proc entry, perfmon disabled");
+		return -1;
+	}
+	perfmon_proc->proc_fops = &pfm_proc_fops;
+	return 0;
+}
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/perfmon_default_smpl.c linux-2.6.25-id/arch/ia64/perfmon/perfmon_default_smpl.c
--- linux-2.6.25-org/arch/ia64/perfmon/perfmon_default_smpl.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/perfmon_default_smpl.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,268 @@
+/*
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file implements the old default sampling buffer format
+ * for the Linux/ia64 perfmon-2 subsystem. This is for backward
+ * compatibility only. use the new default format in perfmon/
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <asm/delay.h>
+#include <linux/smp.h>
+#include <linux/sysctl.h>
+
+#ifdef MODULE
+#define FMT_FLAGS	0
+#else
+#define FMT_FLAGS	PFM_FMTFL_IS_BUILTIN
+#endif
+
+#include <linux/perfmon.h>
+#include <asm-ia64/perfmon_default_smpl.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("perfmon old default sampling format");
+MODULE_LICENSE("GPL");
+
+static int pfm_default_fmt_validate(u32 flags, u16 npmds, void *data)
+{
+	struct pfm_default_smpl_arg *arg = data;
+	size_t min_buf_size;
+
+	if (data == NULL) {
+		PFM_DBG("no argument passed");
+		return -EINVAL;
+	}
+
+	/*
+	 * compute min buf size. All PMD are manipulated as 64bit entities
+	 */
+	min_buf_size = sizeof(struct pfm_default_smpl_hdr)
+	             + (sizeof(struct pfm_default_smpl_entry)
+		     + (npmds*sizeof(u64)));
+
+	PFM_DBG("validate flags=0x%x npmds=%u min_buf_size=%lu "
+		  "buf_size=%lu CPU%d", flags, npmds, min_buf_size,
+		  arg->buf_size, smp_processor_id());
+
+	/*
+	 * must hold at least the buffer header + one minimally sized entry
+	 */
+	if (arg->buf_size < min_buf_size) return -EINVAL;
+
+	return 0;
+}
+
+static int pfm_default_fmt_get_size(unsigned int flags, void *data,
+				    size_t *size)
+{
+	struct pfm_default_smpl_arg *arg = data;
+
+	/*
+	 * size has been validated in default_validate
+	 */
+	*size = arg->buf_size;
+
+	return 0;
+}
+
+static int pfm_default_fmt_init(struct pfm_context *ctx, void *buf,
+				u32 flags, u16 npmds, void *data)
+{
+	struct pfm_default_smpl_hdr *hdr;
+	struct pfm_default_smpl_arg *arg = data;
+
+	hdr = buf;
+
+	hdr->hdr_version      = PFM_DEFAULT_SMPL_VERSION;
+	hdr->hdr_buf_size     = arg->buf_size;
+	hdr->hdr_cur_offs     = sizeof(*hdr);
+	hdr->hdr_overflows    = 0;
+	hdr->hdr_count        = 0;
+
+	PFM_DBG("buffer=%p buf_size=%lu hdr_size=%lu "
+		  "hdr_version=%u cur_offs=%lu",
+		  buf,
+		  hdr->hdr_buf_size,
+		  sizeof(*hdr),
+		  hdr->hdr_version,
+		  hdr->hdr_cur_offs);
+
+	return 0;
+}
+
+static int pfm_default_fmt_handler(void *buf, struct pfm_ovfl_arg *arg,
+				   unsigned long ip, u64 tstamp, void *data)
+{
+	struct pfm_default_smpl_hdr *hdr;
+	struct pfm_default_smpl_entry *ent;
+	void *cur, *last;
+	u64 *e;
+	size_t entry_size;
+	u16 npmds, i, ovfl_pmd;
+
+	hdr         = buf;
+	cur         = buf+hdr->hdr_cur_offs;
+	last        = buf+hdr->hdr_buf_size;
+	ovfl_pmd    = arg->ovfl_pmd;
+
+	/*
+	 * precheck for sanity
+	 */
+	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE) goto full;
+
+	npmds = arg->num_smpl_pmds;
+
+	ent = cur;
+
+	prefetch(arg->smpl_pmds_values);
+
+	entry_size = sizeof(*ent) + (npmds << 3);
+
+	/* position for first pmd */
+	e = (unsigned long *)(ent+1);
+
+	hdr->hdr_count++;
+
+	PFM_DBG_ovfl("count=%lu cur=%p last=%p free_bytes=%lu "
+		       "ovfl_pmd=%d npmds=%u",
+		       hdr->hdr_count,
+		       cur, last,
+		       last-cur,
+		       ovfl_pmd,
+		       npmds);
+
+	/*
+	 * current = task running at the time of the overflow.
+	 *
+	 * per-task mode:
+	 * 	- this is ususally the task being monitored.
+	 * 	  Under certain conditions, it might be a different task
+	 *
+	 * system-wide:
+	 * 	- this is not necessarily the task controlling the session
+	 */
+	ent->pid            = current->pid;
+	ent->ovfl_pmd  	    = ovfl_pmd;
+	ent->last_reset_val = arg->pmd_last_reset;
+
+	/*
+	 * where did the fault happen (includes slot number)
+	 */
+	ent->ip = ip;
+
+	ent->tstamp    = tstamp;
+	ent->cpu       = smp_processor_id();
+	ent->set       = arg->active_set;
+	ent->tgid      = current->tgid;
+
+	/*
+	 * selectively store PMDs in increasing index number
+	 */
+	if (npmds) {
+		u64 *val = arg->smpl_pmds_values;
+		for(i=0; i < npmds; i++) {
+			*e++ = *val++;
+		}
+	}
+
+	/*
+	 * update position for next entry
+	 */
+	hdr->hdr_cur_offs += entry_size;
+	cur               += entry_size;
+
+	/*
+	 * post check to avoid losing the last sample
+	 */
+	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE) goto full;
+
+	/*
+	 * reset before returning from interrupt handler
+	 */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+	return 0;
+full:
+	PFM_DBG_ovfl("smpl buffer full free=%lu, count=%lu",
+		       last-cur, hdr->hdr_count);
+
+	/*
+	 * increment number of buffer overflow.
+	 * important to detect duplicate set of samples.
+	 */
+	hdr->hdr_overflows++;
+
+	/*
+	 * request notification and masking of monitoring.
+	 * Notification is still subject to the overflowed
+	 */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY| PFM_OVFL_CTRL_MASK;
+
+	return -ENOBUFS; /* we are full, sorry */
+}
+
+static int pfm_default_fmt_restart(int is_active, u32 *ovfl_ctrl, void *buf)
+{
+	struct pfm_default_smpl_hdr *hdr;
+
+	hdr = buf;
+
+	hdr->hdr_count    = 0;
+	hdr->hdr_cur_offs = sizeof(*hdr);
+
+	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+
+	return 0;
+}
+
+static int pfm_default_fmt_exit(void *buf)
+{
+	return 0;
+}
+
+static struct pfm_smpl_fmt default_fmt={
+	.fmt_name = "default-old",
+	.fmt_version = 0x10000,
+	.fmt_arg_size = sizeof(struct pfm_default_smpl_arg),
+	.fmt_validate = pfm_default_fmt_validate,
+	.fmt_getsize = pfm_default_fmt_get_size,
+	.fmt_init = pfm_default_fmt_init,
+	.fmt_handler = pfm_default_fmt_handler,
+	.fmt_restart = pfm_default_fmt_restart,
+	.fmt_exit = pfm_default_fmt_exit,
+	.fmt_flags = FMT_FLAGS,
+	.owner= THIS_MODULE
+};
+
+static int pfm_default_fmt_init_module(void)
+{
+	int ret;
+
+	return pfm_fmt_register(&default_fmt);
+	return ret;
+}
+
+static void pfm_default_fmt_cleanup_module(void)
+{
+	pfm_fmt_unregister(&default_fmt);
+}
+
+module_init(pfm_default_fmt_init_module);
+module_exit(pfm_default_fmt_cleanup_module);
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/perfmon_generic.c linux-2.6.25-id/arch/ia64/perfmon/perfmon_generic.c
--- linux-2.6.25-org/arch/ia64/perfmon/perfmon_generic.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/perfmon_generic.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,148 @@
+/*
+ * This file contains the generic PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <asm/pal.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Generic IA-64 PMU description tables");
+MODULE_LICENSE("GPL");
+
+#define RDEP(x)	(1UL << (x))
+
+#define PFM_IA64GEN_MASK_PMCS	(RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7))
+#define PFM_IA64GEN_RSVD	(0xffffffffffff0080UL)
+#define PFM_IA64GEN_NO64	(1UL<<5)
+
+/* forward declaration */
+static struct pfm_pmu_config pfm_ia64gen_pmu_conf;
+
+static struct pfm_arch_pmu_info pfm_ia64gen_pmu_info={
+	.mask_pmcs = {PFM_IA64GEN_MASK_PMCS,},
+};
+
+static struct pfm_regmap_desc pfm_ia64gen_pmc_desc[]={
+/* pmc0  */ PMX_NA,
+/* pmc1  */ PMX_NA,
+/* pmc2  */ PMX_NA,
+/* pmc3  */ PMX_NA,
+/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 4),
+/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 5),
+/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 6),
+/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 7)
+};
+#define PFM_IA64GEN_NUM_PMCS ARRAY_SIZE(pfm_ia64gen_pmc_desc)
+
+static struct pfm_regmap_desc pfm_ia64gen_pmd_desc[]={
+/* pmd0  */ PMX_NA,
+/* pmd1  */ PMX_NA,
+/* pmd2  */ PMX_NA,
+/* pmd3  */ PMX_NA,
+/* pmd4  */ PMD_D(PFM_REG_C, "PMD4", 4),
+/* pmd5  */ PMD_D(PFM_REG_C, "PMD5", 5),
+/* pmd6  */ PMD_D(PFM_REG_C, "PMD6", 6),
+/* pmd7  */ PMD_D(PFM_REG_C, "PMD7", 7)
+};
+#define PFM_IA64GEN_NUM_PMDS ARRAY_SIZE(pfm_ia64gen_pmd_desc)
+
+static int pfm_ia64gen_pmc_check(struct pfm_context *ctx,
+				 struct pfm_event_set *set,
+			         struct pfarg_pmc *req)
+{
+#define PFM_IA64GEN_PMC_PM_POS6	(1UL<< 6)
+	u64 tmpval;
+	int is_system;
+
+	is_system = ctx->flags.system;
+	tmpval = req->reg_value;
+
+	switch(req->reg_num) {
+		case  4:
+		case  5:
+		case  6:
+		case  7:
+			/* set pmc.oi for 64-bit emulation */
+			tmpval |= 1UL << 5;
+
+			if (is_system)
+				tmpval |= PFM_IA64GEN_PMC_PM_POS6;
+			else
+				tmpval &= ~PFM_IA64GEN_PMC_PM_POS6;
+			break;
+
+	}
+	req->reg_value = tmpval;
+
+	return 0;
+}
+
+/*
+ * matches anything
+ */
+static int pfm_ia64gen_probe_pmu(void)
+{
+	u64 pm_buffer[16];
+	pal_perf_mon_info_u_t pm_info;
+
+	/*
+	 * call PAL_PERFMON_INFO to retrieve counter width which
+	 * is implementation specific
+	 */
+	if (ia64_pal_perf_mon_info(pm_buffer, &pm_info))
+		return -1;
+
+	pfm_ia64gen_pmu_conf.counter_width = pm_info.pal_perf_mon_info_s.width;
+
+	return 0;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_ia64gen_pmu_conf={
+	.pmu_name = "Generic IA-64",
+	.counter_width = 0, /* computed from PAL_PERFMON_INFO */
+	.pmd_desc = pfm_ia64gen_pmd_desc,
+	.pmc_desc = pfm_ia64gen_pmc_desc,
+	.probe_pmu = pfm_ia64gen_probe_pmu,
+	.num_pmc_entries = PFM_IA64GEN_NUM_PMCS,
+	.num_pmd_entries = PFM_IA64GEN_NUM_PMDS,
+	.pmc_write_check = pfm_ia64gen_pmc_check,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.arch_info = & pfm_ia64gen_pmu_info
+	/* no read/write checkers */
+};
+
+static int __init pfm_gen_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_ia64gen_pmu_conf);
+}
+
+static void __exit pfm_gen_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_ia64gen_pmu_conf);
+}
+
+module_init(pfm_gen_pmu_init_module);
+module_exit(pfm_gen_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/perfmon_itanium.c linux-2.6.25-id/arch/ia64/perfmon/perfmon_itanium.c
--- linux-2.6.25-org/arch/ia64/perfmon/perfmon_itanium.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/perfmon_itanium.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,229 @@
+/*
+ * This file contains the Itanium PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Itanium (Merced) PMU description tables");
+MODULE_LICENSE("GPL");
+
+#define RDEP(x)	(1ULL << (x))
+
+#define PFM_ITA_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|RDEP(10)|RDEP(11)|\
+			   RDEP(12))
+
+#define PFM_ITA_NO64	(1ULL<<5)
+
+static struct pfm_arch_pmu_info pfm_ita_pmu_info={
+	.mask_pmcs = {PFM_ITA_MASK_PMCS,},
+};
+/* reserved bits are 1 in the mask */
+#define PFM_ITA_RSVD 0xfffffffffc8000a0UL
+/*
+ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
+ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
+ * but this is fine because they are handled separately in the IA-64 specific
+ * code.
+ */
+static struct pfm_regmap_desc pfm_ita_pmc_desc[]={
+/* pmc0  */ PMX_NA,
+/* pmc1  */ PMX_NA,
+/* pmc2  */ PMX_NA,
+/* pmc3  */ PMX_NA,
+/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 4),
+/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 5),
+/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 6),
+/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 7),
+/* pmc8  */ PMC_D(PFM_REG_W  , "PMC8" , 0xfffffffe3ffffff8UL, 0xfff00000001c0000UL, 0, 8),
+/* pmc9  */ PMC_D(PFM_REG_W  , "PMC9" , 0xfffffffe3ffffff8UL, 0xfff00000001c0000UL, 0, 9),
+/* pmc10 */ PMC_D(PFM_REG_W  , "PMC10", 0x0, 0xfffffffff3f0ff30UL, 0, 10),
+/* pmc11 */ PMC_D(PFM_REG_W  , "PMC11", 0x10000000UL, 0xffffffffecf0ff30UL, 0, 11),
+/* pmc12 */ PMC_D(PFM_REG_W  , "PMC12", 0x0, 0xffffffffffff0030UL, 0, 12),
+/* pmc13 */ PMC_D(PFM_REG_W  , "PMC13", 0x3ffff00000001UL, 0xfffffffffffffffeUL, 0, 13),
+/* pmc14 */ PMX_NA,
+/* pmc15 */ PMX_NA,
+/* pmc16 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc24 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc32 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc40 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc48 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc56 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc64 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc72 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc80 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc88 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc96 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc104 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc112 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc120 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc128 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc136 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc144 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc152 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc160 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc168 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc176 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc184 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc192 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc200 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc208 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc216 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc224 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc232 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc240 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc248 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc256 */ PMC_D(PFM_REG_W  , "IBR0", 0x0, 0, 0, 0),
+/* pmc257 */ PMC_D(PFM_REG_W  , "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
+/* pmc258 */ PMC_D(PFM_REG_W  , "IBR2", 0x0, 0, 0, 2),
+/* pmc259 */ PMC_D(PFM_REG_W  , "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
+/* pmc260 */ PMC_D(PFM_REG_W  , "IBR4", 0x0, 0, 0, 4),
+/* pmc261 */ PMC_D(PFM_REG_W  , "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
+/* pmc262 */ PMC_D(PFM_REG_W  , "IBR6", 0x0, 0, 0, 6),
+/* pmc263 */ PMC_D(PFM_REG_W  , "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
+/* pmc264 */ PMC_D(PFM_REG_W  , "DBR0", 0x0, 0, 0, 0),
+/* pmc265 */ PMC_D(PFM_REG_W  , "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
+/* pmc266 */ PMC_D(PFM_REG_W  , "DBR2", 0x0, 0, 0, 2),
+/* pmc267 */ PMC_D(PFM_REG_W  , "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
+/* pmc268 */ PMC_D(PFM_REG_W  , "DBR4", 0x0, 0, 0, 4),
+/* pmc269 */ PMC_D(PFM_REG_W  , "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
+/* pmc270 */ PMC_D(PFM_REG_W  , "DBR6", 0x0, 0, 0, 6),
+/* pmc271 */ PMC_D(PFM_REG_W  , "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
+};
+#define PFM_ITA_NUM_PMCS ARRAY_SIZE(pfm_ita_pmc_desc)
+
+static struct pfm_regmap_desc pfm_ita_pmd_desc[]={
+/* pmd0  */ PMD_D(PFM_REG_I , "PMD0", 0),
+/* pmd1  */ PMD_D(PFM_REG_I , "PMD1", 1),
+/* pmd2  */ PMD_D(PFM_REG_I , "PMD2", 2),
+/* pmd3  */ PMD_D(PFM_REG_I , "PMD3", 3),
+/* pmd4  */ PMD_D(PFM_REG_C , "PMD4", 4),
+/* pmd5  */ PMD_D(PFM_REG_C , "PMD5", 5),
+/* pmd6  */ PMD_D(PFM_REG_C , "PMD6", 6),
+/* pmd7  */ PMD_D(PFM_REG_C , "PMD7", 7),
+/* pmd8  */ PMD_D(PFM_REG_I , "PMD8", 8),
+/* pmd9  */ PMD_D(PFM_REG_I , "PMD9", 9),
+/* pmd10 */ PMD_D(PFM_REG_I , "PMD10", 10),
+/* pmd11 */ PMD_D(PFM_REG_I , "PMD11", 11),
+/* pmd12 */ PMD_D(PFM_REG_I , "PMD12", 12),
+/* pmd13 */ PMD_D(PFM_REG_I , "PMD13", 13),
+/* pmd14 */ PMD_D(PFM_REG_I , "PMD14", 14),
+/* pmd15 */ PMD_D(PFM_REG_I , "PMD15", 15),
+/* pmd16 */ PMD_D(PFM_REG_I , "PMD16", 16),
+/* pmd17 */ PMD_D(PFM_REG_I , "PMD17", 17)
+};
+#define PFM_ITA_NUM_PMDS ARRAY_SIZE(pfm_ita_pmd_desc)
+
+static int pfm_ita_pmc_check(struct pfm_context *ctx,
+			     struct pfm_event_set *set,
+			     struct pfarg_pmc *req)
+{
+#define PFM_ITA_PMC_PM_POS6	(1UL<< 6)
+	struct pfm_arch_context *ctx_arch;
+	u64 tmpval;
+	u16 cnum;
+	int ret = 0, is_system;
+
+	tmpval = req->reg_value;
+	cnum   = req->reg_num;
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+	switch(cnum) {
+		case  4:
+		case  5:
+		case  6:
+		case  7:
+		case 10:
+		case 11:
+		case 12: if (is_system)
+				 tmpval |= PFM_ITA_PMC_PM_POS6;
+			 else
+				 tmpval &= ~PFM_ITA_PMC_PM_POS6;
+			 break;
+	}
+
+	/*
+	 * we must clear the (instruction) debug registers if pmc13.ta bit is
+	 * cleared before they are written (fl_using_dbreg==0) to avoid
+	 * picking up stale information.
+	 */
+	if (cnum == 13 && ((tmpval & 0x1) == 0)
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc13 has pmc13.ta cleared, clearing ibr");
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret) return ret;
+	}
+
+	/*
+	 * we must clear the (data) debug registers if pmc11.pt bit is cleared
+	 * before they are written (fl_using_dbreg==0) to avoid picking up
+	 * stale information.
+	 */
+	if (cnum == 11 && ((tmpval >> 28)& 0x1) == 0
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc11 has pmc11.pt cleared, clearing dbr");
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret) return ret;
+	}
+
+	req->reg_value = tmpval;
+
+	return 0;
+}
+
+static int pfm_ita_probe_pmu(void)
+{
+	return local_cpu_data->family == 0x7 && !ia64_platform_is("hpsim")
+		? 0 : -1;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_ita_pmu_conf={
+	.pmu_name = "Itanium",
+	.counter_width = 32,
+	.pmd_desc = pfm_ita_pmd_desc,
+	.pmc_desc = pfm_ita_pmc_desc,
+	.pmc_write_check = pfm_ita_pmc_check,
+	.num_pmc_entries = PFM_ITA_NUM_PMCS,
+	.num_pmd_entries = PFM_ITA_NUM_PMDS,
+	.probe_pmu = pfm_ita_probe_pmu,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.arch_info = &pfm_ita_pmu_info
+};
+
+static int __init pfm_ita_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_ita_pmu_conf);
+}
+
+static void __exit pfm_ita_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_ita_pmu_conf);
+}
+
+module_init(pfm_ita_pmu_init_module);
+module_exit(pfm_ita_pmu_cleanup_module);
+
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/perfmon_mckinley.c linux-2.6.25-id/arch/ia64/perfmon/perfmon_mckinley.c
--- linux-2.6.25-org/arch/ia64/perfmon/perfmon_mckinley.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/perfmon_mckinley.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,285 @@
+/*
+ * This file contains the McKinley PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Itanium 2 (McKinley) PMU description tables");
+MODULE_LICENSE("GPL");
+
+#define RDEP(x)	(1UL << (x))
+
+#define PFM_MCK_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|RDEP(10)|RDEP(11)|\
+			   RDEP(12))
+
+#define PFM_MCK_NO64	(1UL<<5)
+
+static struct pfm_arch_pmu_info pfm_mck_pmu_info={
+	.mask_pmcs = {PFM_MCK_MASK_PMCS,},
+};
+
+/* reserved bits are 1 in the mask */
+#define PFM_ITA2_RSVD 0xfffffffffc8000a0UL
+
+/*
+ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
+ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
+ * but this is fine because they are handled separately in the IA-64 specific
+ * code.
+ */
+static struct pfm_regmap_desc pfm_mck_pmc_desc[]={
+/* pmc0  */ PMX_NA,
+/* pmc1  */ PMX_NA,
+/* pmc2  */ PMX_NA,
+/* pmc3  */ PMX_NA,
+/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x800020UL, 0xfffffffffc8000a0, PFM_MCK_NO64, 4),
+/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 5),
+/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 6),
+/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 7),
+/* pmc8  */ PMC_D(PFM_REG_W  , "PMC8" , 0xffffffff3fffffffUL, 0xc0000004UL, 0, 8),
+/* pmc9  */ PMC_D(PFM_REG_W  , "PMC9" , 0xffffffff3ffffffcUL, 0xc0000004UL, 0, 9),
+/* pmc10 */ PMC_D(PFM_REG_W  , "PMC10", 0x0, 0xffffffffffff0000UL, 0, 10),
+/* pmc11 */ PMC_D(PFM_REG_W  , "PMC11", 0x0, 0xfffffffffcf0fe30UL, 0, 11),
+/* pmc12 */ PMC_D(PFM_REG_W  , "PMC12", 0x0, 0xffffffffffff0000UL, 0, 12),
+/* pmc13 */ PMC_D(PFM_REG_W  , "PMC13", 0x2078fefefefeUL, 0xfffe1fffe7e7e7e7UL, 0, 13),
+/* pmc14 */ PMC_D(PFM_REG_W  , "PMC14", 0x0db60db60db60db6UL, 0xffffffffffffdb6dUL, 0, 14),
+/* pmc15 */ PMC_D(PFM_REG_W  , "PMC15", 0xfffffff0UL, 0xfffffffffffffff0UL, 0, 15),
+/* pmc16 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc24 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc32 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc40 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc48 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc56 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc64 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc72 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc80 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc88 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc96 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc104 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc112 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc120 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc128 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc136 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc144 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc152 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc160 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc168 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc176 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc184 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc192 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc200 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc208 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc216 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc224 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc232 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc240 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc248 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc256 */ PMC_D(PFM_REG_W  , "IBR0", 0x0, 0, 0, 0),
+/* pmc257 */ PMC_D(PFM_REG_W  , "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
+/* pmc258 */ PMC_D(PFM_REG_W  , "IBR2", 0x0, 0, 0, 2),
+/* pmc259 */ PMC_D(PFM_REG_W  , "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
+/* pmc260 */ PMC_D(PFM_REG_W  , "IBR4", 0x0, 0, 0, 4),
+/* pmc261 */ PMC_D(PFM_REG_W  , "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
+/* pmc262 */ PMC_D(PFM_REG_W  , "IBR6", 0x0, 0, 0, 6),
+/* pmc263 */ PMC_D(PFM_REG_W  , "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
+/* pmc264 */ PMC_D(PFM_REG_W  , "DBR0", 0x0, 0, 0, 0),
+/* pmc265 */ PMC_D(PFM_REG_W  , "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
+/* pmc266 */ PMC_D(PFM_REG_W  , "DBR2", 0x0, 0, 0, 2),
+/* pmc267 */ PMC_D(PFM_REG_W  , "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
+/* pmc268 */ PMC_D(PFM_REG_W  , "DBR4", 0x0, 0, 0, 4),
+/* pmc269 */ PMC_D(PFM_REG_W  , "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
+/* pmc270 */ PMC_D(PFM_REG_W  , "DBR6", 0x0, 0, 0, 6),
+/* pmc271 */ PMC_D(PFM_REG_W  , "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
+};
+#define PFM_MCK_NUM_PMCS ARRAY_SIZE(pfm_mck_pmc_desc)
+
+static struct pfm_regmap_desc pfm_mck_pmd_desc[]={
+/* pmd0  */ PMD_D(PFM_REG_I, "PMD0", 0),
+/* pmd1  */ PMD_D(PFM_REG_I, "PMD1", 1),
+/* pmd2  */ PMD_D(PFM_REG_I, "PMD2", 2),
+/* pmd3  */ PMD_D(PFM_REG_I, "PMD3", 3),
+/* pmd4  */ PMD_D(PFM_REG_C, "PMD4", 4),
+/* pmd5  */ PMD_D(PFM_REG_C, "PMD5", 5),
+/* pmd6  */ PMD_D(PFM_REG_C, "PMD6", 6),
+/* pmd7  */ PMD_D(PFM_REG_C, "PMD7", 7),
+/* pmd8  */ PMD_D(PFM_REG_I, "PMD8", 8),
+/* pmd9  */ PMD_D(PFM_REG_I, "PMD9", 9),
+/* pmd10 */ PMD_D(PFM_REG_I, "PMD10", 10),
+/* pmd11 */ PMD_D(PFM_REG_I, "PMD11", 11),
+/* pmd12 */ PMD_D(PFM_REG_I, "PMD12", 12),
+/* pmd13 */ PMD_D(PFM_REG_I, "PMD13", 13),
+/* pmd14 */ PMD_D(PFM_REG_I, "PMD14", 14),
+/* pmd15 */ PMD_D(PFM_REG_I, "PMD15", 15),
+/* pmd16 */ PMD_D(PFM_REG_I, "PMD16", 16),
+/* pmd17 */ PMD_D(PFM_REG_I, "PMD17", 17)
+};
+#define PFM_MCK_NUM_PMDS ARRAY_SIZE(pfm_mck_pmd_desc)
+
+static int pfm_mck_pmc_check(struct pfm_context *ctx,
+			     struct pfm_event_set *set,
+			     struct pfarg_pmc *req)
+{
+	struct pfm_arch_context *ctx_arch;
+	u64 val8 = 0, val14 = 0, val13 = 0;
+	u64 tmpval;
+	u16 cnum;
+	int ret = 0, check_case1 = 0;
+	int is_system;
+
+	tmpval = req->reg_value;
+	cnum = req->reg_num;
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+#define PFM_MCK_PMC_PM_POS6	(1UL<< 6)
+#define PFM_MCK_PMC_PM_POS4	(1UL<< 4)
+
+	switch(cnum) {
+		case  4:
+		case  5:
+		case  6:
+		case  7:
+		case 11:
+		case 12: if (is_system)
+				 tmpval |= PFM_MCK_PMC_PM_POS6;
+			 else
+				 tmpval &= ~PFM_MCK_PMC_PM_POS6;
+			 break;
+
+		case  8: val8 = tmpval;
+			 val13 = set->pmcs[13];
+			 val14 = set->pmcs[14];
+			 check_case1 = 1;
+			 break;
+
+		case 10: if (is_system)
+				 tmpval |= PFM_MCK_PMC_PM_POS4;
+			 else
+				 tmpval &= ~PFM_MCK_PMC_PM_POS4;
+			 break;
+
+		case 13:
+			 val8 = set->pmcs[8];
+			 val13 = tmpval;
+			 val14 = set->pmcs[14];
+			 check_case1 = 1;
+			 break;
+
+		case 14:
+			 val8 = set->pmcs[8];
+			 val13 = set->pmcs[13];
+			 val14 = tmpval;
+			 check_case1 = 1;
+			 break;
+	}
+
+	/*
+	 * check illegal configuration which can produce inconsistencies
+	 * in tagging i-side events in L1D and L2 caches
+	 */
+	if (check_case1) {
+		ret = (((val13 >> 45) & 0xf) == 0 && ((val8 & 0x1) == 0))
+		    && ((((val14>>1) & 0x3) == 0x2 || ((val14>>1) & 0x3) == 0x0)
+		    ||(((val14>>4) & 0x3) == 0x2 || ((val14>>4) & 0x3) == 0x0));
+
+		if (ret) {
+			PFM_DBG("perfmon: invalid config pmc8=0x%lx "
+				  "pmc13=0x%lx pmc14=0x%lx",
+				  val8, val13, val14);
+			return -EINVAL;
+		}
+	}
+
+	/*
+	 * check if configuration implicitely activates the use of
+	 * the debug registers. If true, then we ensure that this is
+	 * possible and that we do not pick up stale value in the HW
+	 * registers.
+	 *
+	 * We postpone the checks of pmc13 and pmc14 to avoid side effects
+	 * in case of errors
+	 */
+
+	/*
+	 * pmc13 is "active" if:
+	 * 	one of the pmc13.cfg_dbrpXX field is different from 0x3
+	 * AND
+	 * 	at the corresponding pmc13.ena_dbrpXX is set.
+	 */
+	if (cnum == 13 && (tmpval & 0x1e00000000000UL)
+	    && (tmpval & 0x18181818UL) != 0x18181818UL
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc13=0x%lx active", tmpval);
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret) return ret;
+	}
+
+	/*
+	 *  if any pmc14.ibrpX bit is enabled we must clear the ibrs
+	 */
+	if (cnum == 14 && ((tmpval & 0x2222UL) != 0x2222UL)
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc14=0x%lx active", tmpval);
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret) return ret;
+	}
+
+	req->reg_value = tmpval;
+
+	return 0;
+}
+
+static int pfm_mck_probe_pmu(void)
+{
+	return local_cpu_data->family == 0x1f ? 0 : -1;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_mck_pmu_conf={
+	.pmu_name = "Itanium 2",
+	.counter_width = 47,
+	.pmd_desc = pfm_mck_pmd_desc,
+	.pmc_desc = pfm_mck_pmc_desc,
+	.pmc_write_check = pfm_mck_pmc_check,
+	.num_pmc_entries = PFM_MCK_NUM_PMCS,
+	.num_pmd_entries = PFM_MCK_NUM_PMDS,
+	.probe_pmu = pfm_mck_probe_pmu,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.arch_info = &pfm_mck_pmu_info,
+};
+
+static int __init pfm_mck_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_mck_pmu_conf);
+}
+
+static void __exit pfm_mck_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_mck_pmu_conf);
+}
+
+module_init(pfm_mck_pmu_init_module);
+module_exit(pfm_mck_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/ia64/perfmon/perfmon_montecito.c linux-2.6.25-id/arch/ia64/perfmon/perfmon_montecito.c
--- linux-2.6.25-org/arch/ia64/perfmon/perfmon_montecito.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/ia64/perfmon/perfmon_montecito.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,404 @@
+/*
+ * This file contains the McKinley PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <linux/smp.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Dual-Core Itanium 2 (Montecito) PMU description table");
+MODULE_LICENSE("GPL");
+
+#define RDEP(x)	(1UL << (x))
+
+#define PFM_MONT_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|\
+			    RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|\
+			    RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|\
+			    RDEP(37)|RDEP(39)|RDEP(40)|RDEP(42))
+
+#define PFM_MONT_NO64	(1UL<<5)
+
+static struct pfm_arch_pmu_info pfm_mont_pmu_info={
+	.mask_pmcs = {PFM_MONT_MASK_PMCS,},
+};
+
+#define PFM_MONT_RSVD 0xffffffff838000a0UL
+/*
+ *
+ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
+ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
+ * but this is fine because they are handled separately in the IA-64 specific
+ * code.
+ *
+ * For PMC4-PMC15, PMC40: we force pmc.ism=2 (IA-64 mode only)
+ */
+static struct pfm_regmap_desc pfm_mont_pmc_desc[]={
+/* pmc0  */ PMX_NA,
+/* pmc1  */ PMX_NA,
+/* pmc2  */ PMX_NA,
+/* pmc3  */ PMX_NA,
+/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 4),
+/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 5),
+/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 6),
+/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 7),
+/* pmc8  */ PMC_D(PFM_REG_W64, "PMC8" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 8),
+/* pmc9  */ PMC_D(PFM_REG_W64, "PMC9" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 9),
+/* pmc10 */ PMC_D(PFM_REG_W64, "PMC10", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 10),
+/* pmc11 */ PMC_D(PFM_REG_W64, "PMC11", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 11),
+/* pmc12 */ PMC_D(PFM_REG_W64, "PMC12", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 12),
+/* pmc13 */ PMC_D(PFM_REG_W64, "PMC13", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 13),
+/* pmc14 */ PMC_D(PFM_REG_W64, "PMC14", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 14),
+/* pmc15 */ PMC_D(PFM_REG_W64, "PMC15", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 15),
+/* pmc16 */ PMX_NA,
+/* pmc17 */ PMX_NA,
+/* pmc18 */ PMX_NA,
+/* pmc19 */ PMX_NA,
+/* pmc20 */ PMX_NA,
+/* pmc21 */ PMX_NA,
+/* pmc22 */ PMX_NA,
+/* pmc23 */ PMX_NA,
+/* pmc24 */ PMX_NA,
+/* pmc25 */ PMX_NA,
+/* pmc26 */ PMX_NA,
+/* pmc27 */ PMX_NA,
+/* pmc28 */ PMX_NA,
+/* pmc29 */ PMX_NA,
+/* pmc30 */ PMX_NA,
+/* pmc31 */ PMX_NA,
+/* pmc32 */ PMC_D(PFM_REG_W , "PMC32", 0x30f01ffffffffffUL, 0xfcf0fe0000000000UL, 0, 32),
+/* pmc33 */ PMC_D(PFM_REG_W , "PMC33", 0x0, 0xfffffe0000000000UL, 0, 33),
+/* pmc34 */ PMC_D(PFM_REG_W , "PMC34", 0xf01ffffffffffUL, 0xfff0fe0000000000UL, 0, 34),
+/* pmc35 */ PMC_D(PFM_REG_W , "PMC35", 0x0,  0x1ffffffffffUL, 0, 35),
+/* pmc36 */ PMC_D(PFM_REG_W , "PMC36", 0xfffffff0UL, 0xfffffffffffffff0UL, 0, 36),
+/* pmc37 */ PMC_D(PFM_REG_W , "PMC37", 0x0, 0xffffffffffffc000UL, 0, 37),
+/* pmc38 */ PMC_D(PFM_REG_W , "PMC38", 0xdb6UL, 0xffffffffffffdb6dUL, 0, 38),
+/* pmc39 */ PMC_D(PFM_REG_W , "PMC39", 0x0, 0xffffffffffff0030UL, 0, 39),
+/* pmc40 */ PMC_D(PFM_REG_W , "PMC40", 0x2000000UL, 0xfffffffffff0fe30UL, 0, 40),
+/* pmc41 */ PMC_D(PFM_REG_W , "PMC41", 0x00002078fefefefeUL, 0xfffe1fffe7e7e7e7UL, 0, 41),
+/* pmc42 */ PMC_D(PFM_REG_W , "PMC42", 0x0, 0xfff800b0UL, 0, 42),
+/* pmc43 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc48 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc56 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc64 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc72 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc80 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc88 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc96 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc104 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc112 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc120 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc128 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc136 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc144 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc152 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc160 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc168 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc176 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc184 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc192 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc200 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc208 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc216 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc224 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc232 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc240 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc248 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
+/* pmc256 */ PMC_D(PFM_REG_W, "IBR0", 0x0, 0, 0, 0),
+/* pmc257 */ PMC_D(PFM_REG_W, "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
+/* pmc258 */ PMC_D(PFM_REG_W, "IBR2", 0x0, 0, 0, 2),
+/* pmc259 */ PMC_D(PFM_REG_W, "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
+/* pmc260 */ PMC_D(PFM_REG_W, "IBR4", 0x0, 0, 0, 4),
+/* pmc261 */ PMC_D(PFM_REG_W, "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
+/* pmc262 */ PMC_D(PFM_REG_W, "IBR6", 0x0, 0, 0, 6),
+/* pmc263 */ PMC_D(PFM_REG_W, "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
+/* pmc264 */ PMC_D(PFM_REG_W, "DBR0", 0x0, 0, 0, 0),
+/* pmc265 */ PMC_D(PFM_REG_W, "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
+/* pmc266 */ PMC_D(PFM_REG_W, "DBR2", 0x0, 0, 0, 2),
+/* pmc267 */ PMC_D(PFM_REG_W, "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
+/* pmc268 */ PMC_D(PFM_REG_W, "DBR4", 0x0, 0, 0, 4),
+/* pmc269 */ PMC_D(PFM_REG_W, "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
+/* pmc270 */ PMC_D(PFM_REG_W, "DBR6", 0x0, 0, 0, 6),
+/* pmc271 */ PMC_D(PFM_REG_W, "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
+};
+#define PFM_MONT_NUM_PMCS ARRAY_SIZE(pfm_mont_pmc_desc)
+
+static struct pfm_regmap_desc pfm_mont_pmd_desc[]={
+/* pmd0  */ PMX_NA,
+/* pmd1  */ PMX_NA,
+/* pmd2  */ PMX_NA,
+/* pmd3  */ PMX_NA,
+/* pmd4  */ PMD_D(PFM_REG_C, "PMD4", 4),
+/* pmd5  */ PMD_D(PFM_REG_C, "PMD5", 5),
+/* pmd6  */ PMD_D(PFM_REG_C, "PMD6", 6),
+/* pmd7  */ PMD_D(PFM_REG_C, "PMD7", 7),
+/* pmd8  */ PMD_D(PFM_REG_C, "PMD8", 8),
+/* pmd9  */ PMD_D(PFM_REG_C, "PMD9", 9),
+/* pmd10 */ PMD_D(PFM_REG_C, "PMD10", 10),
+/* pmd11 */ PMD_D(PFM_REG_C, "PMD11", 11),
+/* pmd12 */ PMD_D(PFM_REG_C, "PMD12", 12),
+/* pmd13 */ PMD_D(PFM_REG_C, "PMD13", 13),
+/* pmd14 */ PMD_D(PFM_REG_C, "PMD14", 14),
+/* pmd15 */ PMD_D(PFM_REG_C, "PMD15", 15),
+/* pmd16 */ PMX_NA,
+/* pmd17 */ PMX_NA,
+/* pmd18 */ PMX_NA,
+/* pmd19 */ PMX_NA,
+/* pmd20 */ PMX_NA,
+/* pmd21 */ PMX_NA,
+/* pmd22 */ PMX_NA,
+/* pmd23 */ PMX_NA,
+/* pmd24 */ PMX_NA,
+/* pmd25 */ PMX_NA,
+/* pmd26 */ PMX_NA,
+/* pmd27 */ PMX_NA,
+/* pmd28 */ PMX_NA,
+/* pmd29 */ PMX_NA,
+/* pmd30 */ PMX_NA,
+/* pmd31 */ PMX_NA,
+/* pmd32 */ PMD_D(PFM_REG_I, "PMD32", 32),
+/* pmd33 */ PMD_D(PFM_REG_I, "PMD33", 33),
+/* pmd34 */ PMD_D(PFM_REG_I, "PMD34", 34),
+/* pmd35 */ PMD_D(PFM_REG_I, "PMD35", 35),
+/* pmd36 */ PMD_D(PFM_REG_I, "PMD36", 36),
+/* pmd37 */ PMX_NA,
+/* pmd38 */ PMD_D(PFM_REG_I, "PMD38", 38),
+/* pmd39 */ PMD_D(PFM_REG_I, "PMD39", 39),
+/* pmd40 */ PMX_NA,
+/* pmd41 */ PMX_NA,
+/* pmd42 */ PMX_NA,
+/* pmd43 */ PMX_NA,
+/* pmd44 */ PMX_NA,
+/* pmd45 */ PMX_NA,
+/* pmd46 */ PMX_NA,
+/* pmd47 */ PMX_NA,
+/* pmd48 */ PMD_D(PFM_REG_I, "PMD48", 48),
+/* pmd49 */ PMD_D(PFM_REG_I, "PMD49", 49),
+/* pmd50 */ PMD_D(PFM_REG_I, "PMD50", 50),
+/* pmd51 */ PMD_D(PFM_REG_I, "PMD51", 51),
+/* pmd52 */ PMD_D(PFM_REG_I, "PMD52", 52),
+/* pmd53 */ PMD_D(PFM_REG_I, "PMD53", 53),
+/* pmd54 */ PMD_D(PFM_REG_I, "PMD54", 54),
+/* pmd55 */ PMD_D(PFM_REG_I, "PMD55", 55),
+/* pmd56 */ PMD_D(PFM_REG_I, "PMD56", 56),
+/* pmd57 */ PMD_D(PFM_REG_I, "PMD57", 57),
+/* pmd58 */ PMD_D(PFM_REG_I, "PMD58", 58),
+/* pmd59 */ PMD_D(PFM_REG_I, "PMD59", 59),
+/* pmd60 */ PMD_D(PFM_REG_I, "PMD60", 60),
+/* pmd61 */ PMD_D(PFM_REG_I, "PMD61", 61),
+/* pmd62 */ PMD_D(PFM_REG_I, "PMD62", 62),
+/* pmd63 */ PMD_D(PFM_REG_I, "PMD63", 63)
+};
+#define PFM_MONT_NUM_PMDS ARRAY_SIZE(pfm_mont_pmd_desc)
+
+static int pfm_mont_has_ht;
+
+static int pfm_mont_pmc_check(struct pfm_context *ctx,
+			      struct pfm_event_set *set,
+			      struct pfarg_pmc *req)
+{
+	struct pfm_arch_context *ctx_arch;
+	u64 val32 = 0, val38 = 0, val41 = 0;
+	u64 tmpval;
+	u16 cnum;
+	int ret = 0, check_case1 = 0;
+	int is_system;
+
+	tmpval = req->reg_value;
+	cnum = req->reg_num;
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+#define PFM_MONT_PMC_PM_POS6	(1UL<<6)
+#define PFM_MONT_PMC_PM_POS4	(1UL<<4)
+
+	switch(cnum) {
+		case  4:
+		case  5:
+		case  6:
+		case  7:
+		case  8:
+		case  9: if (is_system)
+				 tmpval |= PFM_MONT_PMC_PM_POS6;
+			 else
+				 tmpval &= ~PFM_MONT_PMC_PM_POS6;
+			 break;
+		case 10:
+		case 11:
+		case 12:
+		case 13:
+		case 14:
+		case 15: if ((req->reg_flags & PFM_REGFL_NO_EMUL64) == 0) {
+				 if (pfm_mont_has_ht) {
+					PFM_INFO("perfmon: Errata 121 PMD10/PMD15 cannot be used to overflow"
+						 "when threads on on");
+					return -EINVAL;
+				 }
+			 }
+			 if (is_system)
+				 tmpval |= PFM_MONT_PMC_PM_POS6;
+			 else
+				 tmpval &= ~PFM_MONT_PMC_PM_POS6;
+			 break;
+		case 39:
+		case 40:
+		case 42: if (pfm_mont_has_ht && ((req->reg_value >> 8) & 0x7) == 4) {
+				 PFM_INFO("perfmon: Errata 120: IP-EAR not available when threads are on");
+				 return -EINVAL;
+			 }
+			 if (is_system)
+				 tmpval |= PFM_MONT_PMC_PM_POS6;
+			 else
+				 tmpval &= ~PFM_MONT_PMC_PM_POS6;
+			 break;
+
+		case  32: val32 = tmpval;
+			  val38 = set->pmcs[38];
+			  val41 = set->pmcs[41];
+			  check_case1 = 1;
+			  break;
+
+		case  37:
+			  if (is_system)
+				 tmpval |= PFM_MONT_PMC_PM_POS4;
+			 else
+				 tmpval &= ~PFM_MONT_PMC_PM_POS4;
+			 break;
+
+		case  38: val38 = tmpval;
+			  val32 = set->pmcs[32];
+			  val41 = set->pmcs[41];
+			  check_case1 = 1;
+			  break;
+		case  41: val41 = tmpval;
+			  val32 = set->pmcs[32];
+			  val38 = set->pmcs[38];
+			  check_case1 = 1;
+			  break;
+	}
+
+	if (check_case1) {
+		ret = (((val41 >> 45) & 0xf) == 0 && ((val32>>57) & 0x1) == 0)
+		     && ((((val38>>1) & 0x3) == 0x2 || ((val38>>1) & 0x3) == 0)
+		     || (((val38>>4) & 0x3) == 0x2 || ((val38>>4) & 0x3) == 0));
+		if (ret) {
+			PFM_DBG("perfmon: invalid config pmc38=0x%lx "
+				"pmc41=0x%lx pmc32=0x%lx",
+				val38, val41, val32);
+			return -EINVAL;
+		}
+	}
+
+	/*
+	 * check if configuration implicitely activates the use of the
+	 * debug registers. If true, then we ensure that this is possible
+	 * and that we do not pick up stale value in the HW registers.
+	 */
+
+	/*
+	 *
+	 * pmc41 is "active" if:
+	 * 	one of the pmc41.cfgdtagXX field is different from 0x3
+	 * AND
+	 * 	the corsesponding pmc41.en_dbrpXX is set.
+	 * AND
+	 *	ctx_fl_use_dbr (dbr not yet used)
+	 */
+	if (cnum == 41
+	    && (tmpval & 0x1e00000000000)
+		&& (tmpval & 0x18181818) != 0x18181818
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc41=0x%lx active, clearing dbr", tmpval);
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret) return ret;
+	}
+	/*
+	 * we must clear the (instruction) debug registers if:
+	 * 	pmc38.ig_ibrpX is 0 (enabled)
+	 * and
+	 * 	fl_use_dbr == 0 (dbr not yet used)
+	 */
+	if (cnum == 38 && ((tmpval & 0x492) != 0x492)
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc38=0x%lx active pmc38, clearing ibr", tmpval);
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret) return ret;
+
+	}
+	req->reg_value = tmpval;
+	return 0;
+}
+
+static void pfm_handle_errata(void)
+{
+	pfm_mont_has_ht = 1;
+
+	PFM_INFO("activating workaround for errata 120 "
+		 "(Disable IP-EAR when threads are on)");
+
+	PFM_INFO("activating workaround for Errata 121 "
+		 "(PMC10-PMC15 cannot be used to overflow"
+		 " when threads are on");
+}
+static int pfm_mont_probe_pmu(void)
+{
+	if (local_cpu_data->family != 0x20)
+		return -1;
+
+	/*
+	 * the 2 errata must be activated when
+	 * threads are/can be enabled
+	 */
+	if (is_multithreading_enabled())
+		pfm_handle_errata();
+
+	return 0;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_mont_pmu_conf={
+	.pmu_name = "Montecito",
+	.counter_width = 47,
+	.pmd_desc = pfm_mont_pmd_desc,
+	.pmc_desc = pfm_mont_pmc_desc,
+	.num_pmc_entries = PFM_MONT_NUM_PMCS,
+	.num_pmd_entries = PFM_MONT_NUM_PMDS,
+	.pmc_write_check = pfm_mont_pmc_check,
+	.probe_pmu = pfm_mont_probe_pmu,
+	.version = "1.0",
+	.arch_info = &pfm_mont_pmu_info,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE
+};
+
+static int __init pfm_mont_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_mont_pmu_conf);
+}
+
+static void __exit pfm_mont_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_mont_pmu_conf);
+}
+
+module_init(pfm_mont_pmu_init_module);
+module_exit(pfm_mont_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/mips/Kconfig linux-2.6.25-id/arch/mips/Kconfig
--- linux-2.6.25-org/arch/mips/Kconfig	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/mips/Kconfig	2008-04-23 11:19:37.000000000 +0200
@@ -1930,6 +1930,8 @@
 
 	  If unsure, say Y. Only embedded should say N here.
 
+source "arch/mips/perfmon/Kconfig"
+
 endmenu
 
 config RWSEM_GENERIC_SPINLOCK
diff -Naur linux-2.6.25-org/arch/mips/Makefile linux-2.6.25-id/arch/mips/Makefile
--- linux-2.6.25-org/arch/mips/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/mips/Makefile	2008-04-23 11:19:37.000000000 +0200
@@ -154,6 +154,12 @@
 endif
 
 #
+# Perfmon support
+#
+
+core-$(CONFIG_PERFMON)		+= arch/mips/perfmon/
+
+#
 # Firmware support
 #
 libs-$(CONFIG_ARC)		+= arch/mips/fw/arc/
diff -Naur linux-2.6.25-org/arch/mips/kernel/process.c linux-2.6.25-id/arch/mips/kernel/process.c
--- linux-2.6.25-org/arch/mips/kernel/process.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/mips/kernel/process.c	2008-04-23 11:19:37.000000000 +0200
@@ -27,6 +27,7 @@
 #include <linux/completion.h>
 #include <linux/kallsyms.h>
 #include <linux/random.h>
+#include <linux/perfmon.h>
 
 #include <asm/asm.h>
 #include <asm/bootinfo.h>
@@ -94,6 +95,7 @@
 
 void exit_thread(void)
 {
+  pfm_exit_thread(current);
 }
 
 void flush_thread(void)
@@ -168,6 +170,8 @@
 	if (clone_flags & CLONE_SETTLS)
 		ti->tp_value = regs->regs[7];
 
+	pfm_copy_thread(p);
+
 	return 0;
 }
 
diff -Naur linux-2.6.25-org/arch/mips/kernel/signal.c linux-2.6.25-id/arch/mips/kernel/signal.c
--- linux-2.6.25-org/arch/mips/kernel/signal.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/mips/kernel/signal.c	2008-04-23 11:19:37.000000000 +0200
@@ -20,6 +20,7 @@
 #include <linux/unistd.h>
 #include <linux/compiler.h>
 #include <linux/uaccess.h>
+#include <linux/perfmon.h>
 
 #include <asm/abi.h>
 #include <asm/asm.h>
@@ -696,6 +697,9 @@
 asmlinkage void do_notify_resume(struct pt_regs *regs, void *unused,
 	__u32 thread_info_flags)
 {
+        if (thread_info_flags & _TIF_PERFMON_WORK)
+		pfm_handle_work(regs);
+
 	/* deal with pending signal delivery */
 	if (thread_info_flags & (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK))
 		do_signal(regs);
diff -Naur linux-2.6.25-org/arch/mips/mips-boards/generic/time.c linux-2.6.25-id/arch/mips/mips-boards/generic/time.c
--- linux-2.6.25-org/arch/mips/mips-boards/generic/time.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/mips/mips-boards/generic/time.c	2008-04-23 11:19:37.000000000 +0200
@@ -27,6 +27,7 @@
 #include <linux/time.h>
 #include <linux/timex.h>
 #include <linux/mc146818rtc.h>
+#include <linux/perfmon.h>
 
 #include <asm/mipsregs.h>
 #include <asm/mipsmtregs.h>
diff -Naur linux-2.6.25-org/arch/mips/perfmon/Kconfig linux-2.6.25-id/arch/mips/perfmon/Kconfig
--- linux-2.6.25-org/arch/mips/perfmon/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/mips/perfmon/Kconfig	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,45 @@
+menu "Hardware Performance Monitoring support"
+config PERFMON
+	bool "Perfmon2 performance monitoring interface"
+	default n
+	help
+	Enables the perfmon2 interface to access the hardware
+	performance counters. See <http://perfmon2.sf.net/> for
+	more details.
+
+config PERFMON_FLUSH
+	bool "Flush sampling buffer when modified"
+	depends on PERFMON
+	default n
+	help
+	On some MIPS models, cache aliasing may cause invalid
+	data to be read from the perfmon sampling buffer. Use this option
+	to flush the buffer when it is modified to ensure valid data is
+	visible at the user level.
+
+config PERFMON_ALIGN
+	bool "Align sampling buffer to avoid cache aliasing"
+	depends on PERFMON
+	default n
+	help
+	On some MIPS models, cache aliasing may cause invalid
+	data to be read from the perfmon sampling buffer. By forcing a bigger
+	page alignment (4-page), one can guarantee the buffer virtual address
+	will conflict in the cache with the user level mapping of the buffer
+	thereby ensuring a consistent view by user programs.
+
+config PERFMON_DEBUG
+	bool "Perfmon debugging"
+	depends on PERFMON
+	default n
+	depends on PERFMON
+	help
+	Enables perfmon debugging support
+
+config PERFMON_MIPS64
+	tristate "Support for MIPS64 hardware performance counters"
+	depends on PERFMON
+	default n
+	help
+	Enables support for the MIPS64 hardware performance counters"
+endmenu
diff -Naur linux-2.6.25-org/arch/mips/perfmon/Makefile linux-2.6.25-id/arch/mips/perfmon/Makefile
--- linux-2.6.25-org/arch/mips/perfmon/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/mips/perfmon/Makefile	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,2 @@
+obj-$(CONFIG_PERFMON)		+= perfmon.o
+obj-$(CONFIG_PERFMON_MIPS64)	+= perfmon_mips64.o
diff -Naur linux-2.6.25-org/arch/mips/perfmon/perfmon.c linux-2.6.25-id/arch/mips/perfmon/perfmon.c
--- linux-2.6.25-org/arch/mips/perfmon/perfmon.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/mips/perfmon/perfmon.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,304 @@
+/*
+ * This file implements the MIPS64 specific
+ * support for the perfmon2 interface
+ *
+ * Copyright (c) 2005 Philip J. Mucci
+ *
+ * based on versions for other architectures:
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@htrpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+/*
+ * collect pending overflowed PMDs. Called from pfm_ctxsw()
+ * and from PMU interrupt handler. Must fill in set->povfl_pmds[]
+ * and set->npend_ovfls. Interrupts are masked
+ */
+static void __pfm_get_ovfl_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 new_val, wmask;
+	u64 *used_mask, *intr_pmds;
+	u64 mask[PFM_PMD_BV];
+	unsigned int i, max;
+
+	max = pfm_pmu_conf->regs.max_intr_pmd;
+	intr_pmds = pfm_pmu_conf->regs.intr_pmds;
+	used_mask = set->used_pmds;
+
+	wmask = 1ULL << pfm_pmu_conf->counter_width;
+
+	bitmap_and(cast_ulp(mask),
+		   cast_ulp(intr_pmds),
+		   cast_ulp(used_mask),
+		   max);
+
+	/*
+	 * check all PMD that can generate interrupts
+	 * (that includes counters)
+	 */
+	for (i = 0; i < max; i++) {
+		if (test_bit(i, mask)) {
+			new_val = pfm_arch_read_pmd(ctx, i);
+
+			PFM_DBG_ovfl("pmd%u new_val=0x%llx bit=%d\n",
+				     i, (unsigned long long)new_val,
+				     (new_val&wmask) ? 1 : 0);
+
+ 			if (new_val & wmask) {
+				__set_bit(i, set->povfl_pmds);
+				set->npend_ovfls++;
+			}
+		}
+	}
+}
+
+static void pfm_stop_active(struct task_struct *task, struct pfm_context *ctx,
+			       struct pfm_event_set *set)
+{
+	unsigned int i, max;
+
+	max = pfm_pmu_conf->regs.max_pmc;
+
+	/*
+	 * clear enable bits, assume all pmcs are enable pmcs
+	 */
+	for (i = 0; i < max; i++) {
+		if (test_bit(i, set->used_pmcs))
+			pfm_arch_write_pmc(ctx, i,0);
+	}
+
+	if (set->npend_ovfls)
+		return;
+
+	__pfm_get_ovfl_pmds(ctx, set);
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * Context is locked. Interrupts are masked. Monitoring is active.
+ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
+ *
+ * for per-thread:
+ * 	must stop monitoring for the task
+ *
+ * Return:
+ * 	non-zero : did not save PMDs (as part of stopping the PMU)
+ * 	       0 : saved PMDs (no need to save them in caller)
+ */
+int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
+			      struct pfm_event_set *set)
+{
+	/*
+	 * disable lazy restore of PMC registers.
+	 */
+	set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
+
+	pfm_stop_active(task, ctx, set);
+
+	return 1;
+}
+
+/*
+ * Called from pfm_stop() and pfm_ctxsw()
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-thread:
+ *   task is not necessarily current. If not current task, then
+ *   task is guaranteed stopped and off any cpu. Access to PMU
+ *   is not guaranteed. Interrupts are masked. Context is locked.
+ *   Set is the active set.
+ *
+ * For system-wide:
+ * 	task is current
+ *
+ * must disable active monitoring. ctx cannot be NULL
+ */
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
+		   struct pfm_event_set *set)
+{
+	/*
+	 * no need to go through stop_save()
+	 * if we are already stopped
+	 */
+	if (!ctx->flags.started || ctx->state == PFM_CTX_MASKED)
+		return;
+
+	/*
+	 * stop live registers and collect pending overflow
+	 */
+	if (task == current)
+		pfm_stop_active(task, ctx, set);
+}
+
+/*
+ * called from pfm_start() or pfm_ctxsw() when idle task and
+ * EXCL_IDLE is on.
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-trhead:
+ * 	Task is not necessarily current. If not current task, then task
+ * 	is guaranteed stopped and off any cpu. Access to PMU is not guaranteed.
+ *
+ * For system-wide:
+ * 	task is always current
+ *
+ * must enable active monitoring.
+ */
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
+	            struct pfm_event_set *set)
+{
+	unsigned int i, max_pmc;
+
+	if (task != current)
+		return;
+
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+
+	for (i = 0; i < max_pmc; i++) {
+		if (test_bit(i, set->used_pmcs))
+		    pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
+	}
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
+ * context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMD registers from set.
+ */
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 ovfl_mask, val;
+	u64 *impl_pmds;
+	unsigned int i;
+	unsigned int max_pmd;
+
+	max_pmd = pfm_pmu_conf->regs.max_pmd;
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	impl_pmds = pfm_pmu_conf->regs.pmds;
+
+	/*
+	 * must restore all pmds to avoid leaking
+	 * information to user.
+	 */
+	for (i = 0; i < max_pmd; i++) {
+
+		if (test_bit(i, impl_pmds) == 0)
+			continue;
+
+		val = set->pmds[i].value;
+
+		/*
+		 * set upper bits for counter to ensure
+		 * overflow will trigger
+		 */
+		val &= ovfl_mask;
+
+		pfm_arch_write_pmd(ctx, i, val);
+	}
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw().
+ * Context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMC registers from set, if needed.
+ */
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 *impl_pmcs;
+	unsigned int i, max_pmc;
+
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+	impl_pmcs = pfm_pmu_conf->regs.pmcs;
+
+	/*
+	 * - by default no PMCS measures anything
+	 * - on ctxswout, all used PMCs are disabled (cccr enable bit cleared)
+	 * hence when masked we do not need to restore anything
+	 */
+	if (ctx->state == PFM_CTX_MASKED || ctx->flags.started == 0)
+		return;
+
+	/*
+	 * restore all pmcs
+	 */
+	for (i = 0; i < max_pmc; i++)
+		if (test_bit(i, impl_pmcs))
+			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
+}
+
+char *pfm_arch_get_pmu_module_name(void)
+{
+	switch(cpu_data->cputype) {
+#ifndef CONFIG_SMP
+	case CPU_34K:
+#if defined(CPU_74K)
+	case CPU_74K:
+#endif
+#endif
+	case CPU_SB1:
+	case CPU_SB1A:
+	case CPU_R12000:
+	case CPU_25KF:
+	case CPU_24K:
+	case CPU_20KC:
+	case CPU_5KC:
+	  return "perfmon_mips64";
+	default:
+	  return NULL;
+	}
+	return NULL;
+}
+
+int perfmon_perf_irq(void)
+{
+  /* BLATANTLY STOLEN FROM OPROFILE, then modified */
+  struct pt_regs *regs;
+  unsigned int counters = pfm_pmu_conf->regs.max_pmc;
+  unsigned int control;
+  unsigned int counter;
+
+  regs = get_irq_regs();
+  switch (counters) {
+#define HANDLE_COUNTER(n)						\
+	case n + 1:							\
+		control = read_c0_perfctrl ## n();			\
+		counter = read_c0_perfcntr ## n();			\
+		if ((control & MIPS64_PMC_INT_ENABLE_MASK) &&		\
+		    (counter & MIPS64_PMD_INTERRUPT)) {			\
+			pfm_interrupt_handler(instruction_pointer(regs),\
+					      regs);                    \
+			return(1);					\
+		}
+      HANDLE_COUNTER(3)
+      HANDLE_COUNTER(2)
+      HANDLE_COUNTER(1)
+      HANDLE_COUNTER(0)
+      }
+
+  return 0;
+}
+EXPORT_SYMBOL(perfmon_perf_irq);
diff -Naur linux-2.6.25-org/arch/mips/perfmon/perfmon_mips64.c linux-2.6.25-id/arch/mips/perfmon/perfmon_mips64.c
--- linux-2.6.25-org/arch/mips/perfmon/perfmon_mips64.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/mips/perfmon/perfmon_mips64.c	2008-04-23 11:19:37.000000000 +0200
@@ -0,0 +1,224 @@
+/*
+ * This file contains the MIPS64 and decendent PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2005 Philip Mucci
+ *
+ * Based on perfmon_p6.c:
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+MODULE_AUTHOR("Philip Mucci <mucci@cs.utk.edu>");
+MODULE_DESCRIPTION("MIPS64 PMU description tables");
+MODULE_LICENSE("GPL");
+
+/*
+ * reserved:
+ * 	- bit 63-9
+ * RSVD: reserved bits must be 1
+ */
+#define PFM_MIPS64_PMC_RSVD 0xfffffffffffff810ULL
+#define PFM_MIPS64_PMC_VAL  (1ULL<<4)
+
+extern int null_perf_irq(struct pt_regs *regs);
+extern int (*perf_irq)(struct pt_regs *regs);
+extern int perfmon_perf_irq(struct pt_regs *regs);
+
+static struct pfm_arch_pmu_info pfm_mips64_pmu_info;
+
+static struct pfm_regmap_desc pfm_mips64_pmc_desc[]={
+/* pmc0 */  PMC_D(PFM_REG_I64, "CP0_25_0", PFM_MIPS64_PMC_VAL, PFM_MIPS64_PMC_RSVD, 0, 0),
+/* pmc1 */  PMC_D(PFM_REG_I64, "CP0_25_1", PFM_MIPS64_PMC_VAL, PFM_MIPS64_PMC_RSVD, 0, 1),
+/* pmc2 */  PMC_D(PFM_REG_I64, "CP0_25_2", PFM_MIPS64_PMC_VAL, PFM_MIPS64_PMC_RSVD, 0, 2),
+/* pmc3 */  PMC_D(PFM_REG_I64, "CP0_25_3", PFM_MIPS64_PMC_VAL, PFM_MIPS64_PMC_RSVD, 0, 3)
+};
+#define PFM_MIPS64_NUM_PMCS ARRAY_SIZE(pfm_mips64_pmc_desc)
+
+static struct pfm_regmap_desc pfm_mips64_pmd_desc[]={
+/* pmd0 */ PMD_D(PFM_REG_C, "CP0_25_0", 0),
+/* pmd1 */ PMD_D(PFM_REG_C, "CP0_25_1", 1),
+/* pmd2 */ PMD_D(PFM_REG_C, "CP0_25_2", 2),
+/* pmd3 */ PMD_D(PFM_REG_C, "CP0_25_3", 3)
+};
+#define PFM_MIPS64_NUM_PMDS ARRAY_SIZE(pfm_mips64_pmd_desc)
+
+static int pfm_mips64_probe_pmu(void)
+{
+	struct cpuinfo_mips *c = &current_cpu_data;
+
+	switch (c->cputype) {
+#ifndef CONFIG_SMP
+	case CPU_34K:
+#if defined(CPU_74K)
+	case CPU_74K:
+#endif
+#endif
+	case CPU_SB1:
+	case CPU_SB1A:
+	case CPU_R12000:
+	case CPU_25KF:
+	case CPU_24K:
+	case CPU_20KC:
+	case CPU_5KC:
+		return 0;
+		break;
+	default:
+		PFM_INFO("Unknown cputype 0x%x",c->cputype);
+	}
+	return -1;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_mips64_pmu_conf = {
+	.pmu_name = "MIPS", /* placeholder */
+	.counter_width = 31,
+	.pmd_desc = pfm_mips64_pmd_desc,
+	.pmc_desc = pfm_mips64_pmc_desc,
+	.num_pmc_entries = PFM_MIPS64_NUM_PMCS,
+	.num_pmd_entries = PFM_MIPS64_NUM_PMDS,
+	.probe_pmu = pfm_mips64_probe_pmu,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.arch_info = &pfm_mips64_pmu_info
+};
+
+static inline int n_counters(void)
+{
+	if (!(read_c0_config1() & MIPS64_CONFIG_PMC_MASK))
+		return 0;
+	if (!(read_c0_perfctrl0() & MIPS64_PMC_CTR_MASK))
+		return 1;
+	if (!(read_c0_perfctrl1() & MIPS64_PMC_CTR_MASK))
+		return 2;
+	if (!(read_c0_perfctrl2() & MIPS64_PMC_CTR_MASK))
+		return 3;
+	return 4;
+}
+
+static int __init pfm_mips64_pmu_init_module(void)
+{
+	struct cpuinfo_mips *c = &current_cpu_data;
+	int i, ret, num;
+        u64 temp_mask;
+
+	switch (c->cputype) {
+	case CPU_5KC:
+		pfm_mips64_pmu_conf.pmu_name = "MIPS5KC";
+		break;
+	case CPU_R12000:
+	        pfm_mips64_pmu_conf.pmu_name = "MIPSR12000";
+	        break;
+	case CPU_20KC:
+		pfm_mips64_pmu_conf.pmu_name = "MIPS20KC";
+		break;
+	case CPU_24K:
+		pfm_mips64_pmu_conf.pmu_name = "MIPS24K";
+		break;
+	case CPU_25KF:
+		pfm_mips64_pmu_conf.pmu_name = "MIPS25KF";
+		break;
+	case CPU_SB1:
+		pfm_mips64_pmu_conf.pmu_name = "SB1";
+		break;
+	case CPU_SB1A:
+	pfm_mips64_pmu_conf.pmu_name = "SB1A";
+		break;
+#ifndef CONFIG_SMP
+	case CPU_34K:
+		pfm_mips64_pmu_conf.pmu_name = "MIPS34K";
+		break;
+#if defined(CPU_74K)
+	case CPU_74K:
+		pfm_mips64_pmu_conf.pmu_name = "MIPS74K";
+		break;
+#endif
+#endif
+	default:
+		PFM_INFO("Unknown cputype 0x%x",c->cputype);
+		return -1;
+	}
+
+           /* The R14k and older performance counters have to          */
+	   /* be hard-coded, as there is no support for auto-detection */
+	if ((c->cputype==CPU_R12000) || (c->cputype==CPU_R14000)) {
+	   num=4;
+	}
+        else if (c->cputype==CPU_R10000) {
+	   num=2;
+	}
+        else {
+	   num = n_counters();
+	}
+
+	if (num == 0) {
+		PFM_INFO("cputype 0x%x has no counters",c->cputype);
+		return -1;
+	}
+	/* mark remaining counters unavailable */
+	for(i=num; i < PFM_MIPS64_NUM_PMCS; i++) {
+		pfm_mips64_pmc_desc[i].type = PFM_REG_NA;
+	}
+
+	for(i=num; i < PFM_MIPS64_NUM_PMDS; i++) {
+		pfm_mips64_pmd_desc[i].type = PFM_REG_NA;
+	}
+
+	/* set the PMC_RSVD mask */
+	switch (c->cputype) {
+	case CPU_5KC:
+	case CPU_R10000:
+	case CPU_20KC:
+	   /* 4-bits for event */
+	   temp_mask=0xfffffffffffffe10ULL;
+	   break;
+	case CPU_R12000:
+	case CPU_R14000:
+	   /* 5-bits for event */
+	   temp_mask=0xfffffffffffffc10ULL;
+	   break;
+	default:
+	   /* 6-bits for event */
+	   temp_mask=0xfffffffffffff810ULL;
+	}
+        for(i=0; i< PFM_MIPS64_NUM_PMCS;i++) {
+           pfm_mips64_pmc_desc[i].rsvd_msk=temp_mask;
+	}
+
+	pfm_mips64_pmu_conf.num_pmc_entries = num;
+	pfm_mips64_pmu_conf.num_pmd_entries = num;
+
+	pfm_mips64_pmu_info.pmu_style = c->cputype;
+
+	ret = pfm_pmu_register(&pfm_mips64_pmu_conf);
+	if (ret == 0)
+		perf_irq = perfmon_perf_irq;
+	return ret;
+}
+
+static void __exit pfm_mips64_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_mips64_pmu_conf);
+	perf_irq = null_perf_irq;
+}
+
+module_init(pfm_mips64_pmu_init_module);
+module_exit(pfm_mips64_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/powerpc/Kconfig linux-2.6.25-id/arch/powerpc/Kconfig
--- linux-2.6.25-org/arch/powerpc/Kconfig	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/Kconfig	2008-04-23 11:19:37.000000000 +0200
@@ -199,6 +199,8 @@
 source "arch/powerpc/sysdev/Kconfig"
 source "arch/powerpc/platforms/Kconfig"
 
+source "arch/powerpc/perfmon/Kconfig"
+
 menu "Kernel options"
 
 config HIGHMEM
diff -Naur linux-2.6.25-org/arch/powerpc/Makefile linux-2.6.25-id/arch/powerpc/Makefile
--- linux-2.6.25-org/arch/powerpc/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/Makefile	2008-04-23 11:19:37.000000000 +0200
@@ -147,6 +147,7 @@
 				   arch/powerpc/platforms/
 core-$(CONFIG_MATH_EMULATION)	+= arch/powerpc/math-emu/
 core-$(CONFIG_XMON)		+= arch/powerpc/xmon/
+core-$(CONFIG_PERFMON)		+= arch/powerpc/perfmon/
 
 drivers-$(CONFIG_OPROFILE)	+= arch/powerpc/oprofile/
 
diff -Naur linux-2.6.25-org/arch/powerpc/boot/Makefile linux-2.6.25-id/arch/powerpc/boot/Makefile
--- linux-2.6.25-org/arch/powerpc/boot/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/boot/Makefile	2008-04-23 11:19:37.000000000 +0200
@@ -178,10 +178,15 @@
 endif
 endif
 
+ifneq ($(KBUILD_VERBOSE),0)
+verbose_shell := -x
+endif
+
 # args (to if_changed): 1 = (this rule), 2 = platform, 3 = dts 4=dtb 5=initrd
 quiet_cmd_wrap	= WRAP    $@
-      cmd_wrap	=$(CONFIG_SHELL) $(wrapper) -c -o $@ -p $2 $(CROSSWRAP) \
-		$(if $3, -s $3)$(if $4, -d $4)$(if $5, -i $5) vmlinux
+      cmd_wrap	=$(CONFIG_SHELL) $(verbose_shell) $(wrapper) -c -o $@ -p $2 \
+		$(CROSSWRAP) $(if $3, -s $3)$(if $4, -d $4)$(if $5, -i $5) \
+		vmlinux
 
 image-$(CONFIG_PPC_PSERIES)		+= zImage.pseries
 image-$(CONFIG_PPC_MAPLE)		+= zImage.pseries
diff -Naur linux-2.6.25-org/arch/powerpc/configs/ps3_defconfig linux-2.6.25-id/arch/powerpc/configs/ps3_defconfig
--- linux-2.6.25-org/arch/powerpc/configs/ps3_defconfig	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/configs/ps3_defconfig	2008-04-23 11:19:39.000000000 +0200
@@ -203,6 +203,16 @@
 # CONFIG_FSL_ULI1575 is not set
 
 #
+# Hardware Performance Monitoring support
+#
+CONFIG_PERFMON=y
+# CONFIG_PERFMON_DEBUG is not set
+# CONFIG_PERFMON_POWER4 is not set
+# CONFIG_PERFMON_POWER5 is not set
+# CONFIG_PERFMON_POWER6 is not set
+CONFIG_PERFMON_CELL=m
+
+#
 # Kernel options
 #
 # CONFIG_TICK_ONESHOT is not set
diff -Naur linux-2.6.25-org/arch/powerpc/kernel/align.c linux-2.6.25-id/arch/powerpc/kernel/align.c
--- linux-2.6.25-org/arch/powerpc/kernel/align.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/kernel/align.c	2008-04-23 11:19:39.000000000 +0200
@@ -24,6 +24,7 @@
 #include <asm/system.h>
 #include <asm/cache.h>
 #include <asm/cputable.h>
+#include <asm/emulated_ops.h>
 
 struct aligninfo {
 	unsigned char len;
@@ -696,8 +697,10 @@
 	areg = dsisr & 0x1f;		/* register to update */
 
 #ifdef CONFIG_SPE
-	if ((instr >> 26) == 0x4)
+	if ((instr >> 26) == 0x4) {
+		WARN_EMULATE(spe);
 		return emulate_spe(regs, reg, instr);
+	}
 #endif
 
 	instr = (dsisr >> 10) & 0x7f;
@@ -731,17 +734,21 @@
 	/* A size of 0 indicates an instruction we don't support, with
 	 * the exception of DCBZ which is handled as a special case here
 	 */
-	if (instr == DCBZ)
+	if (instr == DCBZ) {
+		WARN_EMULATE(dcbz);
 		return emulate_dcbz(regs, addr);
+	}
 	if (unlikely(nb == 0))
 		return 0;
 
 	/* Load/Store Multiple instructions are handled in their own
 	 * function
 	 */
-	if (flags & M)
+	if (flags & M) {
+		WARN_EMULATE(multiple);
 		return emulate_multiple(regs, addr, reg, nb,
 					flags, instr, swiz);
+	}
 
 	/* Verify the address of the operand */
 	if (unlikely(user_mode(regs) &&
@@ -758,8 +765,10 @@
 	}
 
 	/* Special case for 16-byte FP loads and stores */
-	if (nb == 16)
+	if (nb == 16) {
+		WARN_EMULATE(fp_pair);
 		return emulate_fp_pair(regs, addr, reg, flags);
+	}
 
 	/* If we are loading, get the data from user space, else
 	 * get it from register values
diff -Naur linux-2.6.25-org/arch/powerpc/kernel/entry_32.S linux-2.6.25-id/arch/powerpc/kernel/entry_32.S
--- linux-2.6.25-org/arch/powerpc/kernel/entry_32.S	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/kernel/entry_32.S	2008-04-23 11:19:39.000000000 +0200
@@ -38,7 +38,7 @@
  * MSR_KERNEL is > 0x10000 on 4xx/Book-E since it include MSR_CE.
  */
 #if MSR_KERNEL >= 0x10000
-#define LOAD_MSR_KERNEL(r, x)	lis r,(x)@h; ori r,r,(x)@l
+#define LOAD_MSR_KERNEL(r, x)	lis r,(x)@ha; ori r,r,(x)@l
 #else
 #define LOAD_MSR_KERNEL(r, x)	li r,(x)
 #endif
diff -Naur linux-2.6.25-org/arch/powerpc/kernel/entry_64.S linux-2.6.25-id/arch/powerpc/kernel/entry_64.S
--- linux-2.6.25-org/arch/powerpc/kernel/entry_64.S	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/kernel/entry_64.S	2008-04-23 11:19:39.000000000 +0200
@@ -608,6 +608,10 @@
 	b	.ret_from_except_lite
 
 1:	bl	.save_nvgprs
+#ifdef CONFIG_PERFMON
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	bl	.pfm_handle_work
+#endif /* CONFIG_PERFMON */
 	li	r3,0
 	addi	r4,r1,STACK_FRAME_OVERHEAD
 	bl	.do_signal
diff -Naur linux-2.6.25-org/arch/powerpc/kernel/process.c linux-2.6.25-id/arch/powerpc/kernel/process.c
--- linux-2.6.25-org/arch/powerpc/kernel/process.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/kernel/process.c	2008-04-23 11:19:39.000000000 +0200
@@ -33,6 +33,7 @@
 #include <linux/mqueue.h>
 #include <linux/hardirq.h>
 #include <linux/utsname.h>
+#include <linux/perfmon.h>
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
@@ -346,6 +347,9 @@
 		new_thread->start_tb = current_tb;
 	}
 #endif
+	if (test_tsk_thread_flag(new, TIF_PERFMON_CTXSW)
+	    || test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW))
+		pfm_ctxsw(prev, new);
 
 	local_irq_save(flags);
 
@@ -497,6 +501,7 @@
 void exit_thread(void)
 {
 	discard_lazy_cpu_state();
+	pfm_exit_thread(current);
 }
 
 void flush_thread(void)
@@ -615,6 +620,7 @@
 #else
 	kregs->nip = (unsigned long)ret_from_fork;
 #endif
+	pfm_copy_thread(p);
 
 	return 0;
 }
diff -Naur linux-2.6.25-org/arch/powerpc/kernel/sysfs.c linux-2.6.25-id/arch/powerpc/kernel/sysfs.c
--- linux-2.6.25-org/arch/powerpc/kernel/sysfs.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/kernel/sysfs.c	2008-04-23 11:19:39.000000000 +0200
@@ -8,6 +8,7 @@
 #include <linux/nodemask.h>
 #include <linux/cpumask.h>
 #include <linux/notifier.h>
+#include <linux/sysctl.h>
 
 #include <asm/current.h>
 #include <asm/processor.h>
@@ -19,6 +20,7 @@
 #include <asm/lppaca.h>
 #include <asm/machdep.h>
 #include <asm/smp.h>
+#include <asm/emulated_ops.h>
 
 static DEFINE_PER_CPU(struct cpu, cpu_devices);
 
@@ -291,12 +293,100 @@
 };
 
 
+#define SYSFS_EMULATED_SETUP(type)					\
+DEFINE_PER_CPU(atomic_long_t, emulated_ ## type);			\
+static ssize_t show_emulated_ ## type (struct sys_device *dev,		\
+				       char *buf)			\
+{									\
+	struct cpu *cpu = container_of(dev, struct cpu, sysdev);	\
+									\
+	return sprintf(buf, "%lu\n",					\
+		       atomic_long_read(&per_cpu(emulated_ ## type,	\
+					cpu->sysdev.id)));		\
+}									\
+									\
+static struct sysdev_attribute emulated_ ## type ## _attr = {		\
+	.attr = { .name = #type, .mode = 0400 },			\
+	.show = show_emulated_ ## type,					\
+};
+
+SYSFS_EMULATED_SETUP(dcba);
+SYSFS_EMULATED_SETUP(dcbz);
+SYSFS_EMULATED_SETUP(fp_pair);
+SYSFS_EMULATED_SETUP(mcrxr);
+SYSFS_EMULATED_SETUP(mfpvr);
+SYSFS_EMULATED_SETUP(multiple);
+SYSFS_EMULATED_SETUP(popcntb);
+SYSFS_EMULATED_SETUP(spe);
+SYSFS_EMULATED_SETUP(string);
+#ifdef CONFIG_MATH_EMULATION
+SYSFS_EMULATED_SETUP(math);
+#elif defined(CONFIG_8XX_MINIMAL_FPEMU)
+SYSFS_EMULATED_SETUP(8xx);
+#endif
+
+static struct attribute *emulated_attrs[] = {
+	&emulated_dcba_attr.attr,
+	&emulated_dcbz_attr.attr,
+	&emulated_fp_pair_attr.attr,
+	&emulated_mcrxr_attr.attr,
+	&emulated_mfpvr_attr.attr,
+	&emulated_multiple_attr.attr,
+	&emulated_popcntb_attr.attr,
+	&emulated_spe_attr.attr,
+	&emulated_string_attr.attr,
+#ifdef CONFIG_MATH_EMULATION
+	&emulated_math_attr.attr,
+#elif defined(CONFIG_8XX_MINIMAL_FPEMU)
+	&emulated_8xx_attr.attr,
+#endif
+	NULL
+};
+
+static struct attribute_group emulated_attr_group = {
+	.attrs = emulated_attrs,
+	.name = "emulated"
+};
+
+int sysctl_warn_emulated;
+
+#ifdef CONFIG_SYSCTL
+static ctl_table warn_emulated_ctl_table[]={
+	{
+		.procname	= "cpu_emulation_warnings",
+		.data		= &sysctl_warn_emulated,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{}
+};
+
+static ctl_table warn_emulated_sysctl_root[] = {
+	{
+		.ctl_name	= CTL_KERN,
+		.procname	= "kernel",
+		.mode		= 0555,
+		.child		= warn_emulated_ctl_table,
+	},
+	{}
+};
+
+static inline void warn_emulated_sysctl_register(void)
+{
+	register_sysctl_table(warn_emulated_sysctl_root);
+}
+#else /* !CONFIG_SYSCTL */
+static inline void warn_emulated_sysctl_register(void) {}
+#endif /* !CONFIG_SYSCTL */
+
+
 static void register_cpu_online(unsigned int cpu)
 {
 	struct cpu *c = &per_cpu(cpu_devices, cpu);
 	struct sys_device *s = &c->sysdev;
 	struct sysdev_attribute *attrs, *pmc_attrs;
-	int i, nattrs;
+	int i, nattrs, res;
 
 	if (!firmware_has_feature(FW_FEATURE_ISERIES) &&
 			cpu_has_feature(CPU_FTR_SMT))
@@ -339,6 +429,11 @@
 
 	if (cpu_has_feature(CPU_FTR_DSCR))
 		sysdev_create_file(s, &attr_dscr);
+
+	res = sysfs_create_group(&s->kobj, &emulated_attr_group);
+	if (res)
+		pr_warning("Cannot create emulated sysfs group for cpu %u\n",
+			   cpu);
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -560,6 +655,8 @@
 			register_cpu_online(cpu);
 	}
 
+	warn_emulated_sysctl_register();
+
 	return 0;
 }
 subsys_initcall(topology_init);
diff -Naur linux-2.6.25-org/arch/powerpc/kernel/traps.c linux-2.6.25-id/arch/powerpc/kernel/traps.c
--- linux-2.6.25-org/arch/powerpc/kernel/traps.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/kernel/traps.c	2008-04-23 11:19:39.000000000 +0200
@@ -53,6 +53,7 @@
 #include <asm/processor.h>
 #endif
 #include <asm/kexec.h>
+#include <asm/emulated_ops.h>
 
 #if defined(CONFIG_DEBUGGER) || defined(CONFIG_KEXEC)
 int (*__debugger)(struct pt_regs *regs);
@@ -763,6 +764,13 @@
 	return 0;
 }
 
+void do_warn_emulate(const char *type)
+{
+	if (printk_ratelimit())
+		pr_warning("%s used emulated %s instruction\n", current->comm,
+			   type);
+}
+
 static int emulate_instruction(struct pt_regs *regs)
 {
 	u32 instword;
@@ -777,31 +785,38 @@
 
 	/* Emulate the mfspr rD, PVR. */
 	if ((instword & INST_MFSPR_PVR_MASK) == INST_MFSPR_PVR) {
+		WARN_EMULATE(mfpvr);
 		rd = (instword >> 21) & 0x1f;
 		regs->gpr[rd] = mfspr(SPRN_PVR);
 		return 0;
 	}
 
 	/* Emulating the dcba insn is just a no-op.  */
-	if ((instword & INST_DCBA_MASK) == INST_DCBA)
+	if ((instword & INST_DCBA_MASK) == INST_DCBA) {
+		WARN_EMULATE(dcba);
 		return 0;
+	}
 
 	/* Emulate the mcrxr insn.  */
 	if ((instword & INST_MCRXR_MASK) == INST_MCRXR) {
 		int shift = (instword >> 21) & 0x1c;
 		unsigned long msk = 0xf0000000UL >> shift;
 
+		WARN_EMULATE(mcrxr);
 		regs->ccr = (regs->ccr & ~msk) | ((regs->xer >> shift) & msk);
 		regs->xer &= ~0xf0000000UL;
 		return 0;
 	}
 
 	/* Emulate load/store string insn. */
-	if ((instword & INST_STRING_GEN_MASK) == INST_STRING)
+	if ((instword & INST_STRING_GEN_MASK) == INST_STRING) {
+		WARN_EMULATE(string);
 		return emulate_string_inst(regs, instword);
+	}
 
 	/* Emulate the popcntb (Population Count Bytes) instruction. */
 	if ((instword & INST_POPCNTB_MASK) == INST_POPCNTB) {
+		WARN_EMULATE(popcntb);
 		return emulate_popcntb_inst(regs, instword);
 	}
 
@@ -990,6 +1005,8 @@
 
 #ifdef CONFIG_MATH_EMULATION
 	errcode = do_mathemu(regs);
+	if (errcode >= 0)
+		WARN_EMULATE(math);
 
 	switch (errcode) {
 	case 0:
@@ -1011,6 +1028,9 @@
 
 #elif defined(CONFIG_8XX_MINIMAL_FPEMU)
 	errcode = Soft_emulate_8xx(regs);
+	if (errcode >= 0)
+		WARN_EMULATE(8xx);
+
 	switch (errcode) {
 	case 0:
 		emulate_single_step(regs);
diff -Naur linux-2.6.25-org/arch/powerpc/mm/slb.c linux-2.6.25-id/arch/powerpc/mm/slb.c
--- linux-2.6.25-org/arch/powerpc/mm/slb.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/mm/slb.c	2008-04-23 11:19:39.000000000 +0200
@@ -30,7 +30,7 @@
 #ifdef DEBUG
 #define DBG(fmt...) udbg_printf(fmt)
 #else
-#define DBG(fmt...)
+#define DBG(fmt...) pr_debug(fmt)
 #endif
 
 extern void slb_allocate_realmode(unsigned long ea);
@@ -279,8 +279,8 @@
 		patch_slb_encoding(slb_compare_rr_to_size,
 				   mmu_slb_size);
 
-		DBG("SLB: linear  LLP = %04x\n", linear_llp);
-		DBG("SLB: io      LLP = %04x\n", io_llp);
+		DBG("SLB: linear  LLP = %04lx\n", linear_llp);
+		DBG("SLB: io      LLP = %04lx\n", io_llp);
 	}
 
 	get_paca()->stab_rr = SLB_NUM_BOLTED;
diff -Naur linux-2.6.25-org/arch/powerpc/oprofile/Makefile linux-2.6.25-id/arch/powerpc/oprofile/Makefile
--- linux-2.6.25-org/arch/powerpc/oprofile/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/oprofile/Makefile	2008-04-23 11:19:39.000000000 +0200
@@ -14,6 +14,7 @@
 oprofile-$(CONFIG_OPROFILE_CELL) += op_model_cell.o \
 		cell/spu_profiler.o cell/vma_map.o \
 		cell/spu_task_sync.o
+oprofile-$(CONFIG_OPROFILE_PS3) += op_model_ps3.o
 oprofile-$(CONFIG_PPC64) += op_model_rs64.o op_model_power4.o op_model_pa6t.o
 oprofile-$(CONFIG_FSL_EMB_PERFMON) += op_model_fsl_emb.o
 oprofile-$(CONFIG_6xx) += op_model_7450.o
diff -Naur linux-2.6.25-org/arch/powerpc/oprofile/common.c linux-2.6.25-id/arch/powerpc/oprofile/common.c
--- linux-2.6.25-org/arch/powerpc/oprofile/common.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/oprofile/common.c	2008-04-23 11:19:39.000000000 +0200
@@ -178,10 +178,23 @@
 
 	switch (cur_cpu_spec->oprofile_type) {
 #ifdef CONFIG_PPC64
-#ifdef CONFIG_OPROFILE_CELL
+#if defined(CONFIG_OPROFILE_CELL) || defined(CONFIG_OPROFILE_PS3)
 		case PPC_OPROFILE_CELL:
-			if (firmware_has_feature(FW_FEATURE_LPAR))
+			printk("%s:%d: \n", __func__, __LINE__);
+			if (firmware_has_feature(FW_FEATURE_PS3_LV1)) {
+				printk("%s:%d: \n", __func__, __LINE__);
+				model = &op_model_ps3;
+				ops->sync_start = model->sync_start;
+				ops->sync_stop = model->sync_stop;
+				break;
+			}
+
+			if (firmware_has_feature(FW_FEATURE_LPAR)) {
+				printk("%s:%d: \n", __func__, __LINE__);
 				return -ENODEV;
+			}
+
+			printk("%s:%d: \n", __func__, __LINE__);
 			model = &op_model_cell;
 			ops->sync_start = model->sync_start;
 			ops->sync_stop = model->sync_stop;
@@ -208,8 +221,10 @@
 			break;
 #endif
 		default:
+			printk("%s:%d: \n", __func__, __LINE__);
 			return -ENODEV;
 	}
+	printk("%s:%d: \n", __func__, __LINE__);
 
 	model->num_counters = cur_cpu_spec->num_pmcs;
 
diff -Naur linux-2.6.25-org/arch/powerpc/oprofile/op_model_ps3.c linux-2.6.25-id/arch/powerpc/oprofile/op_model_ps3.c
--- linux-2.6.25-org/arch/powerpc/oprofile/op_model_ps3.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/oprofile/op_model_ps3.c	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,767 @@
+/*
+ * PS3 OProfile support
+ *
+ * This file based on op_model_cell.c, but the spu profiling has not
+ * been implemented yet.
+ *
+ * Copyright (C) 2007 Sony Computer Entertainment Inc.
+ * Copyright 2007 Sony Corporation.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+
+#include <linux/cpufreq.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/jiffies.h>
+#include <linux/kthread.h>
+#include <linux/oprofile.h>
+#include <linux/percpu.h>
+#include <linux/smp.h>
+#include <linux/spinlock.h>
+#include <linux/timer.h>
+#include <linux/firmware.h>
+#include <linux/io.h>
+#include <linux/ptrace.h>
+#include <asm/cell-pmu.h>
+#include <asm/cputable.h>
+#include <asm/oprofile_impl.h>
+#include <asm/processor.h>
+#include <asm/prom.h>
+#include <asm/reg.h>
+#include <asm/system.h>
+#include <asm/cell-regs.h>
+#include <asm/pmc.h>
+#include <asm/irq_regs.h>
+#include <asm/ps3.h>
+
+#include "../platforms/cell/interrupt.h"
+#include "cell/pr_util.h"
+
+#define OP_ERR(f, x...)  pr_info("pmu: " f "\n", ## x)
+#define OP_DBG(f, x...)  pr_debug("pmu: " f "\n", ## x)
+
+/*
+ * spu_cycle_reset is the number of cycles between samples.
+ * This variable is used for SPU profiling and should ONLY be set
+ * at the beginning of cell_reg_setup; otherwise, it's read-only.
+ */
+static unsigned int spu_cycle_reset;
+
+#define NUM_SPUS_PER_NODE    8
+#define SPU_CYCLES_EVENT_NUM 2	/*  event number for SPU_CYCLES */
+
+#define PPU_CYCLES_EVENT_NUM 1	/*  event number for CYCLES */
+#define PPU_CYCLES_GRP_NUM   1	/* special group number for identifying
+				 * PPU_CYCLES event
+				 */
+#define CBE_COUNT_ALL_CYCLES 0x42800000 /* PPU cycle event specifier */
+
+#define NUM_THREADS 2         /* number of physical threads in
+			       * physical processor
+			       */
+#define NUM_DEBUG_BUS_WORDS 4
+#define NUM_INPUT_BUS_WORDS 2
+
+#define MAX_SPU_COUNT 0xFFFFFF	/* maximum 24 bit LFSR value */
+
+struct pmc_cntrl_data {
+	unsigned long vcntr;
+	unsigned long evnts;
+	unsigned long masks;
+	unsigned long enabled;
+};
+
+struct pm_signal {
+	u16 cpu;		/* Processor to modify */
+	u16 sub_unit;		/* hw subunit this applies to (if applicable)*/
+	short int signal_group; /* Signal Group to Enable/Disable */
+	u8 bus_word;		/* Enable/Disable on this Trace/Trigger/Event
+				 * Bus Word(s) (bitmask)
+				 */
+	u8 bit;			/* Trigger/Event bit (if applicable) */
+};
+
+struct pm_cntrl {
+	u16 enable;
+	u16 stop_at_max;
+	u16 trace_mode;
+	u16 freeze;
+	u16 count_mode;
+};
+
+static struct {
+	u32 group_control;
+	u32 debug_bus_control;
+	struct pm_cntrl pm_cntrl;
+	u32 pm07_cntrl[NR_PHYS_CTRS];
+} pm_regs;
+
+#define GET_SUB_UNIT(x) ((x & 0x0000f000) >> 12)
+#define GET_BUS_WORD(x) ((x & 0x000000f0) >> 4)
+#define GET_BUS_TYPE(x) ((x & 0x00000300) >> 8)
+#define GET_POLARITY(x) ((x & 0x00000002) >> 1)
+#define GET_COUNT_CYCLES(x) (x & 0x00000001)
+#define GET_INPUT_CONTROL(x) ((x & 0x00000004) >> 2)
+
+static DEFINE_PER_CPU(unsigned long[NR_PHYS_CTRS], pmc_values);
+
+static struct pmc_cntrl_data pmc_cntrl[NUM_THREADS][NR_PHYS_CTRS];
+
+/*
+ * Interpetation of hdw_thread:
+ * 0 - even virtual cpus 0, 2, 4,...
+ * 1 - odd virtual cpus 1, 3, 5, ...
+ *
+ * FIXME: this is strictly wrong, we need to clean this up in a number
+ * of places. It works for now. -arnd
+ */
+static u32 hdw_thread;
+
+static u32 virt_cntr_inter_mask;
+static struct timer_list timer_virt_cntr;
+
+/*
+ * pm_signal needs to be global since it is initialized in
+ * cell_reg_setup at the time when the necessary information
+ * is available.
+ */
+static struct pm_signal pm_signal[NR_PHYS_CTRS];
+
+static u32 reset_value[NR_PHYS_CTRS];
+static u32 count_value[NR_PHYS_CTRS];
+static int num_counters;
+static int oprofile_running;
+static DEFINE_SPINLOCK(virt_cntr_lock);
+
+static u32 ctr_enabled;
+
+static unsigned char input_bus[NUM_INPUT_BUS_WORDS];
+
+static u32 ps3_cpu_to_node(int cpu)
+{
+	return 0;
+}
+
+static int pm_activate_signals(u32 node, u32 count)
+{
+	int i, j;
+	struct pm_signal pm_signal_local[NR_PHYS_CTRS];
+
+	/*
+	 * There is no debug setup required for the cycles event.
+	 * Note that only events in the same group can be used.
+	 * Otherwise, there will be conflicts in correctly routing
+	 * the signals on the debug bus.  It is the responsiblity
+	 * of the OProfile user tool to check the events are in
+	 * the same group.
+	 */
+	i = 0;
+	for (j = 0; j < count; j++) {
+		if (pm_signal[j].signal_group != PPU_CYCLES_GRP_NUM) {
+
+			/* fw expects physical cpu # */
+			pm_signal_local[i].cpu = node;
+			pm_signal_local[i].signal_group
+			    = pm_signal[j].signal_group;
+			pm_signal_local[i].bus_word =
+			    pm_signal[j].bus_word;
+			pm_signal_local[i].sub_unit =
+			    pm_signal[j].sub_unit;
+			pm_signal_local[i].bit = pm_signal[j].bit;
+			ps3_set_signal(pm_signal[j].signal_group,
+				       pm_signal[j].bit,
+				       pm_signal[j].sub_unit,
+				       pm_signal[j].bus_word);
+			i++;
+		}
+	}
+	return 0;
+}
+
+/*
+ * PM Signal functions
+ */
+static void set_pm_event(u32 ctr, int event, u32 unit_mask)
+{
+	struct pm_signal *p;
+	u32 signal_bit;
+	u32 bus_word, bus_type, count_cycles, polarity, input_control;
+	int j, i;
+
+	if (event == PPU_CYCLES_EVENT_NUM) {
+		/* Special Event: Count all cpu cycles */
+		pm_regs.pm07_cntrl[ctr] = CBE_COUNT_ALL_CYCLES;
+		p = &(pm_signal[ctr]);
+		p->signal_group = PPU_CYCLES_GRP_NUM;
+		p->bus_word = 1;
+		p->sub_unit = 0;
+		p->bit = 0;
+		goto out;
+	} else {
+		pm_regs.pm07_cntrl[ctr] = 0;
+	}
+
+	bus_word = GET_BUS_WORD(unit_mask);
+	bus_type = GET_BUS_TYPE(unit_mask);
+	count_cycles = GET_COUNT_CYCLES(unit_mask);
+	polarity = GET_POLARITY(unit_mask);
+	input_control = GET_INPUT_CONTROL(unit_mask);
+	signal_bit = (event % 100);
+
+	p = &(pm_signal[ctr]);
+
+	p->signal_group = event / 100;
+	p->bus_word = bus_word;
+	p->sub_unit = GET_SUB_UNIT(unit_mask);
+
+	/*
+	 * This parameter is used to specify the target physical/logical
+	 * PPE/SPE object.
+	 */
+	if (p->signal_group < 42 || 56 < p->signal_group)
+		p->sub_unit = 1;
+
+	pm_regs.pm07_cntrl[ctr] = 0;
+	pm_regs.pm07_cntrl[ctr] |= PM07_CTR_COUNT_CYCLES(count_cycles);
+	pm_regs.pm07_cntrl[ctr] |= PM07_CTR_POLARITY(polarity);
+	pm_regs.pm07_cntrl[ctr] |= PM07_CTR_INPUT_CONTROL(input_control);
+
+	/*
+	 * Some of the islands signal selection is based on 64 bit words.
+	 * The debug bus words are 32 bits, the input words to the performance
+	 * counters are defined as 32 bits.  Need to convert the 64 bit island
+	 * specification to the appropriate 32 input bit and bus word for the
+	 * performance counter event selection.  See the CELL Performance
+	 * monitoring signals manual and the Perf cntr hardware descriptions
+	 * for the details.
+	 */
+	if (input_control == 0) {
+		if (signal_bit > 31) {
+			signal_bit -= 32;
+			if (bus_word == 0x3)
+				bus_word = 0x2;
+			else if (bus_word == 0xc)
+				bus_word = 0x8;
+		}
+
+		if ((bus_type == 0) && p->signal_group >= 60)
+			bus_type = 2;
+		if ((bus_type == 0) &&
+		    (30 <= p->signal_group && p->signal_group <= 40))
+			bus_type = 2;
+		if ((bus_type == 1) && p->signal_group >= 50)
+			bus_type = 0;
+
+		pm_regs.pm07_cntrl[ctr] |= PM07_CTR_INPUT_MUX(signal_bit);
+	} else {
+		pm_regs.pm07_cntrl[ctr] = 0;
+		p->bit = signal_bit;
+	}
+
+	for (i = 0; i < NUM_DEBUG_BUS_WORDS; i++) {
+		if (bus_word & (1 << i)) {
+			pm_regs.debug_bus_control |=
+				(bus_type << (30 - (2 * i)));
+			for (j = 0; j < NUM_INPUT_BUS_WORDS; j++) {
+				if (input_bus[j] == 0xff) {
+					input_bus[j] = i;
+					pm_regs.group_control |=
+						(i << (30 - (2 * j)));
+
+					break;
+				}
+			}
+		}
+	}
+	OP_DBG("pm07_ctrl[%d] : 0x%x", ctr, pm_regs.pm07_cntrl[ctr]);
+	OP_DBG("group_control : 0x%x", pm_regs.group_control);
+	OP_DBG("debug_bus_control : 0x%x", pm_regs.debug_bus_control);
+out:
+	;
+}
+
+static void write_pm_cntrl(int cpu)
+{
+	/*
+	 * Oprofile will use 32 bit counters, set bits 7:10 to 0
+	 * pmregs.pm_cntrl is a global
+	 */
+
+	u32 val = 0;
+	if (pm_regs.pm_cntrl.enable == 1)
+		val |= CBE_PM_ENABLE_PERF_MON;
+
+	if (pm_regs.pm_cntrl.stop_at_max == 1)
+		val |= CBE_PM_STOP_AT_MAX;
+
+	if (pm_regs.pm_cntrl.trace_mode == 1)
+		val |= CBE_PM_TRACE_MODE_SET(pm_regs.pm_cntrl.trace_mode);
+
+	if (pm_regs.pm_cntrl.freeze == 1)
+		val |= CBE_PM_FREEZE_ALL_CTRS;
+
+	/*
+	 * Routine set_count_mode must be called previously to set
+	 * the count mode based on the user selection of user and kernel.
+	 */
+	val |= CBE_PM_COUNT_MODE_SET(pm_regs.pm_cntrl.count_mode);
+	ps3_write_pm(cpu, pm_control, val);
+}
+
+static inline void
+set_count_mode(u32 kernel, u32 user)
+{
+	OP_DBG("set_count_mode k:%d u:%d", kernel, user);
+	/*
+	 * The user must specify user and kernel if they want them. If
+	 *  neither is specified, OProfile will count in hypervisor mode.
+	 *  pm_regs.pm_cntrl is a global
+	 *
+	 *  NOTE : PS3 hypervisor rejects ALL_MODES and HYPERVISOR_MODE.
+	 *         So, ALL_MODES and HYPERVISOR_MODE are changed to
+	 *         PROBLEM_MODE.
+	 */
+	if (kernel) {
+		if (user)
+			pm_regs.pm_cntrl.count_mode =
+				CBE_COUNT_PROBLEM_MODE;
+		else
+			pm_regs.pm_cntrl.count_mode =
+				CBE_COUNT_SUPERVISOR_MODE;
+	} else {
+		if (user)
+			pm_regs.pm_cntrl.count_mode =
+				CBE_COUNT_PROBLEM_MODE;
+		else
+			pm_regs.pm_cntrl.count_mode =
+				CBE_COUNT_PROBLEM_MODE;
+	}
+}
+
+static inline void enable_ctr(u32 cpu, u32 ctr, u32 *pm07_cntrl)
+{
+
+	pm07_cntrl[ctr] |= CBE_PM_CTR_ENABLE;
+	ps3_write_pm07_control(cpu, ctr, pm07_cntrl[ctr]);
+}
+
+static void add_sample(u32 cpu)
+{
+	struct pt_regs *regs;
+	u64 pc;
+	int is_kernel;
+	int i;
+	u32 value;
+
+	regs = get_irq_regs();
+	if (oprofile_running == 1) {
+		pc = regs->nip;
+		is_kernel = is_kernel_addr(pc);
+
+		for (i = 0; i < num_counters; ++i) {
+			value = ps3_read_ctr(cpu, i);
+			if (value >= count_value[i] && count_value[i] != 0) {
+				OP_DBG("pmu:add_sample ctr:%d"
+				       " value:0x%x reset:0x%x count:0x%x",
+				       i, value, reset_value[i],
+				       count_value[i]);
+				oprofile_add_pc(pc, is_kernel, i);
+				ps3_write_ctr(cpu, i, reset_value[i]);
+			}
+		}
+	}
+}
+
+/*
+ * Oprofile is expected to collect data on all CPUs simultaneously.
+ * However, there is one set of performance counters per node.	There are
+ * two hardware threads or virtual CPUs on each node.  Hence, OProfile must
+ * multiplex in time the performance counter collection on the two virtual
+ * CPUs.  The multiplexing of the performance counters is done by this
+ * virtual counter routine.
+ *
+ * The pmc_values used below is defined as 'per-cpu' but its use is
+ * more akin to 'per-node'.  We need to store two sets of counter
+ * values per node -- one for the previous run and one for the next.
+ * The per-cpu[NR_PHYS_CTRS] gives us the storage we need.  Each odd/even
+ * pair of per-cpu arrays is used for storing the previous and next
+ * pmc values for a given node.
+ * NOTE: We use the per-cpu variable to improve cache performance.
+ *
+ * This routine will alternate loading the virtual counters for
+ * virtual CPUs
+ */
+static void cell_virtual_cntr(unsigned long data)
+{
+	int i, prev_hdw_thread, next_hdw_thread;
+	u32 cpu;
+	unsigned long flags;
+
+	/*
+	 * Make sure that the interrupt_hander and the virt counter are
+	 * not both playing with the counters on the same node.
+	 */
+
+	spin_lock_irqsave(&virt_cntr_lock, flags);
+
+	prev_hdw_thread = hdw_thread;
+
+	/* switch the cpu handling the interrupts */
+	hdw_thread = 1 ^ hdw_thread;
+	next_hdw_thread = hdw_thread;
+
+	pm_regs.group_control = 0;
+	pm_regs.debug_bus_control = 0;
+
+	for (i = 0; i < NUM_INPUT_BUS_WORDS; i++)
+		input_bus[i] = 0xff;
+
+	/*
+	 * There are some per thread events.  Must do the
+	 * set event, for the thread that is being started
+	 */
+	for (i = 0; i < num_counters; i++)
+		set_pm_event(i,
+			pmc_cntrl[next_hdw_thread][i].evnts,
+			pmc_cntrl[next_hdw_thread][i].masks);
+
+	/*
+	 * The following is done only once per each node, but
+	 * we need cpu #, not node #, to pass to the cbe_xxx functions.
+	 */
+	for_each_online_cpu(cpu) {
+		if (ps3_get_hw_thread_id(cpu))
+			continue;
+
+		/*
+		 * stop counters, save counter values, restore counts
+		 * for previous thread
+		 */
+		ps3_disable_pm(cpu);
+		ps3_disable_pm_interrupts(cpu);
+
+		/*
+		 * Add sample data at here.
+		 * Because PS3 hypervisor does not have
+		 * the performance monitor interrupt feature.
+		 */
+		add_sample(cpu);
+
+		/*
+		 * Switch to the other thread. Change the interrupt
+		 * and control regs to be scheduled on the CPU
+		 * corresponding to the thread to execute.
+		 */
+		for (i = 0; i < num_counters; i++) {
+			if (pmc_cntrl[next_hdw_thread][i].enabled) {
+				/*
+				 * There are some per thread events.
+				 * Must do the set event, enable_cntr
+				 * for each cpu.
+				 */
+				enable_ctr(cpu, i,
+					   pm_regs.pm07_cntrl);
+			} else {
+				ps3_write_pm07_control(cpu, i, 0);
+			}
+		}
+
+		/* Enable interrupts on the CPU thread that is starting */
+		ps3_enable_pm_interrupts(cpu, next_hdw_thread,
+					 virt_cntr_inter_mask);
+		ps3_enable_pm(cpu);
+	}
+
+	spin_unlock_irqrestore(&virt_cntr_lock, flags);
+
+	mod_timer(&timer_virt_cntr, jiffies + HZ / 10);
+}
+
+static void start_virt_cntrs(void)
+{
+	init_timer(&timer_virt_cntr);
+	timer_virt_cntr.function = cell_virtual_cntr;
+	timer_virt_cntr.data = 0UL;
+	timer_virt_cntr.expires = jiffies + HZ / 10;
+	add_timer(&timer_virt_cntr);
+}
+
+/* This function is called once for all cpus combined */
+static int cell_reg_setup(struct op_counter_config *ctr,
+			struct op_system_config *sys, int num_ctrs)
+{
+	int i, j, cpu;
+	int ret;
+
+	spu_cycle_reset = 0;
+
+	ret = ps3_lpm_open(PS3_LPM_TB_TYPE_NONE, NULL, 0);
+	if (ret) {
+		OP_ERR("lpm_open error. %d", ret);
+		return -EFAULT;
+	}
+
+	if (ctr[0].event == SPU_CYCLES_EVENT_NUM)
+		spu_cycle_reset = ctr[0].count;
+
+	num_counters = num_ctrs;
+
+	pm_regs.group_control = 0;
+	pm_regs.debug_bus_control = 0;
+
+	/* setup the pm_control register */
+	memset(&pm_regs.pm_cntrl, 0, sizeof(struct pm_cntrl));
+	pm_regs.pm_cntrl.stop_at_max = 1;
+	pm_regs.pm_cntrl.trace_mode = 0;
+	pm_regs.pm_cntrl.freeze = 1;
+
+	set_count_mode(sys->enable_kernel, sys->enable_user);
+
+	/* Setup the thread 0 events */
+	for (i = 0; i < num_ctrs; ++i) {
+
+		pmc_cntrl[0][i].evnts = ctr[i].event;
+		pmc_cntrl[0][i].masks = ctr[i].unit_mask;
+		pmc_cntrl[0][i].enabled = ctr[i].enabled;
+		pmc_cntrl[0][i].vcntr = i;
+
+		for_each_possible_cpu(j)
+			per_cpu(pmc_values, j)[i] = 0;
+	}
+
+	/*
+	 * Setup the thread 1 events, map the thread 0 event to the
+	 * equivalent thread 1 event.
+	 */
+	for (i = 0; i < num_ctrs; ++i) {
+		if ((ctr[i].event >= 2100) && (ctr[i].event <= 2111))
+			pmc_cntrl[1][i].evnts = ctr[i].event + 19;
+		else if (ctr[i].event == 2203)
+			pmc_cntrl[1][i].evnts = ctr[i].event;
+		else if ((ctr[i].event >= 2200) && (ctr[i].event <= 2215))
+			pmc_cntrl[1][i].evnts = ctr[i].event + 16;
+		else
+			pmc_cntrl[1][i].evnts = ctr[i].event;
+
+		pmc_cntrl[1][i].masks = ctr[i].unit_mask;
+		pmc_cntrl[1][i].enabled = ctr[i].enabled;
+		pmc_cntrl[1][i].vcntr = i;
+	}
+
+	for (i = 0; i < NUM_INPUT_BUS_WORDS; i++)
+		input_bus[i] = 0xff;
+
+	/*
+	 * Our counters count up, and "count" refers to
+	 * how much before the next interrupt, and we interrupt
+	 * on overflow.	 So we calculate the starting value
+	 * which will give us "count" until overflow.
+	 * Then we set the events on the enabled counters.
+	 */
+	for (i = 0; i < num_counters; ++i) {
+		/* start with virtual counter set 0 */
+		if (pmc_cntrl[0][i].enabled) {
+			reset_value[i] = 0;
+			count_value[i] = ctr[i].count;
+			set_pm_event(i,
+				     pmc_cntrl[0][i].evnts,
+				     pmc_cntrl[0][i].masks);
+
+			/* global, used by cell_cpu_setup */
+			ctr_enabled |= (1 << i);
+		}
+	}
+
+	/* initialize the previous counts for the virtual cntrs */
+	for_each_online_cpu(cpu)
+		for (i = 0; i < num_counters; ++i)
+			per_cpu(pmc_values, cpu)[i] = reset_value[i];
+
+	return 0;
+}
+
+
+
+/* This function is called once for each cpu */
+static int cell_cpu_setup(struct op_counter_config *cntr)
+{
+	u32 cpu = smp_processor_id();
+	u32 num_enabled = 0;
+	int i;
+
+	if (spu_cycle_reset)
+		return 0;
+
+	/* There is one performance monitor per processor chip (i.e. node),
+	 * so we only need to perform this function once per node.
+	 */
+	if (ps3_get_hw_thread_id(cpu))
+		return 0;
+
+	/* Stop all counters */
+	ps3_disable_pm(cpu);
+	ps3_disable_pm_interrupts(cpu);
+
+	ps3_write_pm(cpu, pm_interval, 0);
+	ps3_write_pm(cpu, pm_start_stop, 0);
+	ps3_write_pm(cpu, group_control, pm_regs.group_control);
+	ps3_write_pm(cpu, debug_bus_control, pm_regs.debug_bus_control);
+	write_pm_cntrl(cpu);
+
+	for (i = 0; i < num_counters; ++i) {
+		if (ctr_enabled & (1 << i)) {
+			pm_signal[num_enabled].cpu = ps3_cpu_to_node(cpu);
+			num_enabled++;
+		}
+	}
+
+	return pm_activate_signals(ps3_cpu_to_node(cpu), num_enabled);
+}
+
+
+static int cell_global_start_spu(struct op_counter_config *ctr)
+{
+	return -ENOSYS;
+}
+
+static int cell_global_start_ppu(struct op_counter_config *ctr)
+{
+	u32 cpu, i;
+	u32 interrupt_mask = 0;
+
+	/* This routine gets called once for the system.
+	 * There is one performance monitor per node, so we
+	 * only need to perform this function once per node.
+	 */
+	for_each_online_cpu(cpu) {
+		if (ps3_get_hw_thread_id(cpu))
+			continue;
+
+		interrupt_mask = 0;
+
+		for (i = 0; i < num_counters; ++i) {
+			if (ctr_enabled & (1 << i)) {
+				ps3_write_ctr(cpu, i, reset_value[i]);
+				enable_ctr(cpu, i, pm_regs.pm07_cntrl);
+				interrupt_mask |=
+				    CBE_PM_CTR_OVERFLOW_INTR(i);
+			} else {
+				/* Disable counter */
+				ps3_write_pm07_control(cpu, i, 0);
+			}
+		}
+
+		ps3_get_and_clear_pm_interrupts(cpu);
+		ps3_enable_pm_interrupts(cpu, hdw_thread, interrupt_mask);
+		ps3_enable_pm(cpu);
+	}
+
+	virt_cntr_inter_mask = interrupt_mask;
+	oprofile_running = 1;
+	/* complete the previous store */
+	smp_wmb();
+
+	/*
+	 * NOTE: start_virt_cntrs will result in cell_virtual_cntr() being
+	 * executed which manipulates the PMU.	We start the "virtual counter"
+	 * here so that we do not need to synchronize access to the PMU in
+	 * the above for-loop.
+	 */
+	start_virt_cntrs();
+
+	return 0;
+}
+
+static int cell_global_start(struct op_counter_config *ctr)
+{
+	if (spu_cycle_reset)
+		return cell_global_start_spu(ctr);
+	else
+		return cell_global_start_ppu(ctr);
+}
+
+static void cell_global_stop_spu(void)
+{
+}
+
+static void cell_global_stop_ppu(void)
+{
+	int cpu;
+
+	/*
+	 * This routine will be called once for the system.
+	 * There is one performance monitor per node, so we
+	 * only need to perform this function once per node.
+	 */
+	del_timer_sync(&timer_virt_cntr);
+	oprofile_running = 0;
+	/* complete the previous store */
+	smp_wmb();
+
+	for_each_online_cpu(cpu) {
+		if (ps3_get_hw_thread_id(cpu))
+			continue;
+
+		/* Stop the counters */
+		ps3_disable_pm(cpu);
+
+		/* Deactivate the signals */
+		ps3_set_signal(0, 0, 0, 0);	/*clear all */
+
+		/* Deactivate interrupts */
+		ps3_disable_pm_interrupts(cpu);
+	}
+}
+
+static void cell_global_stop(void)
+{
+	if (spu_cycle_reset)
+		cell_global_stop_spu();
+	else
+		cell_global_stop_ppu();
+}
+
+
+/*
+ * This function is called from the generic OProfile
+ * driver.  When profiling PPUs, we need to do the
+ * generic sync start; otherwise, do spu_sync_start.
+ */
+static int cell_sync_start(void)
+{
+	OP_ERR("PS3 oprofile support");
+	return DO_GENERIC_SYNC;
+}
+
+static int cell_sync_stop(void)
+{
+	int ret;
+
+	ret = ps3_lpm_close();
+	if (ret)
+		OP_ERR("lpm_close error. %d", ret);
+
+	return 1;
+}
+
+struct op_powerpc_model op_model_ps3 = {
+	.reg_setup = cell_reg_setup,
+	.cpu_setup = cell_cpu_setup,
+	.global_start = cell_global_start,
+	.global_stop = cell_global_stop,
+	.sync_start = cell_sync_start,
+	.sync_stop = cell_sync_stop,
+};
diff -Naur linux-2.6.25-org/arch/powerpc/perfmon/Kconfig linux-2.6.25-id/arch/powerpc/perfmon/Kconfig
--- linux-2.6.25-org/arch/powerpc/perfmon/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/perfmon/Kconfig	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,57 @@
+menu "Hardware Performance Monitoring support"
+config PERFMON
+	bool "Perfmon2 performance monitoring interface"
+	default n
+	help
+	Enables the perfmon2 interface to access the hardware
+	performance counters. See <http://perfmon2.sf.net/> for
+	more details.
+
+config PERFMON_DEBUG
+	bool "Perfmon debugging"
+	default n
+	depends on PERFMON
+	help
+	Enables perfmon debugging support
+
+config PERFMON_POWER4
+	tristate "Support for Power4 hardware performance counters"
+	depends on PERFMON && PPC64
+	default n
+	help
+	Enables support for the Power 4 hardware performance counters
+	If unsure, say M.
+
+config PERFMON_POWER5
+	tristate "Support for Power5 hardware performance counters"
+	depends on PERFMON && PPC64
+	default n
+	help
+	Enables support for the Power 5 hardware performance counters
+	If unsure, say M.
+
+config PERFMON_POWER6
+	tristate "Support for Power6 hardware performance counters"
+	depends on PERFMON && PPC64
+	default n
+	help
+	Enables support for the Power 6 hardware performance counters
+	If unsure, say M.
+
+config PERFMON_PPC32
+	tristate "Support for PPC32 hardware performance counters"
+	depends on PERFMON && PPC32
+	default n
+	help
+	Enables support for the PPC32 hardware performance counters
+	If unsure, say M.
+
+config PERFMON_CELL
+	tristate "Support for Cell hardware performance counters"
+	depends on PERFMON && PPC_CELL
+	default n
+	help
+	Enables support for the Cell hardware performance counters.
+	If unsure, say M.
+
+endmenu
diff -Naur linux-2.6.25-org/arch/powerpc/perfmon/Makefile linux-2.6.25-id/arch/powerpc/perfmon/Makefile
--- linux-2.6.25-org/arch/powerpc/perfmon/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/perfmon/Makefile	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,6 @@
+obj-$(CONFIG_PERFMON)		+= perfmon.o
+obj-$(CONFIG_PERFMON_POWER4)	+= perfmon_power4.o
+obj-$(CONFIG_PERFMON_POWER5)	+= perfmon_power5.o
+obj-$(CONFIG_PERFMON_POWER6)	+= perfmon_power6.o
+obj-$(CONFIG_PERFMON_PPC32)	+= perfmon_ppc32.o
+obj-$(CONFIG_PERFMON_CELL)	+= perfmon_cell.o
diff -Naur linux-2.6.25-org/arch/powerpc/perfmon/perfmon.c linux-2.6.25-id/arch/powerpc/perfmon/perfmon.c
--- linux-2.6.25-org/arch/powerpc/perfmon/perfmon.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/perfmon/perfmon.c	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,318 @@
+/*
+ * This file implements the powerpc specific
+ * support for the perfmon2 interface
+ *
+ * Copyright (c) 2005 David Gibson, IBM Corporation.
+ *
+ * based on versions for other architectures:
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/perfmon.h>
+#include <asm/perfmon.h>
+
+static void pfm_stop_active(struct task_struct *task,
+			    struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	BUG_ON(!arch_info->disable_counters || !arch_info->get_ovfl_pmds);
+
+	arch_info->disable_counters(ctx, set);
+
+	if (set->npend_ovfls)
+		return;
+
+	arch_info->get_ovfl_pmds(ctx, set);
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * Context is locked. Interrupts are masked. Monitoring is active.
+ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
+ *
+ * for per-thread:
+ * 	must stop monitoring for the task
+ * Return:
+ * 	non-zero : did not save PMDs (as part of stopping the PMU)
+ * 	       0 : saved PMDs (no need to save them in caller)
+ */
+int pfm_arch_ctxswout_thread(struct task_struct *task,
+			     struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	/*
+	 * disable lazy restore of PMC registers.
+	 */
+	set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
+
+	pfm_stop_active(task, ctx, set);
+
+	if (arch_info->ctxswout_thread)
+		arch_info->ctxswout_thread(task, ctx, set);
+
+	return 1;
+}
+
+/*
+ * Called from pfm_ctxsw
+ */
+void pfm_arch_ctxswin_thread(struct task_struct *task,
+			     struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	if (ctx->state != PFM_CTX_MASKED && ctx->flags.started == 1) {
+		BUG_ON(!arch_info->enable_counters);
+		arch_info->enable_counters(ctx, set);
+	}
+
+	if (arch_info->ctxswin_thread)
+		arch_info->ctxswin_thread(task, ctx, set);
+}
+
+/*
+ * Called from pfm_stop() and idle notifier
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-thread:
+ *   task is not necessarily current. If not current task, then
+ *   task is guaranteed stopped and off any cpu. Access to PMU
+ *   is not guaranteed. Interrupts are masked. Context is locked.
+ *   Set is the active set.
+ *
+ * For system-wide:
+ * 	task is current
+ *
+ * must disable active monitoring. ctx cannot be NULL
+ */
+void pfm_arch_stop(struct task_struct *task,
+		   struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	/*
+	 * no need to go through stop_save()
+	 * if we are already stopped
+	 */
+	if (!ctx->flags.started || ctx->state == PFM_CTX_MASKED)
+		return;
+
+	/*
+	 * stop live registers and collect pending overflow
+	 */
+	if (task == current)
+		pfm_stop_active(task, ctx, set);
+}
+
+/*
+ * Enable active monitoring. Called from pfm_start() and
+ * pfm_arch_unmask_monitoring().
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-thread:
+ * 	Task is not necessarily current. If not current task, then task
+ * 	is guaranteed stopped and off any cpu. No access to PMU if task
+ *	is not current.
+ *
+ * For system-wide:
+ * 	Task is always current
+ */
+void pfm_arch_start(struct task_struct *task,
+		    struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	if (task != current)
+		return;
+
+	BUG_ON(!arch_info->enable_counters);
+
+	arch_info->enable_counters(ctx, set);
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
+ * context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMD registers from set.
+ */
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	u64 *used_pmds;
+	u16 i, num;
+
+	/* The model-specific module can override the default
+	 * restore-PMD method.
+	 */
+	if (arch_info->restore_pmds)
+		return arch_info->restore_pmds(set);
+
+	num = set->nused_pmds;
+	used_pmds = set->used_pmds;
+
+	for (i = 0; num; i++) {
+		if (likely(test_bit(i, used_pmds))) {
+			pfm_write_pmd(ctx, i, set->pmds[i].value);
+			num--;
+		}
+	}
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
+ * context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMC registers from set, if needed.
+ */
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info;
+	u64 *impl_pmcs;
+	unsigned int i, max_pmc, reg;
+
+	/* The model-specific module can override the default
+	 * restore-PMC method.
+	 */
+	arch_info = pfm_pmu_conf->arch_info;
+	if (arch_info->restore_pmcs)
+		return arch_info->restore_pmcs(set);
+
+	/* The "common" powerpc model's enable the counters simply by writing
+	 * all the control registers. Therefore, if we're masked or stopped we
+	 * don't need to bother restoring the PMCs now.
+	 */
+	if (ctx->state == PFM_CTX_MASKED || ctx->flags.started == 0)
+		return;
+
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+	impl_pmcs = pfm_pmu_conf->regs.pmcs;
+
+	/*
+	 * Restore all pmcs in reverse order to ensure the counters aren't
+	 * enabled before their event selectors are set correctly.
+	 */
+	reg = max_pmc - 1;
+	for (i = 0; i < max_pmc; i++) {
+		if (test_bit(reg, impl_pmcs))
+			pfm_arch_write_pmc(ctx, reg, set->pmcs[reg]);
+		reg--;
+	}
+}
+
+int pfm_arch_ctxsw(struct notifier_block *block,
+                   unsigned long object_id, void *arg)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	struct task_struct *p;
+
+	if (!arch_info->get_pid || !arch_info->ctxsw) {
+		return 0;
+	}
+
+	read_lock(&tasklist_lock);
+
+	p = find_task_by_pid(arch_info->get_pid(arg));
+	if (p)
+		get_task_struct(p);
+
+	read_unlock(&tasklist_lock);
+
+	if (p == NULL)
+		return 0;
+
+	arch_info->ctxsw(block, object_id, p, arg);
+
+	put_task_struct(p);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(pfm_arch_ctxsw);
+
+char *pfm_arch_get_pmu_module_name(void)
+{
+	unsigned int pvr = mfspr(SPRN_PVR);
+
+	switch (PVR_VER(pvr)) {
+	case 0x0004: /* 604 */
+	case 0x0009: /* 604e;  */
+	case 0x000A: /* 604ev */
+	case 0x0008: /* 750/740 */
+	case 0x7000: /* 750FX */
+	case 0x7001:
+	case 0x7002: /* 750GX */
+	case 0x000C: /* 7400 */
+	case 0x800C: /* 7410 */
+	case 0x8000: /* 7451/7441 */
+	case 0x8001: /* 7455/7445 */
+	case 0x8002: /* 7457/7447 */
+	case 0x8003: /* 7447A */
+	case 0x8004: /* 7448 */
+		return("perfmon_ppc32");
+	case PV_POWER4:
+	case PV_POWER4p:
+		return "perfmon_power4";
+	case PV_POWER5:
+		return "perfmon_power5";
+	case PV_POWER5p:
+		return "perfmon_power5+";
+	case PV_POWER6:
+		return "perfmon_power6";
+	case PV_970:
+	case PV_970FX:
+	case PV_970MP:
+		return "perfmon_ppc970";
+	case PV_BE:
+		return "perfmon_cell";
+	}
+	return NULL;
+}
+
+void pfm_arch_init_percpu(void)
+{
+#ifdef CONFIG_PPC64
+	extern void ppc64_enable_pmcs(void);
+	ppc64_enable_pmcs();
+#endif
+}
+
+/**
+ * powerpc_irq_handler
+ *
+ * Get the perfmon context that belongs to the current CPU, and call the
+ * model-specific interrupt handler.
+ **/
+void powerpc_irq_handler(struct pt_regs *regs)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	struct pfm_context *ctx;
+
+	if (arch_info->irq_handler) {
+		ctx = __get_cpu_var(pmu_ctx);
+		if (likely(ctx))
+			arch_info->irq_handler(regs, ctx);
+	}
+}
diff -Naur linux-2.6.25-org/arch/powerpc/perfmon/perfmon_cell.c linux-2.6.25-id/arch/powerpc/perfmon/perfmon_cell.c
--- linux-2.6.25-org/arch/powerpc/perfmon/perfmon_cell.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/perfmon/perfmon_cell.c	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,1410 @@
+/*
+ * This file contains the Cell PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright IBM Corporation 2007
+ * (C) Copyright 2007 TOSHIBA CORPORATION
+ *
+ * Based on other Perfmon2 PMU modules.
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <asm/cell-pmu.h>
+#include <asm/cell-regs.h>
+#include <asm/io.h>
+#include <asm/machdep.h>
+#include <asm/rtas.h>
+#include <asm/ps3.h>
+#include "../platforms/cell/spufs/spufs.h"
+#include <asm/perfmon.h>
+
+MODULE_AUTHOR("Kevin Corry <kevcorry@us.ibm.com>, "
+	      "Carl Love <carll@us.ibm.com>");
+MODULE_DESCRIPTION("Cell PMU description table");
+MODULE_LICENSE("GPL");
+
+struct pfm_cell_platform_pmu_info {
+	u32  (*read_ctr)(u32 cpu, u32 ctr);
+	void (*write_ctr)(u32 cpu, u32 ctr, u32 val);
+	void (*write_pm07_control)(u32 cpu, u32 ctr, u32 val);
+	void (*write_pm)(u32 cpu, enum pm_reg_name reg, u32 val);
+	void (*enable_pm)(u32 cpu);
+	void (*disable_pm)(u32 cpu);
+	void (*enable_pm_interrupts)(u32 cpu, u32 thread, u32 mask);
+	u32  (*get_and_clear_pm_interrupts)(u32 cpu);
+	u32  (*get_hw_thread_id)(int cpu);
+	struct cbe_ppe_priv_regs __iomem *(*get_cpu_ppe_priv_regs)(int cpu);
+	struct cbe_pmd_regs __iomem *(*get_cpu_pmd_regs)(int cpu);
+	struct cbe_mic_tm_regs __iomem *(*get_cpu_mic_tm_regs)(int cpu);
+	int (*rtas_token)(const char *service);
+	int (*rtas_call)(int token, int param1, int param2, int *param3, ...);
+};
+
+/*
+ * Mapping from Perfmon logical control registers to Cell hardware registers.
+ */
+static struct pfm_regmap_desc pfm_cell_pmc_desc[] = {
+	/* Per-counter control registers. */
+	PMC_D(PFM_REG_I, "pm0_control",       0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm1_control",       0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm2_control",       0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm3_control",       0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm4_control",       0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm5_control",       0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm6_control",       0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm7_control",       0, 0, 0, 0),
+
+	/* Per-counter RTAS arguments. Each of these registers has three fields.
+	 *   bits 63-48: debug-bus word
+	 *   bits 47-32: sub-unit
+	 *   bits 31-0 : full signal number
+	 *   (MSB = 63, LSB = 0)
+	 */
+	PMC_D(PFM_REG_I, "pm0_event",         0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm1_event",         0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm2_event",         0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm3_event",         0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm4_event",         0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm5_event",         0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm6_event",         0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm7_event",         0, 0, 0, 0),
+
+	/* Global control registers. Same order as enum pm_reg_name. */
+	PMC_D(PFM_REG_I, "group_control",     0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "debug_bus_control", 0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "trace_address",     0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "ext_trace_timer",   0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm_status",         0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm_control",        0, 0, 0, 0),
+	PMC_D(PFM_REG_I, "pm_interval",       0, 0, 0, 0), /* FIX: Does user-space also need read access to this one? */
+	PMC_D(PFM_REG_I, "pm_start_stop",     0, 0, 0, 0),
+};
+#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_cell_pmc_desc)
+
+#define CELL_PMC_GROUP_CONTROL    16
+#define CELL_PMC_PM_STATUS        20
+
+/*
+ * Mapping from Perfmon logical data counters to Cell hardware counters.
+ */
+static struct pfm_regmap_desc pfm_cell_pmd_desc[] = {
+	PMD_D(PFM_REG_C, "pm0", 0),
+	PMD_D(PFM_REG_C, "pm1", 0),
+	PMD_D(PFM_REG_C, "pm2", 0),
+	PMD_D(PFM_REG_C, "pm3", 0),
+	PMD_D(PFM_REG_C, "pm4", 0),
+	PMD_D(PFM_REG_C, "pm5", 0),
+	PMD_D(PFM_REG_C, "pm6", 0),
+	PMD_D(PFM_REG_C, "pm7", 0),
+};
+#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_cell_pmd_desc)
+
+#define PFM_EVENT_PMC_BUS_WORD(x)      (((x) >> 48) & 0x00ff)
+#define PFM_EVENT_PMC_FULL_SIGNAL_NUMBER(x) ((x) & 0xffffffff)
+#define PFM_EVENT_PMC_SIGNAL_GROUP(x) (((x) & 0xffffffff) / 100)
+#define PFM_PM_CTR_INPUT_MUX_BIT(pm07_control) (((pm07_control) >> 26) & 0x1f)
+#define PFM_PM_CTR_INPUT_MUX_GROUP_INDEX(pm07_control) ((pm07_control) >> 31)
+#define PFM_GROUP_CONTROL_GROUP0_WORD(grp_ctrl) ((grp_ctrl) >> 30)
+#define PFM_GROUP_CONTROL_GROUP1_WORD(grp_ctrl) (((grp_ctrl) >> 28) & 0x3)
+#define PFM_NUM_OF_GROUPS 2
+#define PFM_PPU_IU1_THREAD1_BASE_BIT 19
+#define PFM_PPU_XU_THREAD1_BASE_BIT  16
+#define PFM_COUNTER_CTRL_PMC_PPU_TH0 0x100000000ULL
+#define PFM_COUNTER_CTRL_PMC_PPU_TH1 0x200000000ULL
+
+/*
+ * Debug-bus signal handling.
+ *
+ * Some Cell systems have firmware that can handle the debug-bus signal
+ * routing. For systems without this firmware, we have a minimal in-kernel
+ * implementation as well.
+ */
+
+/* The firmware only sees physical CPUs, so divide by 2 if SMT is on. */
+#ifdef CONFIG_SCHED_SMT
+#define RTAS_CPU(cpu) ((cpu) / 2)
+#else
+#define RTAS_CPU(cpu) (cpu)
+#endif
+#define RTAS_BUS_WORD(x)      (u16)(((x) >> 48) & 0x0000ffff)
+#define RTAS_SUB_UNIT(x)      (u16)(((x) >> 32) & 0x0000ffff)
+#define RTAS_SIGNAL_NUMBER(x) (s32)( (x)        & 0xffffffff)
+#define RTAS_SIGNAL_GROUP(x)  (RTAS_SIGNAL_NUMBER(x) / 100)
+
+#define subfunc_RESET		1
+#define subfunc_ACTIVATE	2
+
+#define passthru_ENABLE		1
+#define passthru_DISABLE	2
+
+/**
+ * struct cell_rtas_arg
+ *
+ * @cpu: Processor to modify. Linux numbers CPUs based on SMT IDs, but the
+ *       firmware only sees the physical CPUs. So this value should be the
+ *       SMT ID (from smp_processor_id() or get_cpu()) divided by 2.
+ * @sub_unit: Hardware subunit this applies to (if applicable).
+ * @signal_group: Signal group to enable/disable on the trace bus.
+ * @bus_word: For signal groups that propagate via the trace bus, this trace
+ *            bus word will be used. This is a mask of (1 << TraceBusWord).
+ *            For other signal groups, this specifies the trigger or event bus.
+ * @bit: Trigger/Event bit, if applicable for the signal group.
+ *
+ * An array of these structures are passed to rtas_call() to set up the
+ * signals on the debug bus.
+ **/
+struct cell_rtas_arg {
+	u16 cpu;
+	u16 sub_unit;
+	s16 signal_group;
+	u8 bus_word;
+	u8 bit;
+};
+
+/**
+ * rtas_reset_signals
+ *
+ * Use the firmware RTAS call to disable signal pass-thru and to reset the
+ * debug-bus signals.
+ **/
+static int rtas_reset_signals(u32 cpu)
+{
+	struct cell_rtas_arg signal;
+	u64 real_addr = virt_to_phys(&signal);
+	int rc;
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	memset(&signal, 0, sizeof(signal));
+	signal.cpu = RTAS_CPU(cpu);
+	rc = info->rtas_call(info->rtas_token("ibm,cbe-perftools"),
+		       5, 1, NULL,
+		       subfunc_RESET,
+		       passthru_DISABLE,
+		       real_addr >> 32,
+		       real_addr & 0xffffffff,
+		       sizeof(signal));
+
+	return rc;
+}
+
+/**
+ * rtas_activate_signals
+ *
+ * Use the firmware RTAS call to enable signal pass-thru and to activate the
+ * desired signal groups on the debug-bus.
+ **/
+static int rtas_activate_signals(struct cell_rtas_arg *signals,
+				 int num_signals)
+{
+	u64 real_addr = virt_to_phys(signals);
+	int rc;
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	rc = info->rtas_call(info->rtas_token("ibm,cbe-perftools"),
+		       5, 1, NULL,
+		       subfunc_ACTIVATE,
+		       passthru_ENABLE,
+		       real_addr >> 32,
+		       real_addr & 0xffffffff,
+		       num_signals * sizeof(*signals));
+
+	return rc;
+}
+
+#define HID1_RESET_MASK			(~0x00000001ffffffffUL)
+#define PPU_IU1_WORD0_HID1_EN_MASK	(~0x00000001f0c0802cUL)
+#define PPU_IU1_WORD0_HID1_EN_WORD	( 0x00000001f0400000UL)
+#define PPU_IU1_WORD1_HID1_EN_MASK	(~0x000000010fc08023UL)
+#define PPU_IU1_WORD1_HID1_EN_WORD	( 0x000000010f400001UL)
+#define PPU_XU_WORD0_HID1_EN_MASK	(~0x00000001f038402cUL)
+#define PPU_XU_WORD0_HID1_EN_WORD	( 0x00000001f0080008UL)
+#define PPU_XU_WORD1_HID1_EN_MASK	(~0x000000010f074023UL)
+#define PPU_XU_WORD1_HID1_EN_WORD	( 0x000000010f030002UL)
+
+/* The bus_word field in the cell_rtas_arg structure is a bit-mask
+ * indicating which debug-bus word(s) to use.
+ */
+enum {
+	BUS_WORD_0 = 1,
+	BUS_WORD_1 = 2,
+	BUS_WORD_2 = 4,
+	BUS_WORD_3 = 8,
+};
+
+/* Definitions of the signal-groups that the built-in signal-activation
+ * code can handle.
+ */
+enum {
+	SIG_GROUP_NONE = 0,
+
+	/* 2.x PowerPC Processor Unit (PPU) Signal Groups */
+	SIG_GROUP_PPU_BASE = 20,
+	SIG_GROUP_PPU_IU1 = 21,
+	SIG_GROUP_PPU_XU = 22,
+
+	/* 3.x PowerPC Storage Subsystem (PPSS) Signal Groups */
+	SIG_GROUP_PPSS_BASE = 30,
+
+	/* 4.x Synergistic Processor Unit (SPU) Signal Groups */
+	SIG_GROUP_SPU_BASE = 40,
+
+	/* 5.x Memory Flow Controller (MFC) Signal Groups */
+	SIG_GROUP_MFC_BASE = 50,
+
+	/* 6.x Element )nterconnect Bus (EIB) Signal Groups */
+	SIG_GROUP_EIB_BASE = 60,
+
+	/* 7.x Memory Interface Controller (MIC) Signal Groups */
+	SIG_GROUP_MIC_BASE = 70,
+
+	/* 8.x Cell Broadband Engine Interface (BEI) Signal Groups */
+	SIG_GROUP_BEI_BASE = 80,
+};
+
+/**
+ * rmw_spr
+ *
+ * Read-modify-write for a special-purpose-register.
+ **/
+#define rmw_spr(spr_id, a_mask, o_mask) \
+	do { \
+		u64 value = mfspr(spr_id); \
+		value &= (u64)(a_mask); \
+		value |= (u64)(o_mask); \
+		mtspr((spr_id), value); \
+	} while (0)
+
+/**
+ * rmw_mmio_reg64
+ *
+ * Read-modify-write for a 64-bit MMIO register.
+ **/
+#define rmw_mmio_reg64(mem, a_mask, o_mask) \
+	do { \
+		u64 value = in_be64(&(mem)); \
+		value &= (u64)(a_mask); \
+		value |= (u64)(o_mask); \
+		out_be64(&(mem), value); \
+	} while (0)
+
+/**
+ * rmwb_mmio_reg64
+ *
+ * Set or unset a specified bit within a 64-bit MMIO register.
+ **/
+#define rmwb_mmio_reg64(mem, bit_num, set_bit) \
+	rmw_mmio_reg64((mem), ~(1UL << (63 - (bit_num))), \
+		       ((set_bit) << (63 - (bit_num))))
+
+/**
+ * passthru
+ *
+ * Enable or disable passthru mode in all the Cell signal islands.
+ **/
+static int passthru(u32 cpu, u64 enable)
+{
+	struct cbe_ppe_priv_regs __iomem *ppe_priv_regs;
+	struct cbe_pmd_regs __iomem *pmd_regs;
+	struct cbe_mic_tm_regs __iomem *mic_tm_regs;
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	ppe_priv_regs = info->get_cpu_ppe_priv_regs(cpu);
+	pmd_regs = info->get_cpu_pmd_regs(cpu);
+	mic_tm_regs = info->get_cpu_mic_tm_regs(cpu);
+
+	if (!ppe_priv_regs || !pmd_regs || !mic_tm_regs) {
+		PFM_ERR("Error getting Cell PPE, PMD, and MIC "
+			"register maps: 0x%p, 0x%p, 0x%p",
+			ppe_priv_regs, pmd_regs, mic_tm_regs);
+		return -EINVAL;
+	}
+
+	rmwb_mmio_reg64(ppe_priv_regs->L2_debug1, 61, enable);
+	rmwb_mmio_reg64(ppe_priv_regs->ciu_dr1, 5, enable);
+	rmwb_mmio_reg64(pmd_regs->on_ramp_trace, 39, enable);
+	rmwb_mmio_reg64(mic_tm_regs->MBL_debug, 20, enable);
+
+	return 0;
+}
+
+#define passthru_enable(cpu)  passthru(cpu, 1)
+#define passthru_disable(cpu) passthru(cpu, 0)
+
+static inline void reset_signal_registers(u32 cpu)
+{
+	rmw_spr(SPRN_HID1, HID1_RESET_MASK, 0);
+}
+
+/**
+ * celleb_reset_signals
+ *
+ * Non-rtas version of resetting the debug-bus signals.
+ **/
+static int celleb_reset_signals(u32 cpu)
+{
+	int rc;
+	rc = passthru_disable(cpu);
+	if (!rc)
+		reset_signal_registers(cpu);
+	return rc;
+}
+
+/**
+ * ppu_selection
+ *
+ * Write the HID1 register to connect the specified PPU signal-group to the
+ * debug-bus.
+ **/
+static int ppu_selection(struct cell_rtas_arg *signal)
+{
+	u64 hid1_enable_word = 0;
+	u64 hid1_enable_mask = 0;
+
+	switch (signal->signal_group) {
+
+	case SIG_GROUP_PPU_IU1: /* 2.1 PPU Instruction Unit - Group 1 */
+		switch (signal->bus_word) {
+		case BUS_WORD_0:
+			hid1_enable_mask = PPU_IU1_WORD0_HID1_EN_MASK;
+			hid1_enable_word = PPU_IU1_WORD0_HID1_EN_WORD;
+			break;
+		case BUS_WORD_1:
+			hid1_enable_mask = PPU_IU1_WORD1_HID1_EN_MASK;
+			hid1_enable_word = PPU_IU1_WORD1_HID1_EN_WORD;
+			break;
+		default:
+			PFM_ERR("Invalid bus-word (0x%x) for signal-group %d.",
+				signal->bus_word, signal->signal_group);
+			return -EINVAL;
+		}
+		break;
+
+	case SIG_GROUP_PPU_XU:  /* 2.2 PPU Execution Unit */
+		switch (signal->bus_word) {
+		case BUS_WORD_0:
+			hid1_enable_mask = PPU_XU_WORD0_HID1_EN_MASK;
+			hid1_enable_word = PPU_XU_WORD0_HID1_EN_WORD;
+			break;
+		case BUS_WORD_1:
+			hid1_enable_mask = PPU_XU_WORD1_HID1_EN_MASK;
+			hid1_enable_word = PPU_XU_WORD1_HID1_EN_WORD;
+			break;
+		default:
+			PFM_ERR("Invalid bus-word (0x%x) for signal-group %d.",
+				signal->bus_word, signal->signal_group);
+			return -EINVAL;
+		}
+		break;
+
+	default:
+		PFM_ERR("Signal-group %d not implemented.",
+			signal->signal_group);
+		return -EINVAL;
+	}
+
+	rmw_spr(SPRN_HID1, hid1_enable_mask, hid1_enable_word);
+
+	return 0;
+}
+
+/**
+ * celleb_activate_signals
+ *
+ * Non-rtas version of activating the debug-bus signals.
+ **/
+static int celleb_activate_signals(struct cell_rtas_arg *signals,
+				   int num_signals)
+{
+	int i, rc = -EINVAL;
+
+	for (i = 0; i < num_signals; i++) {
+		switch (signals[i].signal_group) {
+
+		/* 2.x PowerPC Processor Unit (PPU) Signal Selection */
+		case SIG_GROUP_PPU_IU1:
+		case SIG_GROUP_PPU_XU:
+			rc = ppu_selection(signals + i);
+			if (rc)
+				return rc;
+			break;
+
+		default:
+			PFM_ERR("Signal-group %d not implemented.",
+				signals[i].signal_group);
+			return -EINVAL;
+		}
+	}
+
+	if (0 < i)
+		rc = passthru_enable(signals[0].cpu);
+
+	return rc;
+}
+
+/**
+ * ps3_reset_signals
+ *
+ * ps3 version of resetting the debug-bus signals.
+ **/
+static int ps3_reset_signals(u32 cpu)
+{
+#ifdef CONFIG_PPC_PS3
+	return ps3_set_signal(0, 0, 0, 0);
+#else
+	return 0;
+#endif
+}
+
+/**
+ * ps3_activate_signals
+ *
+ * ps3 version of activating the debug-bus signals.
+ **/
+static int ps3_activate_signals(struct cell_rtas_arg *signals,
+				int num_signals)
+{
+#ifdef CONFIG_PPC_PS3
+	int i;
+
+	for (i = 0; i < num_signals; i++)
+		ps3_set_signal(signals[i].signal_group, signals[i].bit,
+			       signals[i].sub_unit, signals[i].bus_word);
+#endif
+	return 0;
+}
+
+
+/**
+ * reset_signals
+ *
+ * Call to the firmware (if available) to reset the debug-bus signals.
+ * Otherwise call the built-in version.
+ **/
+int reset_signals(u32 cpu)
+{
+	int rc;
+
+	if (machine_is(celleb))
+		rc = celleb_reset_signals(cpu);
+	else if (machine_is(ps3))
+		rc = ps3_reset_signals(cpu);
+	else
+		rc = rtas_reset_signals(cpu);
+
+	return rc;
+}
+
+/**
+ * activate_signals
+ *
+ * Call to the firmware (if available) to activate the debug-bus signals.
+ * Otherwise call the built-in version.
+ **/
+int activate_signals(struct cell_rtas_arg *signals, int num_signals)
+{
+	int rc;
+
+	if (machine_is(celleb))
+		rc = celleb_activate_signals(signals, num_signals);
+	else if (machine_is(ps3))
+		rc = ps3_activate_signals(signals, num_signals);
+	else
+		rc = rtas_activate_signals(signals, num_signals);
+
+	return rc;
+}
+
+/**
+ *  pfm_cell_pmc_check
+ *
+ * Verify that we are going to write a valid value to the specified PMC.
+ **/
+int pfm_cell_pmc_check(struct pfm_context *ctx,
+		       struct pfm_event_set *set,
+		       struct pfarg_pmc *req)
+{
+	u16 cnum, reg_num = req->reg_num;
+	s16 signal_group = RTAS_SIGNAL_GROUP(req->reg_value);
+	u8 bus_word = RTAS_BUS_WORD(req->reg_value);
+
+	if (reg_num < NR_CTRS || reg_num >= (NR_CTRS * 2)) {
+		return -EINVAL;
+	}
+
+	switch (signal_group) {
+	case SIG_GROUP_PPU_IU1:
+	case SIG_GROUP_PPU_XU:
+		if ((bus_word != 0) && (bus_word != 1)) {
+			PFM_ERR("Invalid bus word (%d) for signal-group %d",
+				bus_word, signal_group);
+			return -EINVAL;
+		}
+		break;
+	default:
+		PFM_ERR("Signal-group %d not implemented.", signal_group);
+		return -EINVAL;
+	}
+
+	for (cnum = NR_CTRS; cnum < (NR_CTRS * 2); cnum++) {
+		if (test_bit(cnum, cast_ulp(set->used_pmcs)) &&
+		    bus_word == RTAS_BUS_WORD(set->pmcs[cnum]) &&
+		    signal_group != RTAS_SIGNAL_GROUP(set->pmcs[cnum])) {
+			PFM_ERR("Impossible signal-group combination: "
+				"(%u,%u,%d) (%u,%u,%d)",
+				reg_num, bus_word, signal_group, cnum,
+				RTAS_BUS_WORD(set->pmcs[cnum]),
+				RTAS_SIGNAL_GROUP(set->pmcs[cnum]));
+			return  -EBUSY;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * write_pm07_event
+ *
+ * Pull out the RTAS arguments from the 64-bit register value and make the
+ * RTAS activate-signals call.
+ **/
+static void write_pm07_event(int cpu, unsigned int ctr, u64 value)
+{
+	struct cell_rtas_arg signal;
+	s32 signal_number;
+	int rc;
+
+	signal_number = RTAS_SIGNAL_NUMBER(value);
+	if (!signal_number) {
+		/* Don't include counters that are counting cycles. */
+		return;
+	}
+
+	signal.cpu = RTAS_CPU(cpu);
+	signal.bus_word = 1 << RTAS_BUS_WORD(value);
+	signal.sub_unit = RTAS_SUB_UNIT(value);
+	signal.signal_group = signal_number / 100;
+	signal.bit = signal_number % 100;
+
+	rc = activate_signals(&signal, 1);
+	if (rc) {
+		PFM_WARN("%s(%d, %u, %lu): Error calling "
+			 "activate_signals(): %d\n", __FUNCTION__,
+			 cpu, ctr, (unsigned long)value, rc);
+		/* FIX: Could we change this routine to return an error? */
+	}
+}
+
+/**
+ * pfm_cell_probe_pmu
+ *
+ * Simply check the processor version register to see if we're currently
+ * on a Cell system.
+ **/
+static int pfm_cell_probe_pmu(void)
+{
+	unsigned long pvr = mfspr(SPRN_PVR);
+
+	if (PVR_VER(pvr) != PV_BE)
+		return -1;
+
+	return 0;
+}
+
+/**
+ * pfm_cell_write_pmc
+ **/
+static void pfm_cell_write_pmc(unsigned int cnum, u64 value)
+{
+	int cpu = smp_processor_id();
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	if (cnum < NR_CTRS) {
+		info->write_pm07_control(cpu, cnum, value);
+
+	} else if (cnum < NR_CTRS * 2) {
+		write_pm07_event(cpu, cnum - NR_CTRS, value);
+
+	} else if (cnum == CELL_PMC_PM_STATUS) {
+		/* The pm_status register must be treated separately from
+		 * the other "global" PMCs. This call will ensure that
+		 * the interrupts are routed to the correct CPU, as well
+		 * as writing the desired value to the pm_status register.
+		 */
+		info->enable_pm_interrupts(cpu, info->get_hw_thread_id(cpu),
+					   value);
+
+	} else if (cnum < PFM_PM_NUM_PMCS) {
+		info->write_pm(cpu, cnum - (NR_CTRS * 2), value);
+	}
+}
+
+/**
+ * pfm_cell_write_pmd
+ **/
+static void pfm_cell_write_pmd(unsigned int cnum, u64 value)
+{
+	int cpu = smp_processor_id();
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	if (cnum < NR_CTRS) {
+		info->write_ctr(cpu, cnum, value);
+	}
+}
+
+/**
+ * pfm_cell_read_pmd
+ **/
+static u64 pfm_cell_read_pmd(unsigned int cnum)
+{
+	int cpu = smp_processor_id();
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	if (cnum < NR_CTRS) {
+		return info->read_ctr(cpu, cnum);
+	}
+
+	return -EINVAL;
+}
+
+/**
+ * pfm_cell_enable_counters
+ *
+ * Just need to turn on the global disable bit in pm_control.
+ **/
+static void pfm_cell_enable_counters(struct pfm_context *ctx,
+				     struct pfm_event_set *set)
+{
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	if (set->priv_flags & PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE)
+	       return ;
+
+	info->enable_pm(smp_processor_id());
+}
+
+/**
+ * pfm_cell_disable_counters
+ *
+ * Just need to turn off the global disable bit in pm_control.
+ **/
+static void pfm_cell_disable_counters(struct pfm_context *ctx,
+				      struct pfm_event_set *set)
+{
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	info->disable_pm(smp_processor_id());
+	if (machine_is(ps3))
+		reset_signals(smp_processor_id());
+}
+
+/*
+ * Return the thread id of the specified ppu signal.
+ */
+static inline u32 get_target_ppu_thread_id(u32 group, u32 bit)
+{
+	if ((group == SIG_GROUP_PPU_IU1 &&
+	     bit < PFM_PPU_IU1_THREAD1_BASE_BIT) ||
+	    (group == SIG_GROUP_PPU_XU &&
+	     bit < PFM_PPU_XU_THREAD1_BASE_BIT))
+		return 0;
+	else
+		return 1;
+}
+
+/*
+ * Return whether the specified counter is for PPU signal group.
+ */
+static inline int is_counter_for_ppu_sig_grp(u32 counter_control, u32 sig_grp)
+{
+	if (!(counter_control & CBE_PM_CTR_INPUT_CONTROL) &&
+	    (counter_control & CBE_PM_CTR_ENABLE) &&
+	    ((sig_grp == SIG_GROUP_PPU_IU1) || (sig_grp == SIG_GROUP_PPU_XU)))
+		return 1;
+	else
+		return 0;
+}
+
+/*
+ * Search ppu signal groups.
+ */
+static int get_ppu_signal_groups(struct pfm_event_set *set,
+				 u32 *ppu_sig_grp0, u32 *ppu_sig_grp1)
+{
+	u64 pm_event, *used_pmcs = set->used_pmcs;
+	int i, j;
+	u32 grp0_wd, grp1_wd, wd, sig_grp;
+
+	*ppu_sig_grp0 = 0;
+	*ppu_sig_grp1 = 0;
+	grp0_wd = PFM_GROUP_CONTROL_GROUP0_WORD(
+		set->pmcs[CELL_PMC_GROUP_CONTROL]);
+	grp1_wd = PFM_GROUP_CONTROL_GROUP1_WORD(
+		set->pmcs[CELL_PMC_GROUP_CONTROL]);
+
+	for (i = 0, j = 0; (i < NR_CTRS) && (j < PFM_NUM_OF_GROUPS); i++) {
+		if (test_bit(i + NR_CTRS, used_pmcs)) {
+			pm_event = set->pmcs[i + NR_CTRS];
+			wd = PFM_EVENT_PMC_BUS_WORD(pm_event);
+			sig_grp = PFM_EVENT_PMC_SIGNAL_GROUP(pm_event);
+			if ((sig_grp == SIG_GROUP_PPU_IU1) ||
+			    (sig_grp == SIG_GROUP_PPU_XU)) {
+
+				if (wd == grp0_wd && *ppu_sig_grp0 == 0) {
+					*ppu_sig_grp0 = sig_grp;
+					j++;
+				} else if (wd == grp1_wd &&
+					   *ppu_sig_grp1 == 0) {
+					*ppu_sig_grp1 = sig_grp;
+					j++;
+				}
+			}
+		}
+	}
+	return j;
+}
+
+/**
+ * pfm_cell_restore_pmcs
+ *
+ * Write all control register values that are saved in the specified event
+ * set. We could use the pfm_arch_write_pmc() function to restore each PMC
+ * individually (as is done in other architectures), but that results in
+ * multiple RTAS calls. As an optimization, we will setup the RTAS argument
+ * array so we can do all event-control registers in one RTAS call.
+ *
+ * In per-thread mode,
+ * The counter enable bit of the pmX_control PMC is enabled while the target
+ * task runs on the target HW thread.
+ **/
+void pfm_cell_restore_pmcs(struct pfm_event_set *set)
+{
+	u64 ctr_ctrl;
+	u64 *used_pmcs = set->used_pmcs;
+	int i;
+	int cpu = smp_processor_id();
+	u32 current_th_id;
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	for (i = 0; i < NR_CTRS; i++) {
+		ctr_ctrl = set->pmcs[i];
+
+		if (ctr_ctrl & PFM_COUNTER_CTRL_PMC_PPU_TH0) {
+			current_th_id = info->get_hw_thread_id(cpu);
+
+			/*
+			 * Set the counter enable bit down if the current
+			 * HW thread is NOT 0
+			 **/
+			if (current_th_id)
+				ctr_ctrl = ctr_ctrl & ~CBE_PM_CTR_ENABLE;
+
+		} else if (ctr_ctrl & PFM_COUNTER_CTRL_PMC_PPU_TH1) {
+			current_th_id = info->get_hw_thread_id(cpu);
+
+			/*
+			 * Set the counter enable bit down if the current
+			 * HW thread is 0
+			 **/
+			if (!current_th_id)
+				ctr_ctrl = ctr_ctrl & ~CBE_PM_CTR_ENABLE;
+		}
+
+		/* Write the per-counter control register. If the PMC is not
+		 * in use, then it will simply clear the register, which will
+		 * disable the associated counter.
+		 */
+		info->write_pm07_control(cpu, i, ctr_ctrl);
+
+		if (test_bit(i + NR_CTRS, used_pmcs))
+			write_pm07_event(cpu, 0, set->pmcs[i + NR_CTRS]);
+	}
+
+	/* Write all the global PMCs. Need to call pfm_cell_write_pmc()
+	 * instead of cbe_write_pm() due to special handling for the
+	 * pm_status register.
+	 */
+	for (i *= 2; i < PFM_PM_NUM_PMCS; i++)
+		pfm_cell_write_pmc(i, set->pmcs[i]);
+}
+
+/**
+ * pfm_cell_load_context
+ *
+ * In per-thread mode,
+ *  The pmX_control PMCs which are used for PPU IU/XU event are marked with
+ *  the thread id(PFM_COUNTER_CTRL_PMC_PPU_TH0/TH1).
+ **/
+static int pfm_cell_load_context(struct pfm_context *ctx,
+				 struct pfm_event_set *set,
+				 struct task_struct *task)
+{
+	int i;
+	u32 ppu_sig_grp[PFM_NUM_OF_GROUPS] = {SIG_GROUP_NONE, SIG_GROUP_NONE};
+	u32 bit;
+	int index;
+	u32 target_th_id;
+	int ppu_sig_num = 0;
+	struct pfm_event_set *s;
+
+	if (ctx->flags.system)
+		return 0;
+
+	list_for_each_entry(s, &ctx->set_list, list) {
+		ppu_sig_num = get_ppu_signal_groups(s, &ppu_sig_grp[0],
+						    &ppu_sig_grp[1]);
+
+		for (i = 0; i < NR_CTRS; i++) {
+			index = PFM_PM_CTR_INPUT_MUX_GROUP_INDEX(s->pmcs[i]);
+			if (ppu_sig_num &&
+			    (ppu_sig_grp[index] != SIG_GROUP_NONE) &&
+			    is_counter_for_ppu_sig_grp(s->pmcs[i],
+						       ppu_sig_grp[index])) {
+
+				bit = PFM_PM_CTR_INPUT_MUX_BIT(s->pmcs[i]);
+				target_th_id = get_target_ppu_thread_id(
+					ppu_sig_grp[index], bit);
+				if (!target_th_id)
+					s->pmcs[i] |=
+						PFM_COUNTER_CTRL_PMC_PPU_TH0;
+				else
+					s->pmcs[i] |=
+						PFM_COUNTER_CTRL_PMC_PPU_TH1;
+				PFM_DBG("set:%d mark ctr:%d target_thread:%d",
+					s->id, i, target_th_id);
+			}
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * pfm_cell_unload_context
+ *
+ * For system-wide contexts and self-monitored contexts, make the RTAS call
+ * to reset the debug-bus signals.
+ *
+ * For non-self-monitored contexts, the monitored thread will already have
+ * been taken off the CPU and we don't need to do anything additional.
+ **/
+static int pfm_cell_unload_context(struct pfm_context *ctx,
+				   struct task_struct *task)
+{
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	if (task == current || ctx->flags.system) {
+		reset_signals(smp_processor_id());
+	}
+
+	/*
+	 * At the end of monitoring SPE events,
+	 * detaching pfm_ctx from the target task may not be done correctly.
+	 * So, Clear all signals and disable PM
+	 * at here.
+	 */
+	info->disable_pm(smp_processor_id());
+	reset_signals(smp_processor_id());
+
+	return 0;
+}
+
+/**
+ * pfm_cell_ctxswout_thread
+ *
+ * When a monitored thread is switched out (self-monitored or externally
+ * monitored) we need to reset the debug-bus signals so the next context that
+ * gets switched in can start from a clean set of signals.
+ **/
+int pfm_cell_ctxswout_thread(struct task_struct *task,
+			     struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	reset_signals(smp_processor_id());
+	return 0;
+}
+
+/**
+ * pfm_cell_get_ovfl_pmds
+ *
+ * Determine which counters in this set have overflowed and fill in the
+ * set->povfl_pmds mask and set->npend_ovfls count. On Cell, the pm_status
+ * register contains a bit for each counter to indicate overflow. However,
+ * those 8 bits are in the reverse order than what Perfmon2 is expecting,
+ * so we need to reverse the order of the overflow bits.
+ **/
+static void pfm_cell_get_ovfl_pmds(struct pfm_context *ctx,
+				   struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
+	u32 pm_status, ovfl_ctrs;
+	u64 povfl_pmds = 0;
+	int i;
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	if (!ctx_arch->last_read_updated)
+		/* This routine was not called via the interrupt handler.
+		 * Need to start by getting interrupts and updating
+		 * last_read_pm_status.
+		 */
+		ctx_arch->last_read_pm_status =
+ info->get_and_clear_pm_interrupts(smp_processor_id());
+
+	/* Reset the flag that the interrupt handler last read pm_status. */
+	ctx_arch->last_read_updated = 0;
+
+	pm_status = ctx_arch->last_read_pm_status &
+		    set->pmcs[CELL_PMC_PM_STATUS];
+	ovfl_ctrs = CBE_PM_OVERFLOW_CTRS(pm_status);
+
+	/* Reverse the order of the bits in ovfl_ctrs
+	 * and store the result in povfl_pmds.
+	 */
+	for (i = 0; i < PFM_PM_NUM_PMDS; i++) {
+		povfl_pmds = (povfl_pmds << 1) | (ovfl_ctrs & 1);
+		ovfl_ctrs >>= 1;
+	}
+
+	/* Mask povfl_pmds with set->used_pmds to get set->povfl_pmds.
+	 * Count the bits set in set->povfl_pmds to get set->npend_ovfls.
+	 */
+	bitmap_and(set->povfl_pmds, &povfl_pmds,
+		   set->used_pmds, PFM_PM_NUM_PMDS);
+	set->npend_ovfls = bitmap_weight(set->povfl_pmds, PFM_PM_NUM_PMDS);
+}
+
+/**
+ * pfm_cell_acquire_pmu
+ *
+ * acquire PMU resource.
+ * This acquisition is done when the first context is created.
+ **/
+int pfm_cell_acquire_pmu(void)
+{
+#ifdef CONFIG_PPC_PS3
+	int ret;
+
+	if (machine_is(ps3)) {
+		PFM_DBG("");
+		ret = ps3_lpm_open(PS3_LPM_TB_TYPE_INTERNAL, NULL, 0);
+		if (ret) {
+			PFM_ERR("Can't open PS3 lpm. error:%d", ret);
+			return -EFAULT;
+		}
+	}
+#endif
+	return 0;
+}
+
+/**
+ * pfm_cell_release_pmu
+ *
+ * release PMU resource.
+ * actual release happens when last context is destroyed
+ **/
+void pfm_cell_release_pmu(void)
+{
+#ifdef CONFIG_PPC_PS3
+	if (machine_is(ps3))
+		ps3_lpm_close();
+#endif
+}
+
+/**
+ * handle_trace_buffer_interrupts
+ *
+ * This routine is for processing just the interval timer and trace buffer
+ * overflow interrupts. Performance counter interrupts are handled by the
+ * perf_irq_handler() routine, which reads and saves the pm_status register.
+ * This routine should not read the actual pm_status register, but rather
+ * the value passed in.
+ **/
+static void handle_trace_buffer_interrupts(unsigned long iip,
+					   struct pt_regs *regs,
+					   struct pfm_context *ctx,
+					   u32 pm_status)
+{
+	/* FIX: Currently ignoring trace-buffer interrupts. */
+	return;
+}
+
+/**
+ * pfm_cell_irq_handler
+ *
+ * Handler for all Cell performance-monitor interrupts.
+ **/
+static void pfm_cell_irq_handler(struct pt_regs *regs, struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
+	u32 last_read_pm_status;
+	int cpu = smp_processor_id();
+	struct pfm_cell_platform_pmu_info *info =
+		((struct pfm_arch_pmu_info *)
+		 (pfm_pmu_conf->arch_info))->platform_info;
+
+	/* Need to disable and reenable the performance counters to get the
+	 * desired behavior from the hardware. This is specific to the Cell
+	 * PMU hardware.
+	 */
+	info->disable_pm(cpu);
+
+	/* Read the pm_status register to get the interrupt bits. If a
+	 * perfmormance counter overflow interrupt occurred, call the core
+	 * perfmon interrupt handler to service the counter overflow. If the
+	 * interrupt was for the interval timer or the trace_buffer,
+	 * call the interval timer and trace buffer interrupt handler.
+	 *
+	 * The value read from the pm_status register is stored in the
+	 * pmf_arch_context structure for use by other routines. Note that
+	 * reading the pm_status register resets the interrupt flags to zero.
+	 * Hence, it is important that the register is only read in one place.
+	 *
+	 * The pm_status reg interrupt reg format is:
+	 * [pmd0:pmd1:pmd2:pmd3:pmd4:pmd5:pmd6:pmd7:intt:tbf:tbu:]
+	 * - pmd0 to pm7 are the perf counter overflow interrupts.
+	 * - intt is the interval timer overflowed interrupt.
+	 * - tbf is the trace buffer full interrupt.
+	 * - tbu is the trace buffer underflow interrupt.
+	 * - The pmd0 bit is the MSB of the 32 bit register.
+	 */
+	ctx_arch->last_read_pm_status = last_read_pm_status =
+			info->get_and_clear_pm_interrupts(cpu);
+
+	/* Set flag for pfm_cell_get_ovfl_pmds() routine so it knows
+	 * last_read_pm_status was updated by the interrupt handler.
+	 */
+	ctx_arch->last_read_updated = 1;
+
+	if (last_read_pm_status & CBE_PM_ALL_OVERFLOW_INTR)
+		/* At least one counter overflowed. */
+		pfm_interrupt_handler(instruction_pointer(regs), regs);
+
+	if (last_read_pm_status & (CBE_PM_INTERVAL_INTR |
+				   CBE_PM_TRACE_BUFFER_FULL_INTR |
+				   CBE_PM_TRACE_BUFFER_UNDERFLOW_INTR))
+		/* Trace buffer or interval timer overflow. */
+		handle_trace_buffer_interrupts(instruction_pointer(regs),
+					       regs, ctx, last_read_pm_status);
+
+	/* The interrupt settings is the value written to the pm_status
+	 * register. It is saved in the context when the register is
+	 * written.
+	 */
+	info->enable_pm_interrupts(cpu, info->get_hw_thread_id(cpu),
+	ctx->active_set->pmcs[CELL_PMC_PM_STATUS]);
+
+	/* The writes to the various performance counters only writes to a
+	 * latch. The new values (interrupt setting bits, reset counter value
+	 * etc.) are not copied to the actual registers until the performance
+	 * monitor is enabled. In order to get this to work as desired, the
+	 * permormance monitor needs to be disabled while writting to the
+	 * latches. This is a HW design issue.
+	 */
+	info->enable_pm(cpu);
+}
+
+
+static inline u64 update_sub_unit_field(u64 pm_event, u64 spe_id)
+{
+	return ((pm_event & 0xFFFF0000FFFFFFFF) | (spe_id << 32));
+}
+
+static void pfm_prepare_ctxswin_thread(struct pfm_context *ctx, u64 spe_id)
+{
+	struct pfm_event_set *set;
+	int i;
+	u64 signal_group;
+
+	spin_lock(&ctx->lock);
+
+	set = ctx->active_set;
+	while (set != NULL) {
+		for (i = 8; i < 16; i++) {
+			signal_group = set->pmcs[i] & 0x00000000FFFFFFFF;
+			signal_group = signal_group / 100;
+
+			/*
+			 * If the target event is included SPE signal group,
+			 * The sub_unit field in pmX_event pmc is changed to the
+			 * specified spe_id.
+			 */
+			if (SIG_GROUP_SPU_BASE < signal_group &&
+			    signal_group < SIG_GROUP_EIB_BASE) {
+				set->pmcs[i] = update_sub_unit_field(
+					set->pmcs[i], spe_id);
+				set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
+				set->priv_flags &=
+					~PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE;
+				PFM_DBG("pmcs[%d] : 0x%lx", i, set->pmcs[i]);
+			}
+		}
+		if (set->id_next == ctx->active_set->id)
+			break;
+
+		set = list_first_entry(&set->list, struct pfm_event_set, list);
+	}
+
+	spin_unlock(&ctx->lock);
+}
+
+static int pfm_spe_get_pid(void *arg)
+{
+	return ((struct spu *)arg)->pid;
+}
+
+static int pfm_spe_ctxsw(struct notifier_block *block,
+			 unsigned long object_id,
+			 struct task_struct *p, void *arg)
+{
+	struct spu *spu;
+	u64 spe_id;
+
+	spu = arg;
+
+#ifdef CONFIG_PPC_PS3
+	if (machine_is(ps3))
+		spe_id = ps3_get_spe_id(arg);
+	else
+		spe_id = spu->spe_id;
+#else
+	spe_id = spu->spe_id;
+#endif
+
+	if (!p->pfm_context) {
+		PFM_DBG("=== Ignore PFM SPE CTXSW ===");
+		return 0;
+	}
+
+	if (object_id) {
+		PFM_DBG("=== PFM SPE CTXSWIN === 0x%lx", spe_id);
+		if (p->pfm_context)
+			pfm_prepare_ctxswin_thread(p->pfm_context, spe_id);
+		pfm_ctxsw(NULL, p);
+
+	} else {
+		PFM_DBG("=== PFM SPE CTXSWOUT === 0x%lx", spe_id);
+		pfm_ctxsw(p, NULL);
+	}
+
+	return 0;
+}
+
+/*
+ * This function returns whether spe ctxsw should be used for
+ * the pfm context switch.
+ *
+ * The following implementation is very tentative.
+ *  If a pfm target event which is described in pmcs[8] - pmc[16](pmX_event reg)
+ *  is included in SPE signal group, this function returns 1.
+ */
+static int pfm_is_ctxsw_type_spe(struct pfm_event_set *set)
+{
+	int i;
+	u64 signal_group;
+
+	for (i = 8; i < 16; i++) {
+		signal_group = set->pmcs[i] & 0x00000000FFFFFFFF;
+		signal_group = signal_group / 100;
+		if (SIG_GROUP_SPU_BASE < signal_group &&
+		    signal_group < SIG_GROUP_EIB_BASE)
+			return 1;
+	}
+	return 0;
+}
+
+static int pfm_cell_add_ctxsw_hook(struct pfm_context *ctx)
+{
+	int ret;
+
+	if (!pfm_is_ctxsw_type_spe(ctx->active_set)) {
+		set_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
+		return 0;
+	}
+
+	if (ctx->ctxsw_notifier.notifier_call) {
+		set_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
+		return 0;
+	}
+	ctx->ctxsw_notifier.notifier_call = pfm_arch_ctxsw;
+	ctx->ctxsw_notifier.next = NULL;
+	ctx->ctxsw_notifier.priority = 0;
+	if (!ctx->flags.system)
+		ctx->active_set->priv_flags |=
+			PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE;
+
+	ret = spu_switch_event_register(&ctx->ctxsw_notifier);
+	if (ret) {
+		PFM_ERR("Can't register spe_notifier\n");
+		ctx->ctxsw_notifier.notifier_call = NULL;
+		ctx->active_set->priv_flags &=
+			~PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE;
+	}
+
+	return ret;
+}
+
+static int pfm_cell_remove_ctxsw_hook(struct pfm_context *ctx)
+{
+	int ret;
+
+	if (ctx->ctxsw_notifier.notifier_call == NULL) {
+		if (ctx->task)
+			clear_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
+		return 0;
+	}
+	ret = spu_switch_event_unregister(&ctx->ctxsw_notifier);
+	if (ret)
+		PFM_ERR("Can't unregister spe_notifier\n");
+	ctx->ctxsw_notifier.notifier_call = NULL;
+	ctx->ctxsw_notifier.next = NULL;
+	ctx->ctxsw_notifier.priority = 0;
+
+	return ret;
+}
+
+
+
+static struct pfm_cell_platform_pmu_info ps3_platform_pmu_info = {
+#ifdef CONFIG_PPC_PS3
+	.read_ctr                    = ps3_read_ctr,
+	.write_ctr                   = ps3_write_ctr,
+	.write_pm07_control          = ps3_write_pm07_control,
+	.write_pm                    = ps3_write_pm,
+	.enable_pm                   = ps3_enable_pm,
+	.disable_pm                  = ps3_disable_pm,
+	.enable_pm_interrupts        = ps3_enable_pm_interrupts,
+	.get_and_clear_pm_interrupts = ps3_get_and_clear_pm_interrupts,
+	.get_hw_thread_id            = ps3_get_hw_thread_id,
+	.get_cpu_ppe_priv_regs       = NULL,
+	.get_cpu_pmd_regs            = NULL,
+	.get_cpu_mic_tm_regs         = NULL,
+	.rtas_token                  = NULL,
+	.rtas_call                   = NULL,
+#endif
+};
+
+static struct pfm_cell_platform_pmu_info native_platform_pmu_info = {
+#ifdef CONFIG_PPC_CELL_NATIVE
+	.read_ctr                    = cbe_read_ctr,
+	.write_ctr                   = cbe_write_ctr,
+	.write_pm07_control          = cbe_write_pm07_control,
+	.write_pm                    = cbe_write_pm,
+	.enable_pm                   = cbe_enable_pm,
+	.disable_pm                  = cbe_disable_pm,
+	.enable_pm_interrupts        = cbe_enable_pm_interrupts,
+	.get_and_clear_pm_interrupts = cbe_get_and_clear_pm_interrupts,
+	.get_hw_thread_id            = cbe_get_hw_thread_id,
+	.get_cpu_ppe_priv_regs       = cbe_get_cpu_ppe_priv_regs,
+	.get_cpu_pmd_regs            = cbe_get_cpu_pmd_regs,
+	.get_cpu_mic_tm_regs         = cbe_get_cpu_mic_tm_regs,
+	.rtas_token                  = rtas_token,
+	.rtas_call                   = rtas_call,
+#endif
+};
+
+static struct pfm_arch_pmu_info pfm_cell_pmu_info = {
+	.pmu_style        = PFM_POWERPC_PMU_CELL,
+	.add_ctxsw_hook   = pfm_cell_add_ctxsw_hook,
+	.remove_ctxsw_hook = pfm_cell_remove_ctxsw_hook,
+	.ctxsw             = pfm_spe_ctxsw,
+	.get_pid           = pfm_spe_get_pid,
+	.acquire_pmu      = pfm_cell_acquire_pmu,
+	.release_pmu      = pfm_cell_release_pmu,
+	.write_pmc        = pfm_cell_write_pmc,
+	.write_pmd        = pfm_cell_write_pmd,
+	.read_pmd         = pfm_cell_read_pmd,
+	.enable_counters  = pfm_cell_enable_counters,
+	.disable_counters = pfm_cell_disable_counters,
+	.irq_handler      = pfm_cell_irq_handler,
+	.get_ovfl_pmds    = pfm_cell_get_ovfl_pmds,
+	.restore_pmcs     = pfm_cell_restore_pmcs,
+	.ctxswout_thread  = pfm_cell_ctxswout_thread,
+	.load_context     = pfm_cell_load_context,
+	.unload_context   = pfm_cell_unload_context,
+};
+
+static struct pfm_pmu_config pfm_cell_pmu_conf = {
+	.pmu_name = "Cell",
+	.version = "0.1",
+	.counter_width = 32,
+	.pmd_desc = pfm_cell_pmd_desc,
+	.pmc_desc = pfm_cell_pmc_desc,
+	.num_pmc_entries = PFM_PM_NUM_PMCS,
+	.num_pmd_entries = PFM_PM_NUM_PMDS,
+	.probe_pmu  = pfm_cell_probe_pmu,
+	.arch_info = &pfm_cell_pmu_info,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+};
+
+/**
+ * pfm_cell_platform_probe
+ *
+ * If we're on a system without the firmware rtas call available, set up the
+ * PMC write-checker for all the pmX_event control registers.
+ **/
+static void pfm_cell_platform_probe(void)
+{
+	if (machine_is(celleb)) {
+		int cnum;
+		pfm_cell_pmu_conf.pmc_write_check = pfm_cell_pmc_check;
+		for (cnum = NR_CTRS; cnum < (NR_CTRS * 2); cnum++)
+			pfm_cell_pmc_desc[cnum].type |= PFM_REG_WC;
+	}
+
+	if (machine_is(ps3))
+		pfm_cell_pmu_info.platform_info = &ps3_platform_pmu_info;
+	else
+		pfm_cell_pmu_info.platform_info = &native_platform_pmu_info;
+}
+
+static int __init pfm_cell_pmu_init_module(void)
+{
+	pfm_cell_platform_probe();
+	return pfm_pmu_register(&pfm_cell_pmu_conf);
+}
+
+static void __exit pfm_cell_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_cell_pmu_conf);
+}
+
+module_init(pfm_cell_pmu_init_module);
+module_exit(pfm_cell_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/powerpc/perfmon/perfmon_power4.c linux-2.6.25-id/arch/powerpc/perfmon/perfmon_power4.c
--- linux-2.6.25-org/arch/powerpc/perfmon/perfmon_power4.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/perfmon/perfmon_power4.c	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,293 @@
+/*
+ * This file contains the POWER4 PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2007, IBM Corporation.
+ *
+ * Based on a simple modification of perfmon_power5.c for POWER4 by
+ * Corey Ashford <cjashfor@us.ibm.com>.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+MODULE_AUTHOR("Corey Ashford <cjashfor@us.ibm.com>");
+MODULE_DESCRIPTION("POWER4 PMU description table");
+MODULE_LICENSE("GPL");
+
+static struct pfm_regmap_desc pfm_power4_pmc_desc[]={
+/* mmcr0 */ PMC_D(PFM_REG_I, "MMCR0", MMCR0_FC, 0, 0, SPRN_MMCR0),
+/* mmcr1 */ PMC_D(PFM_REG_I, "MMCR1", 0, 0, 0, SPRN_MMCR1),
+/* mmcra */ PMC_D(PFM_REG_I, "MMCRA", 0, 0, 0, SPRN_MMCRA)
+};
+#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_power4_pmc_desc)
+
+/* The TB and PURR registers are read-only. Also, note that the TB register
+ * actually consists of both the 32-bit SPRN_TBRU and SPRN_TBRL registers.
+ * For Perfmon2's purposes, we'll treat it as a single 64-bit register.
+ */
+static struct pfm_regmap_desc pfm_power4_pmd_desc[]={
+/* tb    */ PMD_D((PFM_REG_I|PFM_REG_RO), "TB", SPRN_TBRL),
+/* pmd1  */ PMD_D(PFM_REG_C, "PMC1", SPRN_PMC1),
+/* pmd2  */ PMD_D(PFM_REG_C, "PMC2", SPRN_PMC2),
+/* pmd3  */ PMD_D(PFM_REG_C, "PMC3", SPRN_PMC3),
+/* pmd4  */ PMD_D(PFM_REG_C, "PMC4", SPRN_PMC4),
+/* pmd5  */ PMD_D(PFM_REG_C, "PMC5", SPRN_PMC5),
+/* pmd6  */ PMD_D(PFM_REG_C, "PMC6", SPRN_PMC6),
+/* pmd7  */ PMD_D(PFM_REG_C, "PMC7", SPRN_PMC7),
+/* pmd8  */ PMD_D(PFM_REG_C, "PMC8", SPRN_PMC8)
+};
+#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_power4_pmd_desc)
+
+static int pfm_power4_probe_pmu(void)
+{
+	unsigned long pvr = mfspr(SPRN_PVR);
+	int ver = PVR_VER(pvr);
+
+	if ((ver == PV_POWER4) || (ver == PV_POWER4p))
+		return 0;
+
+	return -1;
+}
+
+static void pfm_power4_write_pmc(unsigned int cnum, u64 value)
+{
+	switch (pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
+	case SPRN_MMCR0:
+		mtspr(SPRN_MMCR0, value);
+		break;
+	case SPRN_MMCR1:
+		mtspr(SPRN_MMCR1, value);
+		break;
+	case SPRN_MMCRA:
+		mtspr(SPRN_MMCRA, value);
+		break;
+	default:
+		BUG();
+	}
+}
+
+static void pfm_power4_write_pmd(unsigned int cnum, u64 value)
+{
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC1:
+		mtspr(SPRN_PMC1, value);
+		break;
+	case SPRN_PMC2:
+		mtspr(SPRN_PMC2, value);
+		break;
+	case SPRN_PMC3:
+		mtspr(SPRN_PMC3, value);
+		break;
+	case SPRN_PMC4:
+		mtspr(SPRN_PMC4, value);
+		break;
+	case SPRN_PMC5:
+		mtspr(SPRN_PMC5, value);
+		break;
+	case SPRN_PMC6:
+		mtspr(SPRN_PMC6, value);
+		break;
+	case SPRN_PMC7:
+		mtspr(SPRN_PMC7, value);
+		break;
+	case SPRN_PMC8:
+		mtspr(SPRN_PMC8, value);
+		break;
+	case SPRN_TBRL:
+	case SPRN_PURR:
+		/* Ignore writes to read-only registers. */
+		break;
+	default:
+		BUG();
+	}
+}
+
+static u64 pfm_power4_read_pmd(unsigned int cnum)
+{
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC1:
+		return mfspr(SPRN_PMC1);
+	case SPRN_PMC2:
+		return mfspr(SPRN_PMC2);
+	case SPRN_PMC3:
+		return mfspr(SPRN_PMC3);
+	case SPRN_PMC4:
+		return mfspr(SPRN_PMC4);
+	case SPRN_PMC5:
+		return mfspr(SPRN_PMC5);
+	case SPRN_PMC6:
+		return mfspr(SPRN_PMC6);
+	case SPRN_PMC7:
+		return mfspr(SPRN_PMC7);
+	case SPRN_PMC8:
+		return mfspr(SPRN_PMC8);
+	case SPRN_TBRL:
+		return ((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL);
+	case SPRN_PURR:
+		if (cpu_has_feature(CPU_FTR_PURR))
+			return mfspr(SPRN_PURR);
+		else
+			return 0;
+	default:
+		BUG();
+	}
+}
+
+/**
+ * pfm_power4_enable_counters
+ *
+ * Just need to load the current values into the control registers.
+ **/
+static void pfm_power4_enable_counters(struct pfm_context *ctx,
+				       struct pfm_event_set *set)
+{
+	unsigned int i, max_pmc;
+
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+
+	for (i = 0; i < max_pmc; i++)
+		if (test_bit(i, set->used_pmcs))
+			pfm_power4_write_pmc(i, set->pmcs[i]);
+}
+
+/**
+ * pfm_power4_disable_counters
+ *
+ * Just need to zero all the control registers.
+ **/
+static void pfm_power4_disable_counters(struct pfm_context *ctx,
+					struct pfm_event_set *set)
+{
+	unsigned int i, max;
+
+	max = pfm_pmu_conf->regs.max_pmc;
+
+	for (i = 0; i < max; i++)
+		if (test_bit(i, set->used_pmcs))
+			pfm_power4_write_pmc(i, 0);
+}
+
+/**
+ * pfm_power4_get_ovfl_pmds
+ *
+ * Determine which counters in this set have overflowed and fill in the
+ * set->povfl_pmds mask and set->npend_ovfls count.
+ **/
+static void pfm_power4_get_ovfl_pmds(struct pfm_context *ctx,
+				     struct pfm_event_set *set)
+{
+	unsigned int i;
+	unsigned int max_pmd = pfm_pmu_conf->regs.max_intr_pmd;
+	u64 *used_pmds = set->used_pmds;
+	u64 *cntr_pmds = pfm_pmu_conf->regs.cnt_pmds;
+	u64 width_mask = 1 << pfm_pmu_conf->counter_width;
+	u64 new_val, mask[PFM_PMD_BV];
+
+	bitmap_and(cast_ulp(mask), cast_ulp(cntr_pmds),
+		   cast_ulp(used_pmds), max_pmd);
+
+	for (i = 0; i < max_pmd; i++) {
+		if (test_bit(i, mask)) {
+			new_val = pfm_power4_read_pmd(i);
+			if (new_val & width_mask) {
+				set_bit(i, set->povfl_pmds);
+				set->npend_ovfls++;
+			}
+		}
+	}
+}
+
+static void pfm_power4_irq_handler(struct pt_regs *regs,
+				   struct pfm_context *ctx)
+{
+	u32 mmcr0;
+	u64 mmcra;
+
+	/* Disable the counters (set the freeze bit) to not polute
+	 * the counts.
+	 */
+	mmcr0 = mfspr(SPRN_MMCR0);
+	mtspr(SPRN_MMCR0, (mmcr0 | MMCR0_FC));
+	mmcra = mfspr(SPRN_MMCRA);
+
+	/* Set the PMM bit (see comment below). */
+	mtmsrd(mfmsr() | MSR_PMM);
+
+	pfm_interrupt_handler(instruction_pointer(regs), regs);
+
+	mmcr0 = mfspr(SPRN_MMCR0);
+	/* Reset the perfmon trigger. */
+	mmcr0 |= MMCR0_PMXE;
+
+	/*
+	 * We must clear the PMAO bit on some (GQ) chips. Just do it
+	 * all the time.
+	 */
+	mmcr0 &= ~MMCR0_PMAO;
+
+	/* Clear the appropriate bits in the MMCRA. */
+	mmcra &= POWER6_MMCRA_THRM | POWER6_MMCRA_OTHER;
+	mtspr(SPRN_MMCRA, mmcra);
+
+	/*
+	 * Now clear the freeze bit, counting will not start until we
+	 * rfid from this exception, because only at that point will
+	 * the PMM bit be cleared.
+	 */
+	mmcr0 &= ~MMCR0_FC;
+	mtspr(SPRN_MMCR0, mmcr0);
+}
+
+struct pfm_arch_pmu_info pfm_power4_pmu_info = {
+	.pmu_style        = PFM_POWERPC_PMU_POWER4,
+	.write_pmc        = pfm_power4_write_pmc,
+	.write_pmd        = pfm_power4_write_pmd,
+	.read_pmd         = pfm_power4_read_pmd,
+	.irq_handler      = pfm_power4_irq_handler,
+	.get_ovfl_pmds    = pfm_power4_get_ovfl_pmds,
+	.enable_counters  = pfm_power4_enable_counters,
+	.disable_counters = pfm_power4_disable_counters,
+};
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_power4_pmu_conf = {
+	.pmu_name = "POWER4",
+	.counter_width = 31,
+	.pmd_desc = pfm_power4_pmd_desc,
+	.pmc_desc = pfm_power4_pmc_desc,
+	.num_pmc_entries = PFM_PM_NUM_PMCS,
+	.num_pmd_entries = PFM_PM_NUM_PMDS,
+	.probe_pmu  = pfm_power4_probe_pmu,
+	.arch_info = &pfm_power4_pmu_info,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE
+};
+	
+static int __init pfm_power4_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_power4_pmu_conf);
+}
+
+static void __exit pfm_power4_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_power4_pmu_conf);
+}
+
+module_init(pfm_power4_pmu_init_module);
+module_exit(pfm_power4_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/powerpc/perfmon/perfmon_power5.c linux-2.6.25-id/arch/powerpc/perfmon/perfmon_power5.c
--- linux-2.6.25-org/arch/powerpc/perfmon/perfmon_power5.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/perfmon/perfmon_power5.c	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,292 @@
+/*
+ * This file contains the POWER5 PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2005 David Gibson, IBM Corporation.
+ *
+ * Based on perfmon_p6.c:
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+MODULE_AUTHOR("David Gibson <dwg@au1.ibm.com>");
+MODULE_DESCRIPTION("POWER5 PMU description table");
+MODULE_LICENSE("GPL");
+
+static struct pfm_regmap_desc pfm_power5_pmc_desc[]={
+/* mmcr0 */ PMC_D(PFM_REG_I, "MMCR0", MMCR0_FC, 0, 0, SPRN_MMCR0),
+/* mmcr1 */ PMC_D(PFM_REG_I, "MMCR1", 0, 0, 0, SPRN_MMCR1),
+/* mmcra */ PMC_D(PFM_REG_I, "MMCRA", 0, 0, 0, SPRN_MMCRA)
+};
+#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_power5_pmc_desc)
+
+/* The TB and PURR registers are read-only. Also, note that the TB register
+ * actually consists of both the 32-bit SPRN_TBRU and SPRN_TBRL registers.
+ * For Perfmon2's purposes, we'll treat it as a single 64-bit register.
+ */
+static struct pfm_regmap_desc pfm_power5_pmd_desc[]={
+/* tb    */ PMD_D((PFM_REG_I|PFM_REG_RO), "TB", SPRN_TBRL),
+/* pmd1  */ PMD_D(PFM_REG_C, "PMC1", SPRN_PMC1),
+/* pmd2  */ PMD_D(PFM_REG_C, "PMC2", SPRN_PMC2),
+/* pmd3  */ PMD_D(PFM_REG_C, "PMC3", SPRN_PMC3),
+/* pmd4  */ PMD_D(PFM_REG_C, "PMC4", SPRN_PMC4),
+/* pmd5  */ PMD_D(PFM_REG_C, "PMC5", SPRN_PMC5),
+/* pmd6  */ PMD_D(PFM_REG_C, "PMC6", SPRN_PMC6),
+/* purr  */ PMD_D((PFM_REG_I|PFM_REG_RO), "PURR", SPRN_PURR),
+};
+#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_power5_pmd_desc)
+
+static int pfm_power5_probe_pmu(void)
+{
+	unsigned long pvr = mfspr(SPRN_PVR);
+
+	if (PVR_VER(pvr) != PV_POWER5)
+		return -1;
+
+	return 0;
+}
+
+static void pfm_power5_write_pmc(unsigned int cnum, u64 value)
+{
+	switch (pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
+	case SPRN_MMCR0:
+		mtspr(SPRN_MMCR0, value);
+		break;
+	case SPRN_MMCR1:
+		mtspr(SPRN_MMCR1, value);
+		break;
+	case SPRN_MMCRA:
+		mtspr(SPRN_MMCRA, value);
+		break;
+	default:
+		BUG();
+	}
+}
+
+static void pfm_power5_write_pmd(unsigned int cnum, u64 value)
+{
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC1:
+		mtspr(SPRN_PMC1, value);
+		break;
+	case SPRN_PMC2:
+		mtspr(SPRN_PMC2, value);
+		break;
+	case SPRN_PMC3:
+		mtspr(SPRN_PMC3, value);
+		break;
+	case SPRN_PMC4:
+		mtspr(SPRN_PMC4, value);
+		break;
+	case SPRN_PMC5:
+		mtspr(SPRN_PMC5, value);
+		break;
+	case SPRN_PMC6:
+		mtspr(SPRN_PMC6, value);
+		break;
+	case SPRN_PMC7:
+		mtspr(SPRN_PMC7, value);
+		break;
+	case SPRN_PMC8:
+		mtspr(SPRN_PMC8, value);
+		break;
+	case SPRN_TBRL:
+	case SPRN_PURR:
+		/* Ignore writes to read-only registers. */
+		break;
+	default:
+		BUG();
+	}
+}
+
+static u64 pfm_power5_read_pmd(unsigned int cnum)
+{
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC1:
+		return mfspr(SPRN_PMC1);
+	case SPRN_PMC2:
+		return mfspr(SPRN_PMC2);
+	case SPRN_PMC3:
+		return mfspr(SPRN_PMC3);
+	case SPRN_PMC4:
+		return mfspr(SPRN_PMC4);
+	case SPRN_PMC5:
+		return mfspr(SPRN_PMC5);
+	case SPRN_PMC6:
+		return mfspr(SPRN_PMC6);
+	case SPRN_PMC7:
+		return mfspr(SPRN_PMC7);
+	case SPRN_PMC8:
+		return mfspr(SPRN_PMC8);
+	case SPRN_TBRL:
+		return ((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL);
+	case SPRN_PURR:
+		if (cpu_has_feature(CPU_FTR_PURR))
+			return mfspr(SPRN_PURR);
+		else
+			return 0;
+	default:
+		BUG();
+	}
+}
+
+/**
+ * pfm_power5_enable_counters
+ *
+ * Just need to load the current values into the control registers.
+ **/
+static void pfm_power5_enable_counters(struct pfm_context *ctx,
+				       struct pfm_event_set *set)
+{
+	unsigned int i, max_pmc;
+
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+
+	for (i = 0; i < max_pmc; i++)
+		if (test_bit(i, set->used_pmcs))
+			pfm_power5_write_pmc(i, set->pmcs[i]);
+}
+
+/**
+ * pfm_power5_disable_counters
+ *
+ * Just need to zero all the control registers.
+ **/
+static void pfm_power5_disable_counters(struct pfm_context *ctx,
+					struct pfm_event_set *set)
+{
+	unsigned int i, max;
+
+	max = pfm_pmu_conf->regs.max_pmc;
+
+	for (i = 0; i < max; i++)
+		if (test_bit(i, set->used_pmcs))
+			pfm_power5_write_pmc(i, 0);
+}
+
+/**
+ * pfm_power5_get_ovfl_pmds
+ *
+ * Determine which counters in this set have overflowed and fill in the
+ * set->povfl_pmds mask and set->npend_ovfls count.
+ **/
+static void pfm_power5_get_ovfl_pmds(struct pfm_context *ctx,
+				     struct pfm_event_set *set)
+{
+	unsigned int i;
+	unsigned int max = pfm_pmu_conf->regs.max_intr_pmd;
+	u64 *used_pmds = set->used_pmds;
+	u64 *intr_pmds = pfm_pmu_conf->regs.intr_pmds;
+	u64 width_mask = 1 << pfm_pmu_conf->counter_width;
+	u64 new_val, mask[PFM_PMD_BV];
+
+	bitmap_and(cast_ulp(mask), cast_ulp(intr_pmds),
+		   cast_ulp(used_pmds), max);
+
+	for (i = 0; i < max; i++) {
+		if (test_bit(i, mask)) {
+			new_val = pfm_power5_read_pmd(i);
+			if (new_val & width_mask) {
+				set_bit(i, set->povfl_pmds);
+				set->npend_ovfls++;
+			}
+		}
+	}
+}
+
+static void pfm_power5_irq_handler(struct pt_regs *regs,
+				   struct pfm_context *ctx)
+{
+	u32 mmcr0;
+	u64 mmcra;
+
+	/* Disable the counters (set the freeze bit) to not polute
+	 * the counts.
+	 */
+	mmcr0 = mfspr(SPRN_MMCR0);
+	mtspr(SPRN_MMCR0, (mmcr0 | MMCR0_FC));
+	mmcra = mfspr(SPRN_MMCRA);
+
+	/* Set the PMM bit (see comment below). */
+	mtmsrd(mfmsr() | MSR_PMM);
+
+	pfm_interrupt_handler(instruction_pointer(regs), regs);
+
+	mmcr0 = mfspr(SPRN_MMCR0);
+	/* Reset the perfmon trigger. */
+	mmcr0 |= MMCR0_PMXE;
+
+	/*
+	 * We must clear the PMAO bit on some (GQ) chips. Just do it
+	 * all the time.
+	 */
+	mmcr0 &= ~MMCR0_PMAO;
+
+	/* Clear the appropriate bits in the MMCRA. */
+	mmcra &= POWER6_MMCRA_THRM | POWER6_MMCRA_OTHER;
+	mtspr(SPRN_MMCRA, mmcra);
+
+	/*
+	 * Now clear the freeze bit, counting will not start until we
+	 * rfid from this exception, because only at that point will
+	 * the PMM bit be cleared.
+	 */
+	mmcr0 &= ~MMCR0_FC;
+	mtspr(SPRN_MMCR0, mmcr0);
+}
+
+struct pfm_arch_pmu_info pfm_power5_pmu_info = {
+	.pmu_style        = PFM_POWERPC_PMU_POWER5,
+	.write_pmc        = pfm_power5_write_pmc,
+	.write_pmd        = pfm_power5_write_pmd,
+	.read_pmd         = pfm_power5_read_pmd,
+	.irq_handler      = pfm_power5_irq_handler,
+	.get_ovfl_pmds    = pfm_power5_get_ovfl_pmds,
+	.enable_counters  = pfm_power5_enable_counters,
+	.disable_counters = pfm_power5_disable_counters,
+};
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_power5_pmu_conf = {
+	.pmu_name = "POWER5",
+	.counter_width = 31,
+	.pmd_desc = pfm_power5_pmd_desc,
+	.pmc_desc = pfm_power5_pmc_desc,
+	.num_pmc_entries = PFM_PM_NUM_PMCS,
+	.num_pmd_entries = PFM_PM_NUM_PMDS,
+	.probe_pmu  = pfm_power5_probe_pmu,
+	.arch_info = &pfm_power5_pmu_info,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE
+};
+
+static int __init pfm_power5_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_power5_pmu_conf);
+}
+
+static void __exit pfm_power5_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_power5_pmu_conf);
+}
+
+module_init(pfm_power5_pmu_init_module);
+module_exit(pfm_power5_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/powerpc/perfmon/perfmon_power6.c linux-2.6.25-id/arch/powerpc/perfmon/perfmon_power6.c
--- linux-2.6.25-org/arch/powerpc/perfmon/perfmon_power6.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/perfmon/perfmon_power6.c	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,495 @@
+/*
+ * This file contains the POWER6 PMU register description tables
+ * and pmc checker used by perfmon.c.  
+ *
+ * Copyright (c) 2007, IBM Corporation
+ *
+ * Based on perfmon_power5.c, and written by Carl Love <carll@us.ibm.com>
+ * and Kevin Corry <kevcorry@us.ibm.com>.  Some fixes and refinement by
+ * Corey Ashford <cjashfor@us.ibm.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+MODULE_AUTHOR("Corey Ashford <cjashfor@us.ibm.com>");
+MODULE_DESCRIPTION("POWER6 PMU description table");
+MODULE_LICENSE("GPL");
+
+static struct pfm_regmap_desc pfm_power6_pmc_desc[]={
+/* mmcr0 */ PMC_D(PFM_REG_I, "MMCR0", MMCR0_FC, 0, 0, SPRN_MMCR0),
+/* mmcr1 */ PMC_D(PFM_REG_I, "MMCR1", 0, 0, 0, SPRN_MMCR1),
+/* mmcra */ PMC_D(PFM_REG_I, "MMCRA", 0, 0, 0, SPRN_MMCRA)
+};
+#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_power6_pmc_desc)
+#define PFM_DELTA_TB    10000   /* Not a real registers */
+#define PFM_DELTA_PURR  10001
+
+/*
+ * counters wrap to zero at transition from 2^32-1 to 2^32.  Note:
+ * interrupt generated at transition from 2^31-1 to 2^31
+ */
+#define OVERFLOW_VALUE    0x100000000UL
+
+/* The TB and PURR registers are read-only. Also, note that the TB register
+ * actually consists of both the 32-bit SPRN_TBRU and SPRN_TBRL registers.
+ * For Perfmon2's purposes, we'll treat it as a single 64-bit register.
+ */
+static struct pfm_regmap_desc pfm_power6_pmd_desc[]={
+	/* On POWER 6 PMC5 and PMC6 are not writable, they do not
+	 * generate interrupts, and do not qualify their counts 
+	 * based on problem mode, supervisor mode or hypervisor mode.
+	 * These two counters are implemented as virtual counters
+	 * to make the appear to work like the other counters.  A 
+	 * kernel timer is used sample the real PMC5 and PMC6 and 
+	 * update the virtual counters.
+	 */
+/* tb    */ PMD_D((PFM_REG_I|PFM_REG_RO), "TB", SPRN_TBRL),
+/* pmd1  */ PMD_D(PFM_REG_C, "PMC1", SPRN_PMC1),
+/* pmd2  */ PMD_D(PFM_REG_C, "PMC2", SPRN_PMC2),
+/* pmd3  */ PMD_D(PFM_REG_C, "PMC3", SPRN_PMC3),
+/* pmd4  */ PMD_D(PFM_REG_C, "PMC4", SPRN_PMC4),
+/* pmd5  */ PMD_D((PFM_REG_I|PFM_REG_V), "PMC5", SPRN_PMC5),
+/* pmd6  */ PMD_D((PFM_REG_I|PFM_REG_V), "PMC6", SPRN_PMC6),
+/* purr  */ PMD_D((PFM_REG_I|PFM_REG_RO), "PURR", SPRN_PURR),
+/* delta purr */ PMD_D((PFM_REG_I|PFM_REG_V), "DELTA_TB", PFM_DELTA_TB),
+/* delta tb   */ PMD_D((PFM_REG_I|PFM_REG_V), "DELTA_PURR", PFM_DELTA_PURR),
+};
+
+#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_power6_pmd_desc)
+
+u32 pmc5_start_save[NR_CPUS];
+u32 pmc6_start_save[NR_CPUS];
+
+static struct timer_list pmc5_6_update[NR_CPUS];
+u64 enable_cntrs_cnt;
+u64 disable_cntrs_cnt;
+u64 call_delta;
+u64 pm5_6_interrupt;
+u64 pm1_4_interrupt;
+/* need ctx_arch for kernel timer.  Can't get it in context of the kernel
+ * timer.
+ */
+struct pfm_arch_context *pmc5_6_ctx_arch[NR_CPUS];
+long int update_time;
+
+static void delta(int cpu_num, struct pfm_arch_context *ctx_arch) {
+	u32 tmp5, tmp6;
+
+	call_delta++;
+
+	tmp5 = (u32) mfspr(SPRN_PMC5);
+	tmp6 = (u32) mfspr(SPRN_PMC6);
+
+	/*
+	 * The following difference calculation relies on 32-bit modular
+	 * arithmetic for the deltas to come out correct (especially in the
+	 * presence of a 32-bit counter wrap).
+	 */
+	ctx_arch->powergs_pmc5 += (u64)(tmp5 - pmc5_start_save[cpu_num]);
+	ctx_arch->powergs_pmc6 += (u64)(tmp6 - pmc6_start_save[cpu_num]);
+
+	pmc5_start_save[cpu_num] = tmp5;
+	pmc6_start_save[cpu_num] = tmp6;
+
+	return;
+}
+
+
+static void pmc5_6_updater(unsigned long cpu_num)
+{ 
+	/* update the virtual pmd 5 and pmd 6 counters */
+
+	delta(cpu_num, pmc5_6_ctx_arch[cpu_num]);
+	mod_timer(&pmc5_6_update[cpu_num], jiffies + update_time);
+}
+
+
+static int pfm_power6_probe_pmu(void)
+{
+	unsigned long pvr = mfspr(SPRN_PVR);
+
+	if (PVR_VER(pvr) != PV_POWER6)
+		return -1;
+	return 0;
+}
+
+static void pfm_power6_write_pmc(unsigned int cnum, u64 value)
+{
+	switch (pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
+	case SPRN_MMCR0:
+		mtspr(SPRN_MMCR0, value);
+		break;
+	case SPRN_MMCR1:
+		mtspr(SPRN_MMCR1, value);
+		break;
+	case SPRN_MMCRA:
+		mtspr(SPRN_MMCRA, value);
+		break;
+	default:
+		BUG();
+	}
+}
+
+static void pfm_power6_write_pmd(unsigned int cnum, u64 value)
+{
+	/* On POWER 6 PMC5 and PMC6 are implemented as 
+	 * virtual counters.  See comment in pfm_power6_pmd_desc 
+	 * definition.
+	 */
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC1:
+		mtspr(SPRN_PMC1, value);
+		break;
+	case SPRN_PMC2:
+		mtspr(SPRN_PMC2, value);
+		break;
+	case SPRN_PMC3:
+		mtspr(SPRN_PMC3, value);
+		break;
+	case SPRN_PMC4:
+		mtspr(SPRN_PMC4, value);
+		break;
+	case SPRN_TBRL:
+	case SPRN_PURR:
+		/* Ignore writes to read-only registers. */
+		break;
+	default:
+		BUG();
+	}
+}
+
+static u64 pfm_power6_sread(struct pfm_context *ctx, unsigned int cnum)
+{
+	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
+	int cpu_num = smp_processor_id();
+
+	/* On POWER 6 PMC5 and PMC6 are implemented as 
+	 * virtual counters.  See comment in pfm_power6_pmd_desc 
+	 * definition.
+	 */
+
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC5:
+		return ctx_arch->powergs_pmc5 + (u64)((u32)mfspr(SPRN_PMC5) - pmc5_start_save[cpu_num]);
+		break;
+
+	case SPRN_PMC6:
+		return ctx_arch->powergs_pmc6 + (u64)((u32)mfspr(SPRN_PMC6) - pmc6_start_save[cpu_num]);
+		break;
+
+	case PFM_DELTA_TB:
+		return ctx_arch->delta_tb + 
+			(((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL))
+			 - ctx_arch->delta_tb_start;
+		break;
+
+	case PFM_DELTA_PURR:
+		return ctx_arch->delta_purr + mfspr(SPRN_PURR) 
+			- ctx_arch->delta_purr_start;
+		break;
+
+	default:
+		BUG();
+	}
+}
+
+void pfm_power6_swrite(struct pfm_context *ctx, unsigned int cnum,
+	u64 val)
+{
+	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
+	int cpu_num = smp_processor_id();
+
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC5:
+		pmc5_start_save[cpu_num] = mfspr(SPRN_PMC5);
+		ctx_arch->powergs_pmc5 = val;
+		break;
+
+	case SPRN_PMC6:
+		pmc6_start_save[cpu_num] = mfspr(SPRN_PMC6);
+		ctx_arch->powergs_pmc6 = val;
+		break;
+
+	case PFM_DELTA_TB:
+		ctx_arch->delta_tb_start = 
+			(((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL));
+		ctx_arch->delta_tb = val;
+		break;
+		
+	case PFM_DELTA_PURR:
+		ctx_arch->delta_purr_start = mfspr(SPRN_PURR);
+		ctx_arch->delta_purr = val;
+		break;
+
+	default:
+		BUG();
+	}
+}
+
+static u64 pfm_power6_read_pmd(unsigned int cnum)
+{
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC1:
+		return mfspr(SPRN_PMC1);
+	case SPRN_PMC2:
+		return mfspr(SPRN_PMC2);
+	case SPRN_PMC3:
+		return mfspr(SPRN_PMC3);
+	case SPRN_PMC4:
+		return mfspr(SPRN_PMC4);
+	case SPRN_TBRL:
+		return ((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL);
+	case SPRN_PURR:
+		if (cpu_has_feature(CPU_FTR_PURR))
+			return mfspr(SPRN_PURR);
+		else
+			return 0;
+	default:
+		BUG();
+	}
+}
+
+/**
+ * pfm_power6_enable_counters
+ *
+ * Just need to load the current values into the control registers.
+ **/
+static void pfm_power6_enable_counters(struct pfm_context *ctx,
+				       struct pfm_event_set *set)
+{
+
+	unsigned int i, max_pmc;
+	int cpu_num = smp_processor_id();
+	struct pfm_arch_context *ctx_arch;
+	
+	enable_cntrs_cnt++;
+
+	/* need the ctx passed down to the routine */
+	ctx_arch = pfm_ctx_arch(ctx);
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+
+	for (i = 0; i < max_pmc; i++)
+		if (test_bit(i, set->used_pmcs))
+			pfm_power6_write_pmc(i, set->pmcs[i]);
+
+	/* save current free running HW event count */
+	pmc5_start_save[cpu_num] = mfspr(SPRN_PMC5);
+	pmc6_start_save[cpu_num] = mfspr(SPRN_PMC6);
+
+	ctx_arch->delta_purr_start = mfspr(SPRN_PURR);
+
+	if (cpu_has_feature(CPU_FTR_PURR))
+		ctx_arch->delta_tb_start =
+			((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL);
+	else
+		ctx_arch->delta_tb_start = 0;
+
+	/* Start kernel timer for this cpu to periodically update
+	 * the virtual counters.
+	 */
+	init_timer(&pmc5_6_update[cpu_num]);
+	pmc5_6_update[cpu_num].function = pmc5_6_updater;
+	pmc5_6_update[cpu_num].data = (unsigned long) cpu_num;
+	pmc5_6_update[cpu_num].expires = jiffies + update_time;
+	/* context for this timer, timer will be removed if context
+	 * is switched because the counters will be stopped first.
+	 * NEEDS WORK, I think this is all ok, a little concerned about a 
+	 * race between the kernel timer going off right as the counters
+	 * are being stopped and the context switching.  Need to think
+	 * about this.
+	 */
+	pmc5_6_ctx_arch[cpu_num] = ctx_arch;  
+	add_timer(&pmc5_6_update[cpu_num]);
+}
+
+/**
+ * pfm_power6_disable_counters
+ *
+ * Just need to zero all the control registers.
+ **/
+static void pfm_power6_disable_counters(struct pfm_context *ctx,
+					struct pfm_event_set *set)
+{
+	unsigned int i, max;
+	struct pfm_arch_context *ctx_arch;
+	int cpu_num = smp_processor_id();
+
+	disable_cntrs_cnt++;
+	ctx_arch = pfm_ctx_arch(ctx);
+	max = pfm_pmu_conf->regs.max_pmc;
+
+	/* delete kernel update timer */
+	del_timer_sync(&pmc5_6_update[cpu_num]);
+
+	for (i = 0; i < max; i++)
+		if (test_bit(i, set->used_pmcs))
+			pfm_power6_write_pmc(i, 0);
+
+	/* Update the virtual pmd 5 and 6 counters from the free running
+	 * HW counters
+	 */
+	delta(cpu_num, ctx_arch);
+
+	ctx_arch->delta_tb += 
+		(((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL))
+		 - ctx_arch->delta_tb_start;
+
+	ctx_arch->delta_purr += mfspr(SPRN_PURR) 
+		- ctx_arch->delta_purr_start;
+}
+
+/**
+ * pfm_power6_get_ovfl_pmds
+ *
+ * Determine which counters in this set have overflowed and fill in the
+ * set->povfl_pmds mask and set->npend_ovfls count.
+ **/
+static void pfm_power6_get_ovfl_pmds(struct pfm_context *ctx,
+				     struct pfm_event_set *set)
+{
+	unsigned int i;
+	unsigned int first_intr_pmd = pfm_pmu_conf->regs.first_intr_pmd;
+	unsigned int max_intr_pmd = pfm_pmu_conf->regs.max_intr_pmd;
+	u64 *used_pmds = set->used_pmds;
+	u64 *cntr_pmds = pfm_pmu_conf->regs.cnt_pmds;
+	u64 width_mask = 1 << pfm_pmu_conf->counter_width;
+	u64 new_val, mask[PFM_PMD_BV];
+
+	bitmap_and(cast_ulp(mask), cast_ulp(cntr_pmds), cast_ulp(used_pmds), max_intr_pmd);
+
+	/* max_intr_pmd is actually the last interrupting pmd register + 1 */
+	for (i = first_intr_pmd; i < max_intr_pmd; i++) {
+		if (test_bit(i, mask)) {
+			new_val = pfm_power6_read_pmd(i);
+			if (new_val & width_mask) {
+				set_bit(i, set->povfl_pmds);
+				set->npend_ovfls++;
+			}
+		}
+	}
+}
+
+static void pfm_power6_irq_handler(struct pt_regs *regs,
+				   struct pfm_context *ctx)
+{
+	u32 mmcr0;
+	u64 mmcra;
+
+	/* Disable the counters (set the freeze bit) to not polute
+	 * the counts.
+	 */
+	mmcr0 = mfspr(SPRN_MMCR0);
+	mtspr(SPRN_MMCR0, (mmcr0 | MMCR0_FC));
+	mmcra = mfspr(SPRN_MMCRA);
+
+	/* Set the PMM bit (see comment below). */
+	mtmsrd(mfmsr() | MSR_PMM);
+
+	pm1_4_interrupt++;
+
+	pfm_interrupt_handler(instruction_pointer(regs), regs);
+
+	mmcr0 = mfspr(SPRN_MMCR0);
+	/* Reset the perfmon trigger. */
+	mmcr0 |= MMCR0_PMXE;
+
+	/*
+	 * We must clear the PMAO bit on some (GQ) chips. Just do it
+	 * all the time.
+	 */
+	mmcr0 &= ~MMCR0_PMAO;
+
+	/* Clear the appropriate bits in the MMCRA. */
+	mmcra &= POWER6_MMCRA_THRM | POWER6_MMCRA_OTHER;
+	mtspr(SPRN_MMCRA, mmcra);
+
+	/*
+	 * Now clear the freeze bit, counting will not start until we
+	 * rfid from this exception, because only at that point will
+	 * the PMM bit be cleared.
+	 */
+	mmcr0 &= ~MMCR0_FC;
+	mtspr(SPRN_MMCR0, mmcr0);
+}
+
+struct pfm_arch_pmu_info pfm_power6_pmu_info = {
+	.pmu_style        = PFM_POWERPC_PMU_POWER6,
+	.write_pmc        = pfm_power6_write_pmc,
+	.write_pmd        = pfm_power6_write_pmd,
+	.read_pmd         = pfm_power6_read_pmd,
+	.irq_handler      = pfm_power6_irq_handler,
+	.get_ovfl_pmds    = pfm_power6_get_ovfl_pmds,
+	.enable_counters  = pfm_power6_enable_counters,
+	.disable_counters = pfm_power6_disable_counters,
+};
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_power6_pmu_conf = {
+	.pmu_name = "POWER6",
+	.counter_width = 31,
+	.pmd_desc = pfm_power6_pmd_desc,
+	.pmc_desc = pfm_power6_pmc_desc,
+	.num_pmc_entries = PFM_PM_NUM_PMCS,
+	.num_pmd_entries = PFM_PM_NUM_PMDS,
+	.probe_pmu  = pfm_power6_probe_pmu,
+	.arch_info = &pfm_power6_pmu_info,
+	.pmd_sread = pfm_power6_sread,
+	.pmd_swrite = pfm_power6_swrite,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE
+};
+	
+static int __init pfm_power6_pmu_init_module(void)
+{
+	int ret;
+	disable_cntrs_cnt=0;
+	enable_cntrs_cnt=0;
+	call_delta=0;
+	pm5_6_interrupt=0;
+	pm1_4_interrupt=0;
+
+	/* calculate the time for updating counters 5 and 6 */
+
+	/*
+	 * MAX_EVENT_RATE assumes a max instruction issue rate of 2
+	 * instructions per clock cycle.  Experience shows that this factor
+	 * of 2 is more than adequate.
+	 */
+
+# define MAX_EVENT_RATE (ppc_proc_freq * 2)
+
+	/*
+	 * Calculate the time, in jiffies, it takes for event counter 5 or
+	 * 6 to completely wrap when counting at the max event rate, and
+	 * then figure on sampling at twice that rate.
+	 */
+	update_time = (((unsigned long)HZ * OVERFLOW_VALUE)
+		       / ((unsigned long)MAX_EVENT_RATE)) / 2;
+
+	ret =  pfm_pmu_register(&pfm_power6_pmu_conf);
+	return ret;
+}
+
+static void __exit pfm_power6_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_power6_pmu_conf);
+}
+
+module_init(pfm_power6_pmu_init_module);
+module_exit(pfm_power6_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/powerpc/perfmon/perfmon_ppc32.c linux-2.6.25-id/arch/powerpc/perfmon/perfmon_ppc32.c
--- linux-2.6.25-org/arch/powerpc/perfmon/perfmon_ppc32.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/perfmon/perfmon_ppc32.c	2008-04-23 11:19:39.000000000 +0200
@@ -0,0 +1,340 @@
+/*
+ * This file contains the PPC32 PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Philip Mucci, mucci@cs.utk.edu
+ *
+ * Based on code from:
+ * Copyright (c) 2005 David Gibson, IBM Corporation.
+ *
+ * Based on perfmon_p6.c:
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <asm/reg.h>
+
+MODULE_AUTHOR("Philip Mucci <mucci@cs.utk.edu>");
+MODULE_DESCRIPTION("PPC32 PMU description table");
+MODULE_LICENSE("GPL");
+
+static struct pfm_pmu_config pfm_ppc32_pmu_conf;
+
+static struct pfm_regmap_desc pfm_ppc32_pmc_desc[] = {
+/* mmcr0 */ PMC_D(PFM_REG_I, "MMCR0", 0x0, 0, 0, SPRN_MMCR0),
+/* mmcr1 */ PMC_D(PFM_REG_I, "MMCR1", 0x0, 0, 0, SPRN_MMCR1),
+/* mmcr2 */ PMC_D(PFM_REG_I, "MMCR2", 0x0, 0, 0, SPRN_MMCR2),
+};
+#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_ppc32_pmc_desc)
+
+static struct pfm_regmap_desc pfm_ppc32_pmd_desc[] = {
+/* pmd0  */ PMD_D(PFM_REG_C, "PMC1", SPRN_PMC1),
+/* pmd1  */ PMD_D(PFM_REG_C, "PMC2", SPRN_PMC2),
+/* pmd2  */ PMD_D(PFM_REG_C, "PMC3", SPRN_PMC3),
+/* pmd3  */ PMD_D(PFM_REG_C, "PMC4", SPRN_PMC4),
+/* pmd4  */ PMD_D(PFM_REG_C, "PMC5", SPRN_PMC5),
+/* pmd5  */ PMD_D(PFM_REG_C, "PMC6", SPRN_PMC6),
+};
+#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_ppc32_pmd_desc)
+
+static void perfmon_perf_irq(struct pt_regs *regs)
+{
+	u32 mmcr0;
+
+	/* BLATANTLY STOLEN FROM OPROFILE, then modified */
+
+	/* set the PMM bit (see comment below) */
+	mtmsr(mfmsr() | MSR_PMM);
+
+	pfm_interrupt_handler(instruction_pointer(regs), regs);
+
+	/* The freeze bit was set by the interrupt.
+	 * Clear the freeze bit, and reenable the interrupt.
+	 * The counters won't actually start until the rfi clears
+	 * the PMM bit.
+	 */
+
+	/* Unfreezes the counters on this CPU, enables the interrupt,
+	 * enables the counters to trigger the interrupt, and sets the
+	 * counters to only count when the mark bit is not set.
+	 */
+	mmcr0 = mfspr(SPRN_MMCR0);
+
+	mmcr0 &= ~(MMCR0_FC | MMCR0_FCM0);
+	mmcr0 |= (MMCR0_FCECE | MMCR0_PMC1CE | MMCR0_PMCnCE | MMCR0_PMXE);
+
+	mtspr(SPRN_MMCR0, mmcr0);
+}
+
+static int pfm_ppc32_probe_pmu(void)
+{
+	enum ppc32_pmu_type pm_type;
+	int nmmcr = 0, npmds = 0, intsok = 0, i;
+	unsigned int pvr;
+	char *str;
+
+	pvr = mfspr(SPRN_PVR);
+
+	switch (PVR_VER(pvr)) {
+	case 0x0004: /* 604 */
+		str = "PPC604";
+		pm_type = PFM_POWERPC_PMU_604;
+		nmmcr = 1;
+		npmds = 2;
+		break;
+	case 0x0009: /* 604e;  */
+	case 0x000A: /* 604ev */
+		str = "PPC604e";
+		pm_type = PFM_POWERPC_PMU_604e;
+		nmmcr = 2;
+		npmds = 4;
+		break;
+	case 0x0008: /* 750/740 */
+		str = "PPC750";
+		pm_type = PFM_POWERPC_PMU_750;
+		nmmcr = 2;
+		npmds = 4;
+		break;
+	case 0x7000: /* 750FX */
+	case 0x7001:
+		str = "PPC750";
+		pm_type = PFM_POWERPC_PMU_750;
+		nmmcr = 2;
+		npmds = 4;
+		if ((pvr & 0xFF0F) >= 0x0203)
+			intsok = 1;
+		break;
+	case 0x7002: /* 750GX */
+		str = "PPC750";
+		pm_type = PFM_POWERPC_PMU_750;
+		nmmcr = 2;
+		npmds = 4;
+		intsok = 1;
+	case 0x000C: /* 7400 */
+		str = "PPC7400";
+		pm_type = PFM_POWERPC_PMU_7400;
+		nmmcr = 3;
+		npmds = 4;
+		break;
+	case 0x800C: /* 7410 */
+		str = "PPC7410";
+		pm_type = PFM_POWERPC_PMU_7400;
+		nmmcr = 3;
+		npmds = 4;
+		if ((pvr & 0xFFFF) >= 0x01103)
+			intsok = 1;
+		break;
+	case 0x8000: /* 7451/7441 */
+	case 0x8001: /* 7455/7445 */
+	case 0x8002: /* 7457/7447 */
+	case 0x8003: /* 7447A */
+	case 0x8004: /* 7448 */
+		str = "PPC7450";
+		pm_type = PFM_POWERPC_PMU_7450;
+		nmmcr = 3; npmds = 6;
+		intsok = 1;
+		break;
+	default:
+		PFM_INFO("Unknown PVR_VER(0x%x)\n", PVR_VER(pvr));
+		return -1;
+	}
+
+	/*
+	 * deconfigure unimplemented registers
+	 */
+	for (i = npmds; i < PFM_PM_NUM_PMDS; i++)
+		pfm_ppc32_pmd_desc[i].type = PFM_REG_NA;
+
+	for (i = nmmcr; i < PFM_PM_NUM_PMCS; i++)
+		pfm_ppc32_pmc_desc[i].type = PFM_REG_NA;
+
+	/*
+	 * update PMU description structure
+	 */
+	pfm_ppc32_pmu_conf.pmu_name = str;
+	pfm_ppc32_pmu_info.pmu_style = pm_type;
+	pfm_ppc32_pmu_conf.num_pmc_entries = nmmcr;
+	pfm_ppc32_pmu_conf.num_pmd_entries = npmds;
+
+	if (intsok == 0)
+		PFM_INFO("Interrupts unlikely to work\n");
+
+	return reserve_pmc_hardware(perfmon_perf_irq);
+}
+
+static void pfm_ppc32_write_pmc(unsigned int cnum, u64 value)
+{
+	switch (pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
+	case SPRN_MMCR0:
+		mtspr(SPRN_MMCR0, value);
+		break;
+	case SPRN_MMCR1:
+		mtspr(SPRN_MMCR1, value);
+		break;
+	case SPRN_MMCR2:
+		mtspr(SPRN_MMCR2, value);
+		break;
+	default:
+		BUG();
+	}
+}
+
+static void pfm_ppc32_write_pmd(unsigned int cnum, u64 value)
+{
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC1:
+		mtspr(SPRN_PMC1, value);
+		break;
+	case SPRN_PMC2:
+		mtspr(SPRN_PMC2, value);
+		break;
+	case SPRN_PMC3:
+		mtspr(SPRN_PMC3, value);
+		break;
+	case SPRN_PMC4:
+		mtspr(SPRN_PMC4, value);
+		break;
+	case SPRN_PMC5:
+		mtspr(SPRN_PMC5, value);
+		break;
+	case SPRN_PMC6:
+		mtspr(SPRN_PMC6, value);
+		break;
+	default:
+		BUG();
+	}
+}
+
+static u64 pfm_ppc32_read_pmd(unsigned int cnum)
+{
+	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case SPRN_PMC1:
+		return mfspr(SPRN_PMC1);
+	case SPRN_PMC2:
+		return mfspr(SPRN_PMC2);
+	case SPRN_PMC3:
+		return mfspr(SPRN_PMC3);
+	case SPRN_PMC4:
+		return mfspr(SPRN_PMC4);
+	case SPRN_PMC5:
+		return mfspr(SPRN_PMC5);
+	case SPRN_PMC6:
+		return mfspr(SPRN_PMC6);
+	default:
+		BUG();
+	}
+}
+
+/**
+ * pfm_ppc32_enable_counters
+ *
+ * Just need to load the current values into the control registers.
+ **/
+static void pfm_ppc32_enable_counters(struct pfm_context *ctx,
+				      struct pfm_event_set *set)
+{
+	unsigned int i, max_pmc;
+
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+
+	for (i = 0; i < max_pmc; i++)
+		if (test_bit(i, set->used_pmcs))
+			pfm_ppc32_write_pmc(i, set->pmcs[i]);
+}
+
+/**
+ * pfm_ppc32_disable_counters
+ *
+ * Just need to zero all the control registers.
+ **/
+static void pfm_ppc32_disable_counters(struct pfm_context *ctx,
+				       struct pfm_event_set *set)
+{
+	unsigned int i, max;
+
+	max = pfm_pmu_conf->regs.max_pmc;
+
+	for (i = 0; i < max; i++)
+		if (test_bit(i, set->used_pmcs))
+			pfm_ppc32_write_pmc(ctx, 0);
+}
+
+/**
+ * pfm_ppc32_get_ovfl_pmds
+ *
+ * Determine which counters in this set have overflowed and fill in the
+ * set->povfl_pmds mask and set->npend_ovfls count.
+ **/
+static void pfm_ppc32_get_ovfl_pmds(struct pfm_context *ctx,
+				    struct pfm_event_set *set)
+{
+	unsigned int i;
+	unsigned int max_pmd = pfm_pmu_conf->regs.max_cnt_pmd;
+	u64 *used_pmds = set->used_pmds;
+	u64 *cntr_pmds = pfm_pmu_conf->regs.cnt_pmds;
+	u64 width_mask = 1 << pfm_pmu_conf->counter_width;
+	u64 new_val, mask[PFM_PMD_BV];
+
+	bitmap_and(cast_ulp(mask), cast_ulp(cntr_pmds),
+		   cast_ulp(used_pmds), max_pmd);
+
+	for (i = 0; i < max_pmd; i++) {
+		if (test_bit(i, mask)) {
+			new_val = pfm_ppc32_read_pmd(i);
+			if (new_val & width_mask) {
+				set_bit(i, set->povfl_pmds);
+				set->npend_ovfls++;
+			}
+		}
+	}
+}
+
+struct pfm_arch_pmu_info pfm_ppc32_pmu_info = {
+	.pmu_style        = PFM_POWERPC_PMU_NONE,
+	.write_pmc        = pfm_ppc32_write_pmc,
+	.write_pmd        = pfm_ppc32_write_pmd,
+	.read_pmd         = pfm_ppc32_read_pmd,
+	.get_ovfl_pmds    = pfm_ppc32_get_ovfl_pmds,
+	.enable_counters  = pfm_ppc32_enable_counters,
+	.disable_counters = pfm_ppc32_disable_counters,
+};
+
+static struct pfm_pmu_config pfm_ppc32_pmu_conf = {
+	.counter_width = 31,
+	.pmd_desc = pfm_ppc32_pmd_desc,
+	.pmc_desc = pfm_ppc32_pmc_desc,
+	.probe_pmu  = pfm_ppc32_probe_pmu,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.version = "0.1",
+	.arch_info = &pfm_ppc32_pmu_info,
+};
+
+static int __init pfm_ppc32_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_ppc32_pmu_conf);
+}
+
+static void __exit pfm_ppc32_pmu_cleanup_module(void)
+{
+	release_pmc_hardware();
+	pfm_pmu_unregister(&pfm_ppc32_pmu_conf);
+}
+
+module_init(pfm_ppc32_pmu_init_module);
+module_exit(pfm_ppc32_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/powerpc/platforms/cell/cbe_regs.c linux-2.6.25-id/arch/powerpc/platforms/cell/cbe_regs.c
--- linux-2.6.25-org/arch/powerpc/platforms/cell/cbe_regs.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/platforms/cell/cbe_regs.c	2008-04-23 11:19:39.000000000 +0200
@@ -33,6 +33,7 @@
 	struct cbe_iic_regs __iomem *iic_regs;
 	struct cbe_mic_tm_regs __iomem *mic_tm_regs;
 	struct cbe_pmd_shadow_regs pmd_shadow_regs;
+	struct cbe_ppe_priv_regs __iomem *ppe_priv_regs;
 } cbe_regs_maps[MAX_CBE];
 static int cbe_regs_map_count;
 
@@ -145,6 +146,23 @@
 }
 EXPORT_SYMBOL_GPL(cbe_get_cpu_mic_tm_regs);
 
+struct cbe_ppe_priv_regs __iomem *cbe_get_ppe_priv_regs(struct device_node *np)
+{
+	struct cbe_regs_map *map = cbe_find_map(np);
+	if (map == NULL)
+		return NULL;
+	return map->ppe_priv_regs;
+}
+
+struct cbe_ppe_priv_regs __iomem *cbe_get_cpu_ppe_priv_regs(int cpu)
+{
+	struct cbe_regs_map *map = cbe_thread_map[cpu].regs;
+	if (map == NULL)
+		return NULL;
+	return map->ppe_priv_regs;
+}
+EXPORT_SYMBOL_GPL(cbe_get_cpu_ppe_priv_regs);
+
 u32 cbe_get_hw_thread_id(int cpu)
 {
 	return cbe_thread_map[cpu].thread_id;
@@ -206,6 +224,11 @@
 		for_each_node_by_type(np, "mic-tm")
 			if (of_get_parent(np) == be)
 				map->mic_tm_regs = of_iomap(np, 0);
+
+		for_each_node_by_type(np, "ppe-mmio")
+			if (of_get_parent(np) == be)
+				map->ppe_priv_regs = of_iomap(np, 0);
+
 	} else {
 		struct device_node *cpu;
 		/* That hack must die die die ! */
@@ -227,6 +250,10 @@
 		prop = of_get_property(cpu, "mic-tm", NULL);
 		if (prop != NULL)
 			map->mic_tm_regs = ioremap(prop->address, prop->len);
+
+		prop = of_get_property(cpu, "ppe-mmio", NULL);
+		if (prop != NULL)
+			map->ppe_priv_regs = ioremap(prop->address, prop->len);
 	}
 }
 
diff -Naur linux-2.6.25-org/arch/powerpc/platforms/cell/spufs/hw_ops.c linux-2.6.25-id/arch/powerpc/platforms/cell/spufs/hw_ops.c
--- linux-2.6.25-org/arch/powerpc/platforms/cell/spufs/hw_ops.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/platforms/cell/spufs/hw_ops.c	2008-04-23 11:19:42.000000000 +0200
@@ -237,6 +237,15 @@
 	spin_unlock_irq(&ctx->spu->register_lock);
 }
 
+static void spu_hw_runcntl_stop(struct spu_context *ctx)
+{
+	spin_lock_irq(&ctx->spu->register_lock);
+	out_be32(&ctx->spu->problem->spu_runcntl_RW, SPU_RUNCNTL_STOP);
+	while (in_be32(&ctx->spu->problem->spu_status_R) & SPU_STATUS_RUNNING)
+		cpu_relax();
+	spin_unlock_irq(&ctx->spu->register_lock);
+}
+
 static void spu_hw_master_start(struct spu_context *ctx)
 {
 	struct spu *spu = ctx->spu;
diff -Naur linux-2.6.25-org/arch/powerpc/platforms/ps3/os-area.c linux-2.6.25-id/arch/powerpc/platforms/ps3/os-area.c
--- linux-2.6.25-org/arch/powerpc/platforms/ps3/os-area.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/platforms/ps3/os-area.c	2008-04-23 11:19:46.000000000 +0200
@@ -17,6 +17,7 @@
  *  along with this program; if not, write to the Free Software
  *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
  */
+#define DEBUG
 
 #include <linux/kernel.h>
 #include <linux/io.h>
diff -Naur linux-2.6.25-org/arch/powerpc/platforms/ps3/setup.c linux-2.6.25-id/arch/powerpc/platforms/ps3/setup.c
--- linux-2.6.25-org/arch/powerpc/platforms/ps3/setup.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/platforms/ps3/setup.c	2008-04-23 11:19:47.000000000 +0200
@@ -95,6 +95,14 @@
 	ps3_sys_manager_power_off(); /* never returns */
 }
 
+static void ps3_halt(void)
+{
+	DBG("%s:%d\n", __func__, __LINE__);
+
+	smp_send_stop();
+	ps3_sys_manager_halt(); /* never returns */
+}
+
 static void ps3_panic(char *str)
 {
 	DBG("%s:%d %s\n", __func__, __LINE__, str);
@@ -105,7 +113,8 @@
 	printk("   Please press POWER button.\n");
 	printk("\n");
 
-	while(1);
+	while(1)
+		lv1_pause(1);
 }
 
 #if defined(CONFIG_FB_PS3) || defined(CONFIG_FB_PS3_MODULE) || \
@@ -266,6 +275,7 @@
 	.progress			= ps3_progress,
 	.restart			= ps3_restart,
 	.power_off			= ps3_power_off,
+	.halt				= ps3_halt,
 #if defined(CONFIG_KEXEC)
 	.kexec_cpu_down			= ps3_kexec_cpu_down,
 	.machine_kexec			= default_machine_kexec,
diff -Naur linux-2.6.25-org/arch/powerpc/platforms/ps3/spu.c linux-2.6.25-id/arch/powerpc/platforms/ps3/spu.c
--- linux-2.6.25-org/arch/powerpc/platforms/ps3/spu.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/powerpc/platforms/ps3/spu.c	2008-04-23 11:19:47.000000000 +0200
@@ -140,6 +140,12 @@
 	pr_debug("%s:%d: shadow:  %lxh\n", func, line, shadow);
 }
 
+inline u64 ps3_get_spe_id(void *arg)
+{
+	return spu_pdata(arg)->spe_id;
+}
+EXPORT_SYMBOL_GPL(ps3_get_spe_id);
+
 static unsigned long get_vas_id(void)
 {
 	unsigned long id;
@@ -168,6 +174,7 @@
 		return result;
 	}
 
+	pr_info("spe_id[%d] : 0x%lx\n", spu->number, ps3_get_spe_id(spu));
 	return result;
 }
 
diff -Naur linux-2.6.25-org/arch/powerpc/platforms/ps3/system-bus-rework.txt linux-2.6.25-id/arch/powerpc/platforms/ps3/system-bus-rework.txt
--- linux-2.6.25-org/arch/powerpc/platforms/ps3/system-bus-rework.txt	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/powerpc/platforms/ps3/system-bus-rework.txt	2008-04-23 11:19:47.000000000 +0200
@@ -0,0 +1,80 @@
+* Status of the system-bus re-work
+
+o=working
+x=not working
+NA=no support planned
+				1st	shut	2nd	insmod	rmmod
+				boot	down	boot
+
+CONFIG_USB_EHCI_HCD		o	o	o	o	o
+CONFIG_USB_OHCI_HCD		o	o	o	o	o
+CONFIG_GELIC_NET		o	o	o	o	o
+CONFIG_FB_PS3			o	o	o	o	x
+CONFIG_SND_PS3			o	o	o	o	o
+CONFIG_PS3_PS3AV		o	o	o	o	o
+CONFIG_PS3_SYS_MANAGER		o	o	o	o	NA(1)
+CONFIG_PS3_VUART		o	o	o	o	o
+CONFIG_PS3_FLASH		o	o	o	o	o
+CONFIG_PS3_DISK			o	o	o	o	o
+CONFIG_PS3_ROM			o	o	o	o	o
+CONFIG_PS3_STORAGE		o	o	o	o	o
+CONFIG_SPU_FS			o	o	o	o	o
+
+-- commands --
+
+64 bit kexec	o
+32 bit kexec	x(2)
+reboot:  	o
+halt		o
+shutdown	o
+poweroff	o
+power button	o
+
+-- notes --
+
+(1) loaded as 'permanent'.
+(2) not working, WIP
+
+--------------------------------------------------------------------------------
+* Bus layout
+
+system-bus --+-- sb -------+-- usb0 --+-- ehci0
+             |             |          +-- ohci0
+             |             |
+             |             +-- usb1 --+-- ehci1
+             |             |          +-- ohci1
+             |             |
+             |             +-- gelic
+             |
+             +-- storage --+-- disk
+             |             +-- flash
+             |             +-- rom
+             |
+             +-- ioc0 -----+-- gfx
+             |             +-- sound
+             |
+             +-- vuart ----+-- av_settings
+                           +-- system_manager
+
+
+bus       | bus  | in   |
+name      | num  | repo |
+----------+------+------+---------------------------------------
+sb        | 04h  | yes  |
+storage   | 05h  | yes  |
+ioc0      | 81h  | no   |
+vuart     | 82h  | no   |
+----------+------+------+---------------------------------------
+
+device        | type   | irq  | dma | mmio |
+--------------+--------+------+-----+------+--------------------
+usb0            sb_04
+usb1            sb_04
+gelic           sb_03
+disk            st_00
+flash           st_0e
+rom             st_05
+gfx
+sound
+av_settings     vu_00
+system_manager  vu_02
diff -Naur linux-2.6.25-org/arch/sparc64/Kconfig linux-2.6.25-id/arch/sparc64/Kconfig
--- linux-2.6.25-org/arch/sparc64/Kconfig	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/Kconfig	2008-04-23 11:19:48.000000000 +0200
@@ -469,6 +469,8 @@
 
 source "fs/Kconfig"
 
+source "arch/sparc64/perfmon/Kconfig"
+
 source "arch/sparc64/Kconfig.debug"
 
 source "security/Kconfig"
diff -Naur linux-2.6.25-org/arch/sparc64/Makefile linux-2.6.25-id/arch/sparc64/Makefile
--- linux-2.6.25-org/arch/sparc64/Makefile	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/Makefile	2008-04-23 11:19:48.000000000 +0200
@@ -32,6 +32,8 @@
 libs-y				+= arch/sparc64/prom/ arch/sparc64/lib/
 drivers-$(CONFIG_OPROFILE)	+= arch/sparc64/oprofile/
 
+core-$(CONFIG_PERFMON)		+= arch/sparc64/perfmon/
+
 boot := arch/sparc64/boot
 
 image tftpboot.img vmlinux.aout: vmlinux
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/cpu.c linux-2.6.25-id/arch/sparc64/kernel/cpu.c
--- linux-2.6.25-org/arch/sparc64/kernel/cpu.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/cpu.c	2008-04-23 11:19:48.000000000 +0200
@@ -73,11 +73,13 @@
 	case SUN4V_CHIP_NIAGARA1:
 		sparc_cpu_type = "UltraSparc T1 (Niagara)";
 		sparc_fpu_type = "UltraSparc T1 integrated FPU";
+		sparc_pmu_type = "niagara";
 		break;
 
 	case SUN4V_CHIP_NIAGARA2:
 		sparc_cpu_type = "UltraSparc T2 (Niagara2)";
 		sparc_fpu_type = "UltraSparc T2 integrated FPU";
+		sparc_pmu_type = "niagara2";
 		break;
 
 	default:
@@ -85,6 +87,7 @@
 		       prom_cpu_compatible);
 		sparc_cpu_type = "Unknown SUN4V CPU";
 		sparc_fpu_type = "Unknown SUN4V FPU";
+		sparc_pmu_type = "Unknown SUN4V PMU";
 		break;
 	}
 }
@@ -117,6 +120,8 @@
 			if (linux_sparc_chips[i].impl == impl) {
 				sparc_cpu_type =
 					linux_sparc_chips[i].cpu_name;
+				sparc_pmu_type =
+					linux_sparc_chips[i].pmu_name;
 				break;
 			}
 		}
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/entry.S linux-2.6.25-id/arch/sparc64/kernel/entry.S
--- linux-2.6.25-org/arch/sparc64/kernel/entry.S	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/entry.S	2008-04-23 11:19:48.000000000 +0200
@@ -2631,3 +2631,43 @@
 	retl
 	 nop
 	.size	sun4v_mmu_demap_all, .-sun4v_mmu_demap_all
+
+	.globl	sun4v_niagara_getperf
+	.type	sun4v_niagara_getperf,#function
+sun4v_niagara_getperf:
+	mov	%o0, %o4
+	mov	HV_FAST_GET_PERFREG, %o5
+	ta	HV_FAST_TRAP
+	stx	%o1, [%o4]
+	retl
+	 nop
+	.size	sun4v_niagara_getperf, .-sun4v_niagara_getperf
+
+	.globl	sun4v_niagara_setperf
+	.type	sun4v_niagara_setperf,#function
+sun4v_niagara_setperf:
+	mov	HV_FAST_SET_PERFREG, %o5
+	ta	HV_FAST_TRAP
+	retl
+	 nop
+	.size	sun4v_niagara_setperf, .-sun4v_niagara_setperf
+
+	.globl	sun4v_niagara2_getperf
+	.type	sun4v_niagara2_getperf,#function
+sun4v_niagara2_getperf:
+	mov	%o0, %o4
+	mov	HV_FAST_N2_GET_PERFREG, %o5
+	ta	HV_FAST_TRAP
+	stx	%o1, [%o4]
+	retl
+	 nop
+	.size	sun4v_niagara2_getperf, .-sun4v_niagara2_getperf
+
+	.globl	sun4v_niagara2_setperf
+	.type	sun4v_niagara2_setperf,#function
+sun4v_niagara2_setperf:
+	mov	HV_FAST_N2_SET_PERFREG, %o5
+	ta	HV_FAST_TRAP
+	retl
+	 nop
+	.size	sun4v_niagara2_setperf, .-sun4v_niagara2_setperf
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/hvapi.c linux-2.6.25-id/arch/sparc64/kernel/hvapi.c
--- linux-2.6.25-org/arch/sparc64/kernel/hvapi.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/hvapi.c	2008-04-23 11:19:48.000000000 +0200
@@ -36,6 +36,7 @@
 	{ .group = HV_GRP_NCS,		.flags = FLAG_PRE_API	},
 	{ .group = HV_GRP_NIAG_PERF,	.flags = FLAG_PRE_API	},
 	{ .group = HV_GRP_FIRE_PERF,				},
+	{ .group = HV_GRP_NIAG2_PERF,				},
 	{ .group = HV_GRP_DIAG,		.flags = FLAG_PRE_API	},
 };
 
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/irq.c linux-2.6.25-id/arch/sparc64/kernel/irq.c
--- linux-2.6.25-org/arch/sparc64/kernel/irq.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/irq.c	2008-04-23 11:19:48.000000000 +0200
@@ -714,6 +714,70 @@
 	set_irq_regs(old_regs);
 }
 
+static void unhandled_perf_irq(struct pt_regs *regs)
+{
+	unsigned long pcr, pic;
+
+	read_pcr(pcr);
+	read_pic(pic);
+
+	write_pcr(0);
+
+	printk(KERN_EMERG "CPU %d: Got unexpected perf counter IRQ.\n",
+	       smp_processor_id());
+	printk(KERN_EMERG "CPU %d: PCR[%016lx] PIC[%016lx]\n",
+	       smp_processor_id(), pcr, pic);
+}
+
+/* Almost a direct copy of the powerpc PMC code.  */
+static DEFINE_SPINLOCK(perf_irq_lock);
+static void *perf_irq_owner_caller; /* mostly for debugging */
+static void (*perf_irq)(struct pt_regs *regs) = unhandled_perf_irq;
+
+/* Invoked from level 15 PIL handler in trap table.  */
+void perfctr_irq(int irq, struct pt_regs *regs)
+{
+	clear_softint(1 << irq);
+	perf_irq(regs);
+}
+
+int register_perfctr_intr(void (*handler)(struct pt_regs *))
+{
+	int ret;
+
+	if (!handler)
+		return -EINVAL;
+
+	spin_lock(&perf_irq_lock);
+	if (perf_irq != unhandled_perf_irq) {
+		printk(KERN_WARNING "register_perfctr_intr: "
+		       "perf IRQ busy (reserved by caller %p)\n",
+		       perf_irq_owner_caller);
+		ret = -EBUSY;
+		goto out;
+	}
+
+	perf_irq_owner_caller = __builtin_return_address(0);
+	perf_irq = handler;
+
+	ret = 0;
+out:
+	spin_unlock(&perf_irq_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(register_perfctr_intr);
+
+void release_perfctr_intr(void (*handler)(struct pt_regs *))
+{
+	spin_lock(&perf_irq_lock);
+	perf_irq_owner_caller = NULL;
+	perf_irq = unhandled_perf_irq;
+	spin_unlock(&perf_irq_lock);
+}
+EXPORT_SYMBOL_GPL(release_perfctr_intr);
+
+
 #ifdef CONFIG_HOTPLUG_CPU
 void fixup_irqs(void)
 {
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/process.c linux-2.6.25-id/arch/sparc64/kernel/process.c
--- linux-2.6.25-org/arch/sparc64/kernel/process.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/process.c	2008-04-23 11:19:48.000000000 +0200
@@ -330,11 +330,7 @@
 			t->utraps[0]--;
 	}
 
-	if (test_and_clear_thread_flag(TIF_PERFCTR)) {
-		t->user_cntd0 = t->user_cntd1 = NULL;
-		t->pcr_reg = 0;
-		write_pcr(0);
-	}
+	pfm_exit_thread(current);
 }
 
 void flush_thread(void)
@@ -356,13 +352,6 @@
 
 	set_thread_wsaved(0);
 
-	/* Turn off performance counters if on. */
-	if (test_and_clear_thread_flag(TIF_PERFCTR)) {
-		t->user_cntd0 = t->user_cntd1 = NULL;
-		t->pcr_reg = 0;
-		write_pcr(0);
-	}
-
 	/* Clear FPU register state. */
 	t->fpsaved[0] = 0;
 	
@@ -548,16 +537,6 @@
 	t->fpsaved[0] = 0;
 
 	if (regs->tstate & TSTATE_PRIV) {
-		/* Special case, if we are spawning a kernel thread from
-		 * a userspace task (via KMOD, NFS, or similar) we must
-		 * disable performance counters in the child because the
-		 * address space and protection realm are changing.
-		 */
-		if (t->flags & _TIF_PERFCTR) {
-			t->user_cntd0 = t->user_cntd1 = NULL;
-			t->pcr_reg = 0;
-			t->flags &= ~_TIF_PERFCTR;
-		}
 		t->kregs->u_regs[UREG_FP] = t->ksp;
 		t->flags |= ((long)ASI_P << TI_FLAG_CURRENT_DS_SHIFT);
 		flush_register_windows();
@@ -595,6 +574,8 @@
 	if (clone_flags & CLONE_SETTLS)
 		t->kregs->u_regs[UREG_G7] = regs->u_regs[UREG_I3];
 
+	pfm_copy_thread(p);
+
 	return 0;
 }
 
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/rtrap.S linux-2.6.25-id/arch/sparc64/kernel/rtrap.S
--- linux-2.6.25-org/arch/sparc64/kernel/rtrap.S	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/rtrap.S	2008-04-23 11:19:48.000000000 +0200
@@ -52,7 +52,7 @@
 		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
 		ldx			[%g6 + TI_FLAGS], %l0
 
-1:		andcc			%l0, (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK), %g0
+1:		andcc			%l0, (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK | _TIF_PERFMON_WORK), %g0
 		be,pt			%xcc, __handle_user_windows_continue
 		 nop
 		mov			%l5, %o1
@@ -73,57 +73,14 @@
 		ba,pt			%xcc, __handle_user_windows_continue
 
 		 andn			%l1, %l4, %l1
-__handle_perfctrs:
-		call			update_perfctrs
-		 wrpr			%g0, RTRAP_PSTATE, %pstate
-		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
-		ldub			[%g6 + TI_WSAVED], %o2
-		brz,pt			%o2, 1f
-		 nop
-		/* Redo userwin+sched+sig checks */
-		call			fault_in_user_windows
-
-		 wrpr			%g0, RTRAP_PSTATE, %pstate
-		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
-		ldx			[%g6 + TI_FLAGS], %l0
-		andcc			%l0, _TIF_NEED_RESCHED, %g0
-		be,pt			%xcc, 1f
-
-		 nop
-		call			schedule
-		 wrpr			%g0, RTRAP_PSTATE, %pstate
-		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
-		ldx			[%g6 + TI_FLAGS], %l0
-1:		andcc			%l0, (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK), %g0
-
-		be,pt			%xcc, __handle_perfctrs_continue
-		 sethi			%hi(TSTATE_PEF), %o0
-		mov			%l5, %o1
-		mov			%l6, %o2
-		add			%sp, PTREGS_OFF, %o0
-		mov			%l0, %o3
-		call			do_notify_resume
-
-		 wrpr			%g0, RTRAP_PSTATE, %pstate
-		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
-		clr			%l6
-		/* Signal delivery can modify pt_regs tstate, so we must
-		 * reload it.
-		 */
-		ldx			[%sp + PTREGS_OFF + PT_V9_TSTATE], %l1
-		sethi			%hi(0xf << 20), %l4
-		and			%l1, %l4, %l4
-		andn			%l1, %l4, %l1
-		ba,pt			%xcc, __handle_perfctrs_continue
-
-		 sethi			%hi(TSTATE_PEF), %o0
 __handle_userfpu:
 		rd			%fprs, %l5
 		andcc			%l5, FPRS_FEF, %g0
 		sethi			%hi(TSTATE_PEF), %o0
 		be,a,pn			%icc, __handle_userfpu_continue
 		 andn			%l1, %o0, %l1
-		ba,a,pt			%xcc, __handle_userfpu_continue
+		ba,pt			%xcc, __handle_userfpu_continue
+		 nop
 
 __handle_signal:
 		mov			%l5, %o1
@@ -215,12 +172,8 @@
 		brnz,pn			%o2, __handle_user_windows
 		 nop
 __handle_user_windows_continue:
-		ldx			[%g6 + TI_FLAGS], %l5
-		andcc			%l5, _TIF_PERFCTR, %g0
 		sethi			%hi(TSTATE_PEF), %o0
-		bne,pn			%xcc, __handle_perfctrs
-__handle_perfctrs_continue:
-		 andcc			%l1, %o0, %g0
+		andcc			%l1, %o0, %g0
 
 		/* This fpdepth clear is necessary for non-syscall rtraps only */
 user_nowork:
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/signal.c linux-2.6.25-id/arch/sparc64/kernel/signal.c
--- linux-2.6.25-org/arch/sparc64/kernel/signal.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/signal.c	2008-04-23 11:19:49.000000000 +0200
@@ -22,6 +22,7 @@
 #include <linux/tty.h>
 #include <linux/binfmts.h>
 #include <linux/bitops.h>
+#include <linux/perfmon.h>
 
 #include <asm/uaccess.h>
 #include <asm/ptrace.h>
@@ -580,6 +581,9 @@
 void do_notify_resume(struct pt_regs *regs, unsigned long orig_i0, int restart_syscall,
 		      unsigned long thread_info_flags)
 {
+	if (thread_info_flags & _TIF_PERFMON_WORK)
+		pfm_handle_work(regs);
+
 	if (thread_info_flags & (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK))
 		do_signal(regs, orig_i0, restart_syscall);
 }
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/smp.c linux-2.6.25-id/arch/sparc64/kernel/smp.c
--- linux-2.6.25-org/arch/sparc64/kernel/smp.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/smp.c	2008-04-23 11:19:49.000000000 +0200
@@ -860,6 +860,28 @@
 				      cpu_online_map);
 }
 
+int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
+			     int nonatomic, int wait)
+{
+	/* prevent preemption and reschedule on another processor */
+	int ret;
+	int me = get_cpu();
+	if (cpu == me) {
+		local_irq_disable();
+		func(info);
+		local_irq_enable();
+		put_cpu();
+		return 0;
+	}
+
+	ret = smp_call_function_mask(func, info, nonatomic, wait,
+				     cpumask_of_cpu(cpu));
+
+	put_cpu();
+	return ret;
+}
+EXPORT_SYMBOL(smp_call_function_single);
+
 void smp_call_function_client(int irq, struct pt_regs *regs)
 {
 	void (*func) (void *info) = call_data->func;
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/sys_sparc.c linux-2.6.25-id/arch/sparc64/kernel/sys_sparc.c
--- linux-2.6.25-org/arch/sparc64/kernel/sys_sparc.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/sys_sparc.c	2008-04-23 11:19:49.000000000 +0200
@@ -857,106 +857,10 @@
 	return ret;
 }
 
-/* Invoked by rtrap code to update performance counters in
- * user space.
- */
-asmlinkage void update_perfctrs(void)
-{
-	unsigned long pic, tmp;
-
-	read_pic(pic);
-	tmp = (current_thread_info()->kernel_cntd0 += (unsigned int)pic);
-	__put_user(tmp, current_thread_info()->user_cntd0);
-	tmp = (current_thread_info()->kernel_cntd1 += (pic >> 32));
-	__put_user(tmp, current_thread_info()->user_cntd1);
-	reset_pic();
-}
-
 asmlinkage long sys_perfctr(int opcode, unsigned long arg0, unsigned long arg1, unsigned long arg2)
 {
-	int err = 0;
-
-	switch(opcode) {
-	case PERFCTR_ON:
-		current_thread_info()->pcr_reg = arg2;
-		current_thread_info()->user_cntd0 = (u64 __user *) arg0;
-		current_thread_info()->user_cntd1 = (u64 __user *) arg1;
-		current_thread_info()->kernel_cntd0 =
-			current_thread_info()->kernel_cntd1 = 0;
-		write_pcr(arg2);
-		reset_pic();
-		set_thread_flag(TIF_PERFCTR);
-		break;
-
-	case PERFCTR_OFF:
-		err = -EINVAL;
-		if (test_thread_flag(TIF_PERFCTR)) {
-			current_thread_info()->user_cntd0 =
-				current_thread_info()->user_cntd1 = NULL;
-			current_thread_info()->pcr_reg = 0;
-			write_pcr(0);
-			clear_thread_flag(TIF_PERFCTR);
-			err = 0;
-		}
-		break;
-
-	case PERFCTR_READ: {
-		unsigned long pic, tmp;
-
-		if (!test_thread_flag(TIF_PERFCTR)) {
-			err = -EINVAL;
-			break;
-		}
-		read_pic(pic);
-		tmp = (current_thread_info()->kernel_cntd0 += (unsigned int)pic);
-		err |= __put_user(tmp, current_thread_info()->user_cntd0);
-		tmp = (current_thread_info()->kernel_cntd1 += (pic >> 32));
-		err |= __put_user(tmp, current_thread_info()->user_cntd1);
-		reset_pic();
-		break;
-	}
-
-	case PERFCTR_CLRPIC:
-		if (!test_thread_flag(TIF_PERFCTR)) {
-			err = -EINVAL;
-			break;
-		}
-		current_thread_info()->kernel_cntd0 =
-			current_thread_info()->kernel_cntd1 = 0;
-		reset_pic();
-		break;
-
-	case PERFCTR_SETPCR: {
-		u64 __user *user_pcr = (u64 __user *)arg0;
-
-		if (!test_thread_flag(TIF_PERFCTR)) {
-			err = -EINVAL;
-			break;
-		}
-		err |= __get_user(current_thread_info()->pcr_reg, user_pcr);
-		write_pcr(current_thread_info()->pcr_reg);
-		current_thread_info()->kernel_cntd0 =
-			current_thread_info()->kernel_cntd1 = 0;
-		reset_pic();
-		break;
-	}
-
-	case PERFCTR_GETPCR: {
-		u64 __user *user_pcr = (u64 __user *)arg0;
-
-		if (!test_thread_flag(TIF_PERFCTR)) {
-			err = -EINVAL;
-			break;
-		}
-		err |= __put_user(current_thread_info()->pcr_reg, user_pcr);
-		break;
-	}
-
-	default:
-		err = -EINVAL;
-		break;
-	};
-	return err;
+	/* Superceded by perfmon2 */
+	return -ENOSYS;
 }
 
 /*
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/traps.c linux-2.6.25-id/arch/sparc64/kernel/traps.c
--- linux-2.6.25-org/arch/sparc64/kernel/traps.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/traps.c	2008-04-23 11:19:49.000000000 +0200
@@ -2473,86 +2473,90 @@
 /* Only invoked on boot processor. */
 void __init trap_init(void)
 {
-	/* Compile time sanity check. */
-	if (TI_TASK != offsetof(struct thread_info, task) ||
-	    TI_FLAGS != offsetof(struct thread_info, flags) ||
-	    TI_CPU != offsetof(struct thread_info, cpu) ||
-	    TI_FPSAVED != offsetof(struct thread_info, fpsaved) ||
-	    TI_KSP != offsetof(struct thread_info, ksp) ||
-	    TI_FAULT_ADDR != offsetof(struct thread_info, fault_address) ||
-	    TI_KREGS != offsetof(struct thread_info, kregs) ||
-	    TI_UTRAPS != offsetof(struct thread_info, utraps) ||
-	    TI_EXEC_DOMAIN != offsetof(struct thread_info, exec_domain) ||
-	    TI_REG_WINDOW != offsetof(struct thread_info, reg_window) ||
-	    TI_RWIN_SPTRS != offsetof(struct thread_info, rwbuf_stkptrs) ||
-	    TI_GSR != offsetof(struct thread_info, gsr) ||
-	    TI_XFSR != offsetof(struct thread_info, xfsr) ||
-	    TI_USER_CNTD0 != offsetof(struct thread_info, user_cntd0) ||
-	    TI_USER_CNTD1 != offsetof(struct thread_info, user_cntd1) ||
-	    TI_KERN_CNTD0 != offsetof(struct thread_info, kernel_cntd0) ||
-	    TI_KERN_CNTD1 != offsetof(struct thread_info, kernel_cntd1) ||
-	    TI_PCR != offsetof(struct thread_info, pcr_reg) ||
-	    TI_PRE_COUNT != offsetof(struct thread_info, preempt_count) ||
-	    TI_NEW_CHILD != offsetof(struct thread_info, new_child) ||
-	    TI_SYS_NOERROR != offsetof(struct thread_info, syscall_noerror) ||
-	    TI_RESTART_BLOCK != offsetof(struct thread_info, restart_block) ||
-	    TI_KUNA_REGS != offsetof(struct thread_info, kern_una_regs) ||
-	    TI_KUNA_INSN != offsetof(struct thread_info, kern_una_insn) ||
-	    TI_FPREGS != offsetof(struct thread_info, fpregs) ||
-	    (TI_FPREGS & (64 - 1)))
-		thread_info_offsets_are_bolixed_dave();
-
-	if (TRAP_PER_CPU_THREAD != offsetof(struct trap_per_cpu, thread) ||
-	    (TRAP_PER_CPU_PGD_PADDR !=
-	     offsetof(struct trap_per_cpu, pgd_paddr)) ||
-	    (TRAP_PER_CPU_CPU_MONDO_PA !=
-	     offsetof(struct trap_per_cpu, cpu_mondo_pa)) ||
-	    (TRAP_PER_CPU_DEV_MONDO_PA !=
-	     offsetof(struct trap_per_cpu, dev_mondo_pa)) ||
-	    (TRAP_PER_CPU_RESUM_MONDO_PA !=
-	     offsetof(struct trap_per_cpu, resum_mondo_pa)) ||
-	    (TRAP_PER_CPU_RESUM_KBUF_PA !=
-	     offsetof(struct trap_per_cpu, resum_kernel_buf_pa)) ||
-	    (TRAP_PER_CPU_NONRESUM_MONDO_PA !=
-	     offsetof(struct trap_per_cpu, nonresum_mondo_pa)) ||
-	    (TRAP_PER_CPU_NONRESUM_KBUF_PA !=
-	     offsetof(struct trap_per_cpu, nonresum_kernel_buf_pa)) ||
-	    (TRAP_PER_CPU_FAULT_INFO !=
-	     offsetof(struct trap_per_cpu, fault_info)) ||
-	    (TRAP_PER_CPU_CPU_MONDO_BLOCK_PA !=
-	     offsetof(struct trap_per_cpu, cpu_mondo_block_pa)) ||
-	    (TRAP_PER_CPU_CPU_LIST_PA !=
-	     offsetof(struct trap_per_cpu, cpu_list_pa)) ||
-	    (TRAP_PER_CPU_TSB_HUGE !=
-	     offsetof(struct trap_per_cpu, tsb_huge)) ||
-	    (TRAP_PER_CPU_TSB_HUGE_TEMP !=
-	     offsetof(struct trap_per_cpu, tsb_huge_temp)) ||
-	    (TRAP_PER_CPU_IRQ_WORKLIST_PA !=
-	     offsetof(struct trap_per_cpu, irq_worklist_pa)) ||
-	    (TRAP_PER_CPU_CPU_MONDO_QMASK !=
-	     offsetof(struct trap_per_cpu, cpu_mondo_qmask)) ||
-	    (TRAP_PER_CPU_DEV_MONDO_QMASK !=
-	     offsetof(struct trap_per_cpu, dev_mondo_qmask)) ||
-	    (TRAP_PER_CPU_RESUM_QMASK !=
-	     offsetof(struct trap_per_cpu, resum_qmask)) ||
-	    (TRAP_PER_CPU_NONRESUM_QMASK !=
-	     offsetof(struct trap_per_cpu, nonresum_qmask)))
-		trap_per_cpu_offsets_are_bolixed_dave();
-
-	if ((TSB_CONFIG_TSB !=
-	     offsetof(struct tsb_config, tsb)) ||
-	    (TSB_CONFIG_RSS_LIMIT !=
-	     offsetof(struct tsb_config, tsb_rss_limit)) ||
-	    (TSB_CONFIG_NENTRIES !=
-	     offsetof(struct tsb_config, tsb_nentries)) ||
-	    (TSB_CONFIG_REG_VAL !=
-	     offsetof(struct tsb_config, tsb_reg_val)) ||
-	    (TSB_CONFIG_MAP_VADDR !=
-	     offsetof(struct tsb_config, tsb_map_vaddr)) ||
-	    (TSB_CONFIG_MAP_PTE !=
-	     offsetof(struct tsb_config, tsb_map_pte)))
-		tsb_config_offsets_are_bolixed_dave();
-
+	BUILD_BUG_ON(TI_TASK != offsetof(struct thread_info, task));
+	BUILD_BUG_ON(TI_FLAGS != offsetof(struct thread_info, flags));
+	BUILD_BUG_ON(TI_CPU != offsetof(struct thread_info, cpu));
+	BUILD_BUG_ON(TI_FPSAVED != offsetof(struct thread_info, fpsaved));
+	BUILD_BUG_ON(TI_KSP != offsetof(struct thread_info, ksp));
+	BUILD_BUG_ON(TI_FAULT_ADDR !=
+		     offsetof(struct thread_info, fault_address));
+	BUILD_BUG_ON(TI_KREGS != offsetof(struct thread_info, kregs));
+	BUILD_BUG_ON(TI_UTRAPS != offsetof(struct thread_info, utraps));
+	BUILD_BUG_ON(TI_EXEC_DOMAIN !=
+		     offsetof(struct thread_info, exec_domain));
+	BUILD_BUG_ON(TI_REG_WINDOW !=
+		     offsetof(struct thread_info, reg_window));
+	BUILD_BUG_ON(TI_RWIN_SPTRS !=
+		     offsetof(struct thread_info, rwbuf_stkptrs));
+	BUILD_BUG_ON(TI_GSR != offsetof(struct thread_info, gsr));
+	BUILD_BUG_ON(TI_XFSR != offsetof(struct thread_info, xfsr));
+	BUILD_BUG_ON(TI_PRE_COUNT !=
+		     offsetof(struct thread_info, preempt_count));
+	BUILD_BUG_ON(TI_NEW_CHILD !=
+		     offsetof(struct thread_info, new_child));
+	BUILD_BUG_ON(TI_SYS_NOERROR !=
+		     offsetof(struct thread_info, syscall_noerror));
+	BUILD_BUG_ON(TI_RESTART_BLOCK !=
+		     offsetof(struct thread_info, restart_block));
+	BUILD_BUG_ON(TI_KUNA_REGS !=
+		     offsetof(struct thread_info, kern_una_regs));
+	BUILD_BUG_ON(TI_KUNA_INSN !=
+		     offsetof(struct thread_info, kern_una_insn));
+	BUILD_BUG_ON(TI_FPREGS != offsetof(struct thread_info, fpregs));
+	BUILD_BUG_ON((TI_FPREGS & (64 - 1)));
+ 
+	BUILD_BUG_ON(TRAP_PER_CPU_THREAD !=
+		     offsetof(struct trap_per_cpu, thread));
+	BUILD_BUG_ON(TRAP_PER_CPU_PGD_PADDR !=
+		     offsetof(struct trap_per_cpu, pgd_paddr));
+	BUILD_BUG_ON(TRAP_PER_CPU_CPU_MONDO_PA !=
+		     offsetof(struct trap_per_cpu, cpu_mondo_pa));
+	BUILD_BUG_ON(TRAP_PER_CPU_DEV_MONDO_PA !=
+		     offsetof(struct trap_per_cpu, dev_mondo_pa));
+	BUILD_BUG_ON(TRAP_PER_CPU_RESUM_MONDO_PA !=
+		     offsetof(struct trap_per_cpu, resum_mondo_pa));
+	BUILD_BUG_ON(TRAP_PER_CPU_RESUM_KBUF_PA !=
+		     offsetof(struct trap_per_cpu, resum_kernel_buf_pa));
+	BUILD_BUG_ON(TRAP_PER_CPU_NONRESUM_MONDO_PA !=
+		     offsetof(struct trap_per_cpu, nonresum_mondo_pa));
+	BUILD_BUG_ON(TRAP_PER_CPU_NONRESUM_KBUF_PA !=
+		     offsetof(struct trap_per_cpu, nonresum_kernel_buf_pa));
+	BUILD_BUG_ON(TRAP_PER_CPU_FAULT_INFO !=
+		     offsetof(struct trap_per_cpu, fault_info));
+	BUILD_BUG_ON(TRAP_PER_CPU_CPU_MONDO_BLOCK_PA !=
+		     offsetof(struct trap_per_cpu, cpu_mondo_block_pa));
+	BUILD_BUG_ON(TRAP_PER_CPU_CPU_LIST_PA !=
+		     offsetof(struct trap_per_cpu, cpu_list_pa));
+	BUILD_BUG_ON(TRAP_PER_CPU_TSB_HUGE !=
+		     offsetof(struct trap_per_cpu, tsb_huge));
+	BUILD_BUG_ON(TRAP_PER_CPU_TSB_HUGE_TEMP !=
+		     offsetof(struct trap_per_cpu, tsb_huge_temp));
+#if 0
+	BUILD_BUG_ON(TRAP_PER_CPU_IRQ_WORKLIST !=
+		     offsetof(struct trap_per_cpu, irq_worklist));
+#endif
+	BUILD_BUG_ON(TRAP_PER_CPU_CPU_MONDO_QMASK !=
+		     offsetof(struct trap_per_cpu, cpu_mondo_qmask));
+	BUILD_BUG_ON(TRAP_PER_CPU_DEV_MONDO_QMASK !=
+		     offsetof(struct trap_per_cpu, dev_mondo_qmask));
+	BUILD_BUG_ON(TRAP_PER_CPU_RESUM_QMASK !=
+		     offsetof(struct trap_per_cpu, resum_qmask));
+	BUILD_BUG_ON(TRAP_PER_CPU_NONRESUM_QMASK !=
+		     offsetof(struct trap_per_cpu, nonresum_qmask));
+ 
+	BUILD_BUG_ON(TSB_CONFIG_TSB !=
+		      offsetof(struct tsb_config, tsb));
+	BUILD_BUG_ON(TSB_CONFIG_RSS_LIMIT !=
+		     offsetof(struct tsb_config, tsb_rss_limit));
+	BUILD_BUG_ON(TSB_CONFIG_NENTRIES !=
+		     offsetof(struct tsb_config, tsb_nentries));
+	BUILD_BUG_ON(TSB_CONFIG_REG_VAL !=
+		     offsetof(struct tsb_config, tsb_reg_val));
+	BUILD_BUG_ON(TSB_CONFIG_MAP_VADDR !=
+		     offsetof(struct tsb_config, tsb_map_vaddr));
+	BUILD_BUG_ON(TSB_CONFIG_MAP_PTE !=
+		     offsetof(struct tsb_config, tsb_map_pte));
+ 
 	/* Attach to the address space of init_task.  On SMP we
 	 * do this in smp.c:smp_callin for other cpus.
 	 */
diff -Naur linux-2.6.25-org/arch/sparc64/kernel/ttable.S linux-2.6.25-id/arch/sparc64/kernel/ttable.S
--- linux-2.6.25-org/arch/sparc64/kernel/ttable.S	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/sparc64/kernel/ttable.S	2008-04-23 11:19:49.000000000 +0200
@@ -61,7 +61,7 @@
 tl0_irq6:	BTRAP(0x46) BTRAP(0x47) BTRAP(0x48) BTRAP(0x49)
 tl0_irq10:	BTRAP(0x4a) BTRAP(0x4b) BTRAP(0x4c) BTRAP(0x4d)
 tl0_irq14:	TRAP_IRQ(timer_interrupt, 14)
-tl0_irq15:	TRAP_IRQ(handler_irq, 15)
+tl0_irq15:	TRAP_IRQ(perfctr_irq, 15)
 tl0_resv050:	BTRAP(0x50) BTRAP(0x51) BTRAP(0x52) BTRAP(0x53) BTRAP(0x54) BTRAP(0x55)
 tl0_resv056:	BTRAP(0x56) BTRAP(0x57) BTRAP(0x58) BTRAP(0x59) BTRAP(0x5a) BTRAP(0x5b)
 tl0_resv05c:	BTRAP(0x5c) BTRAP(0x5d) BTRAP(0x5e) BTRAP(0x5f)
diff -Naur linux-2.6.25-org/arch/sparc64/perfmon/Kconfig linux-2.6.25-id/arch/sparc64/perfmon/Kconfig
--- linux-2.6.25-org/arch/sparc64/perfmon/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/sparc64/perfmon/Kconfig	2008-04-23 11:19:49.000000000 +0200
@@ -0,0 +1,16 @@
+menu "Hardware Performance Monitoring support"
+config PERFMON
+	bool "Perfmon2 performance monitoring interface"
+	default n
+	help
+	Enables the perfmon2 interface to access the hardware
+	performance counters. See <http://perfmon2.sf.net/> for
+	more details.
+
+config PERFMON_DEBUG
+	bool "Perfmon debugging"
+	depends on PERFMON
+	default n
+	help
+	Enables perfmon debugging support
+endmenu
diff -Naur linux-2.6.25-org/arch/sparc64/perfmon/Makefile linux-2.6.25-id/arch/sparc64/perfmon/Makefile
--- linux-2.6.25-org/arch/sparc64/perfmon/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/sparc64/perfmon/Makefile	2008-04-23 11:19:49.000000000 +0200
@@ -0,0 +1 @@
+obj-$(CONFIG_PERFMON)	+= perfmon.o
diff -Naur linux-2.6.25-org/arch/sparc64/perfmon/perfmon.c linux-2.6.25-id/arch/sparc64/perfmon/perfmon.c
--- linux-2.6.25-org/arch/sparc64/perfmon/perfmon.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/sparc64/perfmon/perfmon.c	2008-04-23 11:19:49.000000000 +0200
@@ -0,0 +1,423 @@
+/* perfmon.c: sparc64 perfmon support
+ * 
+ * Copyright (C) 2007 David S. Miller (davem@davemloft.net)
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <linux/irq.h>
+
+#include <asm/system.h>
+#include <asm/spitfire.h>
+#include <asm/hypervisor.h>
+
+struct pcr_ops {
+	void (*write)(u64);
+	u64 (*read)(void);
+};
+
+static void direct_write_pcr(u64 val)
+{
+	write_pcr(val);
+}
+
+static u64 direct_read_pcr(void)
+{
+	u64 pcr;
+
+	read_pcr(pcr);
+
+	return pcr;
+}
+
+static struct pcr_ops direct_pcr_ops = {
+	.write	= direct_write_pcr,
+	.read	= direct_read_pcr,
+};
+
+/* Using the hypervisor call is needed so that we can set the
+ * hypervisor trace bit correctly, which is hyperprivileged.
+ */
+static void n2_write_pcr(u64 val)
+{
+	unsigned long ret;
+
+	ret = sun4v_niagara2_setperf(HV_N2_PERF_SPARC_CTL, val);
+	if (val != HV_EOK)
+		write_pcr(val);
+}
+
+static u64 n2_read_pcr(void)
+{
+	u64 pcr;
+
+	read_pcr(pcr);
+
+	return pcr;
+}
+
+static struct pcr_ops n2_pcr_ops = {
+	.write	= n2_write_pcr,
+	.read	= n2_read_pcr,
+};
+
+static struct pcr_ops *pcr_ops;
+
+void pfm_arch_write_pmc(struct pfm_context *ctx,
+			unsigned int cnum, u64 value)
+{
+	/*
+	 * we only write to the actual register when monitoring is
+	 * active (pfm_start was issued)
+	 */
+	if (ctx && ctx->flags.started == 0)
+		return;
+
+	pcr_ops->write(value);
+}
+
+u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
+{
+	return pcr_ops->read();
+}
+
+/*
+ * collect pending overflowed PMDs. Called from pfm_ctxsw()
+ * and from PMU interrupt handler. Must fill in set->povfl_pmds[]
+ * and set->npend_ovfls. Interrupts are masked
+ */
+static void __pfm_get_ovfl_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	unsigned int max = pfm_pmu_conf->regs.max_intr_pmd;
+	u64 wmask = 1ULL << pfm_pmu_conf->counter_width;
+	u64 *intr_pmds = pfm_pmu_conf->regs.intr_pmds;
+	u64 *used_mask = set->used_pmds;
+	u64 mask[PFM_PMD_BV];
+	unsigned int i;
+
+	bitmap_and(cast_ulp(mask),
+		   cast_ulp(intr_pmds),
+		   cast_ulp(used_mask),
+		   max);
+
+	/*
+	 * check all PMD that can generate interrupts
+	 * (that includes counters)
+	 */
+	for (i = 0; i < max; i++) {
+		if (test_bit(i, mask)) {
+			u64 new_val = pfm_arch_read_pmd(ctx, i);
+
+			PFM_DBG_ovfl("pmd%u new_val=0x%llx bit=%d\n",
+				     i, (unsigned long long)new_val,
+				     (new_val&wmask) ? 1 : 0);
+
+ 			if (new_val & wmask) {
+				__set_bit(i, set->povfl_pmds);
+				set->npend_ovfls++;
+			}
+		}
+	}
+}
+
+static void pfm_stop_active(struct task_struct *task, struct pfm_context *ctx,
+			       struct pfm_event_set *set)
+{
+	unsigned int i, max = pfm_pmu_conf->regs.max_pmc;
+
+	/*
+	 * clear enable bits, assume all pmcs are enable pmcs
+	 */
+	for (i = 0; i < max; i++) {
+		if (test_bit(i, set->used_pmcs))
+			pfm_arch_write_pmc(ctx, i, 0);
+	}
+
+	if (set->npend_ovfls)
+		return;
+
+	__pfm_get_ovfl_pmds(ctx, set);
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * Context is locked. Interrupts are masked. Monitoring is active.
+ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
+ *
+ * for per-thread:
+ * 	must stop monitoring for the task
+ *
+ * Return:
+ * 	non-zero : did not save PMDs (as part of stopping the PMU)
+ * 	       0 : saved PMDs (no need to save them in caller)
+ */
+int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
+			      struct pfm_event_set *set)
+{
+	/*
+	 * disable lazy restore of PMC registers.
+	 */
+	set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
+
+	pfm_stop_active(task, ctx, set);
+
+	return 1;
+}
+
+/*
+ * Called from pfm_stop() and idle notifier
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-thread:
+ *   task is not necessarily current. If not current task, then
+ *   task is guaranteed stopped and off any cpu. Access to PMU
+ *   is not guaranteed. Interrupts are masked. Context is locked.
+ *   Set is the active set.
+ *
+ * For system-wide:
+ * 	task is current
+ *
+ * must disable active monitoring. ctx cannot be NULL
+ */
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
+		   struct pfm_event_set *set)
+{
+	/*
+	 * no need to go through stop_save()
+	 * if we are already stopped
+	 */
+	if (!ctx->flags.started || ctx->state == PFM_CTX_MASKED)
+		return;
+
+	/*
+	 * stop live registers and collect pending overflow
+	 */
+	if (task == current)
+		pfm_stop_active(task, ctx, set);
+}
+
+/*
+ * Enable active monitoring. Called from pfm_start() and
+ * pfm_arch_unmask_monitoring().
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-trhead:
+ * 	Task is not necessarily current. If not current task, then task
+ * 	is guaranteed stopped and off any cpu. Access to PMU is not guaranteed.
+ *
+ * For system-wide:
+ * 	task is always current
+ *
+ * must enable active monitoring.
+ */
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
+	            struct pfm_event_set *set)
+{
+	unsigned int max_pmc = pfm_pmu_conf->regs.max_pmc;
+	unsigned int i;
+
+	if (task != current)
+		return;
+
+	for (i = 0; i < max_pmc; i++) {
+		if (test_bit(i, set->used_pmcs))
+			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
+	}
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
+ * context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMD registers from set.
+ */
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	unsigned int max_pmd = pfm_pmu_conf->regs.max_pmd;
+	u64 ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	u64 *impl_pmds = pfm_pmu_conf->regs.pmds;
+	unsigned int i;
+
+	/*
+	 * must restore all pmds to avoid leaking
+	 * information to user.
+	 */
+	for (i = 0; i < max_pmd; i++) {
+		u64 val;
+
+		if (test_bit(i, impl_pmds) == 0)
+			continue;
+
+		val = set->pmds[i].value;
+
+		/*
+		 * set upper bits for counter to ensure
+		 * overflow will trigger
+		 */
+		val &= ovfl_mask;
+
+		pfm_arch_write_pmd(ctx, i, val);
+	}
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw().
+ * Context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMC registers from set, if needed.
+ */
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	unsigned int max_pmc = pfm_pmu_conf->regs.max_pmc;
+	u64 *impl_pmcs = pfm_pmu_conf->regs.pmcs;
+	unsigned int i;
+
+	/* If we're masked or stopped we don't need to bother restoring
+	 * the PMCs now.
+	 */
+	if (ctx->state == PFM_CTX_MASKED || ctx->flags.started == 0)
+		return;
+
+	/*
+	 * restore all pmcs
+	 */
+	for (i = 0; i < max_pmc; i++)
+		if (test_bit(i, impl_pmcs))
+			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
+}
+
+char *pfm_arch_get_pmu_module_name(void)
+{
+	return NULL;
+}
+
+void perfmon_interrupt(struct pt_regs *regs)
+{
+	pfm_interrupt_handler(instruction_pointer(regs), regs);
+}
+
+static struct pfm_regmap_desc pfm_sparc64_pmc_desc[] = {
+	PMC_D(PFM_REG_I, "PCR", 0, 0, 0, 0),
+};
+
+static struct pfm_regmap_desc pfm_sparc64_pmd_desc[] = {
+	PMD_D(PFM_REG_C, "PIC0", 0),
+	PMD_D(PFM_REG_C, "PIC1", 0),
+};
+
+static int pfm_sparc64_probe(void)
+{
+	return 0;
+}
+
+static struct pfm_pmu_config pmu_sparc64_pmu_conf = {
+	.counter_width	= 31,
+	.pmd_desc	= pfm_sparc64_pmd_desc,
+	.num_pmd_entries= 2,
+	.pmc_desc	= pfm_sparc64_pmc_desc,
+	.num_pmc_entries= 1,
+	.probe_pmu	= pfm_sparc64_probe,
+	.flags		= PFM_PMU_BUILTIN_FLAG,
+	.owner		= THIS_MODULE,
+};
+
+static unsigned long perf_hsvc_group;
+static unsigned long perf_hsvc_major;
+static unsigned long perf_hsvc_minor;
+
+static int __init register_perf_hsvc(void)
+{
+	if (tlb_type == hypervisor) {
+		switch (sun4v_chip_type) {
+		case SUN4V_CHIP_NIAGARA1:
+			perf_hsvc_group = HV_GRP_NIAG_PERF;
+			break;
+
+		case SUN4V_CHIP_NIAGARA2:
+			perf_hsvc_group = HV_GRP_NIAG2_PERF;
+			break;
+
+		default:
+			return -ENODEV;
+		}
+
+
+		perf_hsvc_major = 1;
+		perf_hsvc_minor = 0;
+		if (sun4v_hvapi_register(perf_hsvc_group,
+					 perf_hsvc_major,
+					 &perf_hsvc_minor)) {
+			printk("perfmon: Could not register N2 hvapi.\n");
+			return -ENODEV;
+		}
+	}
+	return 0;
+}
+
+static void unregister_perf_hsvc(void)
+{
+	if (tlb_type != hypervisor)
+		return;
+	sun4v_hvapi_unregister(perf_hsvc_group);
+}
+
+static int __init pfm_sparc64_pmu_init(void)
+{
+	u64 mask;
+	int err;
+
+	err = register_perf_hsvc();
+	if (err)
+		return err;
+
+	if (tlb_type == hypervisor &&
+	    sun4v_chip_type == SUN4V_CHIP_NIAGARA2)
+		pcr_ops = &n2_pcr_ops;
+	else
+		pcr_ops = &direct_pcr_ops;
+
+	if (!strcmp(sparc_pmu_type, "ultra12"))
+		mask = (0xf << 11) | (0xf << 4) | 0x7;
+	else if (!strcmp(sparc_pmu_type, "ultra3") ||
+	    !strcmp(sparc_pmu_type, "ultra3i") ||
+	    !strcmp(sparc_pmu_type, "ultra3+") ||
+	    !strcmp(sparc_pmu_type, "ultra4+"))
+		mask = (0x3f << 11) | (0x3f << 4) | 0x7;
+	else if (!strcmp(sparc_pmu_type, "niagara2"))
+		mask = ((1UL << 63) | (1UL << 62) |
+			(1UL << 31) | (0xfUL << 27) | (0xffUL << 19) |
+			(1UL << 18) | (0xfUL << 14) | (0xff << 6) |
+			(0x3UL << 4) | 0x7UL);
+	else if (!strcmp(sparc_pmu_type, "niagara"))
+		mask = ((1UL << 9) | (1UL << 8) |
+			(0x7UL << 4) | 0x7UL);
+	else {
+		err = -ENODEV;
+		goto out_err;
+	}
+
+	pmu_sparc64_pmu_conf.pmu_name = sparc_pmu_type;
+	pfm_sparc64_pmc_desc[0].rsvd_msk = ~mask;
+
+	return pfm_pmu_register(&pmu_sparc64_pmu_conf);
+
+out_err:
+	unregister_perf_hsvc();
+	return err;
+}
+
+static void __exit pfm_sparc64_pmu_exit(void)
+{
+	unregister_perf_hsvc();
+	return pfm_pmu_unregister(&pmu_sparc64_pmu_conf);
+}
+
+module_init(pfm_sparc64_pmu_init);
+module_exit(pfm_sparc64_pmu_exit);
diff -Naur linux-2.6.25-org/arch/x86/Kconfig linux-2.6.25-id/arch/x86/Kconfig
--- linux-2.6.25-org/arch/x86/Kconfig	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/Kconfig	2008-04-23 11:19:49.000000000 +0200
@@ -1215,6 +1215,8 @@
 
 	  If unsure, say Y.
 
+source "arch/x86/perfmon/Kconfig"
+
 endmenu
 
 config ARCH_ENABLE_MEMORY_HOTPLUG
diff -Naur linux-2.6.25-org/arch/x86/kernel/apic_32.c linux-2.6.25-id/arch/x86/kernel/apic_32.c
--- linux-2.6.25-org/arch/x86/kernel/apic_32.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/apic_32.c	2008-04-23 11:21:39.000000000 +0200
@@ -28,6 +28,7 @@
 #include <linux/acpi_pmtmr.h>
 #include <linux/module.h>
 #include <linux/dmi.h>
+#include <linux/perfmon.h>
 
 #include <asm/atomic.h>
 #include <asm/smp.h>
@@ -621,6 +622,37 @@
 }
 
 /*
+ * Setup extended LVT, AMD specific (K8, family 10h)
+ *
+ * Vector mappings are hard coded. On K8 only offset 0 (APIC500) and
+ * MCE interrupts are supported. Thus MCE offset must be set to 0.
+ */
+
+#define APIC_EILVT_LVTOFF_MCE 0
+#define APIC_EILVT_LVTOFF_IBS 1
+
+static void setup_APIC_eilvt(u8 lvt_off, u8 vector, u8 msg_type, u8 mask)
+{
+	unsigned long reg = (lvt_off << 4) + APIC_EILVT0;
+	unsigned int  v   = (mask << 16) | (msg_type << 8) | vector;
+	apic_write(reg, v);
+}
+
+u8 setup_APIC_eilvt_mce(u8 vector, u8 msg_type, u8 mask)
+{
+	setup_APIC_eilvt(APIC_EILVT_LVTOFF_MCE, vector, msg_type, mask);
+	return APIC_EILVT_LVTOFF_MCE;
+}
+EXPORT_SYMBOL(setup_APIC_eilvt_mce);
+
+u8 setup_APIC_eilvt_ibs(u8 vector, u8 msg_type, u8 mask)
+{
+	setup_APIC_eilvt(APIC_EILVT_LVTOFF_IBS, vector, msg_type, mask);
+	return APIC_EILVT_LVTOFF_IBS;
+}
+EXPORT_SYMBOL(setup_APIC_eilvt_ibs);
+
+/*
  * Local APIC start and shutdown
  */
 
@@ -1307,6 +1339,9 @@
 #ifdef CONFIG_X86_MCE_P4THERMAL
 	set_intr_gate(THERMAL_APIC_VECTOR, thermal_interrupt);
 #endif
+#ifdef CONFIG_PERFMON
+	set_intr_gate(LOCAL_PERFMON_VECTOR, pmu_interrupt);
+#endif
 }
 
 /**
diff -Naur linux-2.6.25-org/arch/x86/kernel/cpu/common.c linux-2.6.25-id/arch/x86/kernel/cpu/common.c
--- linux-2.6.25-org/arch/x86/kernel/cpu/common.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/cpu/common.c	2008-04-23 11:21:39.000000000 +0200
@@ -5,6 +5,7 @@
 #include <linux/module.h>
 #include <linux/percpu.h>
 #include <linux/bootmem.h>
+#include <linux/perfmon.h>
 #include <asm/semaphore.h>
 #include <asm/processor.h>
 #include <asm/i387.h>
@@ -718,6 +719,8 @@
 	current_thread_info()->status = 0;
 	clear_used_math();
 	mxcsr_feature_mask_init();
+
+	pfm_init_percpu();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
diff -Naur linux-2.6.25-org/arch/x86/kernel/entry_32.S linux-2.6.25-id/arch/x86/kernel/entry_32.S
--- linux-2.6.25-org/arch/x86/kernel/entry_32.S	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/entry_32.S	2008-04-23 11:21:41.000000000 +0200
@@ -467,7 +467,7 @@
 	ALIGN
 	RING0_PTREGS_FRAME		# can't unwind into user space anyway
 work_pending:
-	testb $_TIF_NEED_RESCHED, %cl
+	testw $(_TIF_NEED_RESCHED|_TIF_PERFMON_WORK), %cx
 	jz work_notifysig
 work_resched:
 	call schedule
diff -Naur linux-2.6.25-org/arch/x86/kernel/entry_64.S linux-2.6.25-id/arch/x86/kernel/entry_64.S
--- linux-2.6.25-org/arch/x86/kernel/entry_64.S	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/entry_64.S	2008-04-23 11:21:41.000000000 +0200
@@ -729,7 +729,13 @@
 ENTRY(spurious_interrupt)
 	apicinterrupt SPURIOUS_APIC_VECTOR,smp_spurious_interrupt
 END(spurious_interrupt)
-				
+
+#ifdef CONFIG_PERFMON
+ENTRY(pmu_interrupt)
+	apicinterrupt LOCAL_PERFMON_VECTOR,smp_pmu_interrupt
+END(pmu_interrupt)
+#endif
+
 /*
  * Exception entry points.
  */ 		
diff -Naur linux-2.6.25-org/arch/x86/kernel/i8259_64.c linux-2.6.25-id/arch/x86/kernel/i8259_64.c
--- linux-2.6.25-org/arch/x86/kernel/i8259_64.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/i8259_64.c	2008-04-23 11:21:41.000000000 +0200
@@ -11,6 +11,7 @@
 #include <linux/kernel_stat.h>
 #include <linux/sysdev.h>
 #include <linux/bitops.h>
+#include <linux/perfmon.h>
 
 #include <asm/acpi.h>
 #include <asm/atomic.h>
@@ -507,6 +508,9 @@
 	set_intr_gate(SPURIOUS_APIC_VECTOR, spurious_interrupt);
 	set_intr_gate(ERROR_APIC_VECTOR, error_interrupt);
 
+#ifdef CONFIG_PERFMON
+	set_intr_gate(LOCAL_PERFMON_VECTOR, pmu_interrupt);
+#endif
 	if (!acpi_ioapic)
 		setup_irq(2, &irq2);
 }
diff -Naur linux-2.6.25-org/arch/x86/kernel/process_32.c linux-2.6.25-id/arch/x86/kernel/process_32.c
--- linux-2.6.25-org/arch/x86/kernel/process_32.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/process_32.c	2008-04-23 11:21:42.000000000 +0200
@@ -36,6 +36,7 @@
 #include <linux/personality.h>
 #include <linux/tick.h>
 #include <linux/percpu.h>
+#include <linux/perfmon.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -430,6 +431,7 @@
 		tss->x86_tss.io_bitmap_base = INVALID_IO_BITMAP_OFFSET;
 		put_cpu();
 	}
+	pfm_exit_thread(current);
 }
 
 void flush_thread(void)
@@ -486,6 +488,8 @@
 
 	savesegment(gs, p->thread.gs);
 
+	pfm_copy_thread(p);
+
 	tsk = current;
 	if (unlikely(test_tsk_thread_flag(tsk, TIF_IO_BITMAP))) {
 		p->thread.io_bitmap_ptr = kmemdup(tsk->thread.io_bitmap_ptr,
@@ -587,6 +591,10 @@
 #endif
 
 
+	if (test_tsk_thread_flag(next_p, TIF_PERFMON_CTXSW)
+	    || test_tsk_thread_flag(prev_p, TIF_PERFMON_CTXSW))
+		pfm_ctxsw(prev_p, next_p);
+
 	if (!test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
 		/*
 		 * Disable the bitmap via an invalid offset. We still cache
diff -Naur linux-2.6.25-org/arch/x86/kernel/process_64.c linux-2.6.25-id/arch/x86/kernel/process_64.c
--- linux-2.6.25-org/arch/x86/kernel/process_64.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/process_64.c	2008-04-23 11:21:42.000000000 +0200
@@ -36,6 +36,7 @@
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 #include <linux/tick.h>
+#include <linux/perfmon.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -391,6 +392,7 @@
 		t->io_bitmap_max = 0;
 		put_cpu();
 	}
+	pfm_exit_thread(me);
 }
 
 void flush_thread(void)
@@ -494,6 +496,8 @@
 	asm("mov %%es,%0" : "=m" (p->thread.es));
 	asm("mov %%ds,%0" : "=m" (p->thread.ds));
 
+	pfm_copy_thread(p);
+
 	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
 		if (!p->thread.io_bitmap_ptr) {
diff -Naur linux-2.6.25-org/arch/x86/kernel/setup64.c linux-2.6.25-id/arch/x86/kernel/setup64.c
--- linux-2.6.25-org/arch/x86/kernel/setup64.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/setup64.c	2008-04-23 11:21:42.000000000 +0200
@@ -11,6 +11,7 @@
 #include <linux/bootmem.h>
 #include <linux/bitops.h>
 #include <linux/module.h>
+#include <linux/perfmon.h>
 #include <asm/pda.h>
 #include <asm/pgtable.h>
 #include <asm/processor.h>
@@ -341,4 +342,6 @@
 	fpu_init(); 
 
 	raw_local_save_flags(kernel_eflags);
+
+	pfm_init_percpu();
 }
diff -Naur linux-2.6.25-org/arch/x86/kernel/signal_32.c linux-2.6.25-id/arch/x86/kernel/signal_32.c
--- linux-2.6.25-org/arch/x86/kernel/signal_32.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/signal_32.c	2008-04-23 11:21:42.000000000 +0200
@@ -19,6 +19,7 @@
 #include <linux/ptrace.h>
 #include <linux/elf.h>
 #include <linux/binfmts.h>
+#include <linux/perfmon.h>
 #include <asm/processor.h>
 #include <asm/ucontext.h>
 #include <asm/uaccess.h>
@@ -664,6 +665,10 @@
 		clear_thread_flag(TIF_SINGLESTEP);
 	}
 
+	/* process perfmon asynchronous work (e.g. block thread or reset) */
+	if (thread_info_flags & _TIF_PERFMON_WORK)
+		pfm_handle_work(regs);
+
 	/* deal with pending signal delivery */
 	if (thread_info_flags & (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK))
 		do_signal(regs);
diff -Naur linux-2.6.25-org/arch/x86/kernel/signal_64.c linux-2.6.25-id/arch/x86/kernel/signal_64.c
--- linux-2.6.25-org/arch/x86/kernel/signal_64.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/signal_64.c	2008-04-23 11:21:43.000000000 +0200
@@ -19,6 +19,7 @@
 #include <linux/stddef.h>
 #include <linux/personality.h>
 #include <linux/compiler.h>
+#include <linux/perfmon.h>
 #include <asm/ucontext.h>
 #include <asm/uaccess.h>
 #include <asm/i387.h>
@@ -495,6 +496,10 @@
 		clear_thread_flag(TIF_SINGLESTEP);
 	}
 
+	/* process perfmon asynchronous work (e.g. block thread or reset) */
+	if (thread_info_flags & _TIF_PERFMON_WORK)
+		pfm_handle_work(regs);
+
 #ifdef CONFIG_X86_MCE
 	/* notify userspace of pending MCEs */
 	if (thread_info_flags & _TIF_MCE_NOTIFY)
diff -Naur linux-2.6.25-org/arch/x86/kernel/smpboot_32.c linux-2.6.25-id/arch/x86/kernel/smpboot_32.c
--- linux-2.6.25-org/arch/x86/kernel/smpboot_32.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/smpboot_32.c	2008-04-23 11:21:43.000000000 +0200
@@ -36,6 +36,7 @@
 #include <linux/module.h>
 #include <linux/init.h>
 #include <linux/kernel.h>
+#include <linux/perfmon.h>
 
 #include <linux/mm.h>
 #include <linux/sched.h>
@@ -1199,6 +1200,7 @@
 	fixup_irqs(map);
 	/* It's now safe to remove this processor from the online map */
 	cpu_clear(cpu, cpu_online_map);
+	pfm_cpu_disable();
 	return 0;
 }
 
diff -Naur linux-2.6.25-org/arch/x86/kernel/smpboot_64.c linux-2.6.25-id/arch/x86/kernel/smpboot_64.c
--- linux-2.6.25-org/arch/x86/kernel/smpboot_64.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/kernel/smpboot_64.c	2008-04-23 11:21:43.000000000 +0200
@@ -49,6 +49,7 @@
 #include <linux/mc146818rtc.h>
 #include <linux/smp.h>
 #include <linux/kdebug.h>
+#include <linux/perfmon.h>
 
 #include <asm/mtrr.h>
 #include <asm/pgalloc.h>
@@ -1066,6 +1067,7 @@
 	spin_unlock(&vector_lock);
 	remove_cpu_from_maps();
 	fixup_irqs(cpu_online_map);
+	pfm_cpu_disable();
 	return 0;
 }
 
diff -Naur linux-2.6.25-org/arch/x86/oprofile/nmi_int.c linux-2.6.25-id/arch/x86/oprofile/nmi_int.c
--- linux-2.6.25-org/arch/x86/oprofile/nmi_int.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/oprofile/nmi_int.c	2008-04-23 11:21:43.000000000 +0200
@@ -15,6 +15,7 @@
 #include <linux/slab.h>
 #include <linux/moduleparam.h>
 #include <linux/kdebug.h>
+#include <linux/perfmon.h>
 #include <asm/nmi.h>
 #include <asm/msr.h>
 #include <asm/apic.h>
@@ -270,6 +271,7 @@
 	unregister_die_notifier(&profile_exceptions_nb);
 	model->shutdown(cpu_msrs);
 	free_msrs();
+	pfm_release_allcpus();
 }
 
 static void nmi_cpu_start(void *dummy)
diff -Naur linux-2.6.25-org/arch/x86/pci/common.c linux-2.6.25-id/arch/x86/pci/common.c
--- linux-2.6.25-org/arch/x86/pci/common.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/arch/x86/pci/common.c	2008-04-23 11:21:43.000000000 +0200
@@ -76,6 +76,7 @@
  * configuration space.
  */
 DEFINE_SPINLOCK(pci_config_lock);
+EXPORT_SYMBOL(pci_config_lock);
 
 /*
  * Several buggy motherboards address only 16 devices and mirror
diff -Naur linux-2.6.25-org/arch/x86/perfmon/Kconfig linux-2.6.25-id/arch/x86/perfmon/Kconfig
--- linux-2.6.25-org/arch/x86/perfmon/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/Kconfig	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,72 @@
+menu "Hardware Performance Monitoring support"
+config PERFMON
+	bool "Perfmon2 performance monitoring interface"
+	select X86_LOCAL_APIC
+	default n
+	help
+	Enables the perfmon2 interface to access the hardware
+	performance counters. See <http://perfmon2.sf.net/> for
+	more details.
+
+config PERFMON_DEBUG
+	bool "Perfmon debugging"
+	default n
+	depends on PERFMON
+	help
+	Enables perfmon debugging support
+
+config X86_PERFMON_P6
+	tristate "Support for Intel P6/Pentium M processor hardware performance counters"
+	depends on PERFMON && X86_32
+	default n
+	help
+	Enables support for Intel P6-style hardware performance counters.
+	To be used for with Intel Pentium III, PentiumPro, Pentium M processors.
+
+config X86_PERFMON_P4
+	tristate "Support for Intel Pentium 4/Xeon hardware performance counters"
+	depends on PERFMON
+	default n
+	help
+	Enables support for Intel Pentium 4/Xeon (Netburst) hardware performance
+	counters.
+
+config	X86_PERFMON_PEBS_P4
+	tristate "Support for Intel Netburst Precise Event-Based Sampling (PEBS)"
+	depends on PERFMON && X86_PERFMON_P4
+	default n
+	help
+	Enables support for Precise Event-Based Sampling (PEBS) on the Intel
+	Netburst processors such as Pentium 4, Xeon which support it.
+
+config  X86_PERFMON_CORE
+	tristate "Support for Intel Core-based performance counters"
+	depends on PERFMON
+	default n
+	help
+	Enables support for Intel Core-based performance counters. Enable
+	this option to support Intel Core 2 processors.
+
+config	X86_PERFMON_PEBS_CORE
+	tristate "Support for Intel Core Precise Event-Based Sampling (PEBS)"
+	depends on PERFMON && X86_PERFMON_CORE
+	default n
+	help
+	Enables support for Precise Event-Based Sampling (PEBS) on the Intel
+	Core processors.
+
+config  X86_PERFMON_INTEL_ARCH
+	tristate "Support for Intel architectural perfmon v1/v2"
+	depends on PERFMON
+	default n
+	help
+	Enables support for Intel architectural performance counters.
+	This feature was introduced with Intel Core Solo/Core Duo processors.
+
+config	X86_PERFMON_AMD64
+	tristate "Support AMD Athlon64/Opteron64 hardware performance counters"
+	depends on PERFMON
+	default n
+	help
+	Enables support for Athlon64/Opterton64 hardware performance counters.
+endmenu
diff -Naur linux-2.6.25-org/arch/x86/perfmon/Makefile linux-2.6.25-id/arch/x86/perfmon/Makefile
--- linux-2.6.25-org/arch/x86/perfmon/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/Makefile	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,12 @@
+#
+# Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
+# Contributed by Stephane Eranian <eranian@hpl.hp.com>
+#
+obj-$(CONFIG_PERFMON)			+= perfmon.o
+obj-$(CONFIG_X86_PERFMON_P6)		+= perfmon_p6.o
+obj-$(CONFIG_X86_PERFMON_P4)		+= perfmon_p4.o
+obj-$(CONFIG_X86_PERFMON_CORE)		+= perfmon_intel_core.o
+obj-$(CONFIG_X86_PERFMON_INTEL_ARCH)	+= perfmon_intel_arch.o
+obj-$(CONFIG_X86_PERFMON_PEBS_P4)	+= perfmon_pebs_p4_smpl.o
+obj-$(CONFIG_X86_PERFMON_PEBS_CORE)	+= perfmon_pebs_core_smpl.o
+obj-$(CONFIG_X86_PERFMON_AMD64)   	+= perfmon_amd64.o
diff -Naur linux-2.6.25-org/arch/x86/perfmon/perfmon.c linux-2.6.25-id/arch/x86/perfmon/perfmon.c
--- linux-2.6.25-org/arch/x86/perfmon/perfmon.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/perfmon.c	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,1456 @@
+/*
+ * This file implements the X86 specific support for the perfmon2 interface
+ *
+ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * Copyright (c) 2007 Advanced Micro Devices, Inc.
+ * Contributed by Robert Richter <robert.richter@amd.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/interrupt.h>
+#include <linux/perfmon.h>
+#include <linux/kprobes.h>
+#include <linux/kdebug.h>
+
+#include <asm/nmi.h>
+#include <asm/apic.h>
+
+DEFINE_PER_CPU(unsigned long, real_iip);
+DEFINE_PER_CPU(int, pfm_using_nmi);
+
+struct pfm_ds_area_p4 {
+	unsigned long	bts_buf_base;
+	unsigned long	bts_index;
+	unsigned long	bts_abs_max;
+	unsigned long	bts_intr_thres;
+	unsigned long	pebs_buf_base;
+	unsigned long	pebs_index;
+	unsigned long	pebs_abs_max;
+	unsigned long	pebs_intr_thres;
+	u64		pebs_cnt_reset;
+};
+
+struct pfm_ds_area_intel_core {
+	u64	bts_buf_base;
+	u64	bts_index;
+	u64	bts_abs_max;
+	u64	bts_intr_thres;
+	u64	pebs_buf_base;
+	u64	pebs_index;
+	u64	pebs_abs_max;
+	u64	pebs_intr_thres;
+	u64	pebs_cnt_reset;
+};
+
+
+static int (*pfm_has_ovfl)(struct pfm_context *);
+static int (*pfm_stop_save)(struct pfm_context *ctx,
+			    struct pfm_event_set *set);
+
+static inline int get_smt_id(void)
+{
+#ifdef CONFIG_SMP
+	int cpu = smp_processor_id();
+	return (cpu != first_cpu(__get_cpu_var(cpu_sibling_map)));
+#else
+	return 0;
+#endif
+}
+
+void __pfm_write_reg_p4(const struct pfm_arch_ext_reg *xreg, u64 val)
+{
+	u64 pmi;
+	int smt_id;
+
+	smt_id = get_smt_id();
+	/*
+	 * HT is only supported by P4-style PMU
+	 *
+	 * Adjust for T1 if necessary:
+	 *
+	 * - move the T0_OS/T0_USR bits into T1 slots
+	 * - move the OVF_PMI_T0 bits into T1 slot
+	 *
+	 * The P4/EM64T T1 is cleared by description table.
+	 * User only works with T0.
+	 */
+	if (smt_id) {
+		if (xreg->reg_type & PFM_REGT_ESCR) {
+
+			/* copy T0_USR & T0_OS to T1 */
+			val |= ((val & 0xc) >> 2);
+
+			/* clear bits T0_USR & T0_OS */
+			val &= ~0xc;
+
+		} else if (xreg->reg_type & PFM_REGT_CCCR) {
+			pmi = (val >> 26) & 0x1;
+			if (pmi) {
+				val &=~(1UL<<26);
+				val |= 1UL<<27;
+			}
+		}
+	}
+	if (xreg->addrs[smt_id])
+		wrmsrl(xreg->addrs[smt_id], val);
+}
+
+void __pfm_read_reg_p4(const struct pfm_arch_ext_reg *xreg, u64 *val)
+{
+	int smt_id;
+
+	smt_id = get_smt_id();
+
+	if (likely(xreg->addrs[smt_id])) {
+		rdmsrl(xreg->addrs[smt_id], *val);
+		/*
+		 * HT is only supported by P4-style PMU
+		 *
+		 * move the Tx_OS and Tx_USR bits into
+		 * T0 slots setting the T1 slots to zero
+		 */
+		if (xreg->reg_type & PFM_REGT_ESCR) {
+			if (smt_id)
+				*val |= (((*val) & 0x3) << 2);
+
+			/*
+			 * zero out bits that are reserved
+			 * (including T1_OS and T1_USR)
+			 */
+			*val &= PFM_ESCR_RSVD;
+		}
+	} else {
+		*val = 0;
+	}
+}
+
+/*
+ * called from NMI interrupt handler
+ */
+static void __kprobes __pfm_arch_quiesce_pmu_percpu(void)
+{
+	struct pfm_arch_pmu_info *arch_info;
+	unsigned int i;
+
+	arch_info = pfm_pmu_conf->arch_info;
+
+	/*
+	 * quiesce PMU by clearing registers that have enable bits
+	 * (start/stop capabilities).
+	 */
+	for (i = 0; i < arch_info->max_ena; i++)
+		if (test_bit(i, cast_ulp(arch_info->enable_mask)))
+			pfm_arch_write_pmc(NULL, i, 0);
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * set cannot be NULL. Context is locked. Interrupts are masked.
+ *
+ * Caller has already restored all PMD and PMC registers, if
+ * necessary (i.e., lazy restore scheme).
+ *
+ * on X86, there is nothing else to do. Even with PEBS, the
+ * DS area is already restore by pfm_arch_restore_pmcs() which
+ * is systematically called as the lazy restore scheme does not
+ * work for PMCs (stopping is a destructive operation for PMC).
+ */
+void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx,
+			     struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	if (set->npend_ovfls)
+		__get_cpu_var(real_iip) = ctx_arch->saved_real_iip;
+
+	/*
+	 * enable RDPMC on this CPU
+	 */
+	if (ctx_arch->flags.insecure)
+		set_in_cr4(X86_CR4_PCE);
+}
+
+static int pfm_stop_save_p6(struct pfm_context *ctx,
+			    struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	u64 used_mask[PFM_PMC_BV];
+	u64 *cnt_pmds;
+	u64 val, wmask, ovfl_mask;
+	u32 i, count;
+
+	wmask = 1ULL << pfm_pmu_conf->counter_width;
+
+	bitmap_and(cast_ulp(used_mask),
+		   cast_ulp(set->used_pmcs),
+		   cast_ulp(arch_info->enable_mask),
+		   arch_info->max_ena);
+
+	count = bitmap_weight(cast_ulp(used_mask), pfm_pmu_conf->regs.max_pmc);
+
+	/*
+	 * stop monitoring
+	 * Unfortunately, this is very expensive!
+	 * wrmsrl() is serializing.
+	 */
+	for (i = 0; count; i++) {
+		if (test_bit(i, cast_ulp(used_mask))) {
+			wrmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, 0);
+			count--;
+		}
+	}
+
+	/*
+	 * if we already having a pending overflow condition, we simply
+	 * return to take care of this first.
+	 */
+	if (set->npend_ovfls)
+		return 1;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
+
+	/*
+	 * check for pending overflows and save PMDs (combo)
+	 * we employ used_pmds because we also need to save
+	 * and not just check for pending interrupts.
+	 *
+	 * Must check for counting PMDs because of virtual PMDs
+	 */
+	count = set->nused_pmds;
+	for (i = 0; count; i++) {
+		if (test_bit(i, cast_ulp(set->used_pmds))) {
+			val = pfm_arch_read_pmd(ctx, i);
+			if (likely(test_bit(i, cast_ulp(cnt_pmds)))) {
+				if (!(val & wmask)) {
+					__set_bit(i, cast_ulp(set->povfl_pmds));
+					set->npend_ovfls++;
+				}
+				val = (set->pmds[i].value & ~ovfl_mask) | (val & ovfl_mask);
+			}
+			set->pmds[i].value = val;
+			count--;
+		}
+	}
+	/* 0 means: no need to save PMDs at upper level */
+	return 0;
+}
+
+#define PFM_AMD64_IBSFETCHVAL	(1ULL<<49) /* valid fetch sample */
+#define PFM_AMD64_IBSFETCHEN	(1ULL<<48) /* fetch sampling enabled */
+#define PFM_AMD64_IBSOPVAL	(1ULL<<18) /* valid execution sample */
+#define PFM_AMD64_IBSOPEN	(1ULL<<17) /* execution sampling enabled */
+
+/*
+ * Must check for IBS event BEFORE stop_save_p6 because
+ * stopping monitoring does destroy IBS state information
+ * in IBSFETCHCTL/IBSOPCTL because they are tagged as enable
+ * registers.
+ */
+static int pfm_stop_save_amd64(struct pfm_context *ctx,
+			       struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	u64 used_mask[PFM_PMC_BV];
+	u64 *cnt_pmds;
+	u64 val, wmask, ovfl_mask;
+	u32 i, count, use_ibs;
+
+	/*
+	 * IBS used if:
+	 *   - on family 10h processor with IBS
+	 *   - at least one of the IBS PMD registers is used
+	 */
+	use_ibs = (arch_info->flags & PFM_X86_FL_IBS)
+		&& (test_bit(arch_info->ibsfetchctl_pmd, cast_ulp(set->used_pmds))
+		    ||test_bit(arch_info->ibsopctl_pmd, cast_ulp(set->used_pmds)));
+
+	wmask = 1ULL << pfm_pmu_conf->counter_width;
+
+	bitmap_and(cast_ulp(used_mask),
+		   cast_ulp(set->used_pmcs),
+		   cast_ulp(arch_info->enable_mask),
+		   arch_info->max_ena);
+
+	count = bitmap_weight(cast_ulp(used_mask), pfm_pmu_conf->regs.max_pmc);
+
+	/*
+	 * stop monitoring
+	 * Unfortunately, this is very expensive!
+	 * wrmsrl() is serializing.
+	 *
+	 * With IBS, we need to do read-modify-write to preserve the content
+	 * for OpsCTL and FetchCTL because they are also used as PMDs and saved
+	 * below
+	 */
+	if (use_ibs) {
+		for (i = 0; count; i++) {
+			if (test_bit(i, cast_ulp(used_mask))) {
+				if (i == arch_info->ibsfetchctl_pmc) {
+					rdmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, val);
+					val &= ~PFM_AMD64_IBSFETCHEN;
+				} else if (i == arch_info->ibsopctl_pmc) {
+					rdmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, val);
+					val &= ~PFM_AMD64_IBSOPEN;
+				} else
+					val = 0;
+				wrmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, val);
+				count--;
+			}
+		}
+	} else {
+		for (i = 0; count; i++) {
+			if (test_bit(i, cast_ulp(used_mask))) {
+				wrmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, 0);
+				count--;
+			}
+		}
+	}
+
+	/*
+	 * if we already having a pending overflow condition, we simply
+	 * return to take care of this first.
+	 */
+	if (set->npend_ovfls)
+		return 1;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
+
+	/*
+	 * check for pending overflows and save PMDs (combo)
+	 * we employ used_pmds because we also need to save
+	 * and not just check for pending interrupts.
+	 *
+	 * Must check for counting PMDs because of virtual PMDs and IBS
+	 */
+	count = set->nused_pmds;
+	for (i = 0; count; i++) {
+		if (test_bit(i, cast_ulp(set->used_pmds))) {
+			val = pfm_arch_read_pmd(ctx, i);
+			if (likely(test_bit(i, cast_ulp(cnt_pmds)))) {
+				if (!(val & wmask)) {
+					__set_bit(i, cast_ulp(set->povfl_pmds));
+					set->npend_ovfls++;
+				}
+				val = (set->pmds[i].value & ~ovfl_mask) | (val & ovfl_mask);
+			}
+			set->pmds[i].value = val;
+			count--;
+		}
+	}
+
+	/*
+	 * check if IBS contains valid data, and mark the corresponding
+	 * PMD has overflowed
+	 */
+	if (use_ibs) {
+		i = arch_info->ibsfetchctl_pmd;
+		if (set->pmds[i].value & PFM_AMD64_IBSFETCHVAL) {
+			__set_bit(i, cast_ulp(set->povfl_pmds));
+			set->npend_ovfls++;
+		}
+		i = arch_info->ibsopctl_pmd;
+		if (set->pmds[i].value & PFM_AMD64_IBSOPVAL) {
+			__set_bit(i, cast_ulp(set->povfl_pmds));
+			set->npend_ovfls++;
+		}
+	}
+	/* 0 means: no need to save PMDs at upper level */
+	return 0;
+}
+
+static int pfm_stop_save_intel_core(struct pfm_context *ctx,
+				    struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_ds_area_intel_core *ds = NULL;
+	u64 used_mask[PFM_PMC_BV];
+	u64 *cnt_mask;
+	u64 val, wmask, ovfl_mask;
+	u16 count, has_ovfl;
+	u16 i, pebs_idx = ~0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	wmask = 1ULL << pfm_pmu_conf->counter_width;
+
+	/*
+	 * used enable pmc bitmask
+	 */
+	bitmap_and(cast_ulp(used_mask),
+			cast_ulp(set->used_pmcs),
+			cast_ulp(arch_info->enable_mask),
+			arch_info->max_ena);
+
+	count = bitmap_weight(cast_ulp(used_mask), arch_info->max_ena);
+	/*
+	 * stop monitoring
+	 * Unfortunately, this is very expensive!
+	 * wrmsrl() is serializing.
+	 */
+	for (i = 0; count; i++) {
+		if (test_bit(i, cast_ulp(used_mask))) {
+			wrmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, 0);
+			count--;
+		}
+	}
+	/*
+	 * if we already having a pending overflow condition, we simply
+	 * return to take care of this first.
+	 */
+	if (set->npend_ovfls)
+		return 1;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	cnt_mask = pfm_pmu_conf->regs.cnt_pmds;
+
+	if (ctx_arch->flags.use_pebs) {
+		ds = ctx_arch->ds_area;
+		pebs_idx = arch_info->pebs_ctr_idx;
+		PFM_DBG("ds=%p pebs_idx=0x%llx thres=0x%llx",
+			ds,
+			(unsigned long long)ds->pebs_index,
+			(unsigned long long)ds->pebs_intr_thres);
+	}
+
+	/*
+	 * Check for pending overflows and save PMDs (combo)
+	 * We employ used_pmds and not intr_pmds because we must
+	 * also saved on PMD registers.
+	 * Must check for counting PMDs because of virtual PMDs
+	 *
+	 * XXX: should use the ovf_status register instead, yet
+	 *      we would have to check if NMI is used and fallback
+	 *      to individual pmd inspection.
+	 */
+	count = set->nused_pmds;
+
+	for (i = 0; count; i++) {
+		if (test_bit(i, cast_ulp(set->used_pmds))) {
+			val = pfm_arch_read_pmd(ctx, i);
+			if (likely(test_bit(i, cast_ulp(cnt_mask)))) {
+				if (i == pebs_idx)
+					has_ovfl = (ds->pebs_index >= ds->pebs_intr_thres);
+				else
+					has_ovfl = !(val & wmask);
+				if (has_ovfl) {
+					__set_bit(i, cast_ulp(set->povfl_pmds));
+					set->npend_ovfls++;
+				}
+				val = (set->pmds[i].value & ~ovfl_mask) | (val & ovfl_mask);
+			}
+			set->pmds[i].value = val;
+			count--;
+		}
+	}
+	/* 0 means: no need to save PMDs at upper level */
+	return 0;
+}
+
+static int pfm_stop_save_p4(struct pfm_context *ctx,
+			    struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_arch_ext_reg *xrc, *xrd;
+	struct pfm_ds_area_p4 *ds = NULL;
+	u64 used_mask[PFM_PMC_BV];
+	u16 i, j, count, pebs_idx = ~0;
+	u16 max_pmc;
+	u64 cccr, ctr1, ctr2, ovfl_mask;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+	xrc = arch_info->pmc_addrs;
+	xrd = arch_info->pmd_addrs;
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+
+	/*
+	 * build used enable PMC bitmask
+	 * if user did not set any CCCR, then mask is
+	 * empty and there is nothing to do because nothing
+	 * was started
+	 */
+	bitmap_and(cast_ulp(used_mask),
+		   cast_ulp(set->used_pmcs),
+		   cast_ulp(arch_info->enable_mask),
+		   arch_info->max_ena);
+
+	count = bitmap_weight(cast_ulp(used_mask), arch_info->max_ena);
+
+	PFM_DBG_ovfl("npend=%u ena_mask=0x%llx u_pmcs=0x%llx count=%u num=%u",
+		set->npend_ovfls,
+		(unsigned long long)arch_info->enable_mask[0],
+		(unsigned long long)set->used_pmcs[0],
+		count, arch_info->max_ena);
+
+
+	/*
+	 * ensures we do not destroy pending overflow
+	 * information. If pended interrupts are already
+	 * known, then we just stop monitoring.
+	 */
+	if (set->npend_ovfls) {
+		/*
+		 * clear enable bit
+		 * unfortunately, this is very expensive!
+		 */
+		for (i = 0; count; i++) {
+			if (test_bit(i, cast_ulp(used_mask))) {
+				__pfm_write_reg_p4(xrc+i, 0);
+				count--;
+			}
+		}
+		/* need save PMDs at upper level */
+		return 1;
+	}
+
+	if (ctx_arch->flags.use_pebs) {
+		ds = ctx_arch->ds_area;
+		pebs_idx = arch_info->pebs_ctr_idx;
+		PFM_DBG("ds=%p pebs_idx=0x%llx thres=0x%llx",
+			ds,
+			(unsigned long long)ds->pebs_index,
+			(unsigned long long)ds->pebs_intr_thres);
+	}
+
+	/*
+	 * stop monitoring AND collect pending overflow information AND
+	 * save pmds.
+	 *
+	 * We need to access the CCCR twice, once to get overflow info
+	 * and a second to stop monitoring (which destroys the OVF flag)
+	 * Similarly, we need to read the counter twice to check whether
+	 * it did overflow between the CCR read and the CCCR write.
+	 */
+	for (i = 0; count; i++) {
+		if (i != pebs_idx && test_bit(i, cast_ulp(used_mask))) {
+			/*
+			 * controlled counter
+			 */
+			j = xrc[i].ctr;
+
+			/* read CCCR (PMC) value */
+			__pfm_read_reg_p4(xrc+i, &cccr);
+
+			/* read counter (PMD) controlled by PMC */
+			__pfm_read_reg_p4(xrd+j, &ctr1);
+
+			/* clear CCCR value: stop counter but destroy OVF */
+			__pfm_write_reg_p4(xrc+i, 0);
+
+			/* read counter controlled by CCCR again */
+			__pfm_read_reg_p4(xrd+j, &ctr2);
+
+			/*
+			 * there is an overflow if either:
+			 * 	- CCCR.ovf is set (and we just cleared it)
+			 * 	- ctr2 < ctr1
+			 * in that case we set the bit corresponding to the
+			 * overflowed PMD  in povfl_pmds.
+			 */
+			if ((cccr & (1ULL<<31)) || (ctr2 < ctr1)) {
+				__set_bit(j, cast_ulp(set->povfl_pmds));
+				set->npend_ovfls++;
+			}
+			ctr2 = (set->pmds[j].value & ~ovfl_mask) | (ctr2 & ovfl_mask);
+			set->pmds[j].value = ctr2;
+			count--;
+		}
+	}
+	/*
+	 * check for PEBS buffer full and set the corresponding PMD overflow
+	 */
+	if (ctx_arch->flags.use_pebs) {
+		PFM_DBG("ds=%p pebs_idx=0x%lx thres=0x%lx", ds, ds->pebs_index, ds->pebs_intr_thres);
+		if (ds->pebs_index >= ds->pebs_intr_thres
+		    && test_bit(arch_info->pebs_ctr_idx, cast_ulp(set->used_pmds))) {
+			__set_bit(arch_info->pebs_ctr_idx, cast_ulp(set->povfl_pmds));
+			set->npend_ovfls++;
+		}
+	}
+	/* 0 means: no need to save the PMD at higher level */
+	return 0;
+}
+
+/*
+ * Called from pfm_stop() and idle notifier
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-thread:
+ *   task is not necessarily current. If not current task, then
+ *   task is guaranteed stopped and off any cpu. Access to PMU
+ *   is not guaranteed.
+ *
+ * For system-wide:
+ * 	task is current
+ *
+ * must disable active monitoring. ctx cannot be NULL
+ */
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
+		   struct pfm_event_set *set)
+{
+	/*
+	 * no need to go through stop_save()
+	 * if we are already stopped
+	 */
+	if (!ctx->flags.started || ctx->state == PFM_CTX_MASKED)
+		return;
+
+	if (task == current)
+		pfm_stop_save(ctx, set);
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * Context is locked. Interrupts are masked. Monitoring may be active.
+ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
+ *
+ * Must stop monitoring, save pending overflow information
+ *
+ * Return:
+ * 	non-zero : did not save PMDs (as part of stopping the PMU)
+ * 	       0 : saved PMDs (no need to save them in caller)
+ */
+int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
+		             struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * disable lazy restore of PMCS on ctxswin because
+	 * we modify some of them.
+	 */
+	set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
+
+	if (set->npend_ovfls) {
+		ctx_arch->saved_real_iip = __get_cpu_var(real_iip);
+	}
+	/*
+	 * disable RDPMC on this CPU
+	 */
+	if (ctx_arch->flags.insecure)
+		clear_in_cr4(X86_CR4_PCE);
+
+	if (ctx->state == PFM_CTX_MASKED)
+		return 1;
+
+	return pfm_stop_save(ctx, set);
+}
+
+/*
+ * called from pfm_start() and idle notifier
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-thread:
+ * 	Task is not necessarily current. If not current task, then task
+ * 	is guaranteed stopped and off any cpu. No access to PMU is task
+ *	is not current.
+ *
+ * For system-wide:
+ * 	task is always current
+ *
+ * must enable active monitoring.
+ */
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
+		    struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+	u64 *mask;
+	u16 i, num;
+
+	/*
+	 * pfm_start issue while context is masked as no effect.
+	 * This comes from the fact that on x86, masking and stopping
+	 * use the same mechanism, i.e., clearing the enable bits
+	 * of the PMC registers.
+	 */
+	if (ctx->state == PFM_CTX_MASKED)
+		return;
+
+	/*
+	 * cannot restore PMC if no access to PMU. Will be done
+	 * when the thread is switched back in
+	 */
+	if (task != current)
+		return;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * reload DS area pointer.
+	 * Must be done before we restore the PMCs
+	 * avoid a race condition
+	 */
+	if (ctx_arch->flags.use_ds)
+		wrmsrl(MSR_IA32_DS_AREA, (unsigned long)ctx_arch->ds_area);
+	/*
+	 * we must actually install all implemented pmcs registers because
+	 * until started, we do not write any PMC registers.
+	 * Note that registers used  by other subsystems (e.g. NMI) are
+	 * removed from pmcs.
+	 *
+	 * The available registers that are actually not used get their default
+	 * value such that counters do not count anything. As such, we can
+	 * afford to write all of them but then stop only the one we use.
+	 *
+	 * XXX: we may be able to optimize this for non-P4 PMU as pmcs are
+	 * independent from each others.
+	 */
+	num = pfm_pmu_conf->regs.num_pmcs;
+	mask = pfm_pmu_conf->regs.pmcs;
+	for (i = 0; num; i++) {
+		if (test_bit(i, cast_ulp(mask))) {
+			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
+			num--;
+		}
+	}
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw()
+ *
+ * context is locked. Interrupts are masked. Set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore PMD registers
+ */
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 *used_pmds;
+	u16 i, num;
+
+	used_pmds = set->used_pmds;
+	num = set->nused_pmds;
+
+	/*
+	 * we can restore only the PMD we use because:
+	 * 	- you can only read with pfm_read_pmds() the registers
+	 * 	  declared used via pfm_write_pmds(), smpl_pmds, reset_pmds
+	 *
+	 * 	- if cr4.pce=1, only counters are exposed to user. No
+	 * 	  address is ever exposed by counters.
+	 */
+	for (i = 0; num; i++) {
+		if (likely(test_bit(i, cast_ulp(used_pmds)))) {
+			pfm_write_pmd(ctx, i, set->pmds[i].value);
+			num--;
+		}
+	}
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw().
+ * Context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMC registers from set
+ */
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	struct pfm_arch_context *ctx_arch;
+	u64 *mask;
+	u16 i, num;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	/*
+	 * we need to restore PMCs only when:
+	 * 	- context is not masked
+	 * 	- monitoring was activated
+	 *
+	 * Masking monitoring after an overflow does not change the
+	 * value of flags.started
+	 */
+	if (ctx->state == PFM_CTX_MASKED || !ctx->flags.started)
+		return;
+
+	/*
+	 * must restore DS pointer before restoring PMCs
+	 * as this can potentially reactivate monitoring
+	 */
+	if (ctx_arch->flags.use_ds)
+		wrmsrl(MSR_IA32_DS_AREA, (unsigned long)ctx_arch->ds_area);
+
+	/*
+	 * In general, writing MSRs is very expensive, so try to be smart.
+	 *
+	 * P6-style, Core-style:
+	 * 	- pmc are totally independent of each other, there is
+	 * 	  possible side-effect from stale pmcs. Therefore we only
+	 * 	  restore the registers we use
+	 * P4-style:
+	 * 	- must restore everything because there are some dependencies
+	 * 	(e.g., ESCR and CCCR)
+	 */
+	if (arch_info->pmu_style == PFM_X86_PMU_P4) {
+		num = pfm_pmu_conf->regs.num_pmcs;
+		mask = pfm_pmu_conf->regs.pmcs;
+	} else {
+		num = set->nused_pmcs;
+		mask = set->used_pmcs;
+	}
+	for (i = 0; num; i++) {
+		if (test_bit(i, cast_ulp(mask))) {
+			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
+			num--;
+		}
+	}
+}
+
+/*
+ * invoked only when NMI is used. Called from the LOCAL_PERFMON_VECTOR
+ * handler to copy P4 overflow state captured when the NMI triggered.
+ * Given that on P4, stopping monitoring destroy the overflow information
+ * we save it in pfm_has_ovfl_p4() where monitoring is also stopped.
+ *
+ * Here we propagate the overflow state to current active set. The
+ * freeze_pmu() call we not overwrite this state because npend_ovfls
+ * is non-zero.
+ */
+static void pfm_p4_copy_nmi_state(void)
+{
+	struct pfm_context *ctx;
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_event_set *set;
+
+	ctx = __get_cpu_var(pmu_ctx);
+	if (!ctx)
+		return;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	set = ctx->active_set;
+
+	if (ctx_arch->p4->npend_ovfls) {
+		set->npend_ovfls = ctx_arch->p4->npend_ovfls;
+
+		bitmap_copy(cast_ulp(set->povfl_pmds),
+			    cast_ulp(ctx_arch->p4->povfl_pmds),
+			    pfm_pmu_conf->regs.max_pmd);
+
+		ctx_arch->p4->npend_ovfls = 0;
+	}
+}
+
+/*
+ * The PMU interrupt is handled through an interrupt gate, therefore
+ * the CPU automatically clears the EFLAGS.IF, i.e., masking interrupts.
+ *
+ * The perfmon interrupt handler MUST run with interrupts disabled due
+ * to possible race with other, higher priority interrupts, such as timer
+ * or IPI function calls.
+ *
+ * See description in IA-32 architecture manual, Vol 3 section 5.8.1
+ */
+fastcall void smp_pmu_interrupt(struct pt_regs *regs)
+{
+	struct pfm_arch_pmu_info *arch_info;
+	unsigned long iip;
+	int using_nmi;
+
+	using_nmi = __get_cpu_var(pfm_using_nmi);
+
+	ack_APIC_irq();
+
+	irq_enter();
+
+	/*
+	 * when using NMI, pfm_handle_nmi() gets called
+	 * first. It stops monitoring and record the
+	 * iip into real_iip, then it repost the interrupt
+	 * using the lower priority vector LOCAL_PERFMON_VECTOR
+	 *
+	 * On P4, due to the difficulty of detecting overflows
+	 * and stoppping the PMU, pfm_handle_nmi() needs to
+	 * record npend_ovfl and ovfl_pmds in ctx_arch. So
+	 * here we simply copy them back to the set.
+	 */
+	if (using_nmi) {
+		arch_info = pfm_pmu_conf->arch_info;
+		iip = __get_cpu_var(real_iip);
+		if (arch_info->pmu_style == PFM_X86_PMU_P4)
+			pfm_p4_copy_nmi_state();
+	} else
+		iip = instruction_pointer(regs);
+
+	pfm_interrupt_handler(iip, regs);
+
+	/*
+	 * On Intel P6, Pentium M, P4, Intel Core:
+	 * 	- it is necessary to clear the MASK field for the LVTPC
+	 * 	  vector. Otherwise interrupts remain masked. See
+	 * 	  section 8.5.1
+	 * AMD X86-64:
+	 * 	- the documentation does not stipulate the behavior.
+	 * 	  To be safe, we also rewrite the vector to clear the
+	 * 	  mask field
+	 */
+	if (!using_nmi && current_cpu_data.x86_vendor == X86_VENDOR_INTEL)
+		apic_write(APIC_LVTPC, LOCAL_PERFMON_VECTOR);
+
+	irq_exit();
+}
+
+/*
+ * detect is counters have overflowed.
+ * return:
+ * 	0 : no overflow
+ * 	1 : at least one overflow
+ *
+ * used by AMD64 and Intel architectural PMU
+ */
+static int __kprobes pfm_has_ovfl_p6(struct pfm_context *ctx)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	struct pfm_arch_ext_reg *xrd;
+	u64 *cnt_mask;
+	u64 wmask, val;
+	u16 i, num;
+
+	cnt_mask = pfm_pmu_conf->regs.cnt_pmds;
+	num = pfm_pmu_conf->regs.num_counters;
+	wmask = 1ULL << pfm_pmu_conf->counter_width;
+	xrd = arch_info->pmd_addrs;
+
+	for (i = 0; num; i++) {
+		if (test_bit(i, cast_ulp(cnt_mask))) {
+			rdmsrl(xrd[i].addrs[0], val);
+			if (!(val & wmask))
+				return 1;
+			num--;
+		}
+	}
+	return 0;
+}
+
+static int __kprobes pfm_has_ovfl_amd64(struct pfm_context *ctx)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	u64 val;
+	/*
+	 * Check for IBS events
+	 */
+	if (arch_info->flags & PFM_X86_FL_IBS) {
+		rdmsrl(pfm_pmu_conf->pmc_desc[arch_info->ibsfetchctl_pmc].hw_addr, val);
+		if (val & PFM_AMD64_IBSFETCHVAL)
+			return 1;
+		rdmsrl(pfm_pmu_conf->pmc_desc[arch_info->ibsopctl_pmc].hw_addr, val);
+		if (val & PFM_AMD64_IBSOPVAL)
+			return 1;
+	}
+	return pfm_has_ovfl_p6(ctx);
+}
+
+/*
+ * detect is counters have overflowed.
+ * return:
+ * 	0 : no overflow
+ * 	1 : at least one overflow
+ *
+ * used by Intel P4
+ */
+static int __kprobes pfm_has_ovfl_p4(struct pfm_context *ctx)
+{	
+	struct pfm_arch_ext_reg *xrc, *xrd;
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_arch_p4_context *p4;
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	u64 ena_mask[PFM_PMC_BV];
+	u64 cccr, ctr1, ctr2;
+	int n, i, j;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	xrc = arch_info->pmc_addrs;
+	xrd = arch_info->pmd_addrs;
+	p4 = ctx_arch->p4;
+
+	bitmap_and(cast_ulp(ena_mask),
+			cast_ulp(pfm_pmu_conf->regs.pmcs),
+			cast_ulp(arch_info->enable_mask),
+			arch_info->max_ena);
+
+	n = bitmap_weight(cast_ulp(ena_mask), arch_info->max_ena);
+
+	for(i=0; n; i++) {
+		if (!test_bit(i, cast_ulp(ena_mask)))
+			continue;
+		/*
+		 * controlled counter
+		 */
+		j = xrc[i].ctr;
+
+		/* read CCCR (PMC) value */
+		__pfm_read_reg_p4(xrc+i, &cccr);
+
+		/* read counter (PMD) controlled by PMC */
+		__pfm_read_reg_p4(xrd+j, &ctr1);
+
+		/* clear CCCR value: stop counter but destroy OVF */
+		__pfm_write_reg_p4(xrc+i, 0);
+
+		/* read counter controlled by CCCR again */
+		__pfm_read_reg_p4(xrd+j, &ctr2);
+
+		/*
+		 * there is an overflow if either:
+		 * 	- CCCR.ovf is set (and we just cleared it)
+		 * 	- ctr2 < ctr1
+		 * in that case we set the bit corresponding to the
+		 * overflowed PMD in povfl_pmds.
+		 */
+		if ((cccr & (1ULL<<31)) || (ctr2 < ctr1)) {
+			__set_bit(j, cast_ulp(ctx_arch->p4->povfl_pmds));
+			ctx_arch->p4->npend_ovfls++;
+		}
+		p4->saved_cccrs[i] = cccr;
+		n--;
+	}
+	/*
+	 * if there was no overflow, then it means the NMI was not really
+	 * for us, so we have to resume monitoring
+	 */
+	if (unlikely(!ctx_arch->p4->npend_ovfls)) {
+		for(i=0; n; i++) {
+			if (!test_bit(i, cast_ulp(ena_mask)))
+				continue;
+			__pfm_write_reg_p4(xrc+i, ctx_arch->p4->saved_cccrs[i]);
+		}
+	}
+	return 0;
+}
+
+/*
+ * detect is counters have overflowed.
+ * return:
+ * 	0 : no overflow
+ * 	1 : at least one overflow
+ *
+ * used by Intel Core-based processors
+ */
+static int __kprobes pfm_has_ovfl_intel_core(struct pfm_context *ctx)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	struct pfm_arch_ext_reg *xrd;
+	u64 *cnt_mask;
+	u64 wmask, val;
+	u16 i, num;
+
+	cnt_mask = pfm_pmu_conf->regs.cnt_pmds;
+	num = pfm_pmu_conf->regs.num_counters;
+	wmask = 1ULL << pfm_pmu_conf->counter_width;
+	xrd = arch_info->pmd_addrs;
+
+	for (i = 0; num; i++) {
+		if (test_bit(i, cast_ulp(cnt_mask))) {
+			rdmsrl(xrd[i].addrs[0], val);
+			if (!(val & wmask))
+				return 1;
+			num--;
+		}
+	}
+	return 0;
+}
+
+/*
+ * called from notify_die() notifier from an trap handler path. We only
+ * care about NMI related callbacks, and ignore everything else.
+ *
+ * Cannot grab any locks, include the perfmon context lock
+ *
+ * Must detect if NMI interrupt comes from perfmon, and if so it must
+ * stop the PMU and repost a lower-priority interrupt. The perfmon interrupt
+ * handler needs to grab the context lock, thus is cannot be run directly
+ * from the NMI interrupt call path.
+ */
+static int __kprobes pfm_handle_nmi(struct notifier_block *nb, unsigned long val,
+			      void *data)
+{
+	struct die_args *args = data;
+	struct pfm_context *ctx;
+
+	/*
+	 * only NMI related calls
+	 */
+	if (val != DIE_NMI_IPI)
+		return NOTIFY_DONE;
+
+	if (!__get_cpu_var(pfm_using_nmi))
+		return NOTIFY_DONE;
+
+	/*
+	 * perfmon not active on this processor
+	 */
+	ctx = __get_cpu_var(pmu_ctx);
+	if (ctx == NULL) {
+		PFM_DBG_ovfl("ctx NULL");
+		return NOTIFY_DONE;
+	}
+
+	/*
+	 * detect if we have overflows, i.e., NMI interrupt
+	 * caused by PMU
+	 */
+	if (!pfm_has_ovfl(ctx)) {
+		PFM_DBG_ovfl("no ovfl");
+		return NOTIFY_DONE;
+	}
+
+	/*
+	 * we stop the PMU to avoid further overflow before this
+	 * one is treated by lower priority interrupt handler
+	 */
+	__pfm_arch_quiesce_pmu_percpu();
+
+	/*
+	 * record actual instruction pointer
+	 */
+	__get_cpu_var(real_iip) = instruction_pointer(args->regs);
+
+	/*
+	 * post lower priority interrupt (LOCAL_PERFMON_VECTOR)
+	 */
+	pfm_arch_resend_irq();
+
+	pfm_stats_get(ovfl_intr_nmi_count)++;
+
+	/*
+	 * we need to rewrite the APIC vector on Intel
+	 */
+	if (current_cpu_data.x86_vendor == X86_VENDOR_INTEL)
+		apic_write(APIC_LVTPC, APIC_DM_NMI);
+
+	/*
+	 * the notification was for us
+	 */
+	return NOTIFY_STOP;
+}
+
+static struct notifier_block pfm_nmi_nb = {
+	.notifier_call = pfm_handle_nmi
+};
+
+/*
+ * called from pfm_register_pmu_config() after the new
+ * config has been validated. The pfm_session_lock
+ * is held.
+ *
+ * return:
+ * 	< 0 : if error
+ * 	  0 : if success
+ */
+int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
+{
+	struct pfm_arch_pmu_info *arch_info = cfg->arch_info;
+
+	/*
+	 * adust stop routine based on PMU model
+	 *
+	 * P6  : P6, Pentium M, Intel architectural perfmon
+	 * P4  : Xeon, EM64T, P4
+	 * Core: Core 2,
+	 * AMD64: AMD64 (K8, family 10h)
+	 */
+	switch(arch_info->pmu_style) {
+	case PFM_X86_PMU_P4:
+		pfm_stop_save = pfm_stop_save_p4;
+		pfm_has_ovfl  = pfm_has_ovfl_p4;
+		break;
+	case PFM_X86_PMU_P6:
+		pfm_stop_save = pfm_stop_save_p6;
+		pfm_has_ovfl  = pfm_has_ovfl_p6;
+		break;
+	case PFM_X86_PMU_CORE:
+		pfm_stop_save = pfm_stop_save_intel_core;
+		pfm_has_ovfl  = pfm_has_ovfl_intel_core;
+		break;
+	case PFM_X86_PMU_AMD64:
+		pfm_stop_save = pfm_stop_save_amd64;
+		pfm_has_ovfl  = pfm_has_ovfl_amd64;
+		break;
+	default:
+		PFM_INFO("unknown pmu_style=%d", arch_info->pmu_style);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+void pfm_arch_pmu_config_remove(void)
+{
+}
+
+char *pfm_arch_get_pmu_module_name(void)
+{
+	switch(current_cpu_data.x86) {
+	case 6:
+		switch(current_cpu_data.x86_model) {
+		case 3: /* Pentium II */
+		case 7 ... 11:
+		case 13:
+			return "perfmon_p6";
+		case 15: /* Merom */
+		case 23: /* Penryn */
+			return "perfmon_intel_core";
+		default:
+			goto try_arch;
+		}
+	case 15:
+	case 16:
+		/* All Opteron processors */
+		if (current_cpu_data.x86_vendor == X86_VENDOR_AMD)
+			return "perfmon_amd64";
+
+		switch(current_cpu_data.x86_model) {
+		case 0 ... 6:
+			return "perfmon_p4";
+		}
+		/* FALL THROUGH */
+	default:
+try_arch:
+		if (boot_cpu_has(X86_FEATURE_ARCH_PERFMON))
+			return "perfmon_intel_arch";
+		return NULL;
+	}
+	return NULL;
+}
+
+void pfm_arch_resend_irq(void)
+{
+	unsigned long val, dest;
+	/*
+	 * we cannot use hw_resend_irq() because it goes to
+	 * the I/O APIC. We need to go to the Local APIC.
+	 *
+	 * The "int vec" is not the right solution either
+	 * because it triggers a software intr. We need
+	 * to regenerate the interrupt and have it pended
+	 * until we unmask interrupts.
+	 *
+	 * Instead we send ourself an IPI on the perfmon
+	 * vector.
+	 */
+	val  = APIC_DEST_SELF|APIC_INT_ASSERT|
+	       APIC_DM_FIXED|LOCAL_PERFMON_VECTOR;
+
+	dest = apic_read(APIC_ID);
+	apic_write(APIC_ICR2, dest);
+	apic_write(APIC_ICR, val);
+}
+
+DEFINE_PER_CPU(unsigned long, saved_lvtpc);
+
+static void pfm_arch_pmu_acquire_percpu(void *data)
+{
+
+	unsigned int tmp, vec;
+	unsigned long flags = (unsigned long)data;
+	unsigned long lvtpc;
+
+	/*
+	 * we only reprogram the LVTPC vector if we have detected
+	 * no sharing, otherwise it means the APIC is already program
+	 * and we use whatever vector (likely NMI) was used
+	 */
+	if (!(flags & PFM_X86_FL_SHARING)) {
+		vec = flags & PFM_X86_FL_USE_NMI ? APIC_DM_NMI : LOCAL_PERFMON_VECTOR;
+		tmp = apic_read(APIC_LVTERR);
+		apic_write(APIC_LVTERR, tmp | APIC_LVT_MASKED);
+		apic_write(APIC_LVTPC, vec);
+		apic_write(APIC_LVTERR, tmp);
+		PFM_DBG("written LVTPC=0x%x", vec);
+	}
+	lvtpc = (unsigned long)apic_read(APIC_LVTPC);
+	__get_cpu_var(pfm_using_nmi) = lvtpc == APIC_DM_NMI;
+	PFM_DBG("LTVPC=0x%lx using_nmi=%d", lvtpc, __get_cpu_var(pfm_using_nmi));
+}
+
+/*
+ * called from pfm_pmu_acquire() with
+ * pfm_pmu_conf.regs copied from pfm_pmu_conf.full_regs
+ * needs to adjust regs to match current PMU availabilityy
+ *
+ * Caller does recalculate all max/num/first limits on the
+ * pfm_pmu_conf.regs structure.
+ *
+ * interrupts are not masked
+ *
+ *
+ * XXX: until reserve_*_nmi() get fixed by Bjorn to work
+ * correctly whenever the NMI watchdog is not used. We skip
+ * the allocation. Yet we do the percpu initialization.
+ */
+int pfm_arch_pmu_acquire(void)
+{
+	struct pfm_arch_pmu_info *arch_info;
+	struct pfm_regmap_desc *d;
+	struct pfm_arch_ext_reg *pc;
+	u16 i, n, ena = 0, nlost;
+
+	arch_info = pfm_pmu_conf->arch_info;
+	pc = arch_info->pmc_addrs;
+
+	bitmap_zero(cast_ulp(arch_info->enable_mask), PFM_MAX_PMCS);
+	arch_info->flags &= ~PFM_X86_FL_SHARING;
+
+	d = pfm_pmu_conf->pmc_desc;
+	n = pfm_pmu_conf->regs.num_pmcs;
+	nlost = 0;
+	for(i=0; n; i++, d++) {
+		/*
+		 * skip not implemented registers (including those
+		 * already removed by the module)
+		 */
+		if (!(d->type & PFM_REG_I))
+			continue;
+
+		n--;
+
+		if (d->type & PFM_REG_V)
+			continue;
+
+		/*
+		 * reserve register with lower-level allocator
+		 */
+		if (!reserve_evntsel_nmi(d->hw_addr)) {
+			PFM_DBG("pmc%d (%s) in use elsewhere, disabling", i, d->desc);
+			__clear_bit(i, cast_ulp(pfm_pmu_conf->regs.pmcs));
+			nlost++;
+			continue;
+		}
+
+		if (!(pc[i].reg_type & PFM_REGT_EN))
+			continue;
+		__set_bit(i, cast_ulp(arch_info->enable_mask));
+		ena++;
+		arch_info->max_ena = i + 1;
+	}
+
+	PFM_DBG("%u PMCs with enable capability", ena);
+
+	if (!ena) {
+		PFM_INFO("no registers with start/stop capability,"
+			 "try rebooting with nmi_watchdog=0, or check that Oprofile is not running");
+		goto undo;
+	}
+	PFM_DBG("nlost=%d info_flags=0x%x\n", nlost, arch_info->flags);
+	/*
+	 * some PMU models (e.g., P6) do not support sharing
+	 * so check if we found less than the expected number of PMC registers
+	 */
+	if (nlost) {
+		if (arch_info->flags & PFM_X86_FL_NO_SHARING) {
+			PFM_INFO("PMU already used by another subsystem, "
+				 "PMU does not support sharing, "
+				 "try disabling Oprofile or "
+				 "reboot with nmi_watchdog=0");
+			goto undo;
+		}
+		arch_info->flags |= PFM_X86_FL_SHARING;
+	}
+
+	d = pfm_pmu_conf->pmd_desc;
+	n = pfm_pmu_conf->regs.num_pmds;
+	for(i=0; n; i++, d++) {
+		if (!(d->type & PFM_REG_I))
+			continue;
+		n--;
+
+		if (d->type & PFM_REG_V)
+			continue;
+
+		if (!reserve_perfctr_nmi(d->hw_addr)) {
+			PFM_DBG("pmd%d (%s) in use elsewhere, disabling", i, d->desc);
+			__clear_bit(i, cast_ulp(pfm_pmu_conf->regs.pmds));
+			__clear_bit(i, cast_ulp(pfm_pmu_conf->regs.cnt_pmds));
+			__clear_bit(i, cast_ulp(pfm_pmu_conf->regs.rw_pmds));
+		}
+	}
+	/*
+	 * program APIC on each CPU
+	 */
+	on_each_cpu(pfm_arch_pmu_acquire_percpu,
+		    (void *)(unsigned long)arch_info->flags , 0, 1);
+
+	return 0;
+undo:
+	/*
+	 * must undo reservation in case of error
+	 */
+	n = pfm_pmu_conf->regs.max_pmc;
+	d = pfm_pmu_conf->pmc_desc;
+	for(i=0; i < n; i++, d++) {
+		if (!test_bit(i, cast_ulp(pfm_pmu_conf->regs.pmcs)))
+			continue;
+		release_evntsel_nmi(d->hw_addr);
+	}
+	return -EBUSY;
+}
+
+static void pfm_arch_pmu_release_percpu(void *data)
+{
+	__get_cpu_var(pfm_using_nmi) = 0;
+}
+
+/*
+ * called from pfm_pmu_release()
+ * interrupts are not masked
+ */
+void pfm_arch_pmu_release(void)
+{
+	struct pfm_regmap_desc *d;
+	u16 i, n;
+
+	d = pfm_pmu_conf->pmc_desc;
+	n = pfm_pmu_conf->regs.num_pmcs;
+	for(i=0; n; i++, d++) {
+		if (!test_bit(i, cast_ulp(pfm_pmu_conf->regs.pmcs)))
+			continue;
+		release_evntsel_nmi(d->hw_addr);
+		n--;
+		PFM_DBG("pmc%u released", i);
+	}
+	d = pfm_pmu_conf->pmd_desc;
+	n = pfm_pmu_conf->regs.num_pmds;
+	for(i=0; n; i++, d++) {
+		if (!test_bit(i, cast_ulp(pfm_pmu_conf->regs.pmds)))
+			continue;
+		release_perfctr_nmi(d->hw_addr);
+		n--;
+		PFM_DBG("pmd%u released", i);
+	}
+	on_each_cpu(pfm_arch_pmu_release_percpu, NULL , 0, 1);
+}
+
+int pfm_arch_init(void)
+{
+	/*
+	 * we need to register our NMI handler when the kernels boots
+	 * to avoid a deadlock condition with the NMI watchdog or Oprofile
+	 * if we were to try and register/unregister on-demand.
+	 */
+	register_die_notifier(&pfm_nmi_nb);
+	return 0;
+}
diff -Naur linux-2.6.25-org/arch/x86/perfmon/perfmon_amd64.c linux-2.6.25-id/arch/x86/perfmon/perfmon_amd64.c
--- linux-2.6.25-org/arch/x86/perfmon/perfmon_amd64.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/perfmon_amd64.c	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,641 @@
+/*
+ * This file contains the PMU description for the Athlon64 and Opteron64
+ * processors. It supports 32 and 64-bit modes.
+ *
+ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * Copyright (c) 2007 Advanced Micro Devices, Inc.
+ * Contributed by Robert Richter <robert.richter@amd.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <linux/vmalloc.h>
+#include <linux/pci.h>
+#include <asm/apic.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_AUTHOR("Robert Richter <robert.richter@amd.com>");
+MODULE_DESCRIPTION("AMD64 PMU description table");
+MODULE_LICENSE("GPL");
+
+static int force_nmi;
+MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
+module_param(force_nmi, bool, 0600);
+
+static struct pfm_arch_pmu_info pfm_amd64_pmu_info = {
+	.pmc_addrs = {
+/* pmc0  */	{{MSR_K7_EVNTSEL0, 0}, 0, PFM_REGT_EN},
+/* pmc1  */	{{MSR_K7_EVNTSEL1, 0}, 1, PFM_REGT_EN},
+/* pmc2  */	{{MSR_K7_EVNTSEL2, 0}, 2, PFM_REGT_EN},
+/* pmc3  */	{{MSR_K7_EVNTSEL3, 0}, 3, PFM_REGT_EN},
+/* pmc4  */	{{MSR_AMD64_IBSFETCHCTL, 0}, 0, PFM_REGT_EN|PFM_REGT_IBS},
+/* pmc5  */	{{MSR_AMD64_IBSOPCTL, 0}, 0, PFM_REGT_EN|PFM_REGT_IBS},
+	},
+	.pmd_addrs = {
+/* pmd0  */	{{MSR_K7_PERFCTR0, 0}, 0, PFM_REGT_CTR},
+/* pmd1  */	{{MSR_K7_PERFCTR1, 0}, 0, PFM_REGT_CTR},
+/* pmd2  */	{{MSR_K7_PERFCTR2, 0}, 0, PFM_REGT_CTR},
+/* pmd3  */	{{MSR_K7_PERFCTR3, 0}, 0, PFM_REGT_CTR},
+/* pmd4  */	{{MSR_AMD64_IBSFETCHCTL, 0}, 0, PFM_REGT_IBS},
+/* pmd5  */	{{MSR_AMD64_IBSFETCHLINAD, 0}, 0, PFM_REGT_IBS},
+/* pmd6  */	{{MSR_AMD64_IBSFETCHPHYSAD, 0}, 0, PFM_REGT_IBS},
+/* pmd7  */	{{MSR_AMD64_IBSOPCTL, 0}, 0, PFM_REGT_IBS},
+/* pmd8  */	{{MSR_AMD64_IBSOPRIP, 0}, 0, PFM_REGT_IBS},
+/* pmd9  */	{{MSR_AMD64_IBSOPDATA, 0}, 0, PFM_REGT_IBS},
+/* pmd10 */	{{MSR_AMD64_IBSOPDATA2, 0}, 0, PFM_REGT_IBS},
+/* pmd11 */	{{MSR_AMD64_IBSOPDATA3, 0}, 0, PFM_REGT_IBS_EXT},
+/* pmd12 */	{{MSR_AMD64_IBSDCLINAD, 0}, 0, PFM_REGT_IBS_EXT},
+/* pmd13 */	{{MSR_AMD64_IBSDCPHYSAD, 0}, 0, PFM_REGT_IBS_EXT},
+	},
+	.ibsfetchctl_pmc = 4,
+	.ibsfetchctl_pmd = 4,
+	.ibsopctl_pmc = 5,
+	.ibsopctl_pmd = 7,
+	.pmu_style = PFM_X86_PMU_AMD64,
+};
+
+/*
+ * force Local APIC interrupt on overflow
+ */
+#define PFM_K8_VAL	(1ULL<<20)
+#define PFM_K8_NO64	(1ULL<<20)
+
+/*
+ * for performance counter control registers:
+ *
+ * reserved bits must be zero
+ *
+ * for family 15:
+ * - upper 32 bits are reserved
+ *
+ * for family 16:
+ * - bits 36-39 are reserved
+ * - bits 42-63 are reserved
+ */
+#define PFM_K8_RSVD ((~((1ULL<<32)-1)) | (1ULL<<20) | (1ULL<<21))
+#define PFM_16_RSVD ((0x3fffffULL<<42) | (0xfULL<<36) | (1ULL<<20) | (1ULL<<21))
+
+/*
+ * for IBS registers:
+ * 	IBSFETCHCTL: all bits are reserved except bits 57, 48, 15:0
+ * 	IBSOPSCTL  : all bits are reserved except bits 17, 15:0
+ */
+#define PFM_AMD64_IBSFETCHCTL_RSVD	(~((1ULL<<48)|(1ULL<<57)|0xffffULL))
+#define PFM_AMD64_IBSOPCTL_RSVD		(~((1ULL<<17)|0xffffULL))
+
+#define IBSCTL				0x1cc
+#define IBSCTL_LVTOFFSETVAL		(1 << 8)
+
+#define ENABLE_CF8_EXT_CFG		(1ULL << 46)
+
+static struct pfm_regmap_desc pfm_amd64_pmc_desc[] = {
+/* pmc0  */ PMC_D(PFM_REG_I64, "PERFSEL0", PFM_K8_VAL, PFM_K8_RSVD, PFM_K8_NO64, MSR_K7_EVNTSEL0),
+/* pmc1  */ PMC_D(PFM_REG_I64, "PERFSEL1", PFM_K8_VAL, PFM_K8_RSVD, PFM_K8_NO64, MSR_K7_EVNTSEL1),
+/* pmc2  */ PMC_D(PFM_REG_I64, "PERFSEL2", PFM_K8_VAL, PFM_K8_RSVD, PFM_K8_NO64, MSR_K7_EVNTSEL2),
+/* pmc3  */ PMC_D(PFM_REG_I64, "PERFSEL3", PFM_K8_VAL, PFM_K8_RSVD, PFM_K8_NO64, MSR_K7_EVNTSEL3),
+/* pmc4  */ PMC_D(PFM_REG_I,   "IBSFETCHCTL", 0, PFM_AMD64_IBSFETCHCTL_RSVD, 0, MSR_AMD64_IBSFETCHCTL),
+/* pmc5  */ PMC_D(PFM_REG_I,   "IBSOPCTL",    0, PFM_AMD64_IBSOPCTL_RSVD,    0, MSR_AMD64_IBSOPCTL),
+};
+#define PFM_AMD_NUM_PMCS ARRAY_SIZE(pfm_amd64_pmc_desc)
+
+#define PFM_REG_IBS (PFM_REG_I|PFM_REG_INTR)
+static struct pfm_regmap_desc pfm_amd64_pmd_desc[] = {
+/* pmd0  */ PMD_D(PFM_REG_C,   "PERFCTR0",	MSR_K7_PERFCTR0),
+/* pmd1  */ PMD_D(PFM_REG_C,   "PERFCTR1",	MSR_K7_PERFCTR1),
+/* pmd2  */ PMD_D(PFM_REG_C,   "PERFCTR2",	MSR_K7_PERFCTR2),
+/* pmd3  */ PMD_D(PFM_REG_C,   "PERFCTR3",	MSR_K7_PERFCTR3),
+/* pmd4  */ PMD_D(PFM_REG_IBS, "IBSFETCHCTL",	MSR_AMD64_IBSFETCHCTL),
+/* pmd5  */ PMD_D(PFM_REG_IRO, "IBSFETCHLINAD",	MSR_AMD64_IBSFETCHLINAD),
+/* pmd6  */ PMD_D(PFM_REG_IRO, "IBSFETCHPHYSAD", MSR_AMD64_IBSFETCHPHYSAD),
+/* pmd7  */ PMD_D(PFM_REG_IBS, "IBSOPCTL",	MSR_AMD64_IBSOPCTL),
+/* pmd8  */ PMD_D(PFM_REG_IRO, "IBSOPRIP",	MSR_AMD64_IBSOPRIP),
+/* pmd9  */ PMD_D(PFM_REG_IRO, "IBSOPDATA",	MSR_AMD64_IBSOPDATA),
+/* pmd10 */ PMD_D(PFM_REG_IRO, "IBSOPDATA2",	MSR_AMD64_IBSOPDATA2),
+/* pmd11 */ PMD_D(PFM_REG_IRO, "IBSOPDATA3",	MSR_AMD64_IBSOPDATA3),
+/* pmd12 */ PMD_D(PFM_REG_IRO, "IBSDCLINAD",	MSR_AMD64_IBSDCLINAD),
+/* pmd13 */ PMD_D(PFM_REG_IRO, "IBSDCPHYSAD",	MSR_AMD64_IBSDCPHYSAD),
+};
+#define PFM_AMD_NUM_PMDS ARRAY_SIZE(pfm_amd64_pmd_desc)
+
+static struct pfm_context **pfm_nb_sys_owners;
+static struct pfm_context *pfm_nb_task_owner;
+
+static struct pfm_pmu_config pfm_amd64_pmu_conf;
+
+/* Functions for accessing extended PCI config space. Can be removed
+   when Kernel API exists. */
+extern spinlock_t pci_config_lock;
+
+#define PCI_CONF1_ADDRESS(bus, devfn, reg) \
+	(0x80000000 | ((reg & 0xF00) << 16) | ((bus & 0xFF) << 16) \
+	| (devfn << 8) | (reg & 0xFC))
+
+#define is_ibs(x) (pfm_amd64_pmu_info.pmc_addrs[x].reg_type & PFM_REGT_IBS)
+
+static int pci_read(unsigned int seg, unsigned int bus,
+		    unsigned int devfn, int reg, int len, u32 *value)
+{
+	unsigned long flags;
+
+	if ((bus > 255) || (devfn > 255) || (reg > 4095)) {
+		*value = -1;
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&pci_config_lock, flags);
+
+	outl(PCI_CONF1_ADDRESS(bus, devfn, reg), 0xCF8);
+
+	switch (len) {
+	case 1:
+		*value = inb(0xCFC + (reg & 3));
+		break;
+	case 2:
+		*value = inw(0xCFC + (reg & 2));
+		break;
+	case 4:
+		*value = inl(0xCFC);
+		break;
+	}
+
+	spin_unlock_irqrestore(&pci_config_lock, flags);
+
+	return 0;
+}
+
+static int pci_write(unsigned int seg, unsigned int bus,
+		     unsigned int devfn, int reg, int len, u32 value)
+{
+	unsigned long flags;
+
+	if ((bus > 255) || (devfn > 255) || (reg > 4095))
+		return -EINVAL;
+
+	spin_lock_irqsave(&pci_config_lock, flags);
+
+	outl(PCI_CONF1_ADDRESS(bus, devfn, reg), 0xCF8);
+
+	switch (len) {
+	case 1:
+		outb((u8)value, 0xCFC + (reg & 3));
+		break;
+	case 2:
+		outw((u16)value, 0xCFC + (reg & 2));
+		break;
+	case 4:
+		outl((u32)value, 0xCFC);
+		break;
+	}
+
+	spin_unlock_irqrestore(&pci_config_lock, flags);
+
+	return 0;
+}
+
+static inline int
+pci_read_ext_config_dword(struct pci_dev *dev, int where, u32 *val)
+{
+	return pci_read(0, dev->bus->number, dev->devfn, where, 4, val);
+}
+
+static inline int
+pci_write_ext_config_dword(struct pci_dev *dev, int where, u32 val)
+{
+	return pci_write(0, dev->bus->number, dev->devfn, where, 4, val);
+}
+
+static void pfm_amd64_enable_pci_ecs_per_cpu(void)
+{
+	u64 reg;
+	/* enable PCI extended config space */
+	rdmsrl(MSR_AMD64_NB_CFG, reg);
+	if (reg & ENABLE_CF8_EXT_CFG)
+		return;
+	reg |= ENABLE_CF8_EXT_CFG;
+	wrmsrl(MSR_AMD64_NB_CFG, reg);
+}
+
+static void pfm_amd64_setup_eilvt_per_cpu(void *info)
+{
+	u8 lvt_off;
+
+	pfm_amd64_enable_pci_ecs_per_cpu();
+
+	/* program the IBS vector to the perfmon vector */
+	lvt_off =  setup_APIC_eilvt_ibs(LOCAL_PERFMON_VECTOR,
+					APIC_EILVT_MSG_FIX, 0);
+	PFM_DBG("APIC_EILVT%d set to 0x%x", lvt_off, LOCAL_PERFMON_VECTOR);
+	pfm_amd64_pmu_info.ibs_eilvt_off = lvt_off;
+}
+
+static int pfm_amd64_setup_eilvt(void)
+{
+	struct pci_dev *cpu_cfg;
+	int nodes;
+	u32 value = 0;
+
+	/* per CPU setup */
+	on_each_cpu(pfm_amd64_setup_eilvt_per_cpu, NULL, 0, 1);
+
+	nodes = 0;
+	cpu_cfg = NULL;
+	do {
+		cpu_cfg = pci_get_device(PCI_VENDOR_ID_AMD,
+					 PCI_DEVICE_ID_AMD_10H_NB_MISC,
+					 cpu_cfg);
+		if (!cpu_cfg)
+			break;
+		++nodes;
+		pci_write_ext_config_dword(cpu_cfg, IBSCTL,
+					   pfm_amd64_pmu_info.ibs_eilvt_off
+					   | IBSCTL_LVTOFFSETVAL);
+		pci_read_ext_config_dword(cpu_cfg, IBSCTL, &value);
+		if (value != (pfm_amd64_pmu_info.ibs_eilvt_off
+			      | IBSCTL_LVTOFFSETVAL)) {
+			PFM_DBG("Failed to setup IBS LVT offset, "
+				"IBSCTL = 0x%08x", value);
+			return 1;
+		}
+	} while (1);
+
+	if (!nodes) {
+		PFM_DBG("No CPU node configured for IBS");
+		return 1;
+	}
+
+#ifdef CONFIG_X86_64
+	/* Sanity check */
+	/* Works only for 64bit with proper numa implementation. */
+	if (nodes != num_possible_nodes()) {
+		PFM_DBG("Failed to setup CPU node(s) for IBS, "
+			"found: %d, expected %d",
+			nodes, num_possible_nodes());
+		return 1;
+	}
+#endif
+	return 0;
+}
+
+/*
+ * There can only be one user per socket for the Northbridge (NB) events,
+ * so we enforce mutual exclusion as follows:
+ * 	- per-thread : only one context machine-wide can use NB events
+ * 	- system-wide: only one context per processor socket
+ *
+ * Exclusion is enforced at:
+ * 	- pfm_load_context()
+ * 	- pfm_write_pmcs() for attached contexts
+ *
+ * Exclusion is released at:
+ * 	- pfm_unload_context() or any calls that implicitely uses it
+ *
+ * return:
+ * 	0  : successfully acquire NB access
+ * 	< 0:  errno, failed to acquire NB access
+ */
+static int pfm_amd64_acquire_nb(struct pfm_context *ctx)
+{
+	struct pfm_context **entry, *old;
+	int proc_id;
+
+#ifdef CONFIG_SMP
+	proc_id = topology_physical_package_id(smp_processor_id());
+#else
+	proc_id = 0;
+#endif
+
+	if (ctx->flags.system)
+		entry = &pfm_nb_sys_owners[proc_id];
+	else
+		entry = &pfm_nb_task_owner;
+
+	old = cmpxchg(entry, NULL, ctx);
+	if (!old) {
+		if (ctx->flags.system)
+			PFM_DBG("acquired Northbridge event access on socket %u", proc_id);
+		else
+			PFM_DBG("acquired Northbridge event access globally");
+	} else if (old != ctx) {
+		if (ctx->flags.system)
+			PFM_DBG("NorthBridge event conflict on socket %u", proc_id);
+		else
+			PFM_DBG("global NorthBridge event conflict");
+		return -EBUSY;
+	}
+	return 0;
+}
+
+/*
+ * invoked from pfm_write_pmcs() when pfm_nb_sys_owners is not NULL,i.e.,
+ * when we have detected a multi-core processor.
+ *
+ * context is locked, interrupts are masked
+ */
+static int pfm_amd64_pmc_write_check(struct pfm_context *ctx,
+			     struct pfm_event_set *set,
+			     struct pfarg_pmc *req)
+{
+	unsigned int event;
+
+	/*
+	 * delay checking NB event until we load the context
+	 */
+	if (ctx->state == PFM_CTX_UNLOADED)
+		return 0;
+
+	/*
+	 * check event is NB event
+	 */
+	event = (unsigned int)(req->reg_value & 0xff);
+	if (event < 0xee)
+		return 0;
+
+	return pfm_amd64_acquire_nb(ctx);
+}
+
+/*
+ * invoked on pfm_load_context().
+ * context is locked, interrupts are masked
+ */
+static int pfm_amd64_load_context(struct pfm_context *ctx)
+{
+	struct pfm_event_set *set;
+	unsigned int i, n;
+
+	/*
+	 * scan all sets for NB events
+	 */
+	list_for_each_entry(set, &ctx->set_list, list) {
+		n = set->nused_pmcs;
+		for (i = 0; n; i++) {
+			if (!test_bit(i, cast_ulp(set->used_pmcs)))
+				continue;
+
+			if (!is_ibs(i) && (set->pmcs[i] & 0xff) >= 0xee)
+				goto found;
+			n--;
+		}
+	}
+	return 0;
+found:
+	return pfm_amd64_acquire_nb(ctx);
+}
+
+/*
+ * invoked on pfm_unload_context()
+ */
+static int pfm_amd64_unload_context(struct pfm_context *ctx)
+{
+	struct pfm_context **entry, *old;
+	int proc_id;
+
+#ifdef CONFIG_SMP
+	proc_id = topology_physical_package_id(smp_processor_id());
+#else
+	proc_id = 0;
+#endif
+
+	/*
+	 * unload always happens on the monitored CPU in system-wide
+	 */
+	if (ctx->flags.system)
+		entry = &pfm_nb_sys_owners[proc_id];
+	else
+		entry = &pfm_nb_task_owner;
+
+	old = cmpxchg(entry, ctx, NULL);
+	if (old == ctx) {
+		if (ctx->flags.system)
+			PFM_DBG("released NorthBridge on socket %u", proc_id);
+		else
+			PFM_DBG("released NorthBridge events globally");
+	}
+	return 0;
+}
+
+/*
+ * detect if we need to activate NorthBridge event access control
+ */
+static int pfm_amd64_setup_nb_event_control(void)
+{
+	unsigned int c, n = 0;
+	unsigned int max_phys = 0;
+
+#ifdef CONFIG_SMP
+	for_each_possible_cpu(c) {
+		if (cpu_data(c).phys_proc_id > max_phys)
+			max_phys = cpu_data(c).phys_proc_id;
+	}
+#else
+	max_phys = 0;
+#endif
+	if (max_phys > 255) {
+		PFM_INFO("socket id %d is too big to handle", max_phys);
+		return -ENOMEM;
+	}
+
+	n = max_phys + 1;
+	if (n < 2)
+		return 0;
+
+	pfm_nb_sys_owners = vmalloc(n * sizeof(*pfm_nb_sys_owners));
+	if (!pfm_nb_sys_owners)
+		return -ENOMEM;
+
+	memset(pfm_nb_sys_owners, 0, n * sizeof(*pfm_nb_sys_owners));
+	pfm_nb_task_owner = NULL;
+
+	/*
+	 * activate write-checker for PMC registers
+	 */
+	for (c = 0; c < PFM_AMD_NUM_PMCS; c++) {
+		if (!is_ibs(c))
+			pfm_amd64_pmc_desc[c].type |= PFM_REG_WC;
+	}
+
+	pfm_amd64_pmu_info.load_context = pfm_amd64_load_context;
+	pfm_amd64_pmu_info.unload_context = pfm_amd64_unload_context;
+
+	pfm_amd64_pmu_conf.pmc_write_check = pfm_amd64_pmc_write_check;
+
+	PFM_INFO("NorthBridge event access control enabled");
+
+	return 0;
+}
+
+/*
+ * disable registers which are not available on
+ * the host (applies to IBS registers)
+ */
+static void pfm_amd64_check_registers(void)
+{
+	struct pfm_arch_ext_reg *ext_reg;
+	u16 i, has_ibs, has_ibsext;
+
+	has_ibs = pfm_amd64_pmu_info.flags & PFM_X86_FL_IBS;
+	has_ibsext = pfm_amd64_pmu_info.flags & PFM_X86_FL_IBS_EXT;
+
+	PFM_DBG("has_ibs=%d has_ibs_ext=%d", has_ibs, has_ibsext);
+
+	/*
+	 * Scan PMC registers
+	 */
+	ext_reg = pfm_amd64_pmu_info.pmc_addrs;
+	for (i = 0; i < PFM_AMD_NUM_PMCS;  i++, ext_reg++) {
+		if (!has_ibs && ext_reg->reg_type & PFM_REGT_IBS) {
+			ext_reg->reg_type = PFM_REGT_NA;
+			pfm_amd64_pmc_desc[i].type = PFM_REG_NA;
+			PFM_DBG("pmc%u not available", i);
+		}
+		if (!has_ibsext && ext_reg->reg_type & PFM_REGT_IBS_EXT) {
+			ext_reg->reg_type = PFM_REGT_NA;
+			pfm_amd64_pmc_desc[i].type = PFM_REG_NA;
+			PFM_DBG("pmc%u not available", i);
+		}
+	}
+
+	/*
+	 * Scan PMD registers
+	 */
+	ext_reg = pfm_amd64_pmu_info.pmd_addrs;
+	for (i = 0; i < PFM_AMD_NUM_PMDS;  i++, ext_reg++) {
+		if (!has_ibs && ext_reg->reg_type & PFM_REGT_IBS) {
+			ext_reg->reg_type = PFM_REGT_NA;
+			pfm_amd64_pmd_desc[i].type = PFM_REG_NA;
+			PFM_DBG("pmd%u not available", i);
+		}
+		if (!has_ibsext && ext_reg->reg_type & PFM_REGT_IBS_EXT) {
+			ext_reg->reg_type = PFM_REGT_NA;
+			pfm_amd64_pmd_desc[i].type = PFM_REG_NA;
+			PFM_DBG("pmd%u not available", i);
+		}
+
+		/*
+		 * adjust reserved mask for counters
+		 */
+		if (ext_reg->reg_type & PFM_REGT_CTR)
+			pfm_amd64_pmd_desc[i].rsvd_msk = ~((1ULL<<48)-1);
+	}
+	/*
+	 * adjust reserved bit fields for family 16
+	 */
+	if (current_cpu_data.x86 == 16) {
+		for(i=0; i < PFM_AMD_NUM_PMCS; i++)
+			if (pfm_amd64_pmc_desc[i].rsvd_msk == PFM_K8_RSVD)
+				pfm_amd64_pmc_desc[i].rsvd_msk = PFM_16_RSVD;
+	}
+}
+
+static int pfm_amd64_probe_pmu(void)
+{
+	u64 val = 0;
+	if (current_cpu_data.x86_vendor != X86_VENDOR_AMD) {
+		PFM_INFO("not an AMD processor");
+		return -1;
+	}
+
+	switch (current_cpu_data.x86) {
+	case 16:
+		if (current_cpu_data.x86_model >= 2) {
+			/* Family 10h, RevB and later */
+			pfm_amd64_pmu_info.flags |= PFM_X86_FL_IBS_EXT
+				| PFM_X86_FL_USE_EI;
+		}
+		pfm_amd64_pmu_info.flags |= PFM_X86_FL_IBS;
+		rdmsrl(MSR_AMD64_IBSCTL, val);
+	case 15:
+	case  6:
+		PFM_INFO("found family=%d VAL=0x%llx", current_cpu_data.x86, (unsigned long long)val);
+		break;
+	default:
+		PFM_INFO("unsupported family=%d", current_cpu_data.x86);
+		return -1;
+	}
+
+	/*
+	 * check for local APIC (required)
+	 */
+	if (!cpu_has_apic) {
+		PFM_INFO("no local APIC, unsupported");
+		return -1;
+	}
+
+	if (current_cpu_data.x86_max_cores > 1
+	    && pfm_amd64_setup_nb_event_control())
+		return -1;
+
+	if (force_nmi)
+		pfm_amd64_pmu_info.flags |= PFM_X86_FL_USE_NMI;
+
+	/* Setup extended interrupt */
+	if (pfm_amd64_pmu_info.flags & PFM_X86_FL_USE_EI) {
+		if (pfm_amd64_setup_eilvt()) {
+			PFM_INFO("Failed to initialize extended interrupts "
+				 "for IBS");
+			pfm_amd64_pmu_info.flags &= ~(PFM_X86_FL_IBS
+					      | PFM_X86_FL_IBS_EXT
+					      | PFM_X86_FL_USE_EI);
+			PFM_INFO("Unable to use IBS");
+		}
+	}
+
+	if (pfm_amd64_pmu_info.flags & PFM_X86_FL_IBS)
+		PFM_INFO("IBS supported");
+
+	if (pfm_amd64_pmu_info.flags & PFM_X86_FL_IBS_EXT)
+		PFM_INFO("IBS extended registers supported");
+
+	if (pfm_amd64_pmu_info.flags & PFM_X86_FL_USE_EI)
+		PFM_INFO("Using extended interrupts for IBS");
+	else if (pfm_amd64_pmu_info.flags & (PFM_X86_FL_IBS|PFM_X86_FL_IBS_EXT))
+		PFM_INFO("Using performance counter interrupts for IBS");
+
+	pfm_amd64_check_registers();
+
+	return 0;
+}
+
+static struct pfm_pmu_config pfm_amd64_pmu_conf = {
+	.pmu_name = "AMD64",
+	.counter_width = 47,
+	.pmd_desc = pfm_amd64_pmd_desc,
+	.pmc_desc = pfm_amd64_pmc_desc,
+	.num_pmc_entries = PFM_AMD_NUM_PMCS,
+	.num_pmd_entries = PFM_AMD_NUM_PMDS,
+	.probe_pmu = pfm_amd64_probe_pmu,
+	.version = "1.2",
+	.arch_info = &pfm_amd64_pmu_info,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+};
+
+static int __init pfm_amd64_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_amd64_pmu_conf);
+}
+
+static void __exit pfm_amd64_pmu_cleanup_module(void)
+{
+	if (pfm_nb_sys_owners)
+		vfree(pfm_nb_sys_owners);
+
+	pfm_pmu_unregister(&pfm_amd64_pmu_conf);
+}
+
+module_init(pfm_amd64_pmu_init_module);
+module_exit(pfm_amd64_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/x86/perfmon/perfmon_intel_arch.c linux-2.6.25-id/arch/x86/perfmon/perfmon_intel_arch.c
--- linux-2.6.25-org/arch/x86/perfmon/perfmon_intel_arch.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/perfmon_intel_arch.c	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,387 @@
+/*
+ * This file contains the Intel architectural perfmon v1 or v2
+ * description tables.
+ *
+ * Architectural perfmon was introduced with Intel Core Solo/Duo
+ * processors.
+ *
+ * Copyright (c) 2006-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <asm/msr.h>
+#include <asm/apic.h>
+#include <asm/nmi.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Intel architectural perfmon v1");
+MODULE_LICENSE("GPL");
+
+static int force, force_nmi;
+MODULE_PARM_DESC(force, "bool: force module to load succesfully");
+MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
+module_param(force, bool, 0600);
+module_param(force_nmi, bool, 0600);
+
+/*
+ * - upper 32 bits are reserved
+ * - INT: APIC enable bit is reserved (forced to 1)
+ * - bit 21 is reserved
+ *
+ * RSVD: reserved bits are 1
+ */
+#define PFM_IA_PMC_RSVD	((~((1ULL<<32)-1)) \
+			| (1ULL<<20) \
+			| (1ULL<<21))
+
+/*
+ * force Local APIC interrupt on overflow
+ * disable with NO_EMUL64
+ */
+#define PFM_IA_PMC_VAL	(1ULL<<20)
+#define PFM_IA_NO64	(1ULL<<20)
+
+/*
+ * architectuture specifies that:
+ * IA32_PMCx MSR        : starts at 0x0c1 & occupy a contiguous block of MSR
+ * IA32_PERFEVTSELx MSR : starts at 0x186 & occupy a contiguous block of MSR
+ * MSR_GEN_FIXED_CTR0   : starts at 0x309 & occupy a contiguous block of MSR
+ */
+#define MSR_GEN_SEL_BASE	MSR_P6_EVNTSEL0
+#define MSR_GEN_PMC_BASE	MSR_P6_PERFCTR0
+#define MSR_GEN_FIXED_PMC_BASE	MSR_CORE_PERF_FIXED_CTR0
+
+#define PFM_IA_SEL(n)	{ 			\
+	.addrs[0] = MSR_GEN_SEL_BASE+(n),	\
+	.ctr = n,				\
+	.reg_type = PFM_REGT_EN}
+
+#define PFM_IA_CTR(n) {				\
+	.addrs[0] = MSR_GEN_PMC_BASE+(n),	\
+	.ctr = n,				\
+	.reg_type = PFM_REGT_CTR}
+
+#define PFM_IA_FCTR(n) {			\
+	.addrs[0] = MSR_GEN_FIXED_PMC_BASE+(n),	\
+	.ctr = n,				\
+	.reg_type = PFM_REGT_CTR}
+
+/*
+ * layout of EAX for CPUID.0xa leaf function
+ */
+struct pmu_eax {
+        unsigned int version:8;		/* architectural perfmon version */
+        unsigned int num_cnt:8; 	/* number of generic counters */
+        unsigned int cnt_width:8;	/* width of generic counters */
+        unsigned int ebx_length:8;	/* number of architected events */
+};
+
+/*
+ * layout of EDX for CPUID.0xa leaf function when perfmon v2 is detected
+ */
+struct pmu_edx {
+        unsigned int num_cnt:5;		/* number of fixed counters */
+        unsigned int cnt_width:8;	/* width of fixed counters */
+        unsigned int reserved:19;
+};
+
+
+/*
+ * physical addresses of MSR controlling the perfevtsel and counter registers
+ */
+struct pfm_arch_pmu_info pfm_intel_arch_pmu_info={
+	.pmc_addrs = {
+/* pmc0  */	PFM_IA_SEL(0) ,  PFM_IA_SEL(1),  PFM_IA_SEL(2),  PFM_IA_SEL(3),
+/* pmc4  */	PFM_IA_SEL(4) ,  PFM_IA_SEL(5),  PFM_IA_SEL(6),  PFM_IA_SEL(7),
+/* pmc8  */	PFM_IA_SEL(8) ,  PFM_IA_SEL(9), PFM_IA_SEL(10), PFM_IA_SEL(11),
+/* pmc12 */	PFM_IA_SEL(12), PFM_IA_SEL(13), PFM_IA_SEL(14), PFM_IA_SEL(15),
+
+/* pmc16 */	{
+			.addrs[0] = MSR_CORE_PERF_FIXED_CTR_CTRL,
+			.reg_type = PFM_REGT_EN
+		}
+	},
+
+	.pmd_addrs = {
+/* pmd0  */	PFM_IA_CTR(0) ,  PFM_IA_CTR(1),  PFM_IA_CTR(2),  PFM_IA_CTR(3),
+/* pmd4  */	PFM_IA_CTR(4) ,  PFM_IA_CTR(5),  PFM_IA_CTR(6),  PFM_IA_CTR(7),
+/* pmd8  */	PFM_IA_CTR(8) ,  PFM_IA_CTR(9), PFM_IA_CTR(10), PFM_IA_CTR(11),
+/* pmd12 */	PFM_IA_CTR(12), PFM_IA_CTR(13), PFM_IA_CTR(14), PFM_IA_CTR(15),
+
+/* pmd16 */	PFM_IA_FCTR(0), PFM_IA_FCTR(1), PFM_IA_FCTR(2), PFM_IA_FCTR(3),
+/* pmd20 */	PFM_IA_FCTR(4), PFM_IA_FCTR(5), PFM_IA_FCTR(6), PFM_IA_FCTR(7),
+/* pmd24 */	PFM_IA_FCTR(8), PFM_IA_FCTR(9), PFM_IA_FCTR(10), PFM_IA_FCTR(11),
+/* pmd28 */	PFM_IA_FCTR(12), PFM_IA_FCTR(13), PFM_IA_FCTR(14), PFM_IA_FCTR(15)
+	},
+	.pmu_style = PFM_X86_PMU_P6
+};
+
+#define PFM_IA_C(n) {                   \
+	.type = PFM_REG_I64,            \
+	.desc = "PERFEVTSEL"#n,         \
+	.dfl_val = PFM_IA_PMC_VAL,      \
+	.rsvd_msk = PFM_IA_PMC_RSVD,    \
+	.no_emul64_msk = PFM_IA_NO64,   \
+	.hw_addr = MSR_GEN_SEL_BASE+(n) \
+	}
+
+#define PFM_IA_D(n) PMD_D(PFM_REG_C, "PMC"#n, MSR_P6_PERFCTR0+n)
+#define PFM_IA_FD(n) PMD_D(PFM_REG_C, "FIXED_CTR"#n, MSR_CORE_PERF_FIXED_CTR0+n)
+
+static struct pfm_regmap_desc pfm_intel_arch_pmc_desc[]={
+/* pmc0  */ PFM_IA_C(0),  PFM_IA_C(1),   PFM_IA_C(2),  PFM_IA_C(3),
+/* pmc4  */ PFM_IA_C(4),  PFM_IA_C(5),   PFM_IA_C(6),  PFM_IA_C(7),
+/* pmc8  */ PFM_IA_C(8),  PFM_IA_C(9),  PFM_IA_C(10), PFM_IA_C(11),
+/* pmc12 */ PFM_IA_C(12), PFM_IA_C(13), PFM_IA_C(14), PFM_IA_C(15),
+
+/* pmc16 */ { .type = PFM_REG_I,
+	      .desc = "FIXED_CTRL",
+	      .dfl_val = 0x8888888888888888ULL,
+	      .rsvd_msk = 0xccccccccccccccccULL,
+	      .no_emul64_msk = 0,
+	      .hw_addr = MSR_CORE_PERF_FIXED_CTR_CTRL
+	    },
+};
+#define PFM_IA_MAX_PMCS	ARRAY_SIZE(pfm_intel_arch_pmc_desc)
+
+static struct pfm_regmap_desc pfm_intel_arch_pmd_desc[]={
+/* pmd0  */  PFM_IA_D(0),  PFM_IA_D(1),  PFM_IA_D(2),  PFM_IA_D(3),
+/* pmd4  */  PFM_IA_D(4),  PFM_IA_D(5),  PFM_IA_D(6),  PFM_IA_D(7),
+/* pmd8  */  PFM_IA_D(8),  PFM_IA_D(9), PFM_IA_D(10), PFM_IA_D(11),
+/* pmd12 */ PFM_IA_D(12), PFM_IA_D(13), PFM_IA_D(14), PFM_IA_D(15),
+
+/* pmd16 */ PFM_IA_FD(0), PFM_IA_FD(1), PFM_IA_FD(2), PFM_IA_FD(3),
+/* pmd20 */ PFM_IA_FD(4), PFM_IA_FD(5), PFM_IA_FD(6), PFM_IA_FD(7),
+/* pmd24 */ PFM_IA_FD(8), PFM_IA_FD(9), PFM_IA_FD(10), PFM_IA_FD(11),
+/* pmd28 */ PFM_IA_FD(16), PFM_IA_FD(17), PFM_IA_FD(18), PFM_IA_FD(19)
+};
+#define PFM_IA_MAX_PMDS	ARRAY_SIZE(pfm_intel_arch_pmd_desc)
+
+#define PFM_IA_MAX_CNT		16 /* maximum # of generic counters in mapping table */
+#define PFM_IA_MAX_FCNT		16 /* maximum # of fixed counters in mapping table */
+#define PFM_IA_FCNT_BASE	16 /* base index of fixed counters PMD */
+
+static struct pfm_pmu_config pfm_intel_arch_pmu_conf;
+
+static int pfm_intel_arch_check_errata(void)
+{
+	/*
+	 * Core Duo errata AE49 (no fix). Both counters share a single
+	 * enable bit in PERFEVTSEL0
+	 */
+	if (current_cpu_data.x86 == 6 && current_cpu_data.x86_model == 14) {
+		pfm_intel_arch_pmu_info.flags |= PFM_X86_FL_NO_SHARING;
+	}
+	return 0;
+}
+
+static int pfm_intel_arch_probe_pmu(void)
+{
+	union {
+		unsigned int val;
+		struct pmu_eax eax;
+		struct pmu_edx edx;
+	} eax, edx;
+	unsigned int ebx, ecx;
+	unsigned int num_cnt, i;
+	u64 dfl, rsvd;
+
+	edx.val = 0;
+
+	if (!cpu_has_arch_perfmon && force == 0) {
+		PFM_INFO("no support for Intel architectural PMU");
+		return -1;
+	}
+
+	if (!cpu_has_apic) {
+		PFM_INFO("no Local APIC, try rebooting with lapic option");
+		return -1;
+	}
+
+	if (pfm_intel_arch_check_errata())
+		return -1;
+
+	if (force == 0) {
+		/* cpuid() call protected by cpu_has_arch_perfmon */
+		cpuid(0xa, &eax.val, &ebx, &ecx, &edx.val);
+	} else {
+		/* lowest common denominator */
+		eax.eax.version = 1;
+		eax.eax.num_cnt = 2;
+		eax.eax.cnt_width = 40;
+		edx.val = 0;
+	}
+	/*
+	 * reject processors supported by perfmon_intel_core
+	 *
+	 * We need to do this explicitely to avoid depending
+	 * on the link order in case, the modules are compiled as
+	 * builtin.
+	 *
+	 * non Intel processors are rejected by cpu_has_arch_perfmon
+	 */
+	if (current_cpu_data.x86 == 6) {
+		switch(current_cpu_data.x86_model) {
+			case 15: /* Merom: use perfmon_intel_core  */
+				return -1;
+			default:
+				break;
+		}
+	}
+
+	/*
+	 * some 6/15 models have buggy BIOS
+	 */
+	if (eax.eax.version == 0
+	    && current_cpu_data.x86 == 6 && current_cpu_data.x86_model == 15) {
+		PFM_INFO("buggy v2 BIOS, adjusting for 2 generic counters");
+		eax.eax.version = 2;
+		eax.eax.num_cnt = 2;
+		eax.eax.cnt_width = 40;
+	}
+
+	/*
+	 * some v2 BIOSes are incomplete
+	 */
+	if (eax.eax.version == 2 && !edx.edx.num_cnt) {
+		PFM_INFO("buggy v2 BIOS, adjusting for 3 fixed counters");
+		edx.edx.num_cnt = 3;
+		edx.edx.cnt_width = 40;
+	}
+
+	/*
+	 * no fixed counters on earlier versions
+	 */
+	if (eax.eax.version < 2)
+		edx.val = 0;
+
+	PFM_INFO("detected architecural perfmon v%d", eax.eax.version);
+	PFM_INFO("num_gen=%d width=%d num_fixed=%d width=%d",
+		  eax.eax.num_cnt,
+		  eax.eax.cnt_width,
+		  edx.edx.num_cnt,
+		  edx.edx.cnt_width);
+
+	/* number of generic counters */
+	num_cnt = eax.eax.num_cnt;
+
+	if (num_cnt >= PFM_IA_MAX_CNT) {
+		printk(KERN_INFO "perfmon: Limiting number of generic counters to %zu,"
+				 "HW supports %u", PFM_IA_MAX_PMCS, num_cnt);
+		num_cnt = PFM_IA_MAX_CNT;
+
+	}
+
+	/*
+	 * adjust rsvd_msk for generic counters based on actual width
+	 */
+	for(i=0; i < num_cnt; i++)
+		pfm_intel_arch_pmd_desc[i].rsvd_msk = ~((1ULL<<eax.eax.cnt_width)-1);
+
+	/*
+	 * mark unused generic counters as not available
+	 */
+	for(i=num_cnt; i < PFM_IA_MAX_CNT; i++) {
+		pfm_intel_arch_pmd_desc[i].type = PFM_REG_NA;
+		pfm_intel_arch_pmc_desc[i].type = PFM_REG_NA;
+	}
+
+	/*
+	 * now process fixed counters (if any)
+	 */
+	num_cnt = edx.edx.num_cnt;
+
+	/*
+	 * adjust rsvd_msk for fixed counters based on actual width
+	 */
+	for(i=0; i < num_cnt; i++)
+		pfm_intel_arch_pmd_desc[PFM_IA_FCNT_BASE+i].rsvd_msk = ~((1ULL<<edx.edx.cnt_width)-1);
+
+	/*
+	 * mark unused fixed counters as
+	 * unavailable.
+	 * update the rsvd_msk, dfl_val for
+	 * FIXED_CTRL:
+	 * 	rsvd_msk: set all 4 bits
+	 *	dfl_val : clear all 4 bits
+	 */
+	dfl = pfm_intel_arch_pmc_desc[16].dfl_val;
+	rsvd = pfm_intel_arch_pmc_desc[16].rsvd_msk;
+
+	for(i=num_cnt; i < PFM_IA_MAX_FCNT; i++) {
+		pfm_intel_arch_pmd_desc[PFM_IA_FCNT_BASE+i].type = PFM_REG_NA;
+		rsvd |= 0xfULL << (i<<2);
+		dfl &= ~(0xfULL << (i<<2));
+	}
+
+	/*
+	 * FIXED_CTR_CTRL unavailable when no fixed counters are defined
+	 */
+	if (!num_cnt) {
+		pfm_intel_arch_pmc_desc[16].type = PFM_REG_NA;
+	} else {
+		pfm_intel_arch_pmc_desc[16].rsvd_msk = rsvd;
+		pfm_intel_arch_pmc_desc[16].dfl_val = dfl;
+	}
+
+	/*
+	 * Maximum number of entries in both tables (some maybe NA)
+	 */
+	pfm_intel_arch_pmu_conf.num_pmc_entries = PFM_IA_MAX_PMCS;
+	pfm_intel_arch_pmu_conf.num_pmd_entries = PFM_IA_MAX_PMDS;
+
+	if (force_nmi)
+		pfm_intel_arch_pmu_info.flags |= PFM_X86_FL_USE_NMI;
+
+
+	return 0;
+}
+
+/*
+ * Counters may have model-specific width. Yet the documentation says
+ * that only the lower 32 bits can be written to due to the specification
+ * of wrmsr. bits [32-(w-1)] are sign extensions of bit 31. Bits [w-63] must
+ * not be set (see rsvd_msk for PMDs). As such the effective width of a
+ * counter is 31 bits only regardless of what CPUID.0xa returns.
+ *
+ * See IA-32 Intel Architecture Software developer manual Vol 3B chapter 18
+ */
+static struct pfm_pmu_config pfm_intel_arch_pmu_conf={
+	.pmu_name = "Intel architectural",
+	.pmd_desc = pfm_intel_arch_pmd_desc,
+	.counter_width   = 31,
+	.pmc_desc = pfm_intel_arch_pmc_desc,
+	.probe_pmu = pfm_intel_arch_probe_pmu,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.arch_info = &pfm_intel_arch_pmu_info
+};
+
+static int __init pfm_intel_arch_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_intel_arch_pmu_conf);
+}
+
+static void __exit pfm_intel_arch_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_intel_arch_pmu_conf);
+}
+
+module_init(pfm_intel_arch_pmu_init_module);
+module_exit(pfm_intel_arch_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/x86/perfmon/perfmon_intel_core.c linux-2.6.25-id/arch/x86/perfmon/perfmon_intel_core.c
--- linux-2.6.25-org/arch/x86/perfmon/perfmon_intel_core.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/perfmon_intel_core.c	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,248 @@
+/*
+ * This file contains the Intel Core PMU registers description tables.
+ * Intel Core-based processors support architectural perfmon v2 + PEBS
+ *
+ * Copyright (c) 2006-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <asm/nmi.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Intel Core");
+MODULE_LICENSE("GPL");
+
+static int force_nmi;
+MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
+module_param(force_nmi, bool, 0600);
+
+/*
+ * - upper 32 bits are reserved
+ * - INT: APIC enable bit is reserved (forced to 1)
+ * - bit 21 is reserved
+ *
+ *   RSVD: reserved bits must be 1
+ */
+#define PFM_CORE_PMC_RSVD ((~((1ULL<<32)-1)) \
+			| (1ULL<<20)   \
+			| (1ULL<<21))
+
+/*
+ * force Local APIC interrupt on overflow
+ * disable with NO_EMUL64
+ */
+#define PFM_CORE_PMC_VAL	(1ULL<<20)
+#define PFM_CORE_NO64		(1ULL<<20)
+
+#define PFM_CORE_NA { .reg_type = PFM_REGT_NA}
+
+#define PFM_CORE_CA(m, c, t) \
+	{ \
+	  .addrs[0] = m, \
+	  .ctr = c, \
+	  .reg_type = t \
+	}
+/*
+ * physical addresses of MSR for evntsel and perfctr registers
+ *
+ * IMPORTANT:
+ * 	The mapping  was chosen to be compatible with the Intel
+ * 	architectural perfmon, so that applications which only
+ * 	know about the architectural perfmon can work on Core
+ * 	without any changes.
+ *
+ * 	We do not expose the GLOBAL_* registers because:
+ * 	- would be incompatible with architectural perfmon v1
+ * 	  (unless default means, measures everything for GLOBAL_CTRL)
+ *	- would cause a conflict when NMI watchdog is enabled.
+ *
+ */
+struct pfm_arch_pmu_info pfm_core_pmu_info={
+	.pmc_addrs = {
+/* pmc0  */	PFM_CORE_CA(MSR_P6_EVNTSEL0, 0, PFM_REGT_EN),
+/* pmc1  */	PFM_CORE_CA(MSR_P6_EVNTSEL1, 1, PFM_REGT_EN),
+/* pmc2  */	PFM_CORE_NA, PFM_CORE_NA,
+/* pmc4  */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
+/* pmc8  */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
+/* pmc12 */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
+/* pmc16 */	PFM_CORE_CA(MSR_CORE_PERF_FIXED_CTR_CTRL, 0, PFM_REGT_EN),
+/* pmc17 */	PFM_CORE_CA(MSR_IA32_PEBS_ENABLE, 0, PFM_REGT_EN)
+	},
+	.pmd_addrs = {
+/* pmd0  */	PFM_CORE_CA(MSR_P6_PERFCTR0, 0, PFM_REGT_CTR),
+/* pmd1  */	PFM_CORE_CA(MSR_P6_PERFCTR1, 0, PFM_REGT_CTR),
+/* pmd2  */	PFM_CORE_NA, PFM_CORE_NA,
+/* pmd4  */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
+/* pmd8  */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
+/* pmd12 */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
+/* pmd16 */	PFM_CORE_CA(MSR_CORE_PERF_FIXED_CTR0, 0, PFM_REGT_CTR),
+/* pmd17 */	PFM_CORE_CA(MSR_CORE_PERF_FIXED_CTR1, 0, PFM_REGT_CTR),
+/* pmd18 */	PFM_CORE_CA(MSR_CORE_PERF_FIXED_CTR2, 0, PFM_REGT_CTR)
+	},
+	.pebs_ctr_idx = 0, /* IA32_PMC0 */
+	.pmu_style = PFM_X86_PMU_CORE
+};
+
+static struct pfm_regmap_desc pfm_core_pmc_desc[]={
+/* pmc0  */ {
+	      .type = PFM_REG_I64,
+	      .desc = "PERFEVTSEL0",
+	      .dfl_val = PFM_CORE_PMC_VAL,
+	      .rsvd_msk = PFM_CORE_PMC_RSVD,
+	      .no_emul64_msk = PFM_CORE_NO64,
+	      .hw_addr = MSR_P6_EVNTSEL0
+	    },
+/* pmc1  */ {
+	      .type = PFM_REG_I64,
+	      .desc = "PERFEVTSEL1",
+	      .dfl_val = PFM_CORE_PMC_VAL,
+	      .rsvd_msk = PFM_CORE_PMC_RSVD,
+	      .no_emul64_msk = PFM_CORE_NO64,
+	      .hw_addr = MSR_P6_EVNTSEL1
+	    },
+/* pmc2  */ PMX_NA, PMX_NA,
+/* pmc4  */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc8  */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc12 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc16 */ { .type = PFM_REG_I,
+	      .desc = "FIXED_CTRL",
+	      .dfl_val = 0x888ULL,
+	      .rsvd_msk = 0xfffffffffffffcccULL,
+	      .no_emul64_msk = 0,
+	      .hw_addr = MSR_CORE_PERF_FIXED_CTR_CTRL
+	    },
+/* pmc17  */ { .type = PFM_REG_W,
+	      .desc = "PEBS_ENABLE",
+	      .dfl_val = 0,
+	      .rsvd_msk = 0xfffffffffffffffeULL,
+	      .no_emul64_msk = 0,
+	      .hw_addr = MSR_IA32_PEBS_ENABLE
+	    }
+};
+
+#define PFM_CORE_D(n) PMD_D(PFM_REG_C, "PMC"#n, MSR_P6_PERFCTR0+n)
+#define PFM_CORE_FD(n) PMD_D(PFM_REG_C, "FIXED_CTR"#n, MSR_CORE_PERF_FIXED_CTR0+n)
+
+static struct pfm_regmap_desc pfm_core_pmd_desc[]={
+/* pmd0  */ PFM_CORE_D(0),
+/* pmd1  */ PFM_CORE_D(1),
+/* pmd2  */ PMX_NA, PMX_NA,
+/* pmd4  */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmd8  */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmd12 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmd16 */ PFM_CORE_FD(0),
+/* pmd17 */ PFM_CORE_FD(1),
+/* pmd18 */ PFM_CORE_FD(2)
+};
+#define PFM_CORE_NUM_PMCS	ARRAY_SIZE(pfm_core_pmc_desc)
+#define PFM_CORE_NUM_PMDS	ARRAY_SIZE(pfm_core_pmd_desc)
+
+static struct pfm_pmu_config pfm_core_pmu_conf;
+
+static int pfm_core_probe_pmu(void)
+{
+	unsigned int i;
+
+	/*
+	 * Check for Intel Core processor explicitely
+	 * Checking for cpu_has_perfmon is not enough as this
+	 * matches intel Core Duo/Core Solo but none supports
+	 * PEBS.
+	 *
+	 * Intel Core = arch perfmon v2 + PEBS
+	 */
+	if (current_cpu_data.x86 != 6)
+		return -1;
+
+	switch(current_cpu_data.x86_model) {
+		case 15: /* Merom */
+			break;
+		case 23: /* Penryn */
+			break;
+		default:
+			return -1;
+	}
+
+	if (!cpu_has_apic) {
+		PFM_INFO("no Local APIC, unsupported");
+		return -1;
+	}
+
+	PFM_INFO("nmi_watchdog=%d nmi_active=%d force_nmi=%d",
+		nmi_watchdog, atomic_read(&nmi_active), force_nmi);
+
+	/*
+	 * Intel Core processors implement DS and PEBS, no need to check
+	 */
+	pfm_core_pmu_info.flags |= PFM_X86_FL_PMU_DS|PFM_X86_FL_PMU_PEBS;
+	PFM_INFO("PEBS supported, enabled");
+
+	/*
+	 * Core 2 have 40-bit counters (generic, fixed)
+	 */
+	for(i=0; i < PFM_CORE_NUM_PMDS; i++)
+		pfm_core_pmd_desc[i].rsvd_msk = ~((1ULL<<40)-1);
+
+	if (force_nmi)
+		pfm_core_pmu_info.flags |= PFM_X86_FL_USE_NMI;
+
+	return 0;
+}
+
+static int pfm_core_pmc17_check(struct pfm_context *ctx,
+			     struct pfm_event_set *set,
+			     struct pfarg_pmc *req)
+{
+	struct pfm_arch_context *ctx_arch;
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * if user activates PEBS_ENABLE, then we need to have a valid
+	 * DS Area setup. This only happens when the PEBS sampling format is used
+	 * in which case PFM_X86_USE_PEBS is set. We must reject all other requests.
+	 * Otherwise we may pickup stale MSR_IA32_DS_AREA values. It appears
+	 * that a value of 0 for this MSR does crash the system with PEBS_ENABLE=1.
+	 */
+	if (!ctx_arch->flags.use_pebs && req->reg_value) {
+		PFM_DBG("pmc17 (PEBS_ENABLE) can only be used with PEBS sampling format");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * Counters may have model-specific width which can be probed using
+ * the CPUID.0xa leaf. Yet, the documentation says: "
+ * In the initial implementation, only the read bit width is reported
+ * by CPUID, write operations are limited to the low 32 bits.
+ * Bits [w-32] are sign extensions of bit 31. As such the effective width
+ * of a counter is 31 bits only.
+ */
+static struct pfm_pmu_config pfm_core_pmu_conf={
+	.pmu_name = "Intel Core",
+	.pmd_desc = pfm_core_pmd_desc,
+	.counter_width = 31,
+	.num_pmc_entries = PFM_CORE_NUM_PMCS,
+	.num_pmd_entries = PFM_CORE_NUM_PMDS,
+	.pmc_desc = pfm_core_pmc_desc,
+	.probe_pmu = pfm_core_probe_pmu,
+	.version = "1.2",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.arch_info = &pfm_core_pmu_info,
+	.pmc_write_check = pfm_core_pmc17_check
+};
+
+static int __init pfm_core_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_core_pmu_conf);
+}
+
+static void __exit pfm_core_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_core_pmu_conf);
+}
+
+module_init(pfm_core_pmu_init_module);
+module_exit(pfm_core_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/x86/perfmon/perfmon_p4.c linux-2.6.25-id/arch/x86/perfmon/perfmon_p4.c
--- linux-2.6.25-org/arch/x86/perfmon/perfmon_p4.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/perfmon_p4.c	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,411 @@
+/*
+ * This file contains the P4/Xeon PMU register description tables
+ * for both 32 and 64 bit modes.
+ *
+ * Copyright (c) 2005 Intel Corporation
+ * Contributed by Bryan Wilkerson <bryan.p.wilkerson@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <asm/msr.h>
+#include <asm/apic.h>
+#include <asm/nmi.h>
+
+MODULE_AUTHOR("Bryan Wilkerson <bryan.p.wilkerson@intel.com>");
+MODULE_DESCRIPTION("P4/Xeon/EM64T PMU description table");
+MODULE_LICENSE("GPL");
+
+static int force;
+MODULE_PARM_DESC(force, "bool: force module to load succesfully");
+module_param(force, bool, 0600);
+
+static int force_nmi;
+MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
+module_param(force_nmi, bool, 0600);
+
+/*
+ * CCCR default value:
+ * 	- OVF_PMI_T0=1 (bit 26)
+ * 	- OVF_PMI_T1=0 (bit 27) (set if necessary in pfm_write_reg())
+ * 	- all other bits are zero
+ *
+ * OVF_PMI is forced to zero if PFM_REGFL_NO_EMUL64 is set on CCCR
+ */
+#define PFM_CCCR_DFL	(1ULL<<26) | (3ULL<<16)
+
+/*
+ * CCCR reserved fields:
+ * 	- bits 0-11, 25-29, 31-63
+ * 	- OVF_PMI (26-27), override with REGFL_NO_EMUL64
+ *
+ * RSVD: reserved bits must be 1
+ */
+#define PFM_CCCR_RSVD     ~((0xfull<<12)  \
+			| (0x7full<<18) \
+			| (0x1ull<<30))
+
+#define PFM_P4_NO64	(3ULL<<26) /* use 3 even in non HT mode */
+
+/*
+ * With HyperThreading enabled:
+ *
+ *  The ESCRs and CCCRs are divided in half with the top half
+ *  belonging to logical processor 0 and the bottom half going to
+ *  logical processor 1. Thus only half of the PMU resources are
+ *  accessible to applications.
+ *
+ *  PEBS is not available due to the fact that:
+ *  	- MSR_PEBS_MATRIX_VERT is shared between the threads
+ *      - IA32_PEBS_ENABLE is shared between the threads
+ *
+ * With HyperThreading disabled:
+ *
+ * The full set of PMU resources is exposed to applications.
+ *
+ * The mapping is chosen such that PMCxx -> MSR is the same
+ * in HT and non HT mode, if register is present in HT mode.
+ *
+ */
+#define PFM_REGT_NHTESCR (PFM_REGT_ESCR|PFM_REGT_NOHT)
+#define PFM_REGT_NHTCCCR (PFM_REGT_CCCR|PFM_REGT_NOHT|PFM_REGT_EN)
+#define PFM_REGT_NHTPEBS (PFM_REGT_PEBS|PFM_REGT_NOHT|PFM_REGT_EN)
+#define PFM_REGT_NHTCTR  (PFM_REGT_CTR|PFM_REGT_NOHT)
+#define PFM_REGT_ENAC    (PFM_REGT_CCCR|PFM_REGT_EN)
+
+static struct pfm_arch_pmu_info pfm_p4_pmu_info={
+ .pmc_addrs = {
+	/*pmc 0 */    {{MSR_P4_BPU_ESCR0, MSR_P4_BPU_ESCR1}, 0, PFM_REGT_ESCR}, /*   BPU_ESCR0,1 */
+	/*pmc 1 */    {{MSR_P4_IS_ESCR0, MSR_P4_IS_ESCR1}, 0, PFM_REGT_ESCR}, /*    IS_ESCR0,1 */
+	/*pmc 2 */    {{MSR_P4_MOB_ESCR0, MSR_P4_MOB_ESCR1}, 0, PFM_REGT_ESCR}, /*   MOB_ESCR0,1 */
+	/*pmc 3 */    {{MSR_P4_ITLB_ESCR0, MSR_P4_ITLB_ESCR1}, 0, PFM_REGT_ESCR}, /*  ITLB_ESCR0,1 */
+	/*pmc 4 */    {{MSR_P4_PMH_ESCR0, MSR_P4_PMH_ESCR1}, 0, PFM_REGT_ESCR}, /*   PMH_ESCR0,1 */
+	/*pmc 5 */    {{MSR_P4_IX_ESCR0, MSR_P4_IX_ESCR1}, 0, PFM_REGT_ESCR}, /*    IX_ESCR0,1 */
+	/*pmc 6 */    {{MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1}, 0, PFM_REGT_ESCR}, /*   FSB_ESCR0,1 */
+	/*pmc 7 */    {{MSR_P4_BSU_ESCR0, MSR_P4_BSU_ESCR1}, 0, PFM_REGT_ESCR}, /*   BSU_ESCR0,1 */
+	/*pmc 8 */    {{MSR_P4_MS_ESCR0, MSR_P4_MS_ESCR1}, 0, PFM_REGT_ESCR}, /*    MS_ESCR0,1 */
+	/*pmc 9 */    {{MSR_P4_TC_ESCR0, MSR_P4_TC_ESCR1}, 0, PFM_REGT_ESCR}, /*    TC_ESCR0,1 */
+	/*pmc 10*/    {{MSR_P4_TBPU_ESCR0, MSR_P4_TBPU_ESCR1}, 0, PFM_REGT_ESCR}, /*  TBPU_ESCR0,1 */
+	/*pmc 11*/    {{MSR_P4_FLAME_ESCR0, MSR_P4_FLAME_ESCR1}, 0, PFM_REGT_ESCR}, /* FLAME_ESCR0,1 */
+	/*pmc 12*/    {{MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1}, 0, PFM_REGT_ESCR}, /*  FIRM_ESCR0,1 */
+	/*pmc 13*/    {{MSR_P4_SAAT_ESCR0, MSR_P4_SAAT_ESCR1}, 0, PFM_REGT_ESCR}, /*  SAAT_ESCR0,1 */
+	/*pmc 14*/    {{MSR_P4_U2L_ESCR0, MSR_P4_U2L_ESCR1}, 0, PFM_REGT_ESCR}, /*   U2L_ESCR0,1 */
+	/*pmc 15*/    {{MSR_P4_DAC_ESCR0, MSR_P4_DAC_ESCR1}, 0, PFM_REGT_ESCR}, /*   DAC_ESCR0,1 */
+	/*pmc 16*/    {{MSR_P4_IQ_ESCR0, MSR_P4_IQ_ESCR1}, 0, PFM_REGT_ESCR}, /*    IQ_ESCR0,1 (only model 1 and 2) */
+	/*pmc 17*/    {{MSR_P4_ALF_ESCR0, MSR_P4_ALF_ESCR1}, 0, PFM_REGT_ESCR}, /*   ALF_ESCR0,1 */
+	/*pmc 18*/    {{MSR_P4_RAT_ESCR0, MSR_P4_RAT_ESCR1}, 0, PFM_REGT_ESCR}, /*   RAT_ESCR0,1 */
+	/*pmc 19*/    {{MSR_P4_SSU_ESCR0, 0}, 0, PFM_REGT_ESCR}, /*   SSU_ESCR0   */
+	/*pmc 20*/    {{MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1}, 0, PFM_REGT_ESCR}, /*   CRU_ESCR0,1 */
+	/*pmc 21*/    {{MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3}, 0, PFM_REGT_ESCR}, /*   CRU_ESCR2,3 */
+	/*pmc 22*/    {{MSR_P4_CRU_ESCR4, MSR_P4_CRU_ESCR5}, 0, PFM_REGT_ESCR}, /*   CRU_ESCR4,5 */
+
+	/*pmc 23*/    {{MSR_P4_BPU_CCCR0, MSR_P4_BPU_CCCR2}, 0, PFM_REGT_ENAC}, /*   BPU_CCCR0,2 */
+	/*pmc 24*/    {{MSR_P4_BPU_CCCR1, MSR_P4_BPU_CCCR3}, 1, PFM_REGT_ENAC}, /*   BPU_CCCR1,3 */
+	/*pmc 25*/    {{MSR_P4_MS_CCCR0, MSR_P4_MS_CCCR2}, 2, PFM_REGT_ENAC}, /*    MS_CCCR0,2 */
+	/*pmc 26*/    {{MSR_P4_MS_CCCR1, MSR_P4_MS_CCCR3}, 3, PFM_REGT_ENAC}, /*    MS_CCCR1,3 */
+	/*pmc 27*/    {{MSR_P4_FLAME_CCCR0, MSR_P4_FLAME_CCCR2}, 4, PFM_REGT_ENAC}, /* FLAME_CCCR0,2 */
+	/*pmc 28*/    {{MSR_P4_FLAME_CCCR1, MSR_P4_FLAME_CCCR3}, 5, PFM_REGT_ENAC}, /* FLAME_CCCR1,3 */
+	/*pmc 29*/    {{MSR_P4_IQ_CCCR0, MSR_P4_IQ_CCCR2}, 6, PFM_REGT_ENAC}, /*    IQ_CCCR0,2 */
+	/*pmc 30*/    {{MSR_P4_IQ_CCCR1, MSR_P4_IQ_CCCR3}, 7, PFM_REGT_ENAC}, /*    IQ_CCCR1,3 */
+	/*pmc 31*/    {{MSR_P4_IQ_CCCR4, MSR_P4_IQ_CCCR5}, 8, PFM_REGT_ENAC}, /*    IQ_CCCR4,5 */
+	/* non HT extensions */
+	/*pmc 32*/    {{MSR_P4_BPU_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   BPU_ESCR1   */
+	/*pmc 33*/    {{MSR_P4_IS_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    IS_ESCR1   */
+	/*pmc 34*/    {{MSR_P4_MOB_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   MOB_ESCR1   */
+	/*pmc 35*/    {{MSR_P4_ITLB_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*  ITLB_ESCR1   */
+	/*pmc 36*/    {{MSR_P4_PMH_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   PMH_ESCR1   */
+	/*pmc 37*/    {{MSR_P4_IX_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    IX_ESCR1   */
+	/*pmc 38*/    {{MSR_P4_FSB_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   FSB_ESCR1   */
+	/*pmc 39*/    {{MSR_P4_BSU_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   BSU_ESCR1   */
+	/*pmc 40*/    {{MSR_P4_MS_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    MS_ESCR1   */
+	/*pmc 41*/    {{MSR_P4_TC_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    TC_ESCR1   */
+	/*pmc 42*/    {{MSR_P4_TBPU_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*  TBPU_ESCR1   */
+	/*pmc 43*/    {{MSR_P4_FLAME_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /* FLAME_ESCR1   */
+	/*pmc 44*/    {{MSR_P4_FIRM_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*  FIRM_ESCR1   */
+	/*pmc 45*/    {{MSR_P4_SAAT_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*  SAAT_ESCR1   */
+	/*pmc 46*/    {{MSR_P4_U2L_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   U2L_ESCR1   */
+	/*pmc 47*/    {{MSR_P4_DAC_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   DAC_ESCR1   */
+	/*pmc 48*/    {{MSR_P4_IQ_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    IQ_ESCR1   (only model 1 and 2) */
+	/*pmc 49*/    {{MSR_P4_ALF_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   ALF_ESCR1   */
+	/*pmc 50*/    {{MSR_P4_RAT_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   RAT_ESCR1   */
+	/*pmc 51*/    {{MSR_P4_CRU_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   CRU_ESCR1   */
+	/*pmc 52*/    {{MSR_P4_CRU_ESCR3,     0}, 0, PFM_REGT_NHTESCR}, /*   CRU_ESCR3   */
+	/*pmc 53*/    {{MSR_P4_CRU_ESCR5,     0}, 0, PFM_REGT_NHTESCR}, /*   CRU_ESCR5   */
+	/*pmc 54*/    {{MSR_P4_BPU_CCCR1,     0}, 9, PFM_REGT_NHTCCCR}, /*   BPU_CCCR1   */
+	/*pmc 55*/    {{MSR_P4_BPU_CCCR3,     0},10, PFM_REGT_NHTCCCR}, /*   BPU_CCCR3   */
+	/*pmc 56*/    {{MSR_P4_MS_CCCR1,     0},11, PFM_REGT_NHTCCCR}, /*    MS_CCCR1   */
+	/*pmc 57*/    {{MSR_P4_MS_CCCR3,     0},12, PFM_REGT_NHTCCCR}, /*    MS_CCCR3   */
+	/*pmc 58*/    {{MSR_P4_FLAME_CCCR1,     0},13, PFM_REGT_NHTCCCR}, /* FLAME_CCCR1   */
+	/*pmc 59*/    {{MSR_P4_FLAME_CCCR3,     0},14, PFM_REGT_NHTCCCR}, /* FLAME_CCCR3   */
+	/*pmc 60*/    {{MSR_P4_IQ_CCCR2,     0},15, PFM_REGT_NHTCCCR}, /*    IQ_CCCR2   */
+	/*pmc 61*/    {{MSR_P4_IQ_CCCR3,     0},16, PFM_REGT_NHTCCCR}, /*    IQ_CCCR3   */
+	/*pmc 62*/    {{MSR_P4_IQ_CCCR5,     0},17, PFM_REGT_NHTCCCR}, /*    IQ_CCCR5   */
+	/*pmc 63*/    {{0x3f2,     0}, 0, PFM_REGT_NHTPEBS},/* PEBS_MATRIX_VERT */
+	/*pmc 64*/    {{0x3f1,     0}, 0, PFM_REGT_NHTPEBS} /* PEBS_ENABLE   */
+},
+
+.pmd_addrs = {
+	/*pmd 0 */    {{MSR_P4_BPU_PERFCTR0, MSR_P4_BPU_PERFCTR2}, 0, PFM_REGT_CTR},  /*   BPU_CTR0,2  */
+	/*pmd 1 */    {{MSR_P4_BPU_PERFCTR1, MSR_P4_BPU_PERFCTR3}, 0, PFM_REGT_CTR},  /*   BPU_CTR1,3  */
+	/*pmd 2 */    {{MSR_P4_MS_PERFCTR0, MSR_P4_MS_PERFCTR2}, 0, PFM_REGT_CTR},  /*    MS_CTR0,2  */
+	/*pmd 3 */    {{MSR_P4_MS_PERFCTR1, MSR_P4_MS_PERFCTR3}, 0, PFM_REGT_CTR},  /*    MS_CTR1,3  */
+	/*pmd 4 */    {{MSR_P4_FLAME_PERFCTR0, MSR_P4_FLAME_PERFCTR2}, 0, PFM_REGT_CTR},  /* FLAME_CTR0,2  */
+	/*pmd 5 */    {{MSR_P4_FLAME_PERFCTR1, MSR_P4_FLAME_PERFCTR3}, 0, PFM_REGT_CTR},  /* FLAME_CTR1,3  */
+	/*pmd 6 */    {{MSR_P4_IQ_PERFCTR0, MSR_P4_IQ_PERFCTR2}, 0, PFM_REGT_CTR},  /*    IQ_CTR0,2  */
+	/*pmd 7 */    {{MSR_P4_IQ_PERFCTR1, MSR_P4_IQ_PERFCTR3}, 0, PFM_REGT_CTR},  /*    IQ_CTR1,3  */
+	/*pmd 8 */    {{MSR_P4_IQ_PERFCTR4, MSR_P4_IQ_PERFCTR5}, 0, PFM_REGT_CTR},  /*    IQ_CTR4,5  */
+	/*
+	 * non HT extensions
+	 */
+	/*pmd 9 */    {{MSR_P4_BPU_PERFCTR2,     0}, 0, PFM_REGT_NHTCTR},  /*   BPU_CTR2    */
+	/*pmd 10*/    {{MSR_P4_BPU_PERFCTR3,     0}, 0, PFM_REGT_NHTCTR},  /*   BPU_CTR3    */
+	/*pmd 11*/    {{MSR_P4_MS_PERFCTR2,     0}, 0, PFM_REGT_NHTCTR},  /*    MS_CTR2    */
+	/*pmd 12*/    {{MSR_P4_MS_PERFCTR3,     0}, 0, PFM_REGT_NHTCTR},  /*    MS_CTR3    */
+	/*pmd 13*/    {{MSR_P4_FLAME_PERFCTR2,     0}, 0, PFM_REGT_NHTCTR},  /* FLAME_CTR2    */
+	/*pmd 14*/    {{MSR_P4_FLAME_PERFCTR3,     0}, 0, PFM_REGT_NHTCTR},  /* FLAME_CTR3    */
+	/*pmd 15*/    {{MSR_P4_IQ_PERFCTR2,     0}, 0, PFM_REGT_NHTCTR},  /*    IQ_CTR2    */
+	/*pmd 16*/    {{MSR_P4_IQ_PERFCTR3,     0}, 0, PFM_REGT_NHTCTR},  /*    IQ_CTR3    */
+	/*pmd 17*/    {{MSR_P4_IQ_PERFCTR5,     0}, 0, PFM_REGT_NHTCTR},  /*    IQ_CTR5    */
+},
+.pebs_ctr_idx = 8, /* thread0: IQ_CTR4, thread1: IQ_CTR5 */
+.pmu_style = PFM_X86_PMU_P4
+};
+
+static struct pfm_regmap_desc pfm_p4_pmc_desc[]={
+/* pmc0  */ PMC_D(PFM_REG_I, "BPU_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_BPU_ESCR0),
+/* pmc1  */ PMC_D(PFM_REG_I, "IS_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IQ_ESCR0),
+/* pmc2  */ PMC_D(PFM_REG_I, "MOB_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_MOB_ESCR0),
+/* pmc3  */ PMC_D(PFM_REG_I, "ITLB_ESCR0" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_ITLB_ESCR0),
+/* pmc4  */ PMC_D(PFM_REG_I, "PMH_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_PMH_ESCR0),
+/* pmc5  */ PMC_D(PFM_REG_I, "IX_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IX_ESCR0),
+/* pmc6  */ PMC_D(PFM_REG_I, "FSB_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FSB_ESCR0),
+/* pmc7  */ PMC_D(PFM_REG_I, "BSU_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_BSU_ESCR0),
+/* pmc8  */ PMC_D(PFM_REG_I, "MS_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_MS_ESCR0),
+/* pmc9  */ PMC_D(PFM_REG_I, "TC_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_TC_ESCR0),
+/* pmc10 */ PMC_D(PFM_REG_I, "TBPU_ESCR0" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_TBPU_ESCR0),
+/* pmc11 */ PMC_D(PFM_REG_I, "FLAME_ESCR0", 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FLAME_ESCR0),
+/* pmc12 */ PMC_D(PFM_REG_I, "FIRM_ESCR0" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FIRM_ESCR0),
+/* pmc13 */ PMC_D(PFM_REG_I, "SAAT_ESCR0" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_SAAT_ESCR0),
+/* pmc14 */ PMC_D(PFM_REG_I, "U2L_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_U2L_ESCR0),
+/* pmc15 */ PMC_D(PFM_REG_I, "DAC_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_DAC_ESCR0),
+/* pmc16 */ PMC_D(PFM_REG_I, "IQ_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IQ_ESCR0), /* only model 1 and 2*/
+/* pmc17 */ PMC_D(PFM_REG_I, "ALF_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_ALF_ESCR0),
+/* pmc18 */ PMC_D(PFM_REG_I, "RAT_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_RAT_ESCR0),
+/* pmc19 */ PMC_D(PFM_REG_I, "SSU_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_SSU_ESCR0),
+/* pmc20 */ PMC_D(PFM_REG_I, "CRU_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR0),
+/* pmc21 */ PMC_D(PFM_REG_I, "CRU_ESCR2"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR2),
+/* pmc22 */ PMC_D(PFM_REG_I, "CRU_ESCR4"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR4),
+/* pmc23 */ PMC_D(PFM_REG_I64, "BPU_CCCR0"  , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_BPU_CCCR0),
+/* pmc24 */ PMC_D(PFM_REG_I64, "BPU_CCCR1"  , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_BPU_CCCR1),
+/* pmc25 */ PMC_D(PFM_REG_I64, "MS_CCCR0"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_MS_CCCR0),
+/* pmc26 */ PMC_D(PFM_REG_I64, "MS_CCCR1"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_MS_CCCR1),
+/* pmc27 */ PMC_D(PFM_REG_I64, "FLAME_CCCR0", PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_FLAME_CCCR0),
+/* pmc28 */ PMC_D(PFM_REG_I64, "FLAME_CCCR1", PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_FLAME_CCCR1),
+/* pmc29 */ PMC_D(PFM_REG_I64, "IQ_CCCR0"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR0),
+/* pmc30 */ PMC_D(PFM_REG_I64, "IQ_CCCR1"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR1),
+/* pmc31 */ PMC_D(PFM_REG_I64, "IQ_CCCR4"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR4),
+		/* No HT extension */
+/* pmc32 */ PMC_D(PFM_REG_I, "BPU_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_BPU_ESCR1),
+/* pmc33 */ PMC_D(PFM_REG_I, "IS_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IS_ESCR1),
+/* pmc34 */ PMC_D(PFM_REG_I, "MOB_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_MOB_ESCR1),
+/* pmc35 */ PMC_D(PFM_REG_I, "ITLB_ESCR1" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_ITLB_ESCR1),
+/* pmc36 */ PMC_D(PFM_REG_I, "PMH_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_PMH_ESCR1),
+/* pmc37 */ PMC_D(PFM_REG_I, "IX_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IX_ESCR1),
+/* pmc38 */ PMC_D(PFM_REG_I, "FSB_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FSB_ESCR1),
+/* pmc39 */ PMC_D(PFM_REG_I, "BSU_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_BSU_ESCR1),
+/* pmc40 */ PMC_D(PFM_REG_I, "MS_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_MS_ESCR1),
+/* pmc41 */ PMC_D(PFM_REG_I, "TC_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_TC_ESCR1),
+/* pmc42 */ PMC_D(PFM_REG_I, "TBPU_ESCR1" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_TBPU_ESCR1),
+/* pmc43 */ PMC_D(PFM_REG_I, "FLAME_ESCR1", 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FLAME_ESCR1),
+/* pmc44 */ PMC_D(PFM_REG_I, "FIRM_ESCR1" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FIRM_ESCR1),
+/* pmc45 */ PMC_D(PFM_REG_I, "SAAT_ESCR1" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_SAAT_ESCR1),
+/* pmc46 */ PMC_D(PFM_REG_I, "U2L_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_U2L_ESCR1),
+/* pmc47 */ PMC_D(PFM_REG_I, "DAC_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_DAC_ESCR1),
+/* pmc48 */ PMC_D(PFM_REG_I, "IQ_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IQ_ESCR1), /* only model 1 and 2 */
+/* pmc49 */ PMC_D(PFM_REG_I, "ALF_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_ALF_ESCR1),
+/* pmc50 */ PMC_D(PFM_REG_I, "RAT_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_RAT_ESCR1),
+/* pmc51 */ PMC_D(PFM_REG_I, "CRU_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR1),
+/* pmc52 */ PMC_D(PFM_REG_I, "CRU_ESCR3"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR3),
+/* pmc53 */ PMC_D(PFM_REG_I, "CRU_ESCR5"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR5),
+/* pmc54 */ PMC_D(PFM_REG_I64, "BPU_CCCR2"  , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_BPU_CCCR2),
+/* pmc55 */ PMC_D(PFM_REG_I64, "BPU_CCCR3"  , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_BPU_CCCR3),
+/* pmc56 */ PMC_D(PFM_REG_I64, "MS_CCCR2"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_MS_CCCR2),
+/* pmc57 */ PMC_D(PFM_REG_I64, "MS_CCCR3"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_MS_CCCR3),
+/* pmc58 */ PMC_D(PFM_REG_I64, "FLAME_CCCR2", PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_FLAME_CCCR2),
+/* pmc59 */ PMC_D(PFM_REG_I64, "FLAME_CCCR3", PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_FLAME_CCCR3),
+/* pmc60 */ PMC_D(PFM_REG_I64, "IQ_CCCR2"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR2),
+/* pmc61 */ PMC_D(PFM_REG_I64, "IQ_CCCR3"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR3),
+/* pmc62 */ PMC_D(PFM_REG_I64, "IQ_CCCR5"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR5),
+/* pmc63 */ PMC_D(PFM_REG_I, "PEBS_MATRIX_VERT", 0, 0xffffffffffffffecULL, 0, 0x3f2),
+/* pmc64 */ PMC_D(PFM_REG_I, "PEBS_ENABLE", 0, 0xfffffffff8ffe000ULL, 0, 0x3f1)
+};
+#define PFM_P4_NUM_PMCS ARRAY_SIZE(pfm_p4_pmc_desc)
+
+/*
+ * See section 15.10.6.6 for details about the IQ block
+ */
+static struct pfm_regmap_desc pfm_p4_pmd_desc[]={
+/* pmd0  */ PMD_D(PFM_REG_C, "BPU_CTR0", MSR_P4_BPU_PERFCTR0),
+/* pmd1  */ PMD_D(PFM_REG_C, "BPU_CTR1", MSR_P4_BPU_PERFCTR1),
+/* pmd2  */ PMD_D(PFM_REG_C, "MS_CTR0", MSR_P4_MS_PERFCTR0),
+/* pmd3  */ PMD_D(PFM_REG_C, "MS_CTR1", MSR_P4_MS_PERFCTR1),
+/* pmd4  */ PMD_D(PFM_REG_C, "FLAME_CTR0", MSR_P4_FLAME_PERFCTR0),
+/* pmd5  */ PMD_D(PFM_REG_C, "FLAME_CTR1", MSR_P4_FLAME_PERFCTR1),
+/* pmd6  */ PMD_D(PFM_REG_C, "IQ_CTR0", MSR_P4_IQ_PERFCTR0),
+/* pmd7  */ PMD_D(PFM_REG_C, "IQ_CTR1", MSR_P4_IQ_PERFCTR1),
+/* pmd8  */ PMD_D(PFM_REG_C, "IQ_CTR4", MSR_P4_IQ_PERFCTR4),
+		/* no HT extension */
+/* pmd9  */ PMD_D(PFM_REG_C, "BPU_CTR2", MSR_P4_BPU_PERFCTR2),
+/* pmd10 */ PMD_D(PFM_REG_C, "BPU_CTR3", MSR_P4_BPU_PERFCTR3),
+/* pmd11 */ PMD_D(PFM_REG_C, "MS_CTR2", MSR_P4_MS_PERFCTR2),
+/* pmd12 */ PMD_D(PFM_REG_C, "MS_CTR3", MSR_P4_MS_PERFCTR3),
+/* pmd13 */ PMD_D(PFM_REG_C, "FLAME_CTR2", MSR_P4_FLAME_PERFCTR2),
+/* pmd14 */ PMD_D(PFM_REG_C, "FLAME_CTR3", MSR_P4_FLAME_PERFCTR3),
+/* pmd15 */ PMD_D(PFM_REG_C, "IQ_CTR2", MSR_P4_IQ_PERFCTR2),
+/* pmd16 */ PMD_D(PFM_REG_C, "IQ_CTR3", MSR_P4_IQ_PERFCTR3),
+/* pmd17 */ PMD_D(PFM_REG_C, "IQ_CTR5", MSR_P4_IQ_PERFCTR5)
+};
+#define PFM_P4_NUM_PMDS ARRAY_SIZE(pfm_p4_pmd_desc)
+
+/*
+ * Due to hotplug CPU support, threads may not necessarily
+ * be activated at the time the module is inserted. We need
+ * to check whether  they could be activated by looking at
+ * the present CPU (present != online).
+ */
+static int pfm_p4_probe_pmu(void)
+{
+	unsigned int i;
+	int ht_enabled;
+
+	/*
+	 * only works on Intel processors
+	 */
+	if (current_cpu_data.x86_vendor != X86_VENDOR_INTEL) {
+		PFM_INFO("not running on Intel processor");
+		return -1;
+	}
+
+	if (current_cpu_data.x86 != 15) {
+		PFM_INFO("unsupported family=%d", current_cpu_data.x86);
+		return -1;
+	}
+
+	switch(current_cpu_data.x86_model) {
+		case 0 ... 2:
+			break;
+		case 3 ... 6:
+			/*
+			 * IQ_ESCR0, IQ_ESCR1 only present on model 1, 2
+			 */
+			pfm_p4_pmc_desc[16].type = PFM_REG_NA;
+			pfm_p4_pmc_desc[48].type = PFM_REG_NA;
+			break;
+		default:
+			/*
+			 * do not know if they all work the same, so reject
+			 * for now
+			 */
+			if (!force) {
+				PFM_INFO("unsupported model %d",
+					 current_cpu_data.x86_model);
+				return -1;
+			}
+	}
+
+	/*
+	 * check for local APIC (required)
+	 */
+	if (!cpu_has_apic) {
+		PFM_INFO("no local APIC, unsupported");
+		return -1;
+	}
+#ifdef CONFIG_SMP
+	ht_enabled = (cpus_weight(__get_cpu_var(cpu_core_map))
+		   / current_cpu_data.x86_max_cores) > 1;
+#else
+	ht_enabled = 0;
+#endif
+	if (cpu_has_ht) {
+
+		PFM_INFO("HyperThreading supported, status %s",
+			 ht_enabled ? "on": "off");
+		/*
+		 * disable registers not supporting HT
+		 */
+		if (ht_enabled) {
+			PFM_INFO("disabling half the registers for HT");
+			for (i = 0; i < PFM_P4_NUM_PMCS; i++) {
+				if (pfm_p4_pmu_info.pmc_addrs[(i)].reg_type &
+				    PFM_REGT_NOHT)
+					pfm_p4_pmc_desc[i].type = PFM_REG_NA;
+			}
+			for (i = 0; i < PFM_P4_NUM_PMDS; i++) {
+				if (pfm_p4_pmu_info.pmd_addrs[(i)].reg_type &
+				    PFM_REGT_NOHT)
+					pfm_p4_pmd_desc[i].type = PFM_REG_NA;
+			}
+		}
+	}
+
+	if (cpu_has_ds) {
+		PFM_INFO("Data Save Area (DS) supported");
+
+		pfm_p4_pmu_info.flags = PFM_X86_FL_PMU_DS;
+
+		if (cpu_has_pebs) {
+			/*
+			 * PEBS does not work with HyperThreading enabled
+			 */
+	                if (ht_enabled) {
+				PFM_INFO("PEBS supported, status off (because of HT)");
+			} else {
+				pfm_p4_pmu_info.flags |= PFM_X86_FL_PMU_PEBS;
+				PFM_INFO("PEBS supported, status on");
+			}
+		}
+	}
+	if (force_nmi)
+		pfm_p4_pmu_info.flags |= PFM_X86_FL_USE_NMI;
+	return 0;
+}
+
+static struct pfm_pmu_config pfm_p4_pmu_conf={
+	.pmu_name = "Intel P4",
+	.counter_width = 40,
+	.pmd_desc = pfm_p4_pmd_desc,
+	.pmc_desc = pfm_p4_pmc_desc,
+	.num_pmc_entries = PFM_P4_NUM_PMCS,
+	.num_pmd_entries = PFM_P4_NUM_PMDS,
+	.probe_pmu = pfm_p4_probe_pmu,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.arch_info = &pfm_p4_pmu_info
+};
+
+static int __init pfm_p4_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_p4_pmu_conf);
+}
+
+static void __exit pfm_p4_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_p4_pmu_conf);
+}
+
+module_init(pfm_p4_pmu_init_module);
+module_exit(pfm_p4_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/x86/perfmon/perfmon_p6.c linux-2.6.25-id/arch/x86/perfmon/perfmon_p6.c
--- linux-2.6.25-org/arch/x86/perfmon/perfmon_p6.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/perfmon_p6.c	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,164 @@
+/*
+ * This file contains the P6 family processor PMU register description tables
+ *
+ * This module supports original P6 processors
+ * (Pentium II, Pentium Pro, Pentium III) and Pentium M.
+ *
+ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+#include <asm/msr.h>
+#include <asm/nmi.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("P6 PMU description table");
+MODULE_LICENSE("GPL");
+
+static int force_nmi;
+MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
+module_param(force_nmi, bool, 0600);
+
+/*
+ * - upper 32 bits are reserved
+ * - INT: APIC enable bit is reserved (forced to 1)
+ * - bit 21 is reserved
+ * - bit 22 is reserved on PEREVNTSEL1
+ *
+ * RSVD: reserved bits are 1
+ */
+#define PFM_P6_PMC0_RSVD ((~((1ULL<<32)-1)) | (1ULL<<20) | (1ULL<<21))
+#define PFM_P6_PMC1_RSVD ((~((1ULL<<32)-1)) | (1ULL<<20) | (3ULL<<21))
+
+/*
+ * force Local APIC interrupt on overflow
+ * disable with NO_EMUL64
+ */
+#define PFM_P6_PMC_VAL  (1ULL<<20)
+#define PFM_P6_NO64	(1ULL<<20)
+
+/*
+ * PFM_X86_FL_NO_SHARING: because of the single enable bit on MSR_P6_EVNTSEL0
+ * the PMU cannot be shared with NMI watchdog or Oprofile
+ */
+struct pfm_arch_pmu_info pfm_p6_pmu_info={
+	.pmc_addrs = {
+		{{MSR_P6_EVNTSEL0, 0}, 0, PFM_REGT_EN}, /* has enable bit */
+		{{MSR_P6_EVNTSEL1, 0}, 1, PFM_REGT_OTH} /* no enable bit  */
+	},
+	.pmd_addrs = {
+		{{MSR_P6_PERFCTR0, 0}, 0, PFM_REGT_CTR},
+		{{MSR_P6_PERFCTR1, 0}, 0, PFM_REGT_CTR}
+	},
+	.flags = PFM_X86_FL_NO_SHARING,
+	.pmu_style = PFM_X86_PMU_P6
+};
+
+static struct pfm_regmap_desc pfm_p6_pmc_desc[]={
+/* pmc0  */ PMC_D(PFM_REG_I64, "PERFEVTSEL0", PFM_P6_PMC_VAL, PFM_P6_PMC0_RSVD, PFM_P6_NO64, MSR_P6_EVNTSEL0),
+/* pmc1  */ PMC_D(PFM_REG_I64, "PERFEVTSEL1", PFM_P6_PMC_VAL, PFM_P6_PMC1_RSVD, PFM_P6_NO64, MSR_P6_EVNTSEL1)
+};
+#define PFM_P6_NUM_PMCS	ARRAY_SIZE(pfm_p6_pmc_desc)
+
+static struct pfm_regmap_desc pfm_p6_pmd_desc[]={
+/* pmd0  */ PMD_D(PFM_REG_C  , "PERFCTR0", MSR_P6_PERFCTR0),
+/* pmd1  */ PMD_D(PFM_REG_C  , "PERFCTR1", MSR_P6_PERFCTR1)
+};
+#define PFM_P6_NUM_PMDS ARRAY_SIZE(pfm_p6_pmd_desc)
+
+static int pfm_p6_probe_pmu(void)
+{
+	int high, low;
+
+	if (current_cpu_data.x86_vendor != X86_VENDOR_INTEL) {
+		PFM_INFO("not an Intel processor");
+		return -1;
+	}
+
+	/*
+	 * check for P6 processor family
+	 */
+	if (current_cpu_data.x86 != 6) {
+		PFM_INFO("unsupported family=%d", current_cpu_data.x86);
+		return -1;
+	}
+
+	switch(current_cpu_data.x86_model) {
+		case 1: /* Pentium Pro */
+		case 3:
+		case 5: /* Pentium II Deschutes */
+		case 7 ... 11:
+			break;
+		case 13:
+			/* for Pentium M, we need to check if PMU exist */
+			rdmsr(MSR_IA32_MISC_ENABLE, low, high);
+			if (low & (1U << 7))
+				break;
+		default:
+			PFM_INFO("unsupported CPU model %d",
+				 current_cpu_data.x86_model);
+			return -1;
+
+	}
+
+	if (!cpu_has_apic) {
+		PFM_INFO("no Local APIC, try rebooting with lapic");
+		return -1;
+	}
+	/*
+	 * force NMI interrupt?
+	 */
+	if (force_nmi)
+		pfm_p6_pmu_info.flags |= PFM_X86_FL_USE_NMI;
+
+	return 0;
+}
+
+/*
+ * Counters have 40 bits implemented. However they are designed such
+ * that bits [32-39] are sign extensions of bit 31. As such the
+ * effective width of a counter for P6-like PMU is 31 bits only.
+ *
+ * See IA-32 Intel Architecture Software developer manual Vol 3B
+ */
+static struct pfm_pmu_config pfm_p6_pmu_conf={
+	.pmu_name = "Intel P6 processor Family",
+	.counter_width = 31,
+	.pmd_desc = pfm_p6_pmd_desc,
+	.pmc_desc = pfm_p6_pmc_desc,
+	.num_pmc_entries = PFM_P6_NUM_PMCS,
+	.num_pmd_entries = PFM_P6_NUM_PMDS,
+	.probe_pmu = pfm_p6_probe_pmu,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.arch_info = &pfm_p6_pmu_info
+};
+
+static int __init pfm_p6_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_p6_pmu_conf);
+}
+
+static void __exit pfm_p6_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_p6_pmu_conf);
+}
+
+module_init(pfm_p6_pmu_init_module);
+module_exit(pfm_p6_pmu_cleanup_module);
diff -Naur linux-2.6.25-org/arch/x86/perfmon/perfmon_pebs_core_smpl.c linux-2.6.25-id/arch/x86/perfmon/perfmon_pebs_core_smpl.c
--- linux-2.6.25-org/arch/x86/perfmon/perfmon_pebs_core_smpl.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/perfmon_pebs_core_smpl.c	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,252 @@
+/*
+ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file implements the Precise Event Based Sampling (PEBS)
+ * sampling format for Intel Core-based processors.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <asm/msr.h>
+
+#include <linux/perfmon.h>
+#include <asm/perfmon_pebs_core_smpl.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Intel Core Precise Event-Based Sampling (PEBS)");
+MODULE_LICENSE("GPL");
+
+#define ALIGN_PEBS(a, order) \
+		((a)+(1UL<<(order))-1) & ~((1UL<<(order))-1)
+
+#define PEBS_PADDING_ORDER 8 /* log2(256) padding for PEBS alignment constraint */
+
+static int pfm_pebs_core_fmt_validate(u32 flags, u16 npmds, void *data)
+{
+	struct pfm_pebs_core_smpl_arg *arg = data;
+	size_t min_buf_size;
+
+	/*
+	 * need to define at least the size of the buffer
+	 */
+	if (data == NULL) {
+		PFM_DBG("no argument passed");
+		return -EINVAL;
+	}
+
+	/*
+	 * compute min buf size. npmds is the maximum number
+	 * of implemented PMD registers.
+	 */
+	min_buf_size = sizeof(struct pfm_pebs_core_smpl_hdr)
+		     + sizeof(struct pfm_pebs_core_smpl_entry)
+		     + (1UL<<PEBS_PADDING_ORDER); /* padding for alignment */
+
+	PFM_DBG("validate flags=0x%x min_buf_size=%zu buf_size=%zu",
+		  flags,
+		  min_buf_size,
+		  arg->buf_size);
+
+	/*
+	 * must hold at least the buffer header + one minimally sized entry
+	 */
+	if (arg->buf_size < min_buf_size)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int pfm_pebs_core_fmt_get_size(unsigned int flags, void *data, size_t *size)
+{
+	struct pfm_pebs_core_smpl_arg *arg = data;
+
+	/*
+	 * size has been validated in pfm_pebs_core_fmt_validate()
+	 */
+	*size = arg->buf_size + (1UL<<PEBS_PADDING_ORDER);
+
+	return 0;
+}
+
+static int pfm_pebs_core_fmt_init(struct pfm_context *ctx, void *buf,
+			     u32 flags, u16 npmds, void *data)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_pebs_core_smpl_hdr *hdr;
+	struct pfm_pebs_core_smpl_arg *arg = data;
+	u64 pebs_start, pebs_end;
+	struct pfm_ds_area_core *ds;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	hdr = buf;
+	ds = &hdr->ds;
+
+	/*
+	 * align PEBS buffer base
+	 */
+	pebs_start = ALIGN_PEBS((unsigned long)(hdr+1), PEBS_PADDING_ORDER);
+	pebs_end = pebs_start + arg->buf_size + 1;
+
+	hdr->version = PFM_PEBS_CORE_SMPL_VERSION;
+	hdr->buf_size = arg->buf_size;
+	hdr->overflows = 0;
+
+	/*
+	 * express PEBS buffer base as offset from the end of the header
+	 */
+	hdr->start_offs = pebs_start - (unsigned long)(hdr+1);
+
+	/*
+	 * PEBS buffer boundaries
+	 */
+	ds->pebs_buf_base = pebs_start;
+	ds->pebs_abs_max = pebs_end;
+
+	/*
+	 * PEBS starting position
+	 */
+	ds->pebs_index = pebs_start;
+
+	/*
+	 * PEBS interrupt threshold
+	 */
+	ds->pebs_intr_thres = pebs_start
+			    + arg->intr_thres
+			    * sizeof(struct pfm_pebs_core_smpl_entry);
+
+	/*
+	 * save counter reset value for PEBS counter
+	 */
+	ds->pebs_cnt_reset = arg->cnt_reset;
+
+	/*
+	 * keep track of DS AREA
+	 */
+	ctx_arch->ds_area = ds;
+	ctx_arch->flags.use_ds = 1;
+	ctx_arch->flags.use_pebs = 1;
+
+	PFM_DBG("buffer=%p buf_size=%llu offs=%llu pebs_start=0x%llx "
+		  "pebs_end=0x%llx ds=%p pebs_thres=0x%llx cnt_reset=0x%llx",
+		  buf,
+		  (unsigned long long)hdr->buf_size,
+		  (unsigned long long)hdr->start_offs,
+		  (unsigned long long)pebs_start,
+		  (unsigned long long)pebs_end,
+		  ds,
+		  (unsigned long long)ds->pebs_intr_thres,
+		  (unsigned long long)ds->pebs_cnt_reset);
+
+	return 0;
+}
+
+static int pfm_pebs_core_fmt_handler(void *buf, struct pfm_ovfl_arg *arg,
+			       unsigned long ip, u64 tstamp, void *data)
+{
+	struct pfm_pebs_core_smpl_hdr *hdr;
+
+	hdr = buf;
+
+	PFM_DBG_ovfl("buffer full");
+	/*
+	 * increment number of buffer overflows.
+	 * important to detect duplicate set of samples.
+	 */
+	hdr->overflows++;
+
+	/*
+	 * request notification and masking of monitoring.
+	 * Notification is still subject to the overflowed
+	 * register having the FL_NOTIFY flag set.
+	 */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY| PFM_OVFL_CTRL_MASK;
+
+	return -ENOBUFS; /* we are full, sorry */
+}
+
+static int pfm_pebs_core_fmt_restart(int is_active, u32 *ovfl_ctrl,
+				void *buf)
+{
+	struct pfm_pebs_core_smpl_hdr *hdr = buf;
+
+	/*
+	 * reset index to base of buffer
+	 */
+	hdr->ds.pebs_index = hdr->ds.pebs_buf_base;
+
+	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+
+	return 0;
+}
+
+static int pfm_pebs_core_fmt_exit(void *buf)
+{
+	return 0;
+}
+
+static struct pfm_smpl_fmt pebs_core_fmt={
+	.fmt_name = PFM_PEBS_CORE_SMPL_NAME,
+	.fmt_version = 0x1,
+	.fmt_arg_size = sizeof(struct pfm_pebs_core_smpl_arg),
+	.fmt_validate = pfm_pebs_core_fmt_validate,
+	.fmt_getsize = pfm_pebs_core_fmt_get_size,
+	.fmt_init = pfm_pebs_core_fmt_init,
+	.fmt_handler = pfm_pebs_core_fmt_handler,
+	.fmt_restart = pfm_pebs_core_fmt_restart,
+	.fmt_exit = pfm_pebs_core_fmt_exit,
+	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+};
+
+static int __init pfm_pebs_core_fmt_init_module(void)
+{
+	if (!cpu_has_pebs) {
+		PFM_INFO("processor does not have PEBS support");
+		return -1;
+	}
+	/*
+	 * cpu_has_pebs is not enough to identify Intel Core PEBS
+	 * which is different fro Pentium 4 PEBS. Therefore we do
+	 * a more detailed check here
+	 */
+	if (current_cpu_data.x86 != 6) {
+		PFM_INFO("not an Intel Core processor");
+		return -1;
+	}
+
+	switch(current_cpu_data.x86_model) {
+		case 15: /* Merom */
+		case 23: /* Penryn */
+			break;
+		default:
+			PFM_INFO("not an Intel Core processor");
+			return -1;
+	}
+	return pfm_fmt_register(&pebs_core_fmt);
+}
+
+static void __exit pfm_pebs_core_fmt_cleanup_module(void)
+{
+	pfm_fmt_unregister(&pebs_core_fmt);
+}
+
+module_init(pfm_pebs_core_fmt_init_module);
+module_exit(pfm_pebs_core_fmt_cleanup_module);
diff -Naur linux-2.6.25-org/arch/x86/perfmon/perfmon_pebs_p4_smpl.c linux-2.6.25-id/arch/x86/perfmon/perfmon_pebs_p4_smpl.c
--- linux-2.6.25-org/arch/x86/perfmon/perfmon_pebs_p4_smpl.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/arch/x86/perfmon/perfmon_pebs_p4_smpl.c	2008-04-23 11:21:43.000000000 +0200
@@ -0,0 +1,252 @@
+/*
+ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file implements the Precise Event Based Sampling (PEBS)
+ * sampling format. It supports the following processors:
+ *	- 32-bit Pentium 4 or other Netburst-based processors
+ *	- 64-bit Pentium 4 or other Netburst-based processors
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <asm/msr.h>
+
+#include <linux/perfmon.h>
+#include <asm/perfmon_pebs_p4_smpl.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Intel P4 Precise Event-Based Sampling (PEBS)");
+MODULE_LICENSE("GPL");
+
+#define ALIGN_PEBS(a, order) \
+		((a)+(1UL<<(order))-1) & ~((1UL<<(order))-1)
+
+#define PEBS_PADDING_ORDER 8 /* log2(256) padding for PEBS alignment constraint */
+
+static int pfm_pebs_p4_fmt_validate(u32 flags, u16 npmds, void *data)
+{
+	struct pfm_pebs_p4_smpl_arg *arg = data;
+	size_t min_buf_size;
+
+	/*
+	 * need to define at least the size of the buffer
+	 */
+	if (data == NULL) {
+		PFM_DBG("no argument passed");
+		return -EINVAL;
+	}
+
+	/*
+	 * compute min buf size. npmds is the maximum number
+	 * of implemented PMD registers.
+	 */
+	min_buf_size = sizeof(struct pfm_pebs_p4_smpl_hdr)
+		     + sizeof(struct pfm_pebs_p4_smpl_entry)
+		     + (1UL<<PEBS_PADDING_ORDER); /* padding for alignment */
+
+	PFM_DBG("validate flags=0x%x min_buf_size=%zu buf_size=%zu",
+		  flags,
+		  min_buf_size,
+		  arg->buf_size);
+
+	/*
+	 * must hold at least the buffer header + one minimally sized entry
+	 */
+	if (arg->buf_size < min_buf_size)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int pfm_pebs_p4_fmt_get_size(unsigned int flags, void *data, size_t *size)
+{
+	struct pfm_pebs_p4_smpl_arg *arg = data;
+
+	/*
+	 * size has been validated in pfm_pebs_p4_fmt_validate()
+	 */
+	*size = arg->buf_size + (1UL<<PEBS_PADDING_ORDER);
+
+	return 0;
+}
+
+static int pfm_pebs_p4_fmt_init(struct pfm_context *ctx, void *buf,
+			        u32 flags, u16 npmds, void *data)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_pebs_p4_smpl_hdr *hdr;
+	struct pfm_pebs_p4_smpl_arg *arg = data;
+	unsigned long pebs_start, pebs_end;
+	struct pfm_ds_area_p4 *ds;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	hdr = buf;
+	ds = &hdr->ds;
+
+	/*
+	 * align PEBS buffer base
+	 */
+	pebs_start = ALIGN_PEBS((unsigned long)(hdr+1), PEBS_PADDING_ORDER);
+	pebs_end = pebs_start + arg->buf_size + 1;
+
+	hdr->version = PFM_PEBS_P4_SMPL_VERSION;
+	hdr->buf_size = arg->buf_size;
+	hdr->overflows = 0;
+
+	/*
+	 * express PEBS buffer base as offset from the end of the header
+	 */
+	hdr->start_offs = pebs_start - (unsigned long)(hdr+1);
+
+	/*
+	 * PEBS buffer boundaries
+	 */
+	ds->pebs_buf_base = pebs_start;
+	ds->pebs_abs_max = pebs_end;
+
+	/*
+	 * PEBS starting position
+	 */
+	ds->pebs_index = pebs_start;
+
+	/*
+	 * PEBS interrupt threshold
+	 */
+	ds->pebs_intr_thres = pebs_start
+			    + arg->intr_thres
+			    * sizeof(struct pfm_pebs_p4_smpl_entry);
+
+	/*
+	 * save counter reset value for PEBS counter
+	 */
+	ds->pebs_cnt_reset = arg->cnt_reset;
+
+	/*
+	 * keep track of DS AREA
+	 */
+	ctx_arch->ds_area = ds;
+	ctx_arch->flags.use_pebs = 1;
+	ctx_arch->flags.use_ds = 1;
+
+	PFM_DBG("buffer=%p buf_size=%llu offs=%llu pebs_start=0x%lx "
+		  "pebs_end=0x%lx ds=%p pebs_thres=0x%lx cnt_reset=0x%llx",
+		  buf,
+		  (unsigned long long)hdr->buf_size,
+		  (unsigned long long)hdr->start_offs,
+		  pebs_start,
+		  pebs_end,
+		  ds,
+		  ds->pebs_intr_thres,
+		  (unsigned long long)ds->pebs_cnt_reset);
+
+	return 0;
+}
+
+static int pfm_pebs_p4_fmt_handler(void *buf, struct pfm_ovfl_arg *arg,
+			       unsigned long ip, u64 tstamp, void *data)
+{
+	struct pfm_pebs_p4_smpl_hdr *hdr;
+
+	hdr = buf;
+
+	PFM_DBG_ovfl("buffer full");
+	/*
+	 * increment number of buffer overflows.
+	 * important to detect duplicate set of samples.
+	 */
+	hdr->overflows++;
+
+	/*
+	 * request notification and masking of monitoring.
+	 * Notification is still subject to the overflowed
+	 * register having the FL_NOTIFY flag set.
+	 */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY| PFM_OVFL_CTRL_MASK;
+
+	return -ENOBUFS; /* we are full, sorry */
+}
+
+static int pfm_pebs_p4_fmt_restart(int is_active, u32 *ovfl_ctrl,
+				void *buf)
+{
+	struct pfm_pebs_p4_smpl_hdr *hdr = buf;
+
+	/*
+	 * reset index to base of buffer
+	 */
+	hdr->ds.pebs_index = hdr->ds.pebs_buf_base;
+
+	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+
+	return 0;
+}
+
+static int pfm_pebs_p4_fmt_exit(void *buf)
+{
+	return 0;
+}
+
+static struct pfm_smpl_fmt pebs_p4_fmt={
+	.fmt_name = PFM_PEBS_P4_SMPL_NAME,
+	.fmt_version = 0x1,
+	.fmt_arg_size = sizeof(struct pfm_pebs_p4_smpl_arg),
+	.fmt_validate = pfm_pebs_p4_fmt_validate,
+	.fmt_getsize = pfm_pebs_p4_fmt_get_size,
+	.fmt_init = pfm_pebs_p4_fmt_init,
+	.fmt_handler = pfm_pebs_p4_fmt_handler,
+	.fmt_restart = pfm_pebs_p4_fmt_restart,
+	.fmt_exit = pfm_pebs_p4_fmt_exit,
+	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+};
+
+static int __init pfm_pebs_p4_fmt_init_module(void)
+{
+	int ht_enabled;
+
+	if (!cpu_has_pebs) {
+		PFM_INFO("processor does not have PEBS support");
+		return -1;
+	}
+	if (current_cpu_data.x86 != 15) {
+		PFM_INFO("not an Intel Pentium 4");
+		return -1;
+	}
+#ifdef CONFIG_SMP
+	ht_enabled = (cpus_weight(__get_cpu_var(cpu_core_map))
+		   / current_cpu_data.x86_max_cores) > 1;
+#else
+	ht_enabled = 0;
+#endif
+	if (ht_enabled) {
+		PFM_INFO("PEBS not available because HyperThreading is on");
+		return -1;
+	}
+	return pfm_fmt_register(&pebs_p4_fmt);
+}
+
+static void __exit pfm_pebs_p4_fmt_cleanup_module(void)
+{
+	pfm_fmt_unregister(&pebs_p4_fmt);
+}
+
+module_init(pfm_pebs_p4_fmt_init_module);
+module_exit(pfm_pebs_p4_fmt_cleanup_module);
diff -Naur linux-2.6.25-org/drivers/block/ps3disk.c linux-2.6.25-id/drivers/block/ps3disk.c
--- linux-2.6.25-org/drivers/block/ps3disk.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/drivers/block/ps3disk.c	2008-04-23 11:21:43.000000000 +0200
@@ -102,8 +102,7 @@
 		dev_dbg(&dev->sbd.core,
 			"%s:%u: bio %u: %u segs %u sectors from %lu\n",
 			__func__, __LINE__, i, bio_segments(iter.bio),
-			bio_sectors(iter.bio),
-			(unsigned long)iter.bio->bi_sector);
+			bio_sectors(iter.bio), iter.bio->bi_sector);
 
 		size = bvec->bv_len;
 		buf = bvec_kmap_irq(bvec, &flags);
diff -Naur linux-2.6.25-org/drivers/net/Kconfig linux-2.6.25-id/drivers/net/Kconfig
--- linux-2.6.25-org/drivers/net/Kconfig	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/drivers/net/Kconfig	2008-04-23 11:21:43.000000000 +0200
@@ -2376,6 +2376,16 @@
 	  the driver automatically distinguishes the models, you can
 	  safely enable this option even if you have a wireless-less model.
 
+config GELIC_WIRELESS
+       bool "PS3 Wireless support"
+       depends on GELIC_NET
+       help
+        This option adds the support for the wireless feature of PS3.
+        If you have the wireless-less model of PS3 or have no plan to
+        use wireless feature, disabling this option saves memory.  As
+        the driver automatically distinguishes the models, you can
+        safely enable this option even if you have a wireless-less model.
+
 config GIANFAR
 	tristate "Gianfar Ethernet"
 	depends on FSL_SOC
diff -Naur linux-2.6.25-org/drivers/oprofile/oprofile_files.c linux-2.6.25-id/drivers/oprofile/oprofile_files.c
--- linux-2.6.25-org/drivers/oprofile/oprofile_files.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/drivers/oprofile/oprofile_files.c	2008-04-23 11:21:49.000000000 +0200
@@ -117,7 +117,7 @@
 static const struct file_operations dump_fops = {
 	.write		= dump_write,
 };
- 
+
 void oprofile_create_files(struct super_block * sb, struct dentry * root)
 {
 	oprofilefs_create_file(sb, root, "enable", &enable_fops);
diff -Naur linux-2.6.25-org/drivers/ps3/ps3-sys-manager.c linux-2.6.25-id/drivers/ps3/ps3-sys-manager.c
--- linux-2.6.25-org/drivers/ps3/ps3-sys-manager.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/drivers/ps3/ps3-sys-manager.c	2008-04-23 11:21:52.000000000 +0200
@@ -24,6 +24,7 @@
 #include <linux/reboot.h>
 
 #include <asm/firmware.h>
+#include <asm/lv1call.h>
 #include <asm/ps3.h>
 
 #include "vuart.h"
@@ -581,6 +582,23 @@
 	return -EIO;
 }
 
+static void ps3_sys_manager_fin(struct ps3_system_bus_device *dev)
+{
+	ps3_sys_manager_send_request_shutdown(dev);
+
+	pr_emerg("System Halted, OK to turn off power\n");
+
+	while (ps3_sys_manager_handle_msg(dev)) {
+		/* pause until next DEC interrupt */
+		lv1_pause(0);
+	}
+
+	while (1) {
+		/* pause, ignoring DEC interrupt */
+		lv1_pause(1);
+	}
+}
+
 /**
  * ps3_sys_manager_final_power_off - The final platform machine_power_off routine.
  *
diff -Naur linux-2.6.25-org/drivers/ps3/sys-manager-core.c linux-2.6.25-id/drivers/ps3/sys-manager-core.c
--- linux-2.6.25-org/drivers/ps3/sys-manager-core.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/drivers/ps3/sys-manager-core.c	2008-04-23 11:21:54.000000000 +0200
@@ -19,6 +19,7 @@
  */
 
 #include <linux/kernel.h>
+#include <asm/lv1call.h>
 #include <asm/ps3.h>
 
 /**
@@ -50,10 +51,7 @@
 	if (ps3_sys_manager_ops.power_off)
 		ps3_sys_manager_ops.power_off(ps3_sys_manager_ops.dev);
 
-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
-	local_irq_disable();
-	while (1)
-		(void)0;
+	ps3_sys_manager_halt();
 }
 
 void ps3_sys_manager_restart(void)
@@ -61,8 +59,14 @@
 	if (ps3_sys_manager_ops.restart)
 		ps3_sys_manager_ops.restart(ps3_sys_manager_ops.dev);
 
-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
+	ps3_sys_manager_halt();
+}
+
+void ps3_sys_manager_halt(void)
+{
+	pr_emerg("System Halted, OK to turn off power\n");
 	local_irq_disable();
 	while (1)
-		(void)0;
+		lv1_pause(1);
 }
+
diff -Naur linux-2.6.25-org/include/asm-ia64/hw_irq.h linux-2.6.25-id/include/asm-ia64/hw_irq.h
--- linux-2.6.25-org/include/asm-ia64/hw_irq.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-ia64/hw_irq.h	2008-04-23 11:21:56.000000000 +0200
@@ -63,9 +63,9 @@
 #define IA64_NUM_DEVICE_VECTORS		(IA64_LAST_DEVICE_VECTOR - IA64_FIRST_DEVICE_VECTOR + 1)
 
 #define IA64_MCA_RENDEZ_VECTOR		0xe8	/* MCA rendez interrupt */
-#define IA64_PERFMON_VECTOR		0xee	/* performance monitor interrupt vector */
 #define IA64_TIMER_VECTOR		0xef	/* use highest-prio group 15 interrupt for timer */
 #define	IA64_MCA_WAKEUP_VECTOR		0xf0	/* MCA wakeup (must be >MCA_RENDEZ_VECTOR) */
+#define IA64_PERFMON_VECTOR		0xf1	/* performance monitor interrupt vector */
 #define IA64_IPI_LOCAL_TLB_FLUSH	0xfc	/* SMP flush local TLB */
 #define IA64_IPI_RESCHEDULE		0xfd	/* SMP reschedule */
 #define IA64_IPI_VECTOR			0xfe	/* inter-processor interrupt vector */
diff -Naur linux-2.6.25-org/include/asm-ia64/perfmon.h linux-2.6.25-id/include/asm-ia64/perfmon.h
--- linux-2.6.25-org/include/asm-ia64/perfmon.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-ia64/perfmon.h	2008-04-23 11:21:56.000000000 +0200
@@ -1,279 +1,321 @@
 /*
- * Copyright (C) 2001-2003 Hewlett-Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- */
+ * Copyright (c) 2001-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file contains Itanium Processor Family specific definitions
+ * for the perfmon interface.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#ifndef _ASM_IA64_PERFMON_H_
+#define _ASM_IA64_PERFMON_H_
 
-#ifndef _ASM_IA64_PERFMON_H
-#define _ASM_IA64_PERFMON_H
+#ifdef __KERNEL__
 
 /*
- * perfmon comamnds supported on all CPU models
+ * compatibility for version v2.0 of the interface
  */
-#define PFM_WRITE_PMCS		0x01
-#define PFM_WRITE_PMDS		0x02
-#define PFM_READ_PMDS		0x03
-#define PFM_STOP		0x04
-#define PFM_START		0x05
-#define PFM_ENABLE		0x06 /* obsolete */
-#define PFM_DISABLE		0x07 /* obsolete */
-#define PFM_CREATE_CONTEXT	0x08
-#define PFM_DESTROY_CONTEXT	0x09 /* obsolete use close() */
-#define PFM_RESTART		0x0a
-#define PFM_PROTECT_CONTEXT	0x0b /* obsolete */
-#define PFM_GET_FEATURES	0x0c
-#define PFM_DEBUG		0x0d
-#define PFM_UNPROTECT_CONTEXT	0x0e /* obsolete */
-#define PFM_GET_PMC_RESET_VAL	0x0f
-#define PFM_LOAD_CONTEXT	0x10
-#define PFM_UNLOAD_CONTEXT	0x11
+#include <asm/perfmon_compat.h>
+#include <asm/delay.h>
 
 /*
- * PMU model specific commands (may not be supported on all PMU models)
+ * describe the content of the pfm_syst_info field
+ * layout:
+ * bits[00-15] : generic flags
+ * bits[16-31] : arch-specific flags
  */
-#define PFM_WRITE_IBRS		0x20
-#define PFM_WRITE_DBRS		0x21
+#define PFM_ITA_CPUINFO_IDLE_EXCL 0x10000 /* stop monitoring in idle loop */
 
 /*
- * context flags
+ * For some CPUs, the upper bits of a counter must be set in order for the
+ * overflow interrupt to happen. On overflow, the counter has wrapped around,
+ * and the upper bits are cleared. This function may be used to set them back.
  */
-#define PFM_FL_NOTIFY_BLOCK    	 0x01	/* block task on user level notifications */
-#define PFM_FL_SYSTEM_WIDE	 0x02	/* create a system wide context */
-#define PFM_FL_OVFL_NO_MSG	 0x80   /* do not post overflow/end messages for notification */
+static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx, unsigned int cnum)
+{}
 
-/*
- * event set flags
- */
-#define PFM_SETFL_EXCL_IDLE      0x01   /* exclude idle task (syswide only) XXX: DO NOT USE YET */
+static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
+{
+	return 0;
+}
 
-/*
- * PMC flags
- */
-#define PFM_REGFL_OVFL_NOTIFY	0x1	/* send notification on overflow */
-#define PFM_REGFL_RANDOM	0x2	/* randomize sampling interval   */
+static inline void pfm_arch_pmu_config_remove(void)
+{}
 
 /*
- * PMD/PMC/IBR/DBR return flags (ignored on input)
+ * called from __pfm_interrupt_handler(). ctx is not NULL.
+ * ctx is locked. PMU interrupt is masked.
  *
- * Those flags are used on output and must be checked in case EAGAIN is returned
- * by any of the calls using a pfarg_reg_t or pfarg_dbreg_t structure.
- */
-#define PFM_REG_RETFL_NOTAVAIL	(1UL<<31) /* set if register is implemented but not available */
-#define PFM_REG_RETFL_EINVAL	(1UL<<30) /* set if register entry is invalid */
-#define PFM_REG_RETFL_MASK	(PFM_REG_RETFL_NOTAVAIL|PFM_REG_RETFL_EINVAL)
-
-#define PFM_REG_HAS_ERROR(flag)	(((flag) & PFM_REG_RETFL_MASK) != 0)
-
-typedef unsigned char pfm_uuid_t[16];	/* custom sampling buffer identifier type */
-
-/*
- * Request structure used to define a context
- */
-typedef struct {
-	pfm_uuid_t     ctx_smpl_buf_id;	 /* which buffer format to use (if needed) */
-	unsigned long  ctx_flags;	 /* noblock/block */
-	unsigned short ctx_nextra_sets;	 /* number of extra event sets (you always get 1) */
-	unsigned short ctx_reserved1;	 /* for future use */
-	int	       ctx_fd;		 /* return arg: unique identification for context */
-	void	       *ctx_smpl_vaddr;	 /* return arg: virtual address of sampling buffer, is used */
-	unsigned long  ctx_reserved2[11];/* for future use */
-} pfarg_context_t;
+ * must stop all monitoring to ensure handler has consistent view.
+ * must collect overflowed PMDs bitmask  into povfls_pmds and
+ * npend_ovfls. If no interrupt detected then npend_ovfls
+ * must be set to zero.
+ */
+static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{
+	u64 tmp;
+
+	/*
+	 * do not overwrite existing value, must
+	 * process those first (coming from context switch replay)
+	 */
+	if (set->npend_ovfls)
+		return;
+
+	ia64_srlz_d();
+
+	tmp =  ia64_get_pmc(0) & ~0xf;
+
+	set->povfl_pmds[0] = tmp;
+
+	set->npend_ovfls = ia64_popcnt(tmp);
+}
+
+static inline int pfm_arch_init_pmu_config(void)
+{
+	return 0;
+}
+
+static inline void pfm_arch_resend_irq(void)
+{
+	ia64_resend_irq(IA64_PERFMON_VECTOR);
+}
+
+static inline void pfm_arch_serialize(void)
+{
+	ia64_srlz_d();
+}
+
+static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
+{
+	PFM_DBG_ovfl("state=%d", ctx->state);
+	ia64_set_pmc(0, 0);
+	/* no serialization */
+}
+
+static inline void pfm_arch_write_pmc(struct pfm_context *ctx, unsigned int cnum, u64 value)
+{
+	if (cnum < 256) {
+		ia64_set_pmc(pfm_pmu_conf->pmc_desc[cnum].hw_addr, value);
+	} else if (cnum < 264) {
+		ia64_set_ibr(cnum-256, value);
+		ia64_dv_serialize_instruction();
+	} else {
+		ia64_set_dbr(cnum-264, value);
+		ia64_dv_serialize_instruction();
+	}
+}
+
+/*
+ * On IA-64, for per-thread context which have the ITA_FL_INSECURE
+ * flag, it is possible to start/stop monitoring directly from user evel
+ * without calling pfm_start()/pfm_stop. This allows very lightweight
+ * control yet the kernel sometimes needs to know if monitoring is actually
+ * on or off.
+ *
+ * Tracking of this information is normally done by pfm_start/pfm_stop
+ * in flags.started. Here we need to compensate by checking actual
+ * psr bit.
+ */
+static inline int pfm_arch_is_active(struct pfm_context *ctx)
+{
+	return ctx->flags.started || ia64_getreg(_IA64_REG_PSR) & (IA64_PSR_UP|IA64_PSR_PP);
+}
+
+static inline void pfm_arch_write_pmd(struct pfm_context *ctx, unsigned int cnum, u64 value)
+{
+	/*
+	 * for a counting PMD, overflow bit must be cleared
+	 */
+	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_C64)
+		value &= pfm_pmu_conf->ovfl_mask;
+
+	/*
+	 * for counters, write to upper bits are ignored, no need to mask
+	 */
+	ia64_set_pmd(pfm_pmu_conf->pmd_desc[cnum].hw_addr, value);
+}
+
+static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
+{
+	return ia64_get_pmd(pfm_pmu_conf->pmd_desc[cnum].hw_addr);
+}
+
+static inline u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
+{
+	return ia64_get_pmc(pfm_pmu_conf->pmc_desc[cnum].hw_addr);
+}
+
+static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
+					 struct pfm_context *ctx,
+					 struct pfm_event_set *set)
+{
+	struct pt_regs *regs;
+
+	regs = task_pt_regs(task);
+	ia64_psr(regs)->pp = 0;
+}
+
+static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
+					struct pfm_context *ctx,
+					struct pfm_event_set *set)
+{
+	struct pt_regs *regs;
+
+	if (!(set->flags & PFM_ITA_SETFL_INTR_ONLY)) {
+		regs = task_pt_regs(task);
+		ia64_psr(regs)->pp = 1;
+	}
+}
+
+/*
+ * On IA-64, the PMDs are NOT saved by pfm_arch_freeze_pmu()
+ * when entering the PMU interrupt handler, thus, we need
+ * to save them in pfm_switch_sets_from_intr()
+ */
+static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
+					   struct pfm_event_set *set)
+{
+	pfm_save_pmds(ctx, set);
+}
+
+int pfm_arch_context_create(struct pfm_context *ctx, u32 ctx_flags);
+
+static inline void pfm_arch_context_free(struct pfm_context *ctx)
+{}
+
+int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
+			     struct pfm_event_set *set);
+void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx,
+			     struct pfm_event_set *set);
+
+int pfm_arch_unload_context(struct pfm_context *ctx, struct task_struct *task);
+int pfm_arch_load_context(struct pfm_context *ctx, struct pfm_event_set *set,
+			  struct task_struct *task);
+int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags);
+
+void pfm_arch_mask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_unmask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set);
+
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
+
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
+		   struct pfm_event_set *set);
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
+		    struct pfm_event_set *set);
+
+int  pfm_arch_init(void);
+void pfm_arch_init_percpu(void);
+char *pfm_arch_get_pmu_module_name(void);
+
+int __pfm_use_dbregs(struct task_struct *task);
+int  __pfm_release_dbregs(struct task_struct *task);
+int pfm_ia64_mark_dbregs_used(struct pfm_context *ctx,
+			      struct pfm_event_set *set);
+
+void pfm_arch_show_session(struct seq_file *m);
+
+static inline int pfm_arch_pmu_acquire(void)
+{
+	return 0;
+}
+
+static inline void pfm_arch_pmu_release(void)
+{}
+
+/* not necessary on IA-64 */
+static inline void pfm_cacheflush(void *addr, unsigned int len)
+{}
 
 /*
- * Request structure used to write/read a PMC or PMD
+ * miscellaneous architected definitions
  */
-typedef struct {
-	unsigned int	reg_num;	   /* which register */
-	unsigned short	reg_set;	   /* event set for this register */
-	unsigned short	reg_reserved1;	   /* for future use */
-
-	unsigned long	reg_value;	   /* initial pmc/pmd value */
-	unsigned long	reg_flags;	   /* input: pmc/pmd flags, return: reg error */
-
-	unsigned long	reg_long_reset;	   /* reset after buffer overflow notification */
-	unsigned long	reg_short_reset;   /* reset after counter overflow */
-
-	unsigned long	reg_reset_pmds[4]; /* which other counters to reset on overflow */
-	unsigned long	reg_random_seed;   /* seed value when randomization is used */
-	unsigned long	reg_random_mask;   /* bitmask used to limit random value */
-	unsigned long   reg_last_reset_val;/* return: PMD last reset value */
-
-	unsigned long	reg_smpl_pmds[4];  /* which pmds are accessed when PMC overflows */
-	unsigned long	reg_smpl_eventid;  /* opaque sampling event identifier */
-
-	unsigned long   reg_reserved2[3];   /* for future use */
-} pfarg_reg_t;
-
-typedef struct {
-	unsigned int	dbreg_num;		/* which debug register */
-	unsigned short	dbreg_set;		/* event set for this register */
-	unsigned short	dbreg_reserved1;	/* for future use */
-	unsigned long	dbreg_value;		/* value for debug register */
-	unsigned long	dbreg_flags;		/* return: dbreg error */
-	unsigned long	dbreg_reserved2[1];	/* for future use */
-} pfarg_dbreg_t;
-
-typedef struct {
-	unsigned int	ft_version;	/* perfmon: major [16-31], minor [0-15] */
-	unsigned int	ft_reserved;	/* reserved for future use */
-	unsigned long	reserved[4];	/* for future use */
-} pfarg_features_t;
-
-typedef struct {
-	pid_t		load_pid;	   /* process to load the context into */
-	unsigned short	load_set;	   /* first event set to load */
-	unsigned short	load_reserved1;	   /* for future use */
-	unsigned long	load_reserved2[3]; /* for future use */
-} pfarg_load_t;
-
-typedef struct {
-	int		msg_type;		/* generic message header */
-	int		msg_ctx_fd;		/* generic message header */
-	unsigned long	msg_ovfl_pmds[4];	/* which PMDs overflowed */
-	unsigned short  msg_active_set;		/* active set at the time of overflow */
-	unsigned short  msg_reserved1;		/* for future use */
-	unsigned int    msg_reserved2;		/* for future use */
-	unsigned long	msg_tstamp;		/* for perf tuning/debug */
-} pfm_ovfl_msg_t;
-
-typedef struct {
-	int		msg_type;		/* generic message header */
-	int		msg_ctx_fd;		/* generic message header */
-	unsigned long	msg_tstamp;		/* for perf tuning */
-} pfm_end_msg_t;
-
-typedef struct {
-	int		msg_type;		/* type of the message */
-	int		msg_ctx_fd;		/* unique identifier for the context */
-	unsigned long	msg_tstamp;		/* for perf tuning */
-} pfm_gen_msg_t;
-
-#define PFM_MSG_OVFL	1	/* an overflow happened */
-#define PFM_MSG_END	2	/* task to which context was attached ended */
-
-typedef union {
-	pfm_ovfl_msg_t	pfm_ovfl_msg;
-	pfm_end_msg_t	pfm_end_msg;
-	pfm_gen_msg_t	pfm_gen_msg;
-} pfm_msg_t;
+#define PFM_ITA_FCNTR	4 /* first counting monitor (PMC/PMD) */
 
 /*
- * Define the version numbers for both perfmon as a whole and the sampling buffer format.
+ * private event set flags  (set_priv_flags)
  */
-#define PFM_VERSION_MAJ		 2U
-#define PFM_VERSION_MIN		 0U
-#define PFM_VERSION		 (((PFM_VERSION_MAJ&0xffff)<<16)|(PFM_VERSION_MIN & 0xffff))
-#define PFM_VERSION_MAJOR(x)	 (((x)>>16) & 0xffff)
-#define PFM_VERSION_MINOR(x)	 ((x) & 0xffff)
+#define PFM_ITA_SETFL_USE_DBR	0x1000000 /* set uses debug registers */
 
 
 /*
- * miscellaneous architected definitions
+ * Itanium-specific data structures
  */
-#define PMU_FIRST_COUNTER	4	/* first counting monitor (PMC/PMD) */
-#define PMU_MAX_PMCS		256	/* maximum architected number of PMC registers */
-#define PMU_MAX_PMDS		256	/* maximum architected number of PMD registers */
-
-#ifdef __KERNEL__
-
-extern long perfmonctl(int fd, int cmd, void *arg, int narg);
+struct pfm_ia64_context_flags {
+	unsigned int use_dbr:1;	 /* use range restrictions (debug registers) */
+	unsigned int insecure:1; /* insecure monitoring for non-self session */
+	unsigned int reserved:30;/* for future use */
+};
 
-typedef struct {
-	void (*handler)(int irq, void *arg, struct pt_regs *regs);
-} pfm_intr_handler_desc_t;
+struct pfm_arch_context {
+	struct pfm_ia64_context_flags flags;	/* arch specific ctx flags */
+	u64			 ctx_saved_psr_up;/* storage for ctxsw psr_up */
+#ifdef CONFIG_IA64_PERFMON_COMPAT
+	void			*ctx_smpl_vaddr; /* vaddr of user mapping */
+#endif
+};
 
-extern void pfm_save_regs (struct task_struct *);
-extern void pfm_load_regs (struct task_struct *);
+#ifdef CONFIG_IA64_PERFMON_COMPAT
+ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size);
+int pfm_ia64_compat_init(void);
+int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
+			         size_t rsize, struct file *filp);
+#else
+static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size)
+{
+	return -EINVAL;
+}
 
-extern void pfm_exit_thread(struct task_struct *);
-extern int  pfm_use_debug_registers(struct task_struct *);
-extern int  pfm_release_debug_registers(struct task_struct *);
-extern void pfm_syst_wide_update_task(struct task_struct *, unsigned long info, int is_ctxswin);
-extern void pfm_inherit(struct task_struct *task, struct pt_regs *regs);
-extern void pfm_init_percpu(void);
-extern void pfm_handle_work(void);
-extern int  pfm_install_alt_pmu_interrupt(pfm_intr_handler_desc_t *h);
-extern int  pfm_remove_alt_pmu_interrupt(pfm_intr_handler_desc_t *h);
+static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
+					       size_t rsize, struct file *filp)
+{
+	return -EINVAL;
+}
+#endif
 
+extern struct pfm_ia64_pmu_info *pfm_ia64_pmu_info;
 
+#define PFM_ARCH_CTX_SIZE	(sizeof(struct pfm_arch_context))
 
 /*
- * Reset PMD register flags
+ * IA-64 does not need extra alignment requirements for the sampling buffer
  */
-#define PFM_PMD_SHORT_RESET	0
-#define PFM_PMD_LONG_RESET	1
+#define PFM_ARCH_SMPL_ALIGN_SIZE	0
 
-typedef union {
-	unsigned int val;
-	struct {
-		unsigned int notify_user:1;	/* notify user program of overflow */
-		unsigned int reset_ovfl_pmds:1;	/* reset overflowed PMDs */
-		unsigned int block_task:1;	/* block monitored task on kernel exit */
-		unsigned int mask_monitoring:1; /* mask monitors via PMCx.plm */
-		unsigned int reserved:28;	/* for future use */
-	} bits;
-} pfm_ovfl_ctrl_t;
-
-typedef struct {
-	unsigned char	ovfl_pmd;			/* index of overflowed PMD  */
-	unsigned char   ovfl_notify;			/* =1 if monitor requested overflow notification */
-	unsigned short  active_set;			/* event set active at the time of the overflow */
-	pfm_ovfl_ctrl_t ovfl_ctrl;			/* return: perfmon controls to set by handler */
-
-	unsigned long   pmd_last_reset;			/* last reset value of of the PMD */
-	unsigned long	smpl_pmds[4];			/* bitmask of other PMD of interest on overflow */
-	unsigned long   smpl_pmds_values[PMU_MAX_PMDS]; /* values for the other PMDs of interest */
-	unsigned long   pmd_value;			/* current 64-bit value of the PMD */
-	unsigned long	pmd_eventid;			/* eventid associated with PMD */
-} pfm_ovfl_arg_t;
-
-
-typedef struct {
-	char		*fmt_name;
-	pfm_uuid_t	fmt_uuid;
-	size_t		fmt_arg_size;
-	unsigned long	fmt_flags;
-
-	int		(*fmt_validate)(struct task_struct *task, unsigned int flags, int cpu, void *arg);
-	int		(*fmt_getsize)(struct task_struct *task, unsigned int flags, int cpu, void *arg, unsigned long *size);
-	int 		(*fmt_init)(struct task_struct *task, void *buf, unsigned int flags, int cpu, void *arg);
-	int		(*fmt_handler)(struct task_struct *task, void *buf, pfm_ovfl_arg_t *arg, struct pt_regs *regs, unsigned long stamp);
-	int		(*fmt_restart)(struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs);
-	int		(*fmt_restart_active)(struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs);
-	int		(*fmt_exit)(struct task_struct *task, void *buf, struct pt_regs *regs);
 
-	struct list_head fmt_list;
-} pfm_buffer_fmt_t;
+static inline void pfm_release_dbregs(struct task_struct *task)
+{
+	if (task->thread.flags & IA64_THREAD_DBG_VALID)
+		__pfm_release_dbregs(task);
+}
 
-extern int pfm_register_buffer_fmt(pfm_buffer_fmt_t *fmt);
-extern int pfm_unregister_buffer_fmt(pfm_uuid_t uuid);
+#define pfm_use_dbregs(_t)     __pfm_use_dbregs(_t)
 
-/*
- * perfmon interface exported to modules
- */
-extern int pfm_mod_read_pmds(struct task_struct *, void *req, unsigned int nreq, struct pt_regs *regs);
-extern int pfm_mod_write_pmcs(struct task_struct *, void *req, unsigned int nreq, struct pt_regs *regs);
-extern int pfm_mod_write_ibrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs);
-extern int pfm_mod_write_dbrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs);
-
-/*
- * describe the content of the local_cpu_date->pfm_syst_info field
- */
-#define PFM_CPUINFO_SYST_WIDE	0x1	/* if set a system wide session exists */
-#define PFM_CPUINFO_DCR_PP	0x2	/* if set the system wide session has started */
-#define PFM_CPUINFO_EXCL_IDLE	0x4	/* the system wide session excludes the idle task */
-
-/*
- * sysctl control structure. visible to sampling formats
- */
-typedef struct {
-	int	debug;		/* turn on/off debugging via syslog */
-	int	debug_ovfl;	/* turn on/off debug printk in overflow handler */
-	int	fastctxsw;	/* turn on/off fast (unsecure) ctxsw */
-	int	expert_mode;	/* turn on/off value checking */
-} pfm_sysctl_t;
-extern pfm_sysctl_t pfm_sysctl;
 
+struct pfm_arch_pmu_info {
+	unsigned long mask_pmcs[PFM_PMC_BV]; /* PMC to modify when masking monitoring */
+};
 
+DECLARE_PER_CPU(u32, pfm_syst_info);
 #endif /* __KERNEL__ */
-
-#endif /* _ASM_IA64_PERFMON_H */
+#endif /* _ASM_IA64_PERFMON_H_ */
diff -Naur linux-2.6.25-org/include/asm-ia64/perfmon_compat.h linux-2.6.25-id/include/asm-ia64/perfmon_compat.h
--- linux-2.6.25-org/include/asm-ia64/perfmon_compat.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-ia64/perfmon_compat.h	2008-04-23 11:21:56.000000000 +0200
@@ -0,0 +1,168 @@
+/*
+ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This header file contains perfmon interface definition
+ * that are now obsolete and should be dropped in favor
+ * of their equivalent functions as explained below.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+
+#ifndef _ASM_IA64_PERFMON_COMPAT_H_
+#define _ASM_IA64_PERFMON_COMPAT_H_
+
+/*
+ * custom sampling buffer identifier type
+ */
+typedef __u8 pfm_uuid_t[16];
+
+/*
+ * obsolete perfmon commands. Supported only on IA-64 for
+ * backward compatiblity reasons with perfmon v2.0.
+ */
+#define PFM_WRITE_PMCS		0x01 /* use pfm_write_pmcs */
+#define PFM_WRITE_PMDS		0x02 /* use pfm_write_pmds */
+#define PFM_READ_PMDS		0x03 /* use pfm_read_pmds */
+#define PFM_STOP		0x04 /* use pfm_stop */
+#define PFM_START		0x05 /* use pfm_start */
+#define PFM_ENABLE		0x06 /* obsolete */
+#define PFM_DISABLE		0x07 /* obsolete */
+#define PFM_CREATE_CONTEXT	0x08 /* use pfm_create_context */
+#define PFM_DESTROY_CONTEXT	0x09 /* use close() */
+#define PFM_RESTART		0x0a /* use pfm_restart */
+#define PFM_PROTECT_CONTEXT	0x0b /* obsolete */
+#define PFM_GET_FEATURES	0x0c /* use /proc/sys/perfmon */
+#define PFM_DEBUG		0x0d /* /proc/sys/kernel/perfmon/debug */
+#define PFM_UNPROTECT_CONTEXT	0x0e /* obsolete */
+#define PFM_GET_PMC_RESET_VAL	0x0f /* use /proc/perfmon_map */
+#define PFM_LOAD_CONTEXT	0x10 /* use pfm_load_context */
+#define PFM_UNLOAD_CONTEXT	0x11 /* use pfm_unload_context */
+
+/*
+ * PMU model specific commands (may not be supported on all PMU models)
+ */
+#define PFM_WRITE_IBRS		0x20 /* obsolete: use PFM_WRITE_PMCS[256-263] */
+#define PFM_WRITE_DBRS		0x21 /* obsolete: use PFM_WRITE_PMCS[264-271] */
+
+/*
+ * argument to PFM_CREATE_CONTEXT
+ */
+struct pfarg_context {
+	pfm_uuid_t     ctx_smpl_buf_id;	 /* buffer format to use */
+	unsigned long  ctx_flags;	 /* noblock/block */
+	unsigned int   ctx_reserved1;	 /* for future use */
+	int	       ctx_fd;		 /* return: fildesc */
+	void	       *ctx_smpl_vaddr;	 /* return: vaddr of buffer */
+	unsigned long  ctx_reserved3[11];/* for future use */
+};
+
+/*
+ * argument structure for PFM_WRITE_PMCS/PFM_WRITE_PMDS/PFM_WRITE_PMDS
+ */
+struct pfarg_reg {
+	unsigned int	reg_num;	   /* which register */
+	unsigned short	reg_set;	   /* event set for this register */
+	unsigned short	reg_reserved1;	   /* for future use */
+
+	unsigned long	reg_value;	   /* initial pmc/pmd value */
+	unsigned long	reg_flags;	   /* input: pmc/pmd flags, return: reg error */
+
+	unsigned long	reg_long_reset;	   /* reset after buffer overflow notification */
+	unsigned long	reg_short_reset;   /* reset after counter overflow */
+
+	unsigned long	reg_reset_pmds[4]; /* which other counters to reset on overflow */
+	unsigned long	reg_random_seed;   /* seed for randomization */
+	unsigned long	reg_random_mask;   /* random range limit */
+	unsigned long   reg_last_reset_val;/* return: PMD last reset value */
+
+	unsigned long	reg_smpl_pmds[4];  /* pmds to be saved on overflow */
+	unsigned long	reg_smpl_eventid;  /* opaque sampling event id */
+	unsigned long   reg_ovfl_switch_cnt;/* #overflows to switch */
+
+	unsigned long   reg_reserved2[2];   /* for future use */
+};
+
+/*
+ * argument to PFM_WRITE_IBRS/PFM_WRITE_DBRS
+ */
+struct pfarg_dbreg {
+	unsigned int	dbreg_num;		/* which debug register */
+	unsigned short	dbreg_set;		/* event set */
+	unsigned short	dbreg_reserved1;	/* for future use */
+	unsigned long	dbreg_value;		/* value for debug register */
+	unsigned long	dbreg_flags;		/* return: dbreg error */
+	unsigned long	dbreg_reserved2[1];	/* for future use */
+};
+
+/*
+ * argument to PFM_GET_FEATURES
+ */
+struct pfarg_features {
+	unsigned int	ft_version;	/* major [16-31], minor [0-15] */
+	unsigned int	ft_reserved;	/* reserved for future use */
+	unsigned long	reserved[4];	/* for future use */
+};
+
+typedef struct {
+	int		msg_type;		/* generic message header */
+	int		msg_ctx_fd;		/* generic message header */
+	unsigned long	msg_ovfl_pmds[4];	/* which PMDs overflowed */
+	unsigned short  msg_active_set;		/* active set at the time of overflow */
+	unsigned short  msg_reserved1;		/* for future use */
+	unsigned int    msg_reserved2;		/* for future use */
+	unsigned long	msg_tstamp;		/* for perf tuning/debug */
+} pfm_ovfl_msg_t;
+
+typedef struct {
+	int		msg_type;		/* generic message header */
+	int		msg_ctx_fd;		/* generic message header */
+	unsigned long	msg_tstamp;		/* for perf tuning */
+} pfm_end_msg_t;
+
+typedef struct {
+	int		msg_type;		/* type of the message */
+	int		msg_ctx_fd;		/* unique identifier for the context */
+	unsigned long	msg_tstamp;		/* for perf tuning */
+} pfm_gen_msg_t;
+
+typedef union {
+	int type;
+	pfm_ovfl_msg_t	pfm_ovfl_msg;
+	pfm_end_msg_t	pfm_end_msg;
+	pfm_gen_msg_t	pfm_gen_msg;
+} pfm_msg_t;
+
+/*
+ * PMD/PMC return flags in case of error (ignored on input)
+ *
+ * reg_flags layout:
+ * bit 00-15 : generic flags
+ * bits[16-23] : arch-specific flags (see asm/perfmon.h)
+ * bit 24-31 : error codes
+ *
+ * Those flags are used on output and must be checked in case EINVAL is
+ * returned by a command accepting a vector of values and each has a flag
+ * field, such as pfarg_reg or pfarg_reg
+ */
+#define PFM_REG_RETFL_NOTAVAIL	(1<<31) /* not implemented or unaccessible */
+#define PFM_REG_RETFL_EINVAL	(1<<30) /* entry is invalid */
+#define PFM_REG_RETFL_MASK	(PFM_REG_RETFL_NOTAVAIL|\
+				 PFM_REG_RETFL_EINVAL)
+
+#define PFM_REG_HAS_ERROR(flag)	(((flag) & PFM_REG_RETFL_MASK) != 0)
+
+
+#endif /* _ASM_IA64_PERFMON_COMPAT_H_ */
diff -Naur linux-2.6.25-org/include/asm-ia64/perfmon_const.h linux-2.6.25-id/include/asm-ia64/perfmon_const.h
--- linux-2.6.25-org/include/asm-ia64/perfmon_const.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-ia64/perfmon_const.h	2008-04-23 11:21:56.000000000 +0200
@@ -0,0 +1,52 @@
+/*
+ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file contains ia64 specific definitions for the perfmon
+ * interface.
+ *
+ * This file MUST never be included directly. Use linux/perfmon.h.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#ifndef _ASM_IA64_PERFMON_CONST_H_
+#define _ASM_IA64_PERFMON_CONST_H_
+
+#define PFM_ARCH_MAX_PMCS	(256+64)
+#define PFM_ARCH_MAX_PMDS	(256+64)
+
+#define PFM_ARCH_PMD_STK_ARG	8
+#define PFM_ARCH_PMC_STK_ARG	8
+
+/*
+ * Itanium specific context flags
+ *
+ * bits[00-15]: generic flags (see asm/perfmon.h)
+ * bits[16-31]: arch-specific flags
+ */
+#define PFM_ITA_FL_INSECURE 0x10000 /* clear psr.sp on non system, non self-monitoring */
+
+/*
+ * Itanium specific public event set flags (set_flags)
+ *
+ * event set flags layout:
+ * bits[00-15] : generic flags
+ * bits[16-31] : arch-specific flags
+ */
+#define PFM_ITA_SETFL_EXCL_INTR	0x10000	 /* exclude interrupt execution */
+#define PFM_ITA_SETFL_INTR_ONLY	0x20000	 /* include only interrupt execution */
+#define PFM_ITA_SETFL_IDLE_EXCL 0x40000  /* stop monitoring in idle loop */
+
+#endif /* _ASM_IA64_PERFMON_CONST_H_ */
diff -Naur linux-2.6.25-org/include/asm-ia64/perfmon_default_smpl.h linux-2.6.25-id/include/asm-ia64/perfmon_default_smpl.h
--- linux-2.6.25-org/include/asm-ia64/perfmon_default_smpl.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-ia64/perfmon_default_smpl.h	2008-04-23 11:21:56.000000000 +0200
@@ -1,83 +1,106 @@
 /*
- * Copyright (C) 2002-2003 Hewlett-Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
  *
- * This file implements the default sampling buffer format
- * for Linux/ia64 perfmon subsystem.
- */
-#ifndef __PERFMON_DEFAULT_SMPL_H__
-#define __PERFMON_DEFAULT_SMPL_H__ 1
+ * This file implements the old default sampling buffer format
+ * for the perfmon2 subsystem. For IA-64 only.
+ *
+ * It requires the use of the perfmon_compat.h header. It is recommended
+ * that applications be ported to the new format instead.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#ifndef __ASM_IA64_PERFMON_DEFAULT_SMPL_H__
+#define __ASM_IA64_PERFMON_DEFAULT_SMPL_H__ 1
+
+#ifndef __ia64__
+#error "this file must be used for compatibility reasons only on IA-64"
+#endif
 
 #define PFM_DEFAULT_SMPL_UUID { \
-		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82, 0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
+		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82,\
+		0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
 
 /*
  * format specific parameters (passed at context creation)
  */
-typedef struct {
+struct pfm_default_smpl_arg {
 	unsigned long buf_size;		/* size of the buffer in bytes */
 	unsigned int  flags;		/* buffer specific flags */
 	unsigned int  res1;		/* for future use */
 	unsigned long reserved[2];	/* for future use */
-} pfm_default_smpl_arg_t;
+};
 
 /*
  * combined context+format specific structure. Can be passed
- * to PFM_CONTEXT_CREATE
+ * to PFM_CONTEXT_CREATE (not PFM_CONTEXT_CREATE2)
  */
-typedef struct {
-	pfarg_context_t		ctx_arg;
-	pfm_default_smpl_arg_t	buf_arg;
-} pfm_default_smpl_ctx_arg_t;
+struct pfm_default_smpl_ctx_arg {
+	struct pfarg_context		ctx_arg;
+	struct pfm_default_smpl_arg	buf_arg;
+};
 
 /*
  * This header is at the beginning of the sampling buffer returned to the user.
  * It is directly followed by the first record.
  */
-typedef struct {
-	unsigned long	hdr_count;		/* how many valid entries */
-	unsigned long	hdr_cur_offs;		/* current offset from top of buffer */
-	unsigned long	hdr_reserved2;		/* reserved for future use */
-
-	unsigned long	hdr_overflows;		/* how many times the buffer overflowed */
-	unsigned long   hdr_buf_size;		/* how many bytes in the buffer */
-
-	unsigned int	hdr_version;		/* contains perfmon version (smpl format diffs) */
-	unsigned int	hdr_reserved1;		/* for future use */
-	unsigned long	hdr_reserved[10];	/* for future use */
-} pfm_default_smpl_hdr_t;
+struct pfm_default_smpl_hdr {
+	u64	hdr_count;	/* how many valid entries */
+	u64	hdr_cur_offs;	/* current offset from top of buffer */
+	u64	dr_reserved2;	/* reserved for future use */
+
+	u64	hdr_overflows;	/* how many times the buffer overflowed */
+	u64	hdr_buf_size;	/* how many bytes in the buffer */
+
+	u32	hdr_version;	/* smpl format version*/
+	u32	hdr_reserved1;		/* for future use */
+	u64	hdr_reserved[10];	/* for future use */
+};
 
 /*
  * Entry header in the sampling buffer.  The header is directly followed
- * with the values of the PMD registers of interest saved in increasing 
- * index order: PMD4, PMD5, and so on. How many PMDs are present depends 
+ * with the values of the PMD registers of interest saved in increasing
+ * index order: PMD4, PMD5, and so on. How many PMDs are present depends
  * on how the session was programmed.
  *
  * In the case where multiple counters overflow at the same time, multiple
  * entries are written consecutively.
  *
- * last_reset_value member indicates the initial value of the overflowed PMD. 
+ * last_reset_value member indicates the initial value of the overflowed PMD.
  */
-typedef struct {
-        int             pid;                    /* thread id (for NPTL, this is gettid()) */
-        unsigned char   reserved1[3];           /* reserved for future use */
-        unsigned char   ovfl_pmd;               /* index of overflowed PMD */
-
-        unsigned long   last_reset_val;         /* initial value of overflowed PMD */
-        unsigned long   ip;                     /* where did the overflow interrupt happened  */
-        unsigned long   tstamp;                 /* ar.itc when entering perfmon intr. handler */
-
-        unsigned short  cpu;                    /* cpu on which the overfow occured */
-        unsigned short  set;                    /* event set active when overflow ocurred   */
-        int    		tgid;              	/* thread group id (for NPTL, this is getpid()) */
-} pfm_default_smpl_entry_t;
-
-#define PFM_DEFAULT_MAX_PMDS		64 /* how many pmds supported by data structures (sizeof(unsigned long) */
-#define PFM_DEFAULT_MAX_ENTRY_SIZE	(sizeof(pfm_default_smpl_entry_t)+(sizeof(unsigned long)*PFM_DEFAULT_MAX_PMDS))
-#define PFM_DEFAULT_SMPL_MIN_BUF_SIZE	(sizeof(pfm_default_smpl_hdr_t)+PFM_DEFAULT_MAX_ENTRY_SIZE)
+struct pfm_default_smpl_entry {
+	pid_t	pid;		/* thread id (for NPTL, this is gettid()) */
+	uint8_t	reserved1[3];	/* for future use */
+	uint8_t	ovfl_pmd;	/* overflow pmd for this sample */
+	u64	last_reset_val;	/* initial value of overflowed PMD */
+	unsigned long ip;	/* where did the overflow interrupt happened */
+	u64	tstamp; 	/* overflow timetamp */
+	u16	cpu;  		/* cpu on which the overfow occured */
+	u16	set;  		/* event set active when overflow ocurred   */
+	pid_t	tgid;		/* thread group id (for NPTL, this is getpid()) */
+};
+
+#define PFM_DEFAULT_MAX_PMDS		64 /* #pmds supported  */
+#define PFM_DEFAULT_MAX_ENTRY_SIZE	(sizeof(struct pfm_default_smpl_entry)+\
+					 (sizeof(u64)*PFM_DEFAULT_MAX_PMDS))
+#define PFM_DEFAULT_SMPL_MIN_BUF_SIZE	(sizeof(struct pfm_default_smpl_hdr)+\
+					 PFM_DEFAULT_MAX_ENTRY_SIZE)
 
 #define PFM_DEFAULT_SMPL_VERSION_MAJ	2U
-#define PFM_DEFAULT_SMPL_VERSION_MIN	0U
-#define PFM_DEFAULT_SMPL_VERSION	(((PFM_DEFAULT_SMPL_VERSION_MAJ&0xffff)<<16)|(PFM_DEFAULT_SMPL_VERSION_MIN & 0xffff))
+#define PFM_DEFAULT_SMPL_VERSION_MIN 1U
+#define PFM_DEFAULT_SMPL_VERSION (((PFM_DEFAULT_SMPL_VERSION_MAJ&0xffff)<<16)|\
+				    (PFM_DEFAULT_SMPL_VERSION_MIN & 0xffff))
 
-#endif /* __PERFMON_DEFAULT_SMPL_H__ */
+#endif /* __ASM_IA64_PERFMON_DEFAULT_SMPL_H__ */
diff -Naur linux-2.6.25-org/include/asm-ia64/processor.h linux-2.6.25-id/include/asm-ia64/processor.h
--- linux-2.6.25-org/include/asm-ia64/processor.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-ia64/processor.h	2008-04-23 11:21:56.000000000 +0200
@@ -42,7 +42,6 @@
 
 #define IA64_THREAD_FPH_VALID	(__IA64_UL(1) << 0)	/* floating-point high state valid? */
 #define IA64_THREAD_DBG_VALID	(__IA64_UL(1) << 1)	/* debug registers valid? */
-#define IA64_THREAD_PM_VALID	(__IA64_UL(1) << 2)	/* performance registers valid? */
 #define IA64_THREAD_UAC_NOPRINT	(__IA64_UL(1) << 3)	/* don't log unaligned accesses */
 #define IA64_THREAD_UAC_SIGBUS	(__IA64_UL(1) << 4)	/* generate SIGBUS on unaligned acc. */
 #define IA64_THREAD_MIGRATION	(__IA64_UL(1) << 5)	/* require migration
@@ -258,14 +257,6 @@
 #else
 # define INIT_THREAD_IA32
 #endif /* CONFIG_IA32_SUPPORT */
-#ifdef CONFIG_PERFMON
-	void *pfm_context;		     /* pointer to detailed PMU context */
-	unsigned long pfm_needs_checking;    /* when >0, pending perfmon work on kernel exit */
-# define INIT_THREAD_PM		.pfm_context =		NULL,     \
-				.pfm_needs_checking =	0UL,
-#else
-# define INIT_THREAD_PM
-#endif
 	__u64 dbr[IA64_NUM_DBG_REGS];
 	__u64 ibr[IA64_NUM_DBG_REGS];
 	struct ia64_fpreg fph[96];	/* saved/loaded on demand */
@@ -280,7 +271,6 @@
 	.task_size =	DEFAULT_TASK_SIZE,			\
 	.last_fph_cpu =  -1,					\
 	INIT_THREAD_IA32					\
-	INIT_THREAD_PM						\
 	.dbr =		{0, },					\
 	.ibr =		{0, },					\
 	.fph =		{{{{0}}}, }				\
diff -Naur linux-2.6.25-org/include/asm-ia64/system.h linux-2.6.25-id/include/asm-ia64/system.h
--- linux-2.6.25-org/include/asm-ia64/system.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-ia64/system.h	2008-04-23 11:21:56.000000000 +0200
@@ -210,22 +210,18 @@
 extern void ia64_save_extra (struct task_struct *task);
 extern void ia64_load_extra (struct task_struct *task);
 
-#ifdef CONFIG_PERFMON
-  DECLARE_PER_CPU(unsigned long, pfm_syst_info);
-# define PERFMON_IS_SYSWIDE() (__get_cpu_var(pfm_syst_info) & 0x1)
-#else
-# define PERFMON_IS_SYSWIDE() (0)
-#endif
-
-#define IA64_HAS_EXTRA_STATE(t)							\
-	((t)->thread.flags & (IA64_THREAD_DBG_VALID|IA64_THREAD_PM_VALID)	\
-	 || IS_IA32_PROCESS(task_pt_regs(t)) || PERFMON_IS_SYSWIDE())
+#define IA64_HAS_EXTRA_STATE(t)						\
+	(((t)->thread.flags & IA64_THREAD_DBG_VALID)			\
+	 || IS_IA32_PROCESS(task_pt_regs(t)))
 
 #define __switch_to(prev,next,last) do {							 \
 	if (IA64_HAS_EXTRA_STATE(prev))								 \
 		ia64_save_extra(prev);								 \
 	if (IA64_HAS_EXTRA_STATE(next))								 \
 		ia64_load_extra(next);								 \
+	if (test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW)					 \
+	    || test_tsk_thread_flag(next, TIF_PERFMON_CTXSW))					 \
+		pfm_ctxsw(prev, next);								 \
 	ia64_psr(task_pt_regs(next))->dfh = !ia64_is_local_fpu_owner(next);			 \
 	(last) = ia64_switch_to((next));							 \
 } while (0)
diff -Naur linux-2.6.25-org/include/asm-mips/perfmon.h linux-2.6.25-id/include/asm-mips/perfmon.h
--- linux-2.6.25-org/include/asm-mips/perfmon.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-mips/perfmon.h	2008-04-23 11:21:56.000000000 +0200
@@ -0,0 +1,421 @@
+/*
+ * Copyright (c) 2005 Philip Mucci.
+ *
+ * Based on other versions:
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file contains mips64 specific definitions for the perfmon
+ * interface.
+ *
+ * This file MUST never be included directly. Use linux/perfmon.h.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#ifndef _ASM_MIPS64_PERFMON_H_
+#define _ASM_MIPS64_PERFMON_H_
+
+#ifdef __KERNEL__
+
+#include <asm/cacheflush.h>
+
+#define PFM_ARCH_PMD_STK_ARG	2
+#define PFM_ARCH_PMC_STK_ARG	2
+
+struct pfm_arch_pmu_info {
+	u32 pmu_style;
+};
+
+#define MIPS64_CONFIG_PMC_MASK (1 << 4)
+#define MIPS64_PMC_INT_ENABLE_MASK (1 << 4)
+#define MIPS64_PMC_CNT_ENABLE_MASK (0xf)
+#define MIPS64_PMC_EVT_MASK (0x7 << 6)
+#define MIPS64_PMC_CTR_MASK (1 << 31)
+#define MIPS64_PMD_INTERRUPT (1 << 31)
+
+/* Coprocessor register 25 contains the PMU interface. */
+/* Sel 0 is control for counter 0 */
+/* Sel 1 is count for counter 0. */
+/* Sel 2 is control for counter 1. */
+/* Sel 3 is count for counter 1. */
+
+/*
+
+31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4  3 2 1 0
+M  0--------------------------------------------------------------0 Event-- IE U S K EXL
+
+M 31 If this bit is one, another pair of Performance Control
+and Counter registers is implemented at a MTC0
+
+Event 8:5 Counter event enabled for this counter. Possible events
+are listed in Table 6-30. R/W Undefined
+
+IE 4 Counter Interrupt Enable. This bit masks bit 31 of the
+associated count register from the interrupt exception
+request output. R/W 0
+
+U 3 Count in User Mode. When this bit is set, the specified
+event is counted in User Mode. R/W Undefined
+
+S 2 Count in Supervisor Mode. When this bit is set, the
+specified event is counted in Supervisor Mode. R/W Undefined
+
+K 1 Count in Kernel Mode. When this bit is set, count the
+event in Kernel Mode when EXL and ERL both are 0. R/W Undefined
+
+EXL 0 Count when EXL. When this bit is set, count the event
+when EXL = 1 and ERL = 0. R/W Undefined
+*/
+
+static inline void pfm_arch_resend_irq(void)
+{}
+
+static inline void pfm_arch_serialize(void)
+{}
+
+
+static inline void pfm_arch_unfreeze_pmu(void)
+{}
+
+/*
+ * MIPS does not save the PMDs during pfm_arch_intr_freeze_pmu(), thus
+ * this routine needs to do it when switching sets on overflow
+ */
+static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
+					   struct pfm_event_set *set)
+{
+	pfm_save_pmds(ctx, set);
+}
+
+static inline void pfm_arch_write_pmc(struct pfm_context *ctx,
+				      unsigned int cnum, u64 value)
+{
+  /*
+   * we only write to the actual register when monitoring is
+   * active (pfm_start was issued)
+   */
+  if (ctx && (ctx->flags.started == 0))
+	  return;
+
+  switch(pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
+  case 0:
+    write_c0_perfctrl0(value);
+    break;
+  case 1:
+    write_c0_perfctrl1(value);
+    break;
+  case 2:
+    write_c0_perfctrl2(value);
+    break;
+  case 3:
+    write_c0_perfctrl3(value);
+    break;
+  default:
+    BUG();
+  }
+}
+
+static inline void pfm_arch_write_pmd(struct pfm_context *ctx,
+				      unsigned int cnum, u64 value)
+{
+  value &= pfm_pmu_conf->ovfl_mask;
+
+  switch(pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+  case 0:
+    write_c0_perfcntr0(value);
+    break;
+  case 1:
+    write_c0_perfcntr1(value);
+    break;
+  case 2:
+    write_c0_perfcntr2(value);
+    break;
+  case 3:
+    write_c0_perfcntr3(value);
+    break;
+  default:
+    BUG();
+  }
+}
+
+static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
+{
+	switch(pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
+	case 0:
+		return read_c0_perfcntr0();
+		break;
+	case 1:
+		return read_c0_perfcntr1();
+		break;
+	case 2:
+		return read_c0_perfcntr2();
+		break;
+	case 3:
+		return read_c0_perfcntr3();
+		break;
+	default:
+		BUG();
+		return 0;
+	}
+}
+
+static inline u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
+{
+	switch(pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
+	case 0:
+		return read_c0_perfctrl0();
+		break;
+	case 1:
+		return read_c0_perfctrl1();
+		break;
+	case 2:
+		return read_c0_perfctrl2();
+		break;
+	case 3:
+		return read_c0_perfctrl3();
+		break;
+	default:
+		BUG();
+		return 0;
+	}
+}
+
+/*
+ * For some CPUs, the upper bits of a counter must be set in order for the
+ * overflow interrupt to happen. On overflow, the counter has wrapped around,
+ * and the upper bits are cleared. This function may be used to set them back.
+ */
+static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx,
+					   unsigned int cnum)
+{
+  u64 val;
+  val = pfm_arch_read_pmd(ctx, cnum);
+  /* This masks out overflow bit 31 */
+  pfm_arch_write_pmd(ctx, cnum, val);
+}
+
+/*
+ * At certain points, perfmon needs to know if monitoring has been
+ * explicitely started/stopped by user via pfm_start/pfm_stop. The
+ * information is tracked in ctx.flags.started. However on certain
+ * architectures, it may be possible to start/stop directly from
+ * user level with a single assembly instruction bypassing
+ * the kernel. This function must be used to determine by
+ * an arch-specific mean if monitoring is actually started/stopped.
+ */
+static inline int pfm_arch_is_active(struct pfm_context *ctx)
+{
+	return ctx->flags.started;
+}
+
+static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
+					 struct pfm_context *ctx,
+					 struct pfm_event_set *set)
+{}
+
+static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
+                                         struct pfm_context *ctx,
+                                         struct pfm_event_set *set)
+{}
+
+static inline void pfm_arch_ctxswin_thread(struct task_struct *task,
+                                         struct pfm_context *ctx,
+                                         struct pfm_event_set *set)
+{}
+
+int  pfm_arch_is_monitoring_active(struct pfm_context *ctx);
+int  pfm_arch_ctxswout_thread(struct task_struct *task,
+			      struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
+		   struct pfm_event_set *set);
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
+		    struct pfm_event_set *set);
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
+char *pfm_arch_get_pmu_module_name(void);
+
+static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{
+	pfm_arch_stop(current, ctx, set);
+	/*
+	 * we mark monitoring as stopped to avoid
+	 * certain side effects especially in
+	 * pfm_switch_sets_from_intr() on
+	 * pfm_arch_restore_pmcs()
+	 */
+	ctx->flags.started = 0;
+}
+
+/*
+ * unfreeze PMU from pfm_do_interrupt_handler()
+ * ctx may be NULL for spurious
+ */
+static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
+{
+	if (!ctx)
+		return;
+
+	PFM_DBG_ovfl("state=%d", ctx->state);
+
+	ctx->flags.started = 1;
+
+	if (ctx->state == PFM_CTX_MASKED)
+		return;
+
+	pfm_arch_restore_pmcs(ctx, ctx->active_set);
+}
+
+static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
+{
+	return 0;
+}
+
+/*
+ * this function is called from the PMU interrupt handler ONLY.
+ * On MIPS, the PMU is frozen via arch_stop, masking would be implemented
+ * via arch-stop as well. Given that the PMU is already stopped when
+ * entering the interrupt handler, we do not need to stop it again, so
+ * this function is a nop.
+ */
+static inline void pfm_arch_mask_monitoring(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{}
+
+/*
+ * on MIPS masking/unmasking uses the start/stop mechanism, so we simply
+ * need to start here.
+ */
+static inline void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
+					      struct pfm_event_set *set)
+{
+	pfm_arch_start(current, ctx, set);
+}
+
+static inline void pfm_arch_pmu_config_remove(void)
+{}
+
+static inline int pfm_arch_context_create(struct pfm_context *ctx,
+					  u32 ctx_flags)
+{
+	return 0;
+}
+
+static inline void pfm_arch_context_free(struct pfm_context *ctx)
+{}
+
+
+
+
+
+/*
+ * function called from pfm_setfl_sane(). Context is locked
+ * and interrupts are masked.
+ * The value of flags is the value of ctx_flags as passed by
+ * user.
+ *
+ * function must check arch-specific set flags.
+ * Return:
+ * 	1 when flags are valid
+ *      0 on error
+ */
+static inline int
+pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
+{
+	return 0;
+}
+
+static inline int pfm_arch_init(void)
+{
+	return 0;
+}
+
+static inline void pfm_arch_init_percpu(void)
+{}
+
+static inline int pfm_arch_load_context(struct pfm_context *ctx,
+					struct pfm_event_set *set,
+					struct task_struct *task)
+{
+	return 0;
+}
+
+static inline int pfm_arch_unload_context(struct pfm_context *ctx,
+					  struct task_struct *task)
+{
+	return 0;
+}
+
+static inline int pfm_arch_pmu_acquire(void)
+{
+	return 0;
+}
+
+static inline void pfm_arch_pmu_release(void)
+{}
+
+#ifdef CONFIG_PERFMON_FLUSH
+/*
+ * due to cache aliasing problem on MIPS, it is necessary to flush
+ * pages out of the cache when they are modified.
+ */
+static inline void pfm_cacheflush(void *addr, unsigned int len)
+{
+	unsigned long start, end;
+
+	start = (unsigned long)addr & PAGE_MASK;
+	end = ((unsigned long)addr + len + PAGE_SIZE - 1) & PAGE_MASK;
+
+	while(start < end) {
+		flush_data_cache_page(start);
+		start += PAGE_SIZE;
+	}
+}
+#else
+static inline void pfm_cacheflush(void *addr, unsigned int len)
+{}
+#endif
+
+/*
+ * not used for mips
+ */
+static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
+					       size_t rsize, struct file *filp)
+{
+	return -EINVAL;
+}
+static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size)
+{
+	return -EINVAL;
+}
+struct pfm_arch_context {
+	/* empty */
+};
+
+#define PFM_ARCH_CTX_SIZE	sizeof(struct pfm_arch_context)
+/*
+ * MIPS may need extra alignment requirements for the sampling buffer
+ */
+#ifdef CONFIG_PERFMON_SMPL_ALIGN
+#define PFM_ARCH_SMPL_ALIGN_SIZE	0x4000
+#else
+#define PFM_ARCH_SMPL_ALIGN_SIZE	0
+#endif
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_MIPS64_PERFMON_H_ */
diff -Naur linux-2.6.25-org/include/asm-mips/perfmon_const.h linux-2.6.25-id/include/asm-mips/perfmon_const.h
--- linux-2.6.25-org/include/asm-mips/perfmon_const.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-mips/perfmon_const.h	2008-04-23 11:21:56.000000000 +0200
@@ -0,0 +1,30 @@
+/*
+ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file contains mips64 specific definitions for the perfmon
+ * interface.
+ *
+ * This file MUST never be included directly. Use linux/perfmon.h.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#ifndef _ASM_MIPS64_PERFMON_CONST_H_
+#define _ASM_MIPS64_PERFMON_CONST_H_
+
+#define PFM_ARCH_MAX_PMCS	(256+64) /* 256 HW 64 SW */
+#define PFM_ARCH_MAX_PMDS	(256+64) /* 256 HW 64 SW */
+
+#endif /* _ASM_MIPS64_PERFMON_CONST_H_ */
diff -Naur linux-2.6.25-org/include/asm-mips/system.h linux-2.6.25-id/include/asm-mips/system.h
--- linux-2.6.25-org/include/asm-mips/system.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-mips/system.h	2008-04-23 11:21:56.000000000 +0200
@@ -67,6 +67,9 @@
 	__mips_mt_fpaff_switch_to(prev);				\
 	if (cpu_has_dsp)						\
 		__save_dsp(prev);					\
+	if (test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW)		\
+	    || test_tsk_thread_flag(next, TIF_PERFMON_CTXSW))		\
+		pfm_ctxsw(prev, next);					\
 	(last) = resume(prev, next, task_thread_info(next));		\
 } while (0)
 
diff -Naur linux-2.6.25-org/include/asm-mips/thread_info.h linux-2.6.25-id/include/asm-mips/thread_info.h
--- linux-2.6.25-org/include/asm-mips/thread_info.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-mips/thread_info.h	2008-04-23 11:21:56.000000000 +0200
@@ -112,6 +112,7 @@
 #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
 #define TIF_SYSCALL_AUDIT	3	/* syscall auditing active */
 #define TIF_SECCOMP		4	/* secure computing */
+#define TIF_PERFMON_WORK	5	/* work for pfm_handle_work() */
 #define TIF_RESTORE_SIGMASK	9	/* restore signal mask in do_signal() */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
@@ -122,6 +123,7 @@
 #define TIF_32BIT_REGS		22	/* also implies 16/32 fprs */
 #define TIF_32BIT_ADDR		23	/* 32-bit address space (o32/n32) */
 #define TIF_FPUBOUND		24	/* thread bound to FPU-full CPU set */
+#define TIF_PERFMON_CTXSW	25	/* perfmon needs ctxsw calls */
 #define TIF_SYSCALL_TRACE	31	/* syscall trace active */
 
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -138,6 +140,8 @@
 #define _TIF_32BIT_REGS		(1<<TIF_32BIT_REGS)
 #define _TIF_32BIT_ADDR		(1<<TIF_32BIT_ADDR)
 #define _TIF_FPUBOUND		(1<<TIF_FPUBOUND)
+#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
+#define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
 
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK		(0x0000ffef & ~_TIF_SECCOMP)
diff -Naur linux-2.6.25-org/include/asm-powerpc/cell-pmu.h linux-2.6.25-id/include/asm-powerpc/cell-pmu.h
--- linux-2.6.25-org/include/asm-powerpc/cell-pmu.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-powerpc/cell-pmu.h	2008-04-23 11:21:56.000000000 +0200
@@ -61,6 +61,11 @@
 
 /* Macros for the pm_status register. */
 #define CBE_PM_CTR_OVERFLOW_INTR(ctr)      (1 << (31 - ((ctr) & 7)))
+#define CBE_PM_OVERFLOW_CTRS(pm_status)    (((pm_status) >> 24) & 0xff)
+#define CBE_PM_ALL_OVERFLOW_INTR           0xff000000
+#define CBE_PM_INTERVAL_INTR               0x00800000
+#define CBE_PM_TRACE_BUFFER_FULL_INTR      0x00400000
+#define CBE_PM_TRACE_BUFFER_UNDERFLOW_INTR 0x00200000
 
 enum pm_reg_name {
 	group_control,
diff -Naur linux-2.6.25-org/include/asm-powerpc/cell-regs.h linux-2.6.25-id/include/asm-powerpc/cell-regs.h
--- linux-2.6.25-org/include/asm-powerpc/cell-regs.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-powerpc/cell-regs.h	2008-04-23 11:21:56.000000000 +0200
@@ -117,8 +117,9 @@
 	u8	pad_0x0c1c_0x0c20 [4];				/* 0x0c1c */
 #define CBE_PMD_FIR_MODE_M8		0x00800
 	u64	fir_enable_mask;				/* 0x0c20 */
-
-	u8	pad_0x0c28_0x0ca8 [0x0ca8 - 0x0c28];		/* 0x0c28 */
+	u8	pad_0x0c28_0x0c98 [0x0c98 - 0x0c28];		/* 0x0c28 */
+	u64	on_ramp_trace;					/* 0x0c98 */
+	u64	pad_0x0ca0;					/* 0x0ca0 */
 	u64	ras_esc_0;					/* 0x0ca8 */
 	u8	pad_0x0cb0_0x1000 [0x1000 - 0x0cb0];		/* 0x0cb0 */
 };
@@ -218,7 +219,11 @@
 
 
 struct cbe_mic_tm_regs {
-	u8	pad_0x0000_0x0040[0x0040 - 0x0000];		/* 0x0000 */
+	u8	pad_0x0000_0x0010[0x0010 - 0x0000];		/* 0x0000 */
+
+	u64	MBL_debug;					/* 0x0010 */
+
+	u8	pad_0x0018_0x0040[0x0040 - 0x0018];		/* 0x0018 */
 
 	u64	mic_ctl_cnfg2;					/* 0x0040 */
 #define CBE_MIC_ENABLE_AUX_TRC		0x8000000000000000LL
@@ -303,6 +308,25 @@
 extern struct cbe_mic_tm_regs __iomem *cbe_get_mic_tm_regs(struct device_node *np);
 extern struct cbe_mic_tm_regs __iomem *cbe_get_cpu_mic_tm_regs(int cpu);
 
+/*
+ *
+ *  PPE Privileged MMIO Registers definition. (offset 0x500000 - 0x500fff)
+ *
+ */
+struct cbe_ppe_priv_regs {
+	u8	pad_0x0000_0x0858[0x0858 - 0x0000];		/* 0x0000 */
+
+	u64	L2_debug1;					/* 0x0858 */
+
+	u8	pad_0x0860_0x0958[0x0958 - 0x0860];		/* 0x0860 */
+
+	u64	ciu_dr1;					/* 0x0958 */
+
+	u8	pad_0x0960_0x1000[0x1000 - 0x0960];		/* 0x0960 */
+};
+
+extern struct cbe_ppe_priv_regs __iomem *cbe_get_cpu_ppe_priv_regs(int cpu);
+
 /* some utility functions to deal with SMT */
 extern u32 cbe_get_hw_thread_id(int cpu);
 extern u32 cbe_cpu_to_node(int cpu);
diff -Naur linux-2.6.25-org/include/asm-powerpc/emulated_ops.h linux-2.6.25-id/include/asm-powerpc/emulated_ops.h
--- linux-2.6.25-org/include/asm-powerpc/emulated_ops.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-powerpc/emulated_ops.h	2008-04-23 11:21:56.000000000 +0200
@@ -0,0 +1,52 @@
+/*
+ *  Copyright 2007 Sony Corp.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+
+#ifndef _ASM_POWERPC_EMULATED_OPS_H
+#define _ASM_POWERPC_EMULATED_OPS_H
+
+#include <linux/percpu.h>
+
+#include <asm/atomic.h>
+
+DECLARE_PER_CPU(atomic_long_t, emulated_dcba);
+DECLARE_PER_CPU(atomic_long_t, emulated_dcbz);
+DECLARE_PER_CPU(atomic_long_t, emulated_fp_pair);
+DECLARE_PER_CPU(atomic_long_t, emulated_mcrxr);
+DECLARE_PER_CPU(atomic_long_t, emulated_mfpvr);
+DECLARE_PER_CPU(atomic_long_t, emulated_multiple);
+DECLARE_PER_CPU(atomic_long_t, emulated_popcntb);
+DECLARE_PER_CPU(atomic_long_t, emulated_spe);
+DECLARE_PER_CPU(atomic_long_t, emulated_string);
+#ifdef CONFIG_MATH_EMULATION
+DECLARE_PER_CPU(atomic_long_t, emulated_math);
+#elif defined(CONFIG_8XX_MINIMAL_FPEMU)
+DECLARE_PER_CPU(atomic_long_t, emulated_8xx);
+#endif
+
+extern int sysctl_warn_emulated;
+extern void do_warn_emulate(const char *type);
+
+#define WARN_EMULATE(type)						\
+	do {								\
+		atomic_long_inc(&per_cpu(emulated_ ## type,		\
+					 raw_smp_processor_id()));	\
+		if (sysctl_warn_emulated)				\
+			do_warn_emulate(#type);				\
+	} while (0)
+
+
+#endif /* _ASM_POWERPC_EMULATED_OPS_H */
diff -Naur linux-2.6.25-org/include/asm-powerpc/oprofile_impl.h linux-2.6.25-id/include/asm-powerpc/oprofile_impl.h
--- linux-2.6.25-org/include/asm-powerpc/oprofile_impl.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-powerpc/oprofile_impl.h	2008-04-23 11:21:57.000000000 +0200
@@ -59,6 +59,7 @@
 extern struct op_powerpc_model op_model_power4;
 extern struct op_powerpc_model op_model_7450;
 extern struct op_powerpc_model op_model_cell;
+extern struct op_powerpc_model op_model_ps3;
 extern struct op_powerpc_model op_model_pa6t;
 
 
diff -Naur linux-2.6.25-org/include/asm-powerpc/perfmon.h linux-2.6.25-id/include/asm-powerpc/perfmon.h
--- linux-2.6.25-org/include/asm-powerpc/perfmon.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-powerpc/perfmon.h	2008-04-23 11:21:57.000000000 +0200
@@ -0,0 +1,445 @@
+/*
+ * Copyright (c) 2005 David Gibson, IBM Corporation.
+ *
+ * Based on other versions:
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file contains powerpc specific definitions for the perfmon
+ * interface.
+ *
+ * This file MUST never be included directly. Use linux/perfmon.h.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#ifndef _ASM_POWERPC_PERFMON_H_
+#define _ASM_POWERPC_PERFMON_H_
+
+#ifdef __KERNEL__
+
+#include <asm/pmc.h>
+
+enum powerpc_pmu_type {
+	PFM_POWERPC_PMU_NONE,
+	PFM_POWERPC_PMU_604,
+	PFM_POWERPC_PMU_604e,
+	PFM_POWERPC_PMU_750,	/* XXX: Minor event set diffs between IBM and Moto. */
+	PFM_POWERPC_PMU_7400,
+	PFM_POWERPC_PMU_7450,
+	PFM_POWERPC_PMU_POWER4,
+	PFM_POWERPC_PMU_POWER5,
+	PFM_POWERPC_PMU_POWER5p,
+	PFM_POWERPC_PMU_POWER6,
+	PFM_POWERPC_PMU_CELL,
+};
+
+struct pfm_arch_pmu_info {
+	enum powerpc_pmu_type pmu_style;
+
+	int (*add_ctxsw_hook)(struct pfm_context *ctx);
+	int (*remove_ctxsw_hook)(struct pfm_context *ctx);
+        int (*get_pid)(void *arg);
+	int (*ctxsw)(struct notifier_block *block,
+		     unsigned long object_id, struct task_struct *p, void *arg);
+
+	void (*write_pmc)(unsigned int cnum, u64 value);
+	void (*write_pmd)(unsigned int cnum, u64 value);
+
+	u64 (*read_pmd)(unsigned int cnum);
+
+	void (*enable_counters)(struct pfm_context *ctx,
+				struct pfm_event_set *set);
+	void (*disable_counters)(struct pfm_context *ctx,
+				 struct pfm_event_set *set);
+
+	void (*irq_handler)(struct pt_regs *regs, struct pfm_context *ctx);
+	void (*get_ovfl_pmds)(struct pfm_context *ctx,
+			      struct pfm_event_set *set);
+
+	/* The following routines are optional. */
+	void (*restore_pmcs)(struct pfm_event_set *set);
+	void (*restore_pmds)(struct pfm_event_set *set);
+
+	int  (*ctxswout_thread)(struct task_struct *task,
+				struct pfm_context *ctx,
+				struct pfm_event_set *set);
+	void (*ctxswin_thread)(struct task_struct *task,
+			       struct pfm_context *ctx,
+			       struct pfm_event_set *set);
+	int  (*load_context)(struct pfm_context *ctx,
+			     struct pfm_event_set *set,
+			     struct task_struct *task);
+	int  (*unload_context)(struct pfm_context *ctx,
+			       struct task_struct *task);
+	int  (*acquire_pmu)(void);
+	void (*release_pmu)(void);
+	void *platform_info;
+};
+
+#ifdef CONFIG_PPC32
+#define PFM_ARCH_PMD_STK_ARG	6 /* conservative value */
+#define PFM_ARCH_PMC_STK_ARG	6 /* conservative value */
+#else
+#define PFM_ARCH_PMD_STK_ARG	8 /* conservative value */
+#define PFM_ARCH_PMC_STK_ARG	8 /* conservative value */
+#endif
+
+static inline void pfm_arch_resend_irq(void)
+{}
+
+static inline void pfm_arch_serialize(void)
+{}
+
+static inline void pfm_arch_write_pmc(struct pfm_context *ctx,
+				      unsigned int cnum,
+				      u64 value)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	/*
+	 * we only write to the actual register when monitoring is
+	 * active (pfm_start was issued)
+	 */
+	if (ctx && ctx->flags.started == 0)
+		return;
+
+	BUG_ON(!arch_info->write_pmc);
+
+	arch_info->write_pmc(cnum, value);
+}
+
+static inline void pfm_arch_write_pmd(struct pfm_context *ctx,
+				      unsigned int cnum, u64 value)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	value &= pfm_pmu_conf->ovfl_mask;
+
+	BUG_ON(!arch_info->write_pmd);
+
+	arch_info->write_pmd(cnum, value);
+}
+
+static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	BUG_ON(!arch_info->read_pmd);
+
+	return arch_info->read_pmd(cnum);
+}
+
+/*
+ * For some CPUs, the upper bits of a counter must be set in order for the
+ * overflow interrupt to happen. On overflow, the counter has wrapped around,
+ * and the upper bits are cleared. This function may be used to set them back.
+ */
+static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx,
+					   unsigned int cnum)
+{
+	u64 val = pfm_arch_read_pmd(ctx, cnum);
+
+	/* This masks out overflow bit 31 */
+	pfm_arch_write_pmd(ctx, cnum, val);
+}
+
+/*
+ * At certain points, perfmon needs to know if monitoring has been
+ * explicitely started/stopped by user via pfm_start/pfm_stop. The
+ * information is tracked in flags.started. However on certain
+ * architectures, it may be possible to start/stop directly from
+ * user level with a single assembly instruction bypassing
+ * the kernel. This function must be used to determine by
+ * an arch-specific mean if monitoring is actually started/stopped.
+ */
+static inline int pfm_arch_is_active(struct pfm_context *ctx)
+{
+	return ctx->flags.started;
+}
+
+static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
+					 struct pfm_context *ctx,
+					 struct pfm_event_set *set)
+{}
+
+static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
+					struct pfm_context *ctx,
+					struct pfm_event_set *set)
+{}
+
+void pfm_arch_init_percpu(void);
+int  pfm_arch_is_monitoring_active(struct pfm_context *ctx);
+int  pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
+			      struct pfm_event_set *set);
+void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx,
+			     struct pfm_event_set *set);
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
+			  struct pfm_event_set *set);
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
+			   struct pfm_event_set *set);
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
+int  pfm_arch_get_ovfl_pmds(struct pfm_context *ctx,
+			    struct pfm_event_set *set);
+char *pfm_arch_get_pmu_module_name(void);
+/*
+ * called from __pfm_interrupt_handler(). ctx is not NULL.
+ * ctx is locked. PMU interrupt is masked.
+ *
+ * must stop all monitoring to ensure handler has consistent view.
+ * must collect overflowed PMDs bitmask  into povfls_pmds and
+ * npend_ovfls. If no interrupt detected then npend_ovfls
+ * must be set to zero.
+ */
+static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	pfm_arch_stop(current, ctx, set);
+}
+
+void powerpc_irq_handler(struct pt_regs *regs);
+
+/*
+ * unfreeze PMU from pfm_do_interrupt_handler()
+ * ctx may be NULL for spurious
+ */
+static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
+{
+	struct pfm_arch_pmu_info *arch_info;
+
+	if (!ctx)
+		return;
+
+	PFM_DBG_ovfl("state=%d", ctx->state);
+
+	ctx->flags.started = 1;
+
+	if (ctx->state == PFM_CTX_MASKED)
+		return;
+
+	arch_info = pfm_pmu_conf->arch_info;
+	BUG_ON(!arch_info->enable_counters);
+	arch_info->enable_counters(ctx, ctx->active_set);
+}
+
+/*
+ * PowerPC does not save the PMDs during pfm_arch_intr_freeze_pmu(), thus
+ * this routine needs to do it when switching sets on overflow
+ */
+static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
+					   struct pfm_event_set *set)
+{
+	pfm_save_pmds(ctx, set);
+}
+
+/*
+ * this function is called from the PMU interrupt handler ONLY.
+ * On PPC, the PMU is frozen via arch_stop, masking would be implemented
+ * via arch-stop as well. Given that the PMU is already stopped when
+ * entering the interrupt handler, we do not need to stop it again, so
+ * this function is a nop.
+ */
+static inline void pfm_arch_mask_monitoring(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{}
+
+/*
+ * Simply need to start the context in order to unmask.
+ */
+static inline void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
+					      struct pfm_event_set *set)
+{
+	pfm_arch_start(current, ctx, set);
+}
+
+
+static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
+{
+	return 0;
+}
+
+static inline void pfm_arch_pmu_config_remove(void)
+{}
+
+static inline int pfm_arch_context_create(struct pfm_context *ctx,
+					  u32 ctx_flags)
+{
+	return 0;
+}
+
+static inline void pfm_arch_context_free(struct pfm_context *ctx)
+{}
+
+/* not necessary on PowerPC */
+static inline void pfm_cacheflush(void *addr, unsigned int len)
+{}
+
+/*
+ * function called from pfm_setfl_sane(). Context is locked
+ * and interrupts are masked.
+ * The value of flags is the value of ctx_flags as passed by
+ * user.
+ *
+ * function must check arch-specific set flags.
+ * Return:
+ * 	1 when flags are valid
+ *      0 on error
+ */
+static inline int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
+{
+	return 0;
+}
+
+static inline int pfm_arch_init(void)
+{
+	return 0;
+}
+
+static inline int pfm_arch_load_context(struct pfm_context *ctx,
+					struct pfm_event_set *set,
+					struct task_struct *task)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	int rc = 0;
+
+	if (arch_info->load_context)
+		rc = arch_info->load_context(ctx, set, task);
+
+	return rc;
+}
+
+static inline int pfm_arch_unload_context(struct pfm_context *ctx,
+					  struct task_struct *task)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	int rc = 0;
+
+	if (arch_info->unload_context)
+		rc = arch_info->unload_context(ctx, task);
+
+	return rc;
+}
+
+/*
+ * not applicable to powerpc
+ */
+static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
+					       size_t rsize, struct file *filp)
+{
+	return -EINVAL;
+}
+
+static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size)
+{
+	return -EINVAL;
+}
+
+static inline int pfm_arch_pmu_acquire(void)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	int rc = 0;
+
+	if (arch_info->acquire_pmu) {
+		rc = arch_info->acquire_pmu();
+		if (rc)
+			return rc;
+	}
+
+	return reserve_pmc_hardware(powerpc_irq_handler);
+}
+
+static inline void pfm_arch_pmu_release(void)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	if (arch_info->release_pmu)
+		arch_info->release_pmu();
+
+	release_pmc_hardware();
+}
+
+struct pfm_arch_context {
+	/* Cell: Most recent value of the pm_status
+	 * register read by the interrupt handler.
+	 *
+	 * Interrupt handler sets last_read_updated if it
+	 * just read and updated last_read_pm_status
+	 */
+	u32 last_read_pm_status;
+	u32 last_read_updated;
+	u64 powergs_pmc5, powergs_pmc6;
+	u64 delta_tb, delta_tb_start;
+	u64 delta_purr, delta_purr_start;
+};
+
+#define PFM_ARCH_CTX_SIZE sizeof(struct pfm_arch_context)
+/*
+ * PowerPC does not need extra alignment requirements for the sampling buffer
+ */
+#define PFM_ARCH_SMPL_ALIGN_SIZE	0
+
+/*
+ * Hook perfmon context switch functions to get the information of
+ * the target task scheduling
+ */
+static inline void pfm_arch_add_ctxsw_hook(struct pfm_context *ctx)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	if (!ctx)
+		return;
+
+	if (arch_info->add_ctxsw_hook) {
+		arch_info->add_ctxsw_hook(ctx);
+		return;
+	}
+
+	set_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
+
+}
+
+static inline void pfm_arch_remove_ctxsw_hook(struct pfm_context *ctx)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	if (!ctx)
+		return;
+
+	if (arch_info->remove_ctxsw_hook) {
+		arch_info->remove_ctxsw_hook(ctx);
+		return;
+	}
+
+	if (ctx->task)
+		clear_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
+}
+
+int pfm_arch_ctxsw(struct notifier_block *block,
+		   unsigned long object_id, void *arg);
+
+/*
+ * common private event set flags (priv_flags)
+ *
+ * upper 16 bits: for arch-specific use
+ * lower 16 bits: for common use
+ *
+ * See common perfmon.h
+ */
+#define PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE 0x10000
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_POWERPC_PERFMON_H_ */
diff -Naur linux-2.6.25-org/include/asm-powerpc/perfmon_const.h linux-2.6.25-id/include/asm-powerpc/perfmon_const.h
--- linux-2.6.25-org/include/asm-powerpc/perfmon_const.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-powerpc/perfmon_const.h	2008-04-23 11:21:57.000000000 +0200
@@ -0,0 +1,30 @@
+/*
+ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file contains powerpc specific definitions for the perfmon
+ * interface.
+ *
+ * This file MUST never be included directly. Use linux/perfmon.h.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#ifndef _ASM_POWERPC_PERFMON_CONST_H_
+#define _ASM_POWERPC_PERFMON_CONST_H_
+
+#define PFM_ARCH_MAX_PMCS	(256+64) /* 256 HW 64 SW */
+#define PFM_ARCH_MAX_PMDS	(256+64) /* 256 HW 64 SW */
+
+#endif /* _ASM_POWERPC_PERFMON_CONST_H_ */
diff -Naur linux-2.6.25-org/include/asm-powerpc/thread_info.h linux-2.6.25-id/include/asm-powerpc/thread_info.h
--- linux-2.6.25-org/include/asm-powerpc/thread_info.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-powerpc/thread_info.h	2008-04-23 11:22:02.000000000 +0200
@@ -142,10 +142,12 @@
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 #define _TIF_RUNLATCH		(1<<TIF_RUNLATCH)
 #define _TIF_ABI_PENDING	(1<<TIF_ABI_PENDING)
+#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
+#define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
 #define _TIF_SYSCALL_T_OR_A	(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SECCOMP)
 
 #define _TIF_USER_WORK_MASK	( _TIF_SIGPENDING | \
-				 _TIF_NEED_RESCHED | _TIF_RESTORE_SIGMASK)
+				 _TIF_NEED_RESCHED | _TIF_RESTORE_SIGMASK| _TIF_PERFMON_WORK)
 #define _TIF_PERSYSCALL_MASK	(_TIF_RESTOREALL|_TIF_NOERROR)
 
 /* Bits in local_flags */
diff -Naur linux-2.6.25-org/include/asm-sparc/Kbuild linux-2.6.25-id/include/asm-sparc/Kbuild
--- linux-2.6.25-org/include/asm-sparc/Kbuild	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-sparc/Kbuild	2008-04-23 11:22:02.000000000 +0200
@@ -11,5 +11,4 @@
 header-y += vfc_ioctls.h
 
 unifdef-y += fbio.h
-unifdef-y += perfctr.h
 unifdef-y += psr.h
diff -Naur linux-2.6.25-org/include/asm-sparc/perfctr.h linux-2.6.25-id/include/asm-sparc/perfctr.h
--- linux-2.6.25-org/include/asm-sparc/perfctr.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-sparc/perfctr.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,173 +0,0 @@
-/*----------------------------------------
-  PERFORMANCE INSTRUMENTATION  
-  Guillaume Thouvenin           08/10/98
-  David S. Miller               10/06/98
-  ---------------------------------------*/
-#ifndef PERF_COUNTER_API
-#define PERF_COUNTER_API
-
-/* sys_perfctr() interface.  First arg is operation code
- * from enumeration below.  The meaning of further arguments
- * are determined by the operation code.
- *
- * int sys_perfctr(int opcode, unsigned long arg0,
- *                 unsigned long arg1, unsigned long arg2)
- *
- * Pointers which are passed by the user are pointers to 64-bit
- * integers.
- *
- * Once enabled, performance counter state is retained until the
- * process either exits or performs an exec.  That is, performance
- * counters remain enabled for fork/clone children.
- */
-enum perfctr_opcode {
-	/* Enable UltraSparc performance counters, ARG0 is pointer
-	 * to 64-bit accumulator for D0 counter in PIC, ARG1 is pointer
-	 * to 64-bit accumulator for D1 counter.  ARG2 is a pointer to
-	 * the initial PCR register value to use.
-	 */
-	PERFCTR_ON,
-
-	/* Disable UltraSparc performance counters.  The PCR is written
-	 * with zero and the user counter accumulator pointers and
-	 * working PCR register value are forgotten.
-	 */
-	PERFCTR_OFF,
-
-	/* Add current D0 and D1 PIC values into user pointers given
-	 * in PERFCTR_ON operation.  The PIC is cleared before returning.
-	 */
-	PERFCTR_READ,
-
-	/* Clear the PIC register. */
-	PERFCTR_CLRPIC,
-
-	/* Begin using a new PCR value, the pointer to which is passed
-	 * in ARG0.  The PIC is also cleared after the new PCR value is
-	 * written.
-	 */
-	PERFCTR_SETPCR,
-
-	/* Store in pointer given in ARG0 the current PCR register value
-	 * being used.
-	 */
-	PERFCTR_GETPCR
-};
-
-/* I don't want the kernel's namespace to be polluted with this
- * stuff when this file is included.  --DaveM
- */
-#ifndef __KERNEL__
-
-#define  PRIV 0x00000001
-#define  SYS  0x00000002
-#define  USR  0x00000004
-
-/* Pic.S0 Selection Bit Field Encoding, Ultra-I/II  */
-#define  CYCLE_CNT            0x00000000
-#define  INSTR_CNT            0x00000010
-#define  DISPATCH0_IC_MISS    0x00000020
-#define  DISPATCH0_STOREBUF   0x00000030
-#define  IC_REF               0x00000080
-#define  DC_RD                0x00000090
-#define  DC_WR                0x000000A0
-#define  LOAD_USE             0x000000B0
-#define  EC_REF               0x000000C0
-#define  EC_WRITE_HIT_RDO     0x000000D0
-#define  EC_SNOOP_INV         0x000000E0
-#define  EC_RD_HIT            0x000000F0
-
-/* Pic.S0 Selection Bit Field Encoding, Ultra-III  */
-#define  US3_CYCLE_CNT	      	0x00000000
-#define  US3_INSTR_CNT	      	0x00000010
-#define  US3_DISPATCH0_IC_MISS	0x00000020
-#define  US3_DISPATCH0_BR_TGT	0x00000030
-#define  US3_DISPATCH0_2ND_BR	0x00000040
-#define  US3_RSTALL_STOREQ	0x00000050
-#define  US3_RSTALL_IU_USE	0x00000060
-#define  US3_IC_REF		0x00000080
-#define  US3_DC_RD		0x00000090
-#define  US3_DC_WR		0x000000a0
-#define  US3_EC_REF		0x000000c0
-#define  US3_EC_WR_HIT_RTO	0x000000d0
-#define  US3_EC_SNOOP_INV	0x000000e0
-#define  US3_EC_RD_MISS		0x000000f0
-#define  US3_PC_PORT0_RD	0x00000100
-#define  US3_SI_SNOOP		0x00000110
-#define  US3_SI_CIQ_FLOW	0x00000120
-#define  US3_SI_OWNED		0x00000130
-#define  US3_SW_COUNT_0		0x00000140
-#define  US3_IU_BR_MISS_TAKEN	0x00000150
-#define  US3_IU_BR_COUNT_TAKEN	0x00000160
-#define  US3_DISP_RS_MISPRED	0x00000170
-#define  US3_FA_PIPE_COMPL	0x00000180
-#define  US3_MC_READS_0		0x00000200
-#define  US3_MC_READS_1		0x00000210
-#define  US3_MC_READS_2		0x00000220
-#define  US3_MC_READS_3		0x00000230
-#define  US3_MC_STALLS_0	0x00000240
-#define  US3_MC_STALLS_2	0x00000250
-
-/* Pic.S1 Selection Bit Field Encoding, Ultra-I/II  */
-#define  CYCLE_CNT_D1         0x00000000
-#define  INSTR_CNT_D1         0x00000800
-#define  DISPATCH0_IC_MISPRED 0x00001000
-#define  DISPATCH0_FP_USE     0x00001800
-#define  IC_HIT               0x00004000
-#define  DC_RD_HIT            0x00004800
-#define  DC_WR_HIT            0x00005000
-#define  LOAD_USE_RAW         0x00005800
-#define  EC_HIT               0x00006000
-#define  EC_WB                0x00006800
-#define  EC_SNOOP_CB          0x00007000
-#define  EC_IT_HIT            0x00007800
-
-/* Pic.S1 Selection Bit Field Encoding, Ultra-III  */
-#define  US3_CYCLE_CNT_D1	0x00000000
-#define  US3_INSTR_CNT_D1	0x00000800
-#define  US3_DISPATCH0_MISPRED	0x00001000
-#define  US3_IC_MISS_CANCELLED	0x00001800
-#define  US3_RE_ENDIAN_MISS	0x00002000
-#define  US3_RE_FPU_BYPASS	0x00002800
-#define  US3_RE_DC_MISS		0x00003000
-#define  US3_RE_EC_MISS		0x00003800
-#define  US3_IC_MISS		0x00004000
-#define  US3_DC_RD_MISS		0x00004800
-#define  US3_DC_WR_MISS		0x00005000
-#define  US3_RSTALL_FP_USE	0x00005800
-#define  US3_EC_MISSES		0x00006000
-#define  US3_EC_WB		0x00006800
-#define  US3_EC_SNOOP_CB	0x00007000
-#define  US3_EC_IC_MISS		0x00007800
-#define  US3_RE_PC_MISS		0x00008000
-#define  US3_ITLB_MISS		0x00008800
-#define  US3_DTLB_MISS		0x00009000
-#define  US3_WC_MISS		0x00009800
-#define  US3_WC_SNOOP_CB	0x0000a000
-#define  US3_WC_SCRUBBED	0x0000a800
-#define  US3_WC_WB_WO_READ	0x0000b000
-#define  US3_PC_SOFT_HIT	0x0000c000
-#define  US3_PC_SNOOP_INV	0x0000c800
-#define  US3_PC_HARD_HIT	0x0000d000
-#define  US3_PC_PORT1_RD	0x0000d800
-#define  US3_SW_COUNT_1		0x0000e000
-#define  US3_IU_STAT_BR_MIS_UNTAKEN	0x0000e800
-#define  US3_IU_STAT_BR_COUNT_UNTAKEN	0x0000f000
-#define  US3_PC_MS_MISSES	0x0000f800
-#define  US3_MC_WRITES_0	0x00010800
-#define  US3_MC_WRITES_1	0x00011000
-#define  US3_MC_WRITES_2	0x00011800
-#define  US3_MC_WRITES_3	0x00012000
-#define  US3_MC_STALLS_1	0x00012800
-#define  US3_MC_STALLS_3	0x00013000
-#define  US3_RE_RAW_MISS	0x00013800
-#define  US3_FM_PIPE_COMPLETION	0x00014000
-
-struct vcounter_struct {
-  unsigned long long vcnt0;
-  unsigned long long vcnt1;
-};
-
-#endif /* !(__KERNEL__) */
-
-#endif /* !(PERF_COUNTER_API) */
diff -Naur linux-2.6.25-org/include/asm-sparc64/Kbuild linux-2.6.25-id/include/asm-sparc64/Kbuild
--- linux-2.6.25-org/include/asm-sparc64/Kbuild	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-sparc64/Kbuild	2008-04-23 11:22:02.000000000 +0200
@@ -21,4 +21,3 @@
 header-y += watchdog.h
 
 unifdef-y += fbio.h
-unifdef-y += perfctr.h
diff -Naur linux-2.6.25-org/include/asm-sparc64/hypervisor.h linux-2.6.25-id/include/asm-sparc64/hypervisor.h
--- linux-2.6.25-org/include/asm-sparc64/hypervisor.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-sparc64/hypervisor.h	2008-04-23 11:22:02.000000000 +0200
@@ -2713,6 +2713,30 @@
  */
 #define HV_FAST_SET_PERFREG		0x101
 
+#define HV_N2_PERF_SPARC_CTL		0x0
+#define HV_N2_PERF_DRAM_CTL0		0x1
+#define HV_N2_PERF_DRAM_CNT0		0x2
+#define HV_N2_PERF_DRAM_CTL1		0x3
+#define HV_N2_PERF_DRAM_CNT1		0x4
+#define HV_N2_PERF_DRAM_CTL2		0x5
+#define HV_N2_PERF_DRAM_CNT2		0x6
+#define HV_N2_PERF_DRAM_CTL3		0x7
+#define HV_N2_PERF_DRAM_CNT3		0x8
+
+#define HV_FAST_N2_GET_PERFREG		0x104
+#define HV_FAST_N2_SET_PERFREG		0x105
+
+#ifndef __ASSEMBLY__
+extern unsigned long sun4v_niagara_getperf(unsigned long reg,
+					   unsigned long *val);
+extern unsigned long sun4v_niagara_setperf(unsigned long reg,
+					   unsigned long val);
+extern unsigned long sun4v_niagara2_getperf(unsigned long reg,
+					    unsigned long *val);
+extern unsigned long sun4v_niagara2_setperf(unsigned long reg,
+					    unsigned long val);
+#endif
+
 /* MMU statistics services.
  *
  * The hypervisor maintains MMU statistics and privileged code provides
@@ -2922,6 +2946,7 @@
 #define HV_GRP_NCS			0x0103
 #define HV_GRP_NIAG_PERF		0x0200
 #define HV_GRP_FIRE_PERF		0x0201
+#define HV_GRP_NIAG2_PERF		0x0202
 #define HV_GRP_DIAG			0x0300
 
 #ifndef __ASSEMBLY__
diff -Naur linux-2.6.25-org/include/asm-sparc64/irq.h linux-2.6.25-id/include/asm-sparc64/irq.h
--- linux-2.6.25-org/include/asm-sparc64/irq.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-sparc64/irq.h	2008-04-23 11:22:02.000000000 +0200
@@ -67,6 +67,9 @@
 extern void __init init_IRQ(void);
 extern void fixup_irqs(void);
 
+extern int register_perfctr_intr(void (*handler)(struct pt_regs *));
+extern void release_perfctr_intr(void (*handler)(struct pt_regs *));
+
 static inline void set_softint(unsigned long bits)
 {
 	__asm__ __volatile__("wr	%0, 0x0, %%set_softint"
diff -Naur linux-2.6.25-org/include/asm-sparc64/perfctr.h linux-2.6.25-id/include/asm-sparc64/perfctr.h
--- linux-2.6.25-org/include/asm-sparc64/perfctr.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-sparc64/perfctr.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,173 +0,0 @@
-/*----------------------------------------
-  PERFORMANCE INSTRUMENTATION  
-  Guillaume Thouvenin           08/10/98
-  David S. Miller               10/06/98
-  ---------------------------------------*/
-#ifndef PERF_COUNTER_API
-#define PERF_COUNTER_API
-
-/* sys_perfctr() interface.  First arg is operation code
- * from enumeration below.  The meaning of further arguments
- * are determined by the operation code.
- *
- * int sys_perfctr(int opcode, unsigned long arg0,
- *                 unsigned long arg1, unsigned long arg2)
- *
- * Pointers which are passed by the user are pointers to 64-bit
- * integers.
- *
- * Once enabled, performance counter state is retained until the
- * process either exits or performs an exec.  That is, performance
- * counters remain enabled for fork/clone children.
- */
-enum perfctr_opcode {
-	/* Enable UltraSparc performance counters, ARG0 is pointer
-	 * to 64-bit accumulator for D0 counter in PIC, ARG1 is pointer
-	 * to 64-bit accumulator for D1 counter.  ARG2 is a pointer to
-	 * the initial PCR register value to use.
-	 */
-	PERFCTR_ON,
-
-	/* Disable UltraSparc performance counters.  The PCR is written
-	 * with zero and the user counter accumulator pointers and
-	 * working PCR register value are forgotten.
-	 */
-	PERFCTR_OFF,
-
-	/* Add current D0 and D1 PIC values into user pointers given
-	 * in PERFCTR_ON operation.  The PIC is cleared before returning.
-	 */
-	PERFCTR_READ,
-
-	/* Clear the PIC register. */
-	PERFCTR_CLRPIC,
-
-	/* Begin using a new PCR value, the pointer to which is passed
-	 * in ARG0.  The PIC is also cleared after the new PCR value is
-	 * written.
-	 */
-	PERFCTR_SETPCR,
-
-	/* Store in pointer given in ARG0 the current PCR register value
-	 * being used.
-	 */
-	PERFCTR_GETPCR
-};
-
-/* I don't want the kernel's namespace to be polluted with this
- * stuff when this file is included.  --DaveM
- */
-#ifndef __KERNEL__
-
-#define  PRIV 0x00000001
-#define  SYS  0x00000002
-#define  USR  0x00000004
-
-/* Pic.S0 Selection Bit Field Encoding, Ultra-I/II  */
-#define  CYCLE_CNT            0x00000000
-#define  INSTR_CNT            0x00000010
-#define  DISPATCH0_IC_MISS    0x00000020
-#define  DISPATCH0_STOREBUF   0x00000030
-#define  IC_REF               0x00000080
-#define  DC_RD                0x00000090
-#define  DC_WR                0x000000A0
-#define  LOAD_USE             0x000000B0
-#define  EC_REF               0x000000C0
-#define  EC_WRITE_HIT_RDO     0x000000D0
-#define  EC_SNOOP_INV         0x000000E0
-#define  EC_RD_HIT            0x000000F0
-
-/* Pic.S0 Selection Bit Field Encoding, Ultra-III  */
-#define  US3_CYCLE_CNT	      	0x00000000
-#define  US3_INSTR_CNT	      	0x00000010
-#define  US3_DISPATCH0_IC_MISS	0x00000020
-#define  US3_DISPATCH0_BR_TGT	0x00000030
-#define  US3_DISPATCH0_2ND_BR	0x00000040
-#define  US3_RSTALL_STOREQ	0x00000050
-#define  US3_RSTALL_IU_USE	0x00000060
-#define  US3_IC_REF		0x00000080
-#define  US3_DC_RD		0x00000090
-#define  US3_DC_WR		0x000000a0
-#define  US3_EC_REF		0x000000c0
-#define  US3_EC_WR_HIT_RTO	0x000000d0
-#define  US3_EC_SNOOP_INV	0x000000e0
-#define  US3_EC_RD_MISS		0x000000f0
-#define  US3_PC_PORT0_RD	0x00000100
-#define  US3_SI_SNOOP		0x00000110
-#define  US3_SI_CIQ_FLOW	0x00000120
-#define  US3_SI_OWNED		0x00000130
-#define  US3_SW_COUNT_0		0x00000140
-#define  US3_IU_BR_MISS_TAKEN	0x00000150
-#define  US3_IU_BR_COUNT_TAKEN	0x00000160
-#define  US3_DISP_RS_MISPRED	0x00000170
-#define  US3_FA_PIPE_COMPL	0x00000180
-#define  US3_MC_READS_0		0x00000200
-#define  US3_MC_READS_1		0x00000210
-#define  US3_MC_READS_2		0x00000220
-#define  US3_MC_READS_3		0x00000230
-#define  US3_MC_STALLS_0	0x00000240
-#define  US3_MC_STALLS_2	0x00000250
-
-/* Pic.S1 Selection Bit Field Encoding, Ultra-I/II  */
-#define  CYCLE_CNT_D1         0x00000000
-#define  INSTR_CNT_D1         0x00000800
-#define  DISPATCH0_IC_MISPRED 0x00001000
-#define  DISPATCH0_FP_USE     0x00001800
-#define  IC_HIT               0x00004000
-#define  DC_RD_HIT            0x00004800
-#define  DC_WR_HIT            0x00005000
-#define  LOAD_USE_RAW         0x00005800
-#define  EC_HIT               0x00006000
-#define  EC_WB                0x00006800
-#define  EC_SNOOP_CB          0x00007000
-#define  EC_IT_HIT            0x00007800
-
-/* Pic.S1 Selection Bit Field Encoding, Ultra-III  */
-#define  US3_CYCLE_CNT_D1	0x00000000
-#define  US3_INSTR_CNT_D1	0x00000800
-#define  US3_DISPATCH0_MISPRED	0x00001000
-#define  US3_IC_MISS_CANCELLED	0x00001800
-#define  US3_RE_ENDIAN_MISS	0x00002000
-#define  US3_RE_FPU_BYPASS	0x00002800
-#define  US3_RE_DC_MISS		0x00003000
-#define  US3_RE_EC_MISS		0x00003800
-#define  US3_IC_MISS		0x00004000
-#define  US3_DC_RD_MISS		0x00004800
-#define  US3_DC_WR_MISS		0x00005000
-#define  US3_RSTALL_FP_USE	0x00005800
-#define  US3_EC_MISSES		0x00006000
-#define  US3_EC_WB		0x00006800
-#define  US3_EC_SNOOP_CB	0x00007000
-#define  US3_EC_IC_MISS		0x00007800
-#define  US3_RE_PC_MISS		0x00008000
-#define  US3_ITLB_MISS		0x00008800
-#define  US3_DTLB_MISS		0x00009000
-#define  US3_WC_MISS		0x00009800
-#define  US3_WC_SNOOP_CB	0x0000a000
-#define  US3_WC_SCRUBBED	0x0000a800
-#define  US3_WC_WB_WO_READ	0x0000b000
-#define  US3_PC_SOFT_HIT	0x0000c000
-#define  US3_PC_SNOOP_INV	0x0000c800
-#define  US3_PC_HARD_HIT	0x0000d000
-#define  US3_PC_PORT1_RD	0x0000d800
-#define  US3_SW_COUNT_1		0x0000e000
-#define  US3_IU_STAT_BR_MIS_UNTAKEN	0x0000e800
-#define  US3_IU_STAT_BR_COUNT_UNTAKEN	0x0000f000
-#define  US3_PC_MS_MISSES	0x0000f800
-#define  US3_MC_WRITES_0	0x00010800
-#define  US3_MC_WRITES_1	0x00011000
-#define  US3_MC_WRITES_2	0x00011800
-#define  US3_MC_WRITES_3	0x00012000
-#define  US3_MC_STALLS_1	0x00012800
-#define  US3_MC_STALLS_3	0x00013000
-#define  US3_RE_RAW_MISS	0x00013800
-#define  US3_FM_PIPE_COMPLETION	0x00014000
-
-struct vcounter_struct {
-  unsigned long long vcnt0;
-  unsigned long long vcnt1;
-};
-
-#endif /* !(__KERNEL__) */
-
-#endif /* !(PERF_COUNTER_API) */
diff -Naur linux-2.6.25-org/include/asm-sparc64/perfmon.h linux-2.6.25-id/include/asm-sparc64/perfmon.h
--- linux-2.6.25-org/include/asm-sparc64/perfmon.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-sparc64/perfmon.h	2008-04-23 11:22:02.000000000 +0200
@@ -0,0 +1,298 @@
+#ifndef _SPARC64_PERFMON_H_
+#define _SPARC64_PERFMON_H_
+
+#ifdef __KERNEL__
+
+#include <linux/irq.h>
+#include <asm-sparc64/system.h>
+
+#define PFM_ARCH_PMD_STK_ARG	2
+#define PFM_ARCH_PMC_STK_ARG	1
+
+struct pfm_arch_pmu_info {
+	u32 pmu_style;
+};
+
+static inline void pfm_arch_resend_irq(void)
+{
+}
+
+static inline void pfm_arch_serialize(void)
+{
+}
+
+static inline void pfm_arch_unfreeze_pmu(void)
+{
+}
+
+/*
+ * SPARC does not save the PMDs during pfm_arch_intr_freeze_pmu(), thus
+ * this routine needs to do it when switching sets on overflow
+ */
+static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
+						struct pfm_event_set *set)
+{
+	pfm_save_pmds(ctx, set);
+}
+
+extern void pfm_arch_write_pmc(struct pfm_context *ctx,
+			       unsigned int cnum, u64 value);
+extern u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum);
+
+static inline void pfm_arch_write_pmd(struct pfm_context *ctx,
+				      unsigned int cnum, u64 value)
+{
+	u64 pic;
+
+	value &= pfm_pmu_conf->ovfl_mask;
+
+	read_pic(pic);
+
+	switch(cnum) {
+	case 0:
+		pic = (pic & 0xffffffff00000000UL) |
+			(value & 0xffffffffUL);
+		break;
+	case 1:
+		pic = (pic & 0xffffffffUL) |
+			(value << 32UL);
+		break;
+	default:
+		BUG();
+	}
+
+	write_pic(pic);
+}
+
+static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx,
+				    unsigned int cnum)
+{
+	u64 pic;
+
+	read_pic(pic);
+
+	switch(cnum) {
+	case 0:
+		return pic & 0xffffffffUL;
+	case 1:
+		return pic >> 32UL;
+	default:
+		BUG();
+		return 0;
+	}
+}
+
+/*
+ * For some CPUs, the upper bits of a counter must be set in order for the
+ * overflow interrupt to happen. On overflow, the counter has wrapped around,
+ * and the upper bits are cleared. This function may be used to set them back.
+ */
+static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx,
+					   unsigned int cnum)
+{
+	u64 val = pfm_arch_read_pmd(ctx, cnum);
+
+	/* This masks out overflow bit 31 */
+	pfm_arch_write_pmd(ctx, cnum, val);
+}
+
+/*
+ * At certain points, perfmon needs to know if monitoring has been
+ * explicitely started/stopped by user via pfm_start/pfm_stop. The
+ * information is tracked in ctx.flags.started. However on certain
+ * architectures, it may be possible to start/stop directly from
+ * user level with a single assembly instruction bypassing
+ * the kernel. This function must be used to determine by
+ * an arch-specific mean if monitoring is actually started/stopped.
+ */
+static inline int pfm_arch_is_active(struct pfm_context *ctx)
+{
+	return ctx->flags.started;
+}
+
+static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
+					 struct pfm_context *ctx,
+					 struct pfm_event_set *set)
+{
+}
+
+static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
+					struct pfm_context *ctx,
+					struct pfm_event_set *set)
+{
+}
+
+static inline void pfm_arch_ctxswin_thread(struct task_struct *task,
+					   struct pfm_context *ctx,
+					   struct pfm_event_set *set)
+{
+}
+
+int  pfm_arch_is_monitoring_active(struct pfm_context *ctx);
+int  pfm_arch_ctxswout_thread(struct task_struct *task,
+			      struct pfm_context *ctx,
+			      struct pfm_event_set *set);
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
+		   struct pfm_event_set *set);
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
+		    struct pfm_event_set *set);
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
+char *pfm_arch_get_pmu_module_name(void);
+
+static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{
+	pfm_arch_stop(current, ctx, set);
+	/*
+	 * we mark monitoring as stopped to avoid
+	 * certain side effects especially in
+	 * pfm_switch_sets_from_intr() on
+	 * pfm_arch_restore_pmcs()
+	 */
+	ctx->flags.started = 0;
+}
+
+/*
+ * unfreeze PMU from pfm_do_interrupt_handler()
+ * ctx may be NULL for spurious
+ */
+static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
+{
+	if (!ctx)
+		return;
+
+	PFM_DBG_ovfl("state=%d", ctx->state);
+
+	ctx->flags.started = 1;
+
+	if (ctx->state == PFM_CTX_MASKED)
+		return;
+
+	pfm_arch_restore_pmcs(ctx, ctx->active_set);
+}
+
+static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
+{
+	return 0;
+}
+
+/*
+ * this function is called from the PMU interrupt handler ONLY.
+ * On SPARC, the PMU is frozen via arch_stop, masking would be implemented
+ * via arch-stop as well. Given that the PMU is already stopped when
+ * entering the interrupt handler, we do not need to stop it again, so
+ * this function is a nop.
+ */
+static inline void pfm_arch_mask_monitoring(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{
+}
+
+/*
+ * on MIPS masking/unmasking uses the start/stop mechanism, so we simply
+ * need to start here.
+ */
+static inline void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
+					      struct pfm_event_set *set)
+{
+	pfm_arch_start(current, ctx, set);
+}
+
+static inline void pfm_arch_pmu_config_remove(void)
+{
+}
+
+static inline int pfm_arch_context_create(struct pfm_context *ctx,
+					  u32 ctx_flags)
+{
+	return 0;
+}
+
+static inline void pfm_arch_context_free(struct pfm_context *ctx)
+{
+}
+
+/*
+ * function called from pfm_setfl_sane(). Context is locked
+ * and interrupts are masked.
+ * The value of flags is the value of ctx_flags as passed by
+ * user.
+ *
+ * function must check arch-specific set flags.
+ * Return:
+ * 	1 when flags are valid
+ *      0 on error
+ */
+static inline int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
+{
+	return 0;
+}
+
+static inline int pfm_arch_init(void)
+{
+	return 0;
+}
+
+static inline void pfm_arch_init_percpu(void)
+{
+}
+
+static inline int pfm_arch_load_context(struct pfm_context *ctx,
+					struct pfm_event_set *set,
+					struct task_struct *task)
+{
+	return 0;
+}
+
+static inline int pfm_arch_unload_context(struct pfm_context *ctx,
+					  struct task_struct *task)
+{
+	return 0;
+}
+
+extern void perfmon_interrupt(struct pt_regs *);
+
+static inline int pfm_arch_pmu_acquire(void)
+{
+	return register_perfctr_intr(perfmon_interrupt);
+}
+
+static inline void pfm_arch_pmu_release(void)
+{
+	release_perfctr_intr(perfmon_interrupt);
+}
+
+/*
+ * not used for sparc
+ */
+static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
+					       size_t rsize, struct file *filp)
+{
+	return -EINVAL;
+}
+
+static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+					   char __user *buf,
+					   int non_block,
+					   size_t size)
+{
+	return -EINVAL;
+}
+struct pfm_arch_context {
+	/* empty */
+};
+
+#define PFM_ARCH_CTX_SIZE	sizeof(struct pfm_arch_context)
+/*
+ * SPARC needs extra alignment for the sampling buffer
+ */
+#define PFM_ARCH_SMPL_ALIGN_SIZE	(16 * 1024)
+
+static inline void pfm_cacheflush(void *addr, unsigned int len)
+{
+}
+
+#endif /* __KERNEL__ */
+
+#endif /* _SPARC64_PERFMON_H_ */
diff -Naur linux-2.6.25-org/include/asm-sparc64/perfmon_const.h linux-2.6.25-id/include/asm-sparc64/perfmon_const.h
--- linux-2.6.25-org/include/asm-sparc64/perfmon_const.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-sparc64/perfmon_const.h	2008-04-23 11:22:02.000000000 +0200
@@ -0,0 +1,7 @@
+#ifndef _SPARC64_PERFMON_CONST_H_
+#define _SPARC64_PERFMON_CONST_H_
+
+#define PFM_ARCH_MAX_PMCS	2
+#define PFM_ARCH_MAX_PMDS	3
+
+#endif /* _SPARC64_PERFMON_CONST_H_ */
diff -Naur linux-2.6.25-org/include/asm-sparc64/system.h linux-2.6.25-id/include/asm-sparc64/system.h
--- linux-2.6.25-org/include/asm-sparc64/system.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-sparc64/system.h	2008-04-23 11:22:02.000000000 +0200
@@ -32,6 +32,10 @@
 
 extern char reboot_command[];
 
+extern char *sparc_cpu_type;
+extern char *sparc_fpu_type;
+extern char *sparc_pmu_type;
+
 /* These are here in an effort to more fully work around Spitfire Errata
  * #51.  Essentially, if a memory barrier occurs soon after a mispredicted
  * branch, the chip can stop executing instructions until a trap occurs.
@@ -104,15 +108,13 @@
 #define write_pcr(__p) __asm__ __volatile__("wr	%0, 0x0, %%pcr" : : "r" (__p))
 #define read_pic(__p)  __asm__ __volatile__("rd %%pic, %0" : "=r" (__p))
 
-/* Blackbird errata workaround.  See commentary in
- * arch/sparc64/kernel/smp.c:smp_percpu_timer_interrupt()
- * for more information.
- */
-#define reset_pic()    						\
-	__asm__ __volatile__("ba,pt	%xcc, 99f\n\t"		\
+/* Blackbird errata workaround.  */
+#define write_pic(val)    					\
+	__asm__ __volatile__("ba,pt	%%xcc, 99f\n\t"		\
 			     ".align	64\n"			\
-			  "99:wr	%g0, 0x0, %pic\n\t"	\
-			     "rd	%pic, %g0")
+			  "99:wr	%0, 0x0, %%pic\n\t"	\
+			     "rd	%%pic, %%g0" : : "r" (val))
+#define reset_pic()	write_pic(0)
 
 #ifndef __ASSEMBLY__
 
@@ -145,14 +147,9 @@
 	 * and 2 stores in this critical code path.  -DaveM
 	 */
 #define switch_to(prev, next, last)					\
-do {	if (test_thread_flag(TIF_PERFCTR)) {				\
-		unsigned long __tmp;					\
-		read_pcr(__tmp);					\
-		current_thread_info()->pcr_reg = __tmp;			\
-		read_pic(__tmp);					\
-		current_thread_info()->kernel_cntd0 += (unsigned int)(__tmp);\
-		current_thread_info()->kernel_cntd1 += ((__tmp) >> 32);	\
-	}								\
+do {	if (test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW)		\
+	    || test_tsk_thread_flag(next, TIF_PERFMON_CTXSW))		\
+		pfm_ctxsw(prev, next);					\
 	flush_tlb_pending();						\
 	save_and_clear_fpu();						\
 	/* If you are tempted to conditionalize the following */	\
@@ -196,11 +193,6 @@
 	        "l1", "l2", "l3", "l4", "l5", "l6", "l7",		\
 	  "i0", "i1", "i2", "i3", "i4", "i5",				\
 	  "o0", "o1", "o2", "o3", "o4", "o5",       "o7");		\
-	/* If you fuck with this, update ret_from_syscall code too. */	\
-	if (test_thread_flag(TIF_PERFCTR)) {				\
-		write_pcr(current_thread_info()->pcr_reg);		\
-		reset_pic();						\
-	}								\
 } while(0)
 
 static inline unsigned long xchg32(__volatile__ unsigned int *m, unsigned int val)
diff -Naur linux-2.6.25-org/include/asm-sparc64/thread_info.h linux-2.6.25-id/include/asm-sparc64/thread_info.h
--- linux-2.6.25-org/include/asm-sparc64/thread_info.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-sparc64/thread_info.h	2008-04-23 11:22:02.000000000 +0200
@@ -59,11 +59,6 @@
 	unsigned long		gsr[7];
 	unsigned long		xfsr[7];
 
-	__u64			__user *user_cntd0;
-	__u64			__user *user_cntd1;
-	__u64			kernel_cntd0, kernel_cntd1;
-	__u64			pcr_reg;
-
 	struct restart_block	restart_block;
 
 	struct pt_regs		*kern_una_regs;
@@ -97,15 +92,10 @@
 #define TI_RWIN_SPTRS	0x000003c8	
 #define TI_GSR		0x00000400
 #define TI_XFSR		0x00000438
-#define TI_USER_CNTD0	0x00000470
-#define TI_USER_CNTD1	0x00000478
-#define TI_KERN_CNTD0	0x00000480
-#define TI_KERN_CNTD1	0x00000488
-#define TI_PCR		0x00000490
-#define TI_RESTART_BLOCK 0x00000498
-#define TI_KUNA_REGS	0x000004c0
-#define TI_KUNA_INSN	0x000004c8
-#define TI_FPREGS	0x00000500
+#define TI_RESTART_BLOCK 0x00000470
+#define TI_KUNA_REGS	0x00000498
+#define TI_KUNA_INSN	0x000004a0
+#define TI_FPREGS	0x000004c0
 
 /* We embed this in the uppermost byte of thread_info->flags */
 #define FAULT_CODE_WRITE	0x01	/* Write access, implies D-TLB	   */
@@ -221,11 +211,11 @@
 #define TIF_RESTORE_SIGMASK	1	/* restore signal mask in do_signal() */
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
-#define TIF_PERFCTR		4	/* performance counters active */
+/* Bit 4 is available */
 #define TIF_UNALIGNED		5	/* allowed to do unaligned accesses */
 #define TIF_NEWSIGNALS		6	/* wants new-style signals */
 #define TIF_32BIT		7	/* 32-bit binary */
-/* flag bit 8 is available */
+#define TIF_PERFMON_WORK	8	/* work for pfm_handle_work() */
 #define TIF_SECCOMP		9	/* secure computing */
 #define TIF_SYSCALL_AUDIT	10	/* syscall auditing active */
 /* flag bit 11 is available */
@@ -236,23 +226,25 @@
 #define TIF_ABI_PENDING		12
 #define TIF_MEMDIE		13
 #define TIF_POLLING_NRFLAG	14
+#define TIF_PERFMON_CTXSW	15	/* perfmon needs ctxsw calls */
 
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
 #define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
-#define _TIF_PERFCTR		(1<<TIF_PERFCTR)
 #define _TIF_UNALIGNED		(1<<TIF_UNALIGNED)
 #define _TIF_NEWSIGNALS		(1<<TIF_NEWSIGNALS)
 #define _TIF_32BIT		(1<<TIF_32BIT)
+#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
 #define _TIF_SECCOMP		(1<<TIF_SECCOMP)
 #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_ABI_PENDING	(1<<TIF_ABI_PENDING)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
+#define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
 
 #define _TIF_USER_WORK_MASK	((0xff << TI_FLAG_WSAVED_SHIFT) | \
 				 (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK | \
-				  _TIF_NEED_RESCHED | _TIF_PERFCTR))
+				  _TIF_NEED_RESCHED))
 
 #endif /* __KERNEL__ */
 
diff -Naur linux-2.6.25-org/include/asm-x86/hw_irq_64.h linux-2.6.25-id/include/asm-x86/hw_irq_64.h
--- linux-2.6.25-org/include/asm-x86/hw_irq_64.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-x86/hw_irq_64.h	2008-04-23 11:22:10.000000000 +0200
@@ -84,6 +84,7 @@
  * sources per level' errata.
  */
 #define LOCAL_TIMER_VECTOR	0xef
+#define LOCAL_PERFMON_VECTOR	0xee
 
 /*
  * First APIC vector available to drivers: (vectors 0x30-0xee)
@@ -91,7 +92,7 @@
  * levels. (0x80 is the syscall vector)
  */
 #define FIRST_DEVICE_VECTOR	(IRQ15_VECTOR + 2)
-#define FIRST_SYSTEM_VECTOR	0xef   /* duplicated in irq.h */
+#define FIRST_SYSTEM_VECTOR	0xee   /* duplicated in irq.h */
 
 
 #ifndef __ASSEMBLY__
diff -Naur linux-2.6.25-org/include/asm-x86/irq_64.h linux-2.6.25-id/include/asm-x86/irq_64.h
--- linux-2.6.25-org/include/asm-x86/irq_64.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-x86/irq_64.h	2008-04-23 11:22:10.000000000 +0200
@@ -29,7 +29,7 @@
  */
 #define NR_VECTORS 256
 
-#define FIRST_SYSTEM_VECTOR	0xef   /* duplicated in hw_irq.h */
+#define FIRST_SYSTEM_VECTOR	0xee   /* duplicated in hw_irq.h */
 
 #define NR_IRQS (NR_VECTORS + (32 *NR_CPUS))
 #define NR_IRQ_VECTORS NR_IRQS
diff -Naur linux-2.6.25-org/include/asm-x86/mach-default/entry_arch.h linux-2.6.25-id/include/asm-x86/mach-default/entry_arch.h
--- linux-2.6.25-org/include/asm-x86/mach-default/entry_arch.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-x86/mach-default/entry_arch.h	2008-04-23 11:22:10.000000000 +0200
@@ -31,4 +31,8 @@
 BUILD_INTERRUPT(thermal_interrupt,THERMAL_APIC_VECTOR)
 #endif
 
+#ifdef CONFIG_PERFMON
+BUILD_INTERRUPT(pmu_interrupt,LOCAL_PERFMON_VECTOR)
+#endif
+
 #endif
diff -Naur linux-2.6.25-org/include/asm-x86/mach-default/irq_vectors.h linux-2.6.25-id/include/asm-x86/mach-default/irq_vectors.h
--- linux-2.6.25-org/include/asm-x86/mach-default/irq_vectors.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-x86/mach-default/irq_vectors.h	2008-04-23 11:22:10.000000000 +0200
@@ -56,6 +56,7 @@
  * sources per level' errata.
  */
 #define LOCAL_TIMER_VECTOR	0xef
+#define LOCAL_PERFMON_VECTOR	0xee
 
 /*
  * First APIC vector available to drivers: (vectors 0x30-0xee)
@@ -63,7 +64,7 @@
  * levels. (0x80 is the syscall vector)
  */
 #define FIRST_DEVICE_VECTOR	0x31
-#define FIRST_SYSTEM_VECTOR	0xef
+#define FIRST_SYSTEM_VECTOR	0xee
 
 #define TIMER_IRQ 0
 
diff -Naur linux-2.6.25-org/include/asm-x86/msr-index.h linux-2.6.25-id/include/asm-x86/msr-index.h
--- linux-2.6.25-org/include/asm-x86/msr-index.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-x86/msr-index.h	2008-04-23 11:22:10.000000000 +0200
@@ -83,6 +83,7 @@
 /* AMD64 MSRs. Not complete. See the architecture manual for a more
    complete list. */
 
+#define MSR_AMD64_NB_CFG		0xc001001f
 #define MSR_AMD64_IBSFETCHCTL		0xc0011030
 #define MSR_AMD64_IBSFETCHLINAD		0xc0011031
 #define MSR_AMD64_IBSFETCHPHYSAD	0xc0011032
diff -Naur linux-2.6.25-org/include/asm-x86/perfmon.h linux-2.6.25-id/include/asm-x86/perfmon.h
--- linux-2.6.25-org/include/asm-x86/perfmon.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-x86/perfmon.h	2008-04-23 11:22:10.000000000 +0200
@@ -0,0 +1,520 @@
+/*
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * Copyright (c) 2007 Advanced Micro Devices, Inc.
+ * Contributed by Robert Richter <robert.richter@amd.com>
+ *
+ * This file contains X86 Processor Family specific definitions
+ * for the perfmon interface. This covers P6, Pentium M, P4/Xeon
+ * (32-bit and 64-bit, i.e., EM64T) and AMD X86-64.
+ *
+ * This file MUST never be included directly. Use linux/perfmon.h.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#ifndef _ASM_X86_PERFMON_H_
+#define _ASM_X86_PERFMON_H_
+
+#ifdef __KERNEL__
+
+#ifdef CONFIG_4KSTACKS
+#define PFM_ARCH_PMD_STK_ARG	2
+#define PFM_ARCH_PMC_STK_ARG	2
+#else
+#define PFM_ARCH_PMD_STK_ARG	4 /* about 700 bytes of stack space */
+#define PFM_ARCH_PMC_STK_ARG	4 /* about 200 bytes of stack space */
+#endif
+
+/*
+ * For P4:
+ * - bits 31 - 63 reserved
+ * - T1_OS and T1_USR bits are reserved - set depending on logical proc
+ *      user mode application should use T0_OS and T0_USR to indicate
+ * RSVD: reserved bits must be 1
+ */
+#define PFM_ESCR_RSVD  ~0x000000007ffffffcULL
+
+/*
+ * bitmask for reg_type
+ */
+#define PFM_REGT_NA		0x0000	/* not available */
+#define PFM_REGT_EN		0x0001	/* has enable bit (cleared on ctxsw) */
+#define PFM_REGT_ESCR		0x0002	/* P4: ESCR */
+#define PFM_REGT_CCCR		0x0004	/* P4: CCCR */
+#define PFM_REGT_PEBS		0x0010	/* PEBS related */
+#define PFM_REGT_NOHT		0x0020	/* unavailable with HT */
+#define PFM_REGT_CTR		0x0040	/* counter */
+#define PFM_REGT_OTH		0x0080	/* other type of register */
+#define PFM_REGT_IBS		0x0100	/* IBS register set */
+#define PFM_REGT_IBS_EXT	0x0200	/* IBS extended register set */
+
+/*
+ * This design and the partitioning of resources for SMT (hyper threads)
+ * is very static and limited due to limitations in the number of ESCRs
+ * and CCCRs per group.
+ */
+#define MAX_SMT_ID 1
+
+/*
+ * For extended register information in addition to address that is used
+ * at runtime to figure out the mapping of reg addresses to logical procs
+ * and association of registers to hardware specific features
+ */
+struct pfm_arch_ext_reg {
+	/*
+	 * one each for the logical CPUs.  Index 0 corresponds to T0 and
+	 * index 1 corresponds to T1.  Index 1 can be zero if no T1
+	 * complement reg exists.
+	 */
+	unsigned long addrs[MAX_SMT_ID+1];
+	unsigned int ctr;	/* for CCCR/PERFEVTSEL, associated counter */
+	unsigned int reg_type;
+};
+
+typedef int (*pfm_check_session_t)(struct pfm_context *ctx);
+
+struct pfm_arch_pmu_info {
+	struct pfm_arch_ext_reg pmc_addrs[PFM_MAX_PMCS];
+	struct pfm_arch_ext_reg pmd_addrs[PFM_MAX_PMDS];
+	u64 enable_mask[PFM_PMC_BV]; /* PMC registers with enable bit */
+
+	u16 max_ena;		/* highest enable bit + 1 */
+	u16 flags;		/* PMU feature flags */
+	u16 pebs_ctr_idx;	/* index of PEBS counter for overflow */
+	u16 reserved;		/* for future use */
+
+	/*
+	 * optional callbacks invoked by pfm_arch_*load_context()
+	 */
+	int (*load_context)(struct pfm_context *ctx);
+	int (*unload_context)(struct pfm_context *ctx);
+
+	u16 ibsfetchctl_pmc;	/* AMD: index of IBSFETCHCTL PMC register */
+	u16 ibsfetchctl_pmd;	/* AMD: index of IBSFETCHCTL PMD register */
+	u16 ibsopctl_pmc;	/* AMD: index of IBSOPCTL PMC register */
+	u16 ibsopctl_pmd;	/* AMD: index of IBSOPCTL PMD register */
+	u8  ibs_eilvt_off;	/* AMD: extended interrupt LVT offset */
+	u8  pmu_style;		/* type of PMU: P4, P6, CORE, AMD64 */
+};
+
+/*
+ * X86 PMU style
+ */
+#define PFM_X86_PMU_P4		1 /* Intel P4/Xeon/EM64T processor PMU */
+#define PFM_X86_PMU_P6		2 /* Intel P6/Pentium M */
+#define PFM_X86_PMU_CORE	3 /* Intel Core PMU */
+#define PFM_X86_PMU_AMD64	4 /* AMD64 PMU (K8, family 10h) */
+
+/*
+ * PMU feature flags
+ */
+#define PFM_X86_FL_PMU_DS	0x01	/* Intel: support for Data Save Area (DS) */
+#define PFM_X86_FL_PMU_PEBS	0x02	/* Intel: support PEBS (implies DS) */
+#define PFM_X86_FL_USE_NMI	0x04	/* user asking for NMI */
+#define PFM_X86_FL_NO_SHARING	0x08	/* no sharing with other subsystems */
+#define PFM_X86_FL_SHARING	0x10	/* PMU is being shared */
+#define PFM_X86_FL_IBS		0x20	/* AMD: PMU has IBS support */
+#define PFM_X86_FL_IBS_EXT	0x40	/* AMD: PMU has IBSext support */
+#define PFM_X86_FL_USE_EI	0x80	/* AMD: PMU uses extended interrupts */
+
+/*
+ * architecture specific context extension.
+ * located at: (struct pfm_arch_context *)(ctx+1)
+ */
+
+struct pfm_arch_p4_context {
+	u32	npend_ovfls;	/* P4 NMI #pending ovfls */
+	u32	reserved;
+	u64	povfl_pmds[PFM_PMD_BV]; /* P4 NMI overflowed counters */
+	u64	saved_cccrs[PFM_MAX_PMCS];
+};
+
+struct pfm_x86_context_flags {
+	unsigned int insecure:1;  /* insecure monitoring for non-self session */
+	unsigned int use_pebs:1;  /* PEBS used */
+	unsigned int use_ds:1;    /* DS used */
+	unsigned int reserved:29; /* for future use */
+};
+
+struct pfm_arch_context {
+	u64				saved_real_iip;	/* instr pointer of last NMI intr (ctxsw) */
+	struct pfm_x86_context_flags	flags;		/* arch-specific flags */
+	void				*ds_area;	/* address of DS management area */
+	struct pfm_arch_p4_context	*p4;		/* P4 specific state */
+};
+
+void __pfm_read_reg_p4(const struct pfm_arch_ext_reg *xreg, u64 *val);
+void __pfm_write_reg_p4(const struct pfm_arch_ext_reg *xreg, u64 val);
+
+
+extern int  pfm_arch_init(void);
+extern void pfm_arch_resend_irq(void);
+
+static inline void pfm_arch_serialize(void)
+{}
+
+/*
+ * on x86, the PMDs are already saved by pfm_arch_freeze_pmu()
+ * when entering the PMU interrupt handler, thus, we do not need
+ * to save them again in pfm_switch_sets_from_intr()
+ */
+static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
+						struct pfm_event_set *set)
+{}
+
+/*
+ * in certain situations, ctx may be NULL
+ */
+static inline void pfm_arch_write_pmc(struct pfm_context *ctx, unsigned int cnum, u64 value)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	/*
+	 * we only write to the actual register when monitoring is
+	 * active (pfm_start was issued)
+	 */
+	if (ctx && ctx->flags.started == 0)
+		return;
+
+	PFM_DBG_ovfl("pfm_arch_write_pmc(0x%lx, 0x%Lx)",
+		     pfm_pmu_conf->pmc_desc[cnum].hw_addr,
+		     (unsigned long long) value);
+
+	if (arch_info->pmu_style == PFM_X86_PMU_P4)
+		__pfm_write_reg_p4(&arch_info->pmc_addrs[cnum], value);
+	else
+		wrmsrl(pfm_pmu_conf->pmc_desc[cnum].hw_addr, value);
+}
+
+static inline void pfm_arch_write_pmd(struct pfm_context *ctx, unsigned int cnum, u64 value)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+
+	/*
+	 * to make sure the counter overflows, we set the bits from
+	 * bit 31 till the width of the counters.
+	 * We clear any other unimplemented bits.
+	 */
+	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_C64)
+		value = (value | ~pfm_pmu_conf->ovfl_mask)
+		      & ~pfm_pmu_conf->pmd_desc[cnum].rsvd_msk;
+
+	PFM_DBG_ovfl("pfm_arch_write_pmd(0x%lx, 0x%Lx)",
+		     pfm_pmu_conf->pmd_desc[cnum].hw_addr,
+		     (unsigned long long) value);
+
+	if (arch_info->pmu_style == PFM_X86_PMU_P4)
+		__pfm_write_reg_p4(&arch_info->pmd_addrs[cnum], value);
+	else
+		wrmsrl(pfm_pmu_conf->pmd_desc[cnum].hw_addr, value);
+}
+
+static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	u64 tmp;
+	if (arch_info->pmu_style == PFM_X86_PMU_P4)
+		__pfm_read_reg_p4(&arch_info->pmd_addrs[cnum], &tmp);
+	else
+		rdmsrl(pfm_pmu_conf->pmd_desc[cnum].hw_addr, tmp);
+
+	PFM_DBG_ovfl("pfm_arch_read_pmd(0x%lx) = 0x%Lx",
+		     pfm_pmu_conf->pmd_desc[cnum].hw_addr,
+		     (unsigned long long) tmp);
+	return tmp;
+}
+
+static inline u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	u64 tmp;
+	if (arch_info->pmu_style == PFM_X86_PMU_P4)
+		__pfm_read_reg_p4(&arch_info->pmc_addrs[cnum], &tmp);
+	else
+		rdmsrl(pfm_pmu_conf->pmc_desc[cnum].hw_addr, tmp);
+
+	PFM_DBG_ovfl("pfm_arch_read_pmc(0x%lx) = 0x%016Lx",
+		     pfm_pmu_conf->pmc_desc[cnum].hw_addr,
+		     (unsigned long long) tmp);
+	return tmp;
+}
+
+/*
+ * At certain points, perfmon needs to know if monitoring has been
+ * explicitely started/stopped by user via pfm_start/pfm_stop. The
+ * information is tracked in flags.started. However on certain
+ * architectures, it may be possible to start/stop directly from
+ * user level with a single assembly instruction bypassing
+ * the kernel. This function is used to determine by
+ * an arch-specific mean if monitoring is actually started/stopped.
+ */
+static inline int pfm_arch_is_active(struct pfm_context *ctx)
+{
+	return ctx->flags.started;
+}
+
+static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
+					 struct pfm_context *ctx,
+					 struct pfm_event_set *set)
+{}
+
+static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
+					struct pfm_context *ctx,
+					struct pfm_event_set *set)
+{}
+
+static inline void pfm_arch_init_percpu(void)
+{}
+
+/* not necessary on IA-64 */
+static inline void pfm_cacheflush(void *addr, unsigned int len)
+{}
+
+int  pfm_arch_ctxswout_thread(struct task_struct *task,
+			      struct pfm_context *ctx,
+			      struct pfm_event_set *set);
+
+void pfm_arch_ctxswin_thread(struct task_struct *task,
+			     struct pfm_context *ctx,
+			     struct pfm_event_set *set);
+
+void pfm_arch_stop(struct task_struct *task,
+		   struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_start(struct task_struct *task,
+		    struct pfm_context *ctx, struct pfm_event_set *set);
+
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
+int  pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg);
+void pfm_arch_pmu_config_remove(void);
+char *pfm_arch_get_pmu_module_name(void);
+
+static inline int pfm_arch_unload_context(struct pfm_context *ctx,
+					  struct task_struct *task)
+{
+	struct pfm_arch_pmu_info *arch_info;
+	struct pfm_arch_context *ctx_arch;
+	int ret = 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	arch_info = pfm_pmu_conf->arch_info;
+	if (arch_info->unload_context) {
+		ret = arch_info->unload_context(ctx);
+	}
+
+	if (ctx_arch->flags.insecure) {
+		PFM_DBG("clear cr4.pce");
+		clear_in_cr4(X86_CR4_PCE);
+	}
+
+	return ret;
+}
+
+static inline int pfm_arch_load_context(struct pfm_context *ctx,
+					struct pfm_event_set *set,
+					struct task_struct *task)
+{
+	struct pfm_arch_pmu_info *arch_info;
+	struct pfm_arch_context *ctx_arch;
+	int ret = 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * RDPMC is automatically authorized in system-wide and
+	 * also in self-monitoring per-thread context.
+	 * It may be authorized in other situations if the
+	 * PFM_X86_FL_INSECURE flags was set
+	 */
+	if (ctx->flags.system || task == current) {
+		PFM_DBG("set cr4.pce");
+		set_in_cr4(X86_CR4_PCE);
+		ctx_arch->flags.insecure = 1;
+	}
+
+	arch_info = pfm_pmu_conf->arch_info;
+	if (arch_info->load_context) {
+		ret = arch_info->load_context(ctx);
+	}
+	return ret;
+}
+
+/*
+ * this function is called from the PMU interrupt handler ONLY.
+ * On x86, the PMU is frozen via arch_stop, masking would be implemented
+ * via arch-stop as well. Given that the PMU is already stopped when
+ * entering the interrupt handler, we do not need to stop it again, so
+ * this function is a nop.
+ */
+static inline void pfm_arch_mask_monitoring(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{}
+
+/*
+ * on x86 masking/unmasking uses the start/stop mechanism, so we simply
+ * need to start here.
+ */
+static inline void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
+					      struct pfm_event_set *set)
+{
+	pfm_arch_start(current, ctx, set);
+}
+
+/*
+ * called from __pfm_interrupt_handler(). ctx is not NULL.
+ * ctx is locked. interrupts are masked
+ *
+ * The following actions must take place:
+ *  - stop all monitoring to ensure handler has consistent view.
+ *  - collect overflowed PMDs bitmask into povfls_pmds and
+ *    npend_ovfls. If no interrupt detected then npend_ovfls
+ *    must be set to zero.
+ */
+static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{
+	/*
+	 * on X86, freezing is equivalent to stopping
+	 */
+	pfm_arch_stop(current, ctx, set);
+
+	/*
+	 * we mark monitoring as stopped to avoid
+	 * certain side effects especially in
+	 * pfm_switch_sets_from_intr() and
+	 * pfm_arch_restore_pmcs()
+	 */
+	ctx->flags.started = 0;
+}
+
+/*
+ * unfreeze PMU from pfm_do_interrupt_handler().
+ * ctx may be NULL for spurious interrupts.
+ * interrupts are masked.
+ */
+static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
+{
+	if (ctx == NULL)
+		return;
+
+	PFM_DBG_ovfl("state=%d", ctx->state);
+
+	/*
+	 * restore flags.started which is cleared in
+	 * pfm_arch_intr_freeze_pmu()
+	 */
+	ctx->flags.started = 1;
+
+	if (ctx->state == PFM_CTX_MASKED)
+		return;
+
+	pfm_arch_restore_pmcs(ctx, ctx->active_set);
+}
+
+
+/*
+ * function called from pfm_setfl_sane(). Context is locked
+ * and interrupts are masked.
+ * The value of flags is the value of ctx_flags as passed by
+ * user.
+ *
+ * function must check arch-specific set flags.
+ * Return:
+ *      1 when flags are valid
+ *      0 on error
+ */
+static inline int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
+{
+	return 0;
+}
+
+int pfm_arch_pmu_acquire(void);
+void pfm_arch_pmu_release(void);
+
+/*
+ * For some CPUs, the upper bits of a counter must be set in order for the
+ * overflow interrupt to happen. On overflow, the counter has wrapped around,
+ * and the upper bits are cleared. This function may be used to set them back.
+ *
+ * x86: The current version loses whatever is remaining in the counter,
+ * which is usually has a small count. In order not to loose this count,
+ * we do a read-modify-write to set the upper bits while preserving the
+ * low-order bits. This is slow but works.
+ */
+static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx, unsigned int cnum)
+{
+	u64 val;
+	val = pfm_arch_read_pmd(ctx, cnum);
+	pfm_arch_write_pmd(ctx, cnum, val);
+}
+
+/*
+ * not used for i386/x86_64
+ */
+static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
+					       size_t rsize, struct file *filp)
+{
+	return -EINVAL;
+}
+static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size)
+{
+	return -EINVAL;
+}
+
+static inline int pfm_arch_context_create(struct pfm_context *ctx, u32 ctx_flags)
+{
+	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
+	struct pfm_arch_context *ctx_arch;
+
+	if (arch_info->pmu_style != PFM_X86_PMU_P4)
+		return 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	ctx_arch->p4 = kzalloc(sizeof(*(ctx_arch->p4)), GFP_KERNEL);
+	if (!ctx_arch->p4)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static inline void pfm_arch_context_free(struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * we do not check if P4, because it would be NULL and
+	 * kfree can deal with NULL
+	 */
+	kfree(ctx_arch->p4);
+}
+
+#define PFM_ARCH_CTX_SIZE	(sizeof(struct pfm_arch_context))
+/*
+ * i386/x86_64 do not need extra alignment requirements for the sampling buffer
+ */
+#define PFM_ARCH_SMPL_ALIGN_SIZE	0
+
+asmlinkage void  pmu_interrupt(void);
+
+#endif /* __KERNEL__ */
+
+#endif /* _ASM_X86_PERFMON_H_ */
diff -Naur linux-2.6.25-org/include/asm-x86/perfmon_const.h linux-2.6.25-id/include/asm-x86/perfmon_const.h
--- linux-2.6.25-org/include/asm-x86/perfmon_const.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-x86/perfmon_const.h	2008-04-23 11:22:10.000000000 +0200
@@ -0,0 +1,30 @@
+/*
+ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file contains i386/x86_64 specific definitions for the perfmon
+ * interface.
+ *
+ * This file MUST never be included directly. Use linux/perfmon.h.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#ifndef _ASM_X86_PERFMON_CONST_H_
+#define _ASM_X86_PERFMON_CONST_H_
+
+#define PFM_ARCH_MAX_PMCS	(256+64) /* 256 HW 64 SW */
+#define PFM_ARCH_MAX_PMDS	(256+64) /* 256 HW 64 SW */
+
+#endif /* _ASM_X86_PERFMON_CONST_H_ */
diff -Naur linux-2.6.25-org/include/asm-x86/perfmon_pebs_core_smpl.h linux-2.6.25-id/include/asm-x86/perfmon_pebs_core_smpl.h
--- linux-2.6.25-org/include/asm-x86/perfmon_pebs_core_smpl.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-x86/perfmon_pebs_core_smpl.h	2008-04-23 11:22:10.000000000 +0200
@@ -0,0 +1,164 @@
+/*
+ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ *
+ * This file implements the sampling format to support Intel
+ * Precise Event Based Sampling (PEBS) feature of Intel Core
+ * processors, such as Intel Core 2.
+ *
+ * What is PEBS?
+ * ------------
+ *  This is a hardware feature to enhance sampling by providing
+ *  better precision as to where a sample is taken. This avoids the
+ *  typical skew in the instruction one can observe with any
+ *  interrupt-based sampling technique.
+ *
+ *  PEBS also lowers sampling overhead significantly by having the
+ *  processor store samples instead of the OS. PMU interrupt are only
+ *  generated after multiple samples are written.
+ *
+ *  Another benefit of PEBS is that samples can be captured inside
+ *  critical sections where interrupts are masked.
+ *
+ * How does it work?
+ *  PEBS effectively implements a Hw buffer. The Os must pass a region
+ *  of memory where samples are to be stored. The region can have any
+ *  size. The OS must also specify the sampling period to reload. The PMU
+ *  will interrupt when it reaches the end of the buffer or a specified
+ *  threshold location inside the memory region.
+ *
+ *  The description of the buffer is stored in the Data Save Area (DS).
+ *  The samples are stored sequentially in the buffer. The format of the
+ *  buffer is fixed and specified in the PEBS documentation.  The sample
+ *  format does not change between 32-bit and 64-bit modes unlike on the
+ *  Pentium 4 version of PEBS.
+ *
+ *  PEBS does not work when HyperThreading is enabled due to certain MSR
+ *  being shared being to two threads.
+ *
+ *  What does the format do?
+ *   It provides access to the PEBS feature for both 32-bit and 64-bit
+ *   processors that support it.
+ *
+ *   The same code and data structures are used for both 32-bit and 64-bi
+ *   modes. A single format name is used for both modes. In 32-bit mode,
+ *   some of the extended registers are written to zero in each sample.
+ *
+ *   It is important to realize that the format provides a zero-copy
+ *   environment for the samples, i.e,, the OS never touches the
+ *   samples. Whatever the processor write is directly accessible to
+ *   the user.
+ *
+ *   Parameters to the buffer can be passed via pfm_create_context() in
+ *   the pfm_pebs_smpl_arg structure.
+ */
+#ifndef __PERFMON_PEBS_CORE_SMPL_H__
+#define __PERFMON_PEBS_CORE_SMPL_H__ 1
+
+/*
+ * The 32-bit and 64-bit formats are identical, thus we use only
+ * one name for the format.
+ */
+#define PFM_PEBS_CORE_SMPL_NAME	"pebs_core"
+
+/*
+ * format specific parameters (passed at context creation)
+ *
+ * intr_thres: index from start of buffer of entry where the
+ * PMU interrupt must be triggered. It must be several samples
+ * short of the end of the buffer.
+ */
+struct pfm_pebs_core_smpl_arg {
+	u64 cnt_reset;	  /* counter reset value */
+	size_t buf_size;  /* size of the PEBS buffer in bytes */
+	size_t intr_thres;/* index of PEBS interrupt threshold entry */
+	u64 reserved[6];  /* for future use */
+};
+
+/*
+ * Data Save Area (32 and 64-bit mode)
+ *
+ * The DS area is exposed to the user. To determine the number
+ * of samples available in PEBS, it is necessary to substract
+ * pebs_index from pebs_base.
+ *
+ * Layout of the structure is mandated by hardware and specified
+ * in the Intel documentation.
+ */
+struct pfm_ds_area_core {
+	u64 bts_buf_base;
+	u64 bts_index;
+	u64 bts_abs_max;
+	u64 bts_intr_thres;
+	u64 pebs_buf_base;
+	u64 pebs_index;
+	u64 pebs_abs_max;
+	u64 pebs_intr_thres;
+	u64 pebs_cnt_reset;
+};
+
+/*
+ * This header is at the beginning of the sampling buffer returned to the user.
+ *
+ * Because of PEBS alignement constraints, the actual PEBS buffer area does
+ * not necessarily begin right after the header. The hdr_start_offs must be
+ * used to compute the first byte of the buffer. The offset is defined as
+ * the number of bytes between the end of the header and the beginning of
+ * the buffer. As such the formula is:
+ * 	actual_buffer = (unsigned long)(hdr+1)+hdr->hdr_start_offs
+ */
+struct pfm_pebs_core_smpl_hdr {
+	u64 overflows;			/* #overflows for buffer */
+	size_t buf_size;		/* bytes in the buffer */
+	size_t start_offs; 		/* actual buffer start offset */
+	u32 version;			/* smpl format version */
+	u32 reserved1;			/* for future use */
+	u64 reserved2[5];		/* for future use */
+	struct pfm_ds_area_core ds;	/* data save area */
+};
+
+/*
+ * Sample format as mandated by Intel documentation.
+ * The same format is used in both 32 and 64 bit modes.
+ */
+struct pfm_pebs_core_smpl_entry {
+	u64	eflags;
+	u64	ip;
+	u64	eax;
+	u64	ebx;
+	u64	ecx;
+	u64	edx;
+	u64	esi;
+	u64	edi;
+	u64	ebp;
+	u64	esp;
+	u64	r8;	/* 0 in 32-bit mode */
+	u64	r9;	/* 0 in 32-bit mode */
+	u64	r10;	/* 0 in 32-bit mode */
+	u64	r11;	/* 0 in 32-bit mode */
+	u64	r12;	/* 0 in 32-bit mode */
+	u64	r13;	/* 0 in 32-bit mode */
+	u64	r14;	/* 0 in 32-bit mode */
+	u64	r15;	/* 0 in 32-bit mode */
+};
+
+#define PFM_PEBS_CORE_SMPL_VERSION_MAJ 1U
+#define PFM_PEBS_CORE_SMPL_VERSION_MIN 0U
+#define PFM_PEBS_CORE_SMPL_VERSION (((PFM_PEBS_CORE_SMPL_VERSION_MAJ&0xffff)<<16)|\
+				   (PFM_PEBS_CORE_SMPL_VERSION_MIN & 0xffff))
+
+#endif /* __PERFMON_PEBS_CORE_SMPL_H__ */
diff -Naur linux-2.6.25-org/include/asm-x86/perfmon_pebs_p4_smpl.h linux-2.6.25-id/include/asm-x86/perfmon_pebs_p4_smpl.h
--- linux-2.6.25-org/include/asm-x86/perfmon_pebs_p4_smpl.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/asm-x86/perfmon_pebs_p4_smpl.h	2008-04-23 11:22:10.000000000 +0200
@@ -0,0 +1,193 @@
+/*
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ *
+ * This file implements the sampling format to support Intel
+ * Precise Event Based Sampling (PEBS) feature of Pentium 4
+ * and other Netburst-based processors. Not to be used for
+ * Intel Core-based processors.
+ *
+ * What is PEBS?
+ * ------------
+ *  This is a hardware feature to enhance sampling by providing
+ *  better precision as to where a sample is taken. This avoids the
+ *  typical skew in the instruction one can observe with any
+ *  interrupt-based sampling technique.
+ *
+ *  PEBS also lowers sampling overhead significantly by having the
+ *  processor store samples instead of the OS. PMU interrupt are only
+ *  generated after multiple samples are written.
+ *
+ *  Another benefit of PEBS is that samples can be captured inside
+ *  critical sections where interrupts are masked.
+ *
+ * How does it work?
+ *  PEBS effectively implements a Hw buffer. The Os must pass a region
+ *  of memory where samples are to be stored. The region can have any
+ *  size. The OS must also specify the sampling period to reload. The PMU
+ *  will interrupt when it reaches the end of the buffer or a specified
+ *  threshold location inside the memory region.
+ *
+ *  The description of the buffer is stored in the Data Save Area (DS).
+ *  The samples are stored sequentially in the buffer. The format of the
+ *  buffer is fixed and specified in the PEBS documentation.  The sample
+ *  format changes between 32-bit and 64-bit modes due to extended register
+ *  file.
+ *
+ *  PEBS does not work when HyperThreading is enabled due to certain MSR
+ *  being shared being to two threads.
+ *
+ *  What does the format do?
+ *   It provides access to the PEBS feature for both 32-bit and 64-bit
+ *   processors that support it.
+ *
+ *   The same code is used for both 32-bit and 64-bit modes, but different
+ *   format names are used because the two modes are not compatible due to
+ *   data model and register file differences. Similarly the public data
+ *   structures describing the samples are different.
+ *
+ *   It is important to realize that the format provides a zero-copy environment
+ *   for the samples, i.e,, the OS never touches the samples. Whatever the
+ *   processor write is directly accessible to the user.
+ *
+ *   Parameters to the buffer can be passed via pfm_create_context() in
+ *   the pfm_pebs_smpl_arg structure.
+ *
+ *   It is not possible to mix a 32-bit PEBS application on top of a 64-bit
+ *   host kernel.
+ */
+#ifndef __PERFMON_PEBS_P4_SMPL_H__
+#define __PERFMON_PEBS_P4_SMPL_H__ 1
+
+#ifdef __i386__
+/*
+ * The 32-bit and 64-bit formats are not compatible, thus we have
+ * two different identifications so that 32-bit programs running on
+ * 64-bit OS will fail to use the 64-bit PEBS support.
+ */
+#define PFM_PEBS_P4_SMPL_NAME	"pebs32_p4"
+#else
+#define PFM_PEBS_P4_SMPL_NAME	"pebs64_p4"
+#endif
+
+/*
+ * format specific parameters (passed at context creation)
+ *
+ * intr_thres: index from start of buffer of entry where the
+ * PMU interrupt must be triggered. It must be several samples
+ * short of the end of the buffer.
+ */
+struct pfm_pebs_p4_smpl_arg {
+	u64 cnt_reset;	  /* counter reset value */
+	size_t buf_size;  /* size of the PEBS buffer in bytes */
+	size_t intr_thres;/* index of PEBS interrupt threshold entry */
+	u64 reserved[6];  /* for future use */
+};
+
+/*
+ * Data Save Area (32 and 64-bit mode)
+ *
+ * The DS area must be exposed to the user because this is the only
+ * way to report on the number of valid entries recorded by the CPU.
+ * This is required when the buffer is not full, i..e, there was not
+ * PMU interrupt.
+ *
+ * Layout of the structure is mandated by hardware and specified in
+ * the Intel documentation.
+ */
+struct pfm_ds_area_p4 {
+	unsigned long	bts_buf_base;
+	unsigned long	bts_index;
+	unsigned long	bts_abs_max;
+	unsigned long	bts_intr_thres;
+	unsigned long	pebs_buf_base;
+	unsigned long	pebs_index;
+	unsigned long	pebs_abs_max;
+	unsigned long	pebs_intr_thres;
+	u64     	pebs_cnt_reset;
+};
+
+/*
+ * This header is at the beginning of the sampling buffer returned to the user.
+ *
+ * Because of PEBS alignement constraints, the actual PEBS buffer area does
+ * not necessarily begin right after the header. The hdr_start_offs must be
+ * used to compute the first byte of the buffer. The offset is defined as
+ * the number of bytes between the end of the header and the beginning of
+ * the buffer. As such the formula is:
+ * 	actual_buffer = (unsigned long)(hdr+1)+hdr->hdr_start_offs
+ */
+struct pfm_pebs_p4_smpl_hdr {
+	u64 overflows;			/* #overflows for buffer */
+	size_t buf_size;		/* bytes in the buffer */
+	size_t start_offs; 		/* actual buffer start offset */
+	u32 version;			/* smpl format version */
+	u32 reserved1;			/* for future use */
+	u64 reserved2[5];		/* for future use */
+	struct pfm_ds_area_p4 ds;	/* data save area */
+};
+
+/*
+ * 64-bit PEBS record format is described in
+ * http://www.intel.com/technology/64bitextensions/30083502.pdf
+ *
+ * The format does not peek at samples. The sample structure is only
+ * used to ensure that the buffer is large enough to accomodate one
+ * sample.
+ */
+#ifdef __i386__
+struct pfm_pebs_p4_smpl_entry {
+	u32	eflags;
+	u32	ip;
+	u32	eax;
+	u32	ebx;
+	u32	ecx;
+	u32	edx;
+	u32	esi;
+	u32	edi;
+	u32	ebp;
+	u32	esp;
+};
+#else
+struct pfm_pebs_p4_smpl_entry {
+	u64	eflags;
+	u64	ip;
+	u64	eax;
+	u64	ebx;
+	u64	ecx;
+	u64	edx;
+	u64	esi;
+	u64	edi;
+	u64	ebp;
+	u64	esp;
+	u64	r8;
+	u64	r9;
+	u64	r10;
+	u64	r11;
+	u64	r12;
+	u64	r13;
+	u64	r14;
+	u64	r15;
+};
+#endif
+
+#define PFM_PEBS_P4_SMPL_VERSION_MAJ 1U
+#define PFM_PEBS_P4_SMPL_VERSION_MIN 0U
+#define PFM_PEBS_P4_SMPL_VERSION (((PFM_PEBS_P4_SMPL_VERSION_MAJ&0xffff)<<16)|\
+				   (PFM_PEBS_P4_SMPL_VERSION_MIN & 0xffff))
+
+#endif /* __PERFMON_PEBS_P4_SMPL_H__ */
diff -Naur linux-2.6.25-org/include/asm-x86/thread_info_64.h linux-2.6.25-id/include/asm-x86/thread_info_64.h
--- linux-2.6.25-org/include/asm-x86/thread_info_64.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/asm-x86/thread_info_64.h	2008-04-23 11:22:10.000000000 +0200
@@ -104,6 +104,7 @@
  * Warning: layout of LSW is hardcoded in entry.S
  */
 #define TIF_SYSCALL_TRACE	0	/* syscall trace active */
+#define TIF_PERFMON_WORK	1	/* work for pfm_handle_work() */
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_SINGLESTEP		4	/* reenable singlestep on user return*/
diff -Naur linux-2.6.25-org/include/linux/pci_ids.h linux-2.6.25-id/include/linux/pci_ids.h
--- linux-2.6.25-org/include/linux/pci_ids.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/linux/pci_ids.h	2008-04-23 11:22:10.000000000 +0200
@@ -497,6 +497,8 @@
 #define PCI_DEVICE_ID_AMD_K8_NB_ADDRMAP	0x1101
 #define PCI_DEVICE_ID_AMD_K8_NB_MEMCTL	0x1102
 #define PCI_DEVICE_ID_AMD_K8_NB_MISC	0x1103
+#define PCI_DEVICE_ID_AMD_10H_NB	0x1200
+#define PCI_DEVICE_ID_AMD_10H_NB_MISC	0x1203
 #define PCI_DEVICE_ID_AMD_LANCE		0x2000
 #define PCI_DEVICE_ID_AMD_LANCE_HOME	0x2001
 #define PCI_DEVICE_ID_AMD_SCSI		0x2020
diff -Naur linux-2.6.25-org/include/linux/perfmon.h linux-2.6.25-id/include/linux/perfmon.h
--- linux-2.6.25-org/include/linux/perfmon.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/linux/perfmon.h	2008-04-23 11:22:10.000000000 +0200
@@ -0,0 +1,753 @@
+/*
+ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+
+#ifndef __LINUX_PERFMON_H__
+#define __LINUX_PERFMON_H__
+
+#ifdef CONFIG_PERFMON
+
+/*
+ * include arch-specific constants
+ *
+ * constants are split for arch-specific perfmon.h
+ * to avoid cyclic dependency with the structures defined
+ * in this file.
+ */
+#include <asm/perfmon_const.h>
+
+#define PFM_MAX_PMCS	PFM_ARCH_MAX_PMCS
+#define PFM_MAX_PMDS	PFM_ARCH_MAX_PMDS
+
+/*
+ * number of elements for each type of bitvector
+ * all bitvectors use u64 fixed size type on all architectures.
+ */
+#define PFM_BVSIZE(x)	(((x)+(sizeof(u64)<<3)-1) / (sizeof(u64)<<3))
+#define PFM_PMD_BV	PFM_BVSIZE(PFM_MAX_PMDS)
+#define PFM_PMC_BV	PFM_BVSIZE(PFM_MAX_PMCS)
+
+/*
+ * PMC/PMD flags to use with pfm_write_pmds() or pfm_write_pmcs()
+ *
+ * event means:
+ * 	- for counters: when the 64-bit counter overflows
+ * 	- for others  : when the PMD generates an interrupt
+ *
+ * PFM_REGFL_NO_EMUL64: must be set of the PMC controlling the counting PMD
+ *
+ * reg_flags layout:
+ * bit 00-15 : generic flags
+ * bit 16-23 : arch-specific flags
+ * bit 24-31 : error codes
+ */
+#define PFM_REGFL_OVFL_NOTIFY	0x1	/* PMD: send notification on event */
+#define PFM_REGFL_RANDOM	0x2	/* PMD: randomize value after event */
+#define PFM_REGFL_NO_EMUL64	0x4	/* PMC: no 64-bit emulation for counter */
+
+/*
+ * event set flags layout:
+ * bits[00-15] : generic flags
+ * bits[16-31] : arch-specific flags (see asm/perfmon.h)
+ */
+#define PFM_SETFL_OVFL_SWITCH	0x01 /* enable switch on overflow */
+#define PFM_SETFL_TIME_SWITCH	0x02 /* enable switch on timeout */
+
+/*
+ * argument to pfm_create_context() system call
+ * structure shared with user level
+ */
+struct pfarg_ctx {
+	__u32		ctx_flags;	  /* noblock/block/syswide */
+	__u32		ctx_reserved1;	  /* ret arg: fd for context */
+	__u64		ctx_reserved2[7]; /* for future use */
+};
+
+/*
+ * context flags (ctx_flags)
+ *
+ * bits[00-15]: generic flags
+ * bits[16-31]: arch-specific flags (see perfmon_const.h)
+ */
+#define PFM_FL_NOTIFY_BLOCK    	 0x01	/* block task on user notifications */
+#define PFM_FL_SYSTEM_WIDE	 0x02	/* create a system wide context */
+#define PFM_FL_OVFL_NO_MSG	 0x80   /* no overflow msgs */
+
+/*
+ * argument to pfm_write_pmcs() system call.
+ * structure shared with user level
+ */
+struct pfarg_pmc {
+	__u16 reg_num;		/* which register */
+	__u16 reg_set;		/* event set for this register */
+	__u32 reg_flags;	/* REGFL flags */
+	__u64 reg_value;	/* pmc value */
+	__u64 reg_reserved2[4];	/* for future use */
+};
+
+/*
+ * argument to pfm_write_pmds() and pfm_read_pmds() system calls.
+ * structure shared with user level
+ */
+struct pfarg_pmd {
+	__u16 reg_num;	   	/* which register */
+	__u16 reg_set;	   	/* event set for this register */
+	__u32 reg_flags; 	/* REGFL flags */
+	__u64 reg_value;	/* initial pmc/pmd value */
+	__u64 reg_long_reset;	/* value to reload after notification */
+	__u64 reg_short_reset;  /* reset after counter overflow */
+	__u64 reg_last_reset_val;	/* return: PMD last reset value */
+	__u64 reg_ovfl_switch_cnt;	/* #overflows before switch */
+	__u64 reg_reset_pmds[PFM_PMD_BV]; /* reset on overflow */
+	__u64 reg_smpl_pmds[PFM_PMD_BV];  /* record in sample */
+	__u64 reg_smpl_eventid; /* opaque event identifier */
+	__u64 reg_random_mask; 	/* bitmask used to limit random value */
+	__u32 reg_random_seed;  /* seed for randomization (OBSOLETE) */
+	__u32 reg_reserved2[7];	/* for future use */
+};
+
+/*
+ * optional argument to pfm_start() system call. Pass NULL if not needed.
+ * structure shared with user level
+ */
+struct pfarg_start {
+	__u16 start_set;	/* event set to start with */
+	__u16 start_reserved1;	/* for future use */
+	__u32 start_reserved2;	/* for future use */
+	__u64 reserved3[3];	/* for future use */
+};
+
+/*
+ * argument to pfm_load_context() system call.
+ * structure shared with user level
+ */
+struct pfarg_load {
+	__u32	load_pid;	   /* thread or CPU to attach to */
+	__u16	load_set;	   /* set to load first */
+	__u16	load_reserved1;	   /* for future use */
+	__u64	load_reserved2[3]; /* for future use */
+};
+
+/*
+ * argument to pfm_create_evtsets() and pfm_delete_evtsets() system calls.
+ * structure shared with user level.
+ */
+struct pfarg_setdesc {
+	__u16	set_id;		  /* which set */
+	__u16	set_reserved1;	  /* for future use */
+	__u32	set_flags; 	  /* SETFL flags  */
+	__u64	set_timeout;	  /* req/eff switch timeout in nsecs */
+	__u64	reserved[6];	  /* for future use */
+};
+
+/*
+ * argument to pfm_getinfo_evtsets() system call.
+ * structure shared with user level
+ */
+struct pfarg_setinfo {
+	__u16	set_id;			/* which set */
+	__u16	set_reserved1;		/* for future use */
+	__u32	set_flags;		/* out: SETFL flags */
+	__u64 	set_ovfl_pmds[PFM_PMD_BV]; /* out: last ovfl PMDs */
+	__u64	set_runs;		/* out: #times the set was active */
+	__u64	set_timeout;		/* out: effective/leftover switch timeout in nsecs */
+	__u64	set_act_duration;	/* out: time set was active in nsecs */
+	__u64	set_avail_pmcs[PFM_PMC_BV];/* out: available PMCs */
+	__u64	set_avail_pmds[PFM_PMD_BV];/* out: available PMDs */
+	__u64	set_reserved3[6];	/* for future use */
+};
+
+/*
+ * default value for the user and group security parameters in
+ * /proc/sys/kernel/perfmon/sys_group
+ * /proc/sys/kernel/perfmon/task_group
+ */
+#define PFM_GROUP_PERM_ANY	-1	/* any user/group */
+
+/*
+ * overflow notification message.
+ * structure shared with user level
+ */
+struct pfarg_ovfl_msg {
+	__u32 		msg_type;	/* message type: PFM_MSG_OVFL */
+	__u32		msg_ovfl_pid;	/* process id */
+	__u16 		msg_active_set;	/* active set at overflow */
+	__u16 		msg_ovfl_cpu;	/* cpu of PMU interrupt */
+	__u32		msg_ovfl_tid;	/* thread id */
+	__u64		msg_ovfl_ip;    /* IP on PMU intr */
+	__u64		msg_ovfl_pmds[PFM_PMD_BV];/* overflowed PMDs */
+};
+
+#define PFM_MSG_OVFL	1	/* an overflow happened */
+#define PFM_MSG_END	2	/* task to which context was attached ended */
+
+/*
+ * generic notification message (union).
+ * union shared with user level
+ */
+union pfarg_msg {
+	__u32	type;
+	struct pfarg_ovfl_msg pfm_ovfl_msg;
+};
+
+/*
+ * perfmon version number
+ */
+#define PFM_VERSION_MAJ		 2U
+#define PFM_VERSION_MIN		 7U
+#define PFM_VERSION		 (((PFM_VERSION_MAJ&0xffff)<<16)|\
+				  (PFM_VERSION_MIN & 0xffff))
+#define PFM_VERSION_MAJOR(x)	 (((x)>>16) & 0xffff)
+#define PFM_VERSION_MINOR(x)	 ((x) & 0xffff)
+
+/*
+ * This part of the header file is meant for kernel level code only including
+ * kernel modules
+ */
+#ifdef __KERNEL__
+
+#include <linux/file.h>
+#include <linux/seq_file.h>
+#include <linux/interrupt.h>
+#include <linux/kobject.h>
+
+/*
+ * perfmon context state
+ */
+#define PFM_CTX_UNLOADED	1 /* context is not loaded onto any task */
+#define PFM_CTX_LOADED		2 /* context is loaded onto a task */
+#define PFM_CTX_MASKED		3 /* context is loaded, monitoring is masked */
+#define PFM_CTX_ZOMBIE		4 /* context lost owner but is still attached */
+
+/*
+ * depth of message queue
+ *
+ * Depth cannot be bigger than 255 (see reset_count)
+ */
+#define PFM_MSGS_ORDER		3 /* log2(number of messages) */
+#define PFM_MSGS_COUNT		(1<<PFM_MSGS_ORDER) /* number of messages */
+#define PFM_MSGQ_MASK		(PFM_MSGS_COUNT-1)
+
+/*
+ * type of PMD reset for pfm_reset_pmds() or pfm_switch_sets*()
+ */
+#define PFM_PMD_RESET_SHORT	1	/* use short reset value */
+#define PFM_PMD_RESET_LONG	2	/* use long reset value  */
+
+/*
+ * describe the content of the pfm_syst_info field
+ * layout:
+ * 	bits[00-15] : generic
+ *	bits[16-31] : arch-specific flags (see asm/perfmon.h)
+ */
+#define PFM_CPUINFO_TIME_SWITCH	0x1 /* current set is time-switched */
+
+struct pfm_controls {
+	int	debug;		/* debugging via syslog */
+	int	debug_ovfl;	/* overflow handling debugging */
+	gid_t	sys_group;	/* gid to create a syswide context */
+	gid_t	task_group;	/* gid to create a per-task context */
+	size_t	arg_mem_max;	/* maximum vector argument size */
+	size_t	smpl_buffer_mem_max; /* max buf mem, -1 for infinity */
+	int pmd_read;
+};
+DECLARE_PER_CPU(u32, pfm_syst_info);
+DECLARE_PER_CPU(struct task_struct *, pmu_owner);
+DECLARE_PER_CPU(struct pfm_context *, pmu_ctx);
+DECLARE_PER_CPU(u64, pmu_activation_number);
+DECLARE_PER_CPU(struct pfm_stats, pfm_stats);
+DECLARE_PER_CPU(struct hrtimer, pfm_hrtimer);
+
+/*
+ * logging
+ */
+#define PFM_ERR(f, x...)  printk(KERN_ERR     "perfmon: " f "\n", ## x)
+#define PFM_WARN(f, x...) printk(KERN_WARNING "perfmon: " f "\n", ## x)
+#define PFM_LOG(f, x...)  printk(KERN_NOTICE  "perfmon: " f "\n", ## x)
+#define PFM_INFO(f, x...) printk(KERN_INFO    "perfmon: " f "\n", ## x)
+
+/*
+ * debugging
+ *
+ * Printk rate limiting is enforced to avoid getting flooded with too many
+ * error messages on the console (which could render the machine unresponsive).
+ * To get full debug output (turn off ratelimit):
+ * 	$ echo 0 >/proc/sys/kernel/printk_ratelimit
+ */
+#ifdef CONFIG_PERFMON_DEBUG
+#define PFM_DBG(f, x...) \
+	do { \
+		if (unlikely(pfm_controls.debug >0 && printk_ratelimit())) { \
+			printk("perfmon: %s.%d: CPU%d [%d]: " f "\n", \
+			       __FUNCTION__, __LINE__, \
+			       smp_processor_id(), current->pid , ## x); \
+		} \
+	} while (0)
+
+#define PFM_DBG_ovfl(f, x...) \
+	do { \
+		if (unlikely(pfm_controls.debug_ovfl >0 && printk_ratelimit())) { \
+			printk("perfmon: %s.%d: CPU%d [%d]: " f "\n", \
+			       __FUNCTION__, __LINE__, \
+			       smp_processor_id(), current->pid , ## x); \
+		} \
+	} while (0)
+#else
+#define PFM_DBG(f, x...)	do {} while(0)
+#define PFM_DBG_ovfl(f, x...)	do {} while(0)
+#endif
+
+/*
+ * PMD information
+ */
+struct pfm_pmd {
+	u64 value;		/* currnet 64-bit value */
+	u64 lval;		/* last reset value */
+	u64 ovflsw_thres;	 /* #overflows left before switching */
+	u64 long_reset;		/* reset value on sampling overflow */
+	u64 short_reset;    	/* reset value on overflow */
+	u64 reset_pmds[PFM_PMD_BV];  /* pmds to reset on overflow */
+	u64 smpl_pmds[PFM_PMD_BV];   /* pmds to record on overflow */
+	u64 mask;		 /* mask for generator */
+	u64 ovflsw_ref_thres;	 /* #overflows before switching to next set */
+	u64 eventid;	 	 /* overflow event identifier */
+	u32 flags;		 /* notify/do not notify */
+};
+
+/*
+ * perfmon context: encapsulates all the state of a monitoring session
+ */
+struct pfm_event_set {
+	u16 id;
+	u16 id_next;			/* which set to go to from this one */
+	u32 flags;			/* public set flags */
+	u64 runs;			/* number of activations */
+	struct list_head list;		/* next in the ordered list */
+	u32 priv_flags;			/* private flags */
+	u32 npend_ovfls;		/* number of pending PMD overflow */
+
+	u64 used_pmds[PFM_PMD_BV];    /* used PMDs */
+	u64 povfl_pmds[PFM_PMD_BV];   /* pending overflowed PMDs */
+	u64 ovfl_pmds[PFM_PMD_BV];    /* last overflowed PMDs */
+	u64 reset_pmds[PFM_PMD_BV];   /* union of PMDs to reset */
+	u64 ovfl_notify[PFM_PMD_BV];  /* notify on overflow */
+	u64 pmcs[PFM_MAX_PMCS];	      /* PMC values */
+
+	u16 nused_pmds;			    /* max number of used PMDs */
+	u16 nused_pmcs;			    /* max number of used PMCs */
+
+	struct pfm_pmd pmds[PFM_MAX_PMDS];  /* 64-bit SW PMDs */
+
+	ktime_t hrtimer_exp;		/* switch timeout reference */
+	ktime_t hrtimer_rem;		/* per-thread remainder timeout */
+
+	u64 duration_start;		    /* start ns */
+	u64 duration;			    /* total active ns */
+	u64 used_pmcs[PFM_PMC_BV];    /* used PMCs (keep for arbitration) */
+};
+
+/*
+ * common private event set flags (priv_flags)
+ *
+ * upper 16 bits: for arch-specific use
+ * lower 16 bits: for common use
+ */
+#define PFM_SETFL_PRIV_MOD_PMDS 0x1 /* PMD register(s) modified */
+#define PFM_SETFL_PRIV_MOD_PMCS 0x2 /* PMC register(s) modified */
+#define PFM_SETFL_PRIV_SWITCH	0x4 /* must switch set on restart */
+#define PFM_SETFL_PRIV_MOD_BOTH	(PFM_SETFL_PRIV_MOD_PMDS | PFM_SETFL_PRIV_MOD_PMCS)
+
+/*
+ * context flags
+ */
+struct pfm_context_flags {
+	unsigned int block:1;		/* task blocks on user notifications */
+	unsigned int system:1;		/* do system wide monitoring */
+	unsigned int no_msg:1;		/* no message sent on overflow */
+	unsigned int switch_ovfl:1;	/* switch set on counter ovfl */
+	unsigned int switch_time:1;	/* switch set on timeout */
+	unsigned int started:1;		/* pfm_start() issued */
+	unsigned int work_type:2;	/* type of work for pfm_handle_work */
+	unsigned int mmap_nlock:1;	/* no lock in pfm_release_buf_space */
+	unsigned int ia64_v20_compat:1;	/* context is IA-64 v2.0 mode */
+	unsigned int can_restart:8;	/* allowed to issue a PFM_RESTART */
+	unsigned int reset_count:8;	/* number of pending resets */
+	unsigned int reserved:6;	/* for future use */
+};
+
+/*
+ * values for work_type (TIF_PERFMON_WORK must be set)
+ */
+#define PFM_WORK_NONE	0	/* nothing to do */
+#define PFM_WORK_RESET	1	/* reset overflowed counters */
+#define PFM_WORK_BLOCK	2	/* block current thread */
+#define PFM_WORK_ZOMBIE	3	/* cleanup zombie context */
+
+/*
+ * check_mask bitmask values for pfm_check_task_state()
+ */
+#define PFM_CMD_STOPPED		0x01	/* command needs thread stopped */
+#define PFM_CMD_UNLOADED	0x02	/* command needs ctx unloaded */
+#define PFM_CMD_UNLOAD		0x04	/* command is unload */
+
+#include <linux/perfmon_pmu.h>
+#include <linux/perfmon_fmt.h>
+
+/*
+ * context: encapsulates all the state of a monitoring session
+ */
+struct pfm_context {
+	spinlock_t		lock;	/* context protection */
+
+	struct pfm_context_flags flags;	/*  flags */
+	u32			state;	/* state */
+	struct task_struct 	*task;	/* attached task */
+
+	struct completion       restart_complete;/* block on notification */
+	u64 			last_act;	/* last activation */
+	u32			last_cpu;   	/* last CPU used (SMP only) */
+	u32			cpu;		/* cpu bound to context */
+
+	struct pfm_smpl_fmt	*smpl_fmt;	/* buffer format callbacks */
+	void			*smpl_addr;	/* user smpl buffer base */
+	size_t			smpl_size;	/* user smpl buffer size */
+	void			*real_smpl_addr;/* actual smpl buffer base */
+	size_t			real_smpl_size; /* actual smpl buffer size */
+
+	wait_queue_head_t 	msgq_wait;	/* pfm_read() wait queue */
+
+	union pfarg_msg		msgq[PFM_MSGS_COUNT];
+	int			msgq_head;
+	int			msgq_tail;
+
+	struct fasync_struct	*async_queue;
+
+	struct pfm_event_set	*active_set;  /* active set */
+	struct list_head	set_list;    /* ordered list of sets */
+
+	/*
+	 * save stack space by allocating temporary variables for
+	 * pfm_overflow_handler() in pfm_context
+	 */
+	struct pfm_ovfl_arg 	ovfl_arg;
+	u64			ovfl_ovfl_notify[PFM_PMD_BV];
+
+	/*
+	 * task context switch notifier.
+	 */
+	struct notifier_block   ctxsw_notifier;
+};
+
+static inline struct pfm_arch_context *pfm_ctx_arch(struct pfm_context *c)
+{
+	return (struct pfm_arch_context *)(c+1);
+}
+
+static inline void pfm_set_pmu_owner(struct task_struct *task,
+				     struct pfm_context *ctx)
+{
+	BUG_ON(task && task->pid == 0);
+	__get_cpu_var(pmu_owner) = task;
+	__get_cpu_var(pmu_ctx) = ctx;
+}
+
+static inline void pfm_inc_activation(void)
+{
+	__get_cpu_var(pmu_activation_number)++;
+}
+
+static inline void pfm_set_activation(struct pfm_context *ctx)
+{
+	ctx->last_act = __get_cpu_var(pmu_activation_number);
+}
+
+static inline void pfm_set_last_cpu(struct pfm_context *ctx, int cpu)
+{
+	ctx->last_cpu = cpu;
+}
+
+extern struct pfm_pmu_config  *pfm_pmu_conf;
+extern struct pfm_controls pfm_controls;
+extern int perfmon_disabled;
+
+int  pfm_get_args(void __user *ureq, size_t sz, size_t lsz, void *laddr,
+		  void **req, void **to_free);
+
+int pfm_get_task(struct pfm_context *ctx, pid_t pid, struct task_struct **task);
+int pfm_get_smpl_arg(char __user *fmt_uname, void __user *uaddr, size_t usize, void **arg,
+		     struct pfm_smpl_fmt **fmt);
+
+int pfm_alloc_fd(struct file **cfile);
+
+int __pfm_write_pmcs(struct pfm_context *ctx, struct pfarg_pmc *req, int count);
+int __pfm_write_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count,
+		     int compat);
+int __pfm_read_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count);
+int __pfm_load_context(struct pfm_context *ctx, struct pfarg_load *req,
+		       struct task_struct *task);
+int __pfm_unload_context(struct pfm_context *ctx, int *can_release);
+int __pfm_stop(struct pfm_context *ctx, int *release_info);
+int  __pfm_restart(struct pfm_context *ctx, int *unblock);
+int __pfm_start(struct pfm_context *ctx, struct pfarg_start *start);
+int __pfm_delete_evtsets(struct pfm_context *ctx, void *arg, int count);
+int __pfm_getinfo_evtsets(struct pfm_context *ctx, struct pfarg_setinfo *req,
+			  int count);
+int __pfm_create_evtsets(struct pfm_context *ctx, struct pfarg_setdesc *req,
+			int count);
+
+int __pfm_create_context(struct pfarg_ctx *req,
+			 struct pfm_smpl_fmt *fmt,
+			 void *fmt_arg,
+			 int mode,
+			 struct pfm_context **new_ctx);
+
+int pfm_check_task_state(struct pfm_context *ctx, int check_mask,
+			 unsigned long *flags);
+
+struct pfm_event_set *pfm_find_set(struct pfm_context *ctx, u16 set_id,
+				   int alloc);
+
+struct pfm_context *pfm_get_ctx(int fd);
+
+void pfm_context_free(struct pfm_context *ctx);
+struct pfm_context *pfm_context_alloc(void);
+int pfm_pmu_conf_get(int autoload);
+void pfm_pmu_conf_put(void);
+
+int pfm_pmu_acquire(void);
+void pfm_pmu_release(void);
+
+int pfm_reserve_session(int is_system, u32 cpu);
+int pfm_release_session(int is_system, u32 cpu);
+
+int pfm_reserve_allcpus(void);
+int pfm_release_allcpus(void);
+
+int pfm_smpl_buffer_alloc(struct pfm_context *ctx, size_t rsize);
+int pfm_reserve_buf_space(size_t size);
+void pfm_release_buf_space(struct pfm_context *ctx, size_t size);
+
+struct pfm_smpl_fmt *pfm_smpl_fmt_get(char *name);
+void pfm_smpl_fmt_put(struct pfm_smpl_fmt *fmt);
+
+int  pfm_init_sysfs(void);
+int  pfm_init_debugfs(void);
+ssize_t pfm_sysfs_session_show(char *buf, size_t sz, int what);
+int pfm_sysfs_remove_pmu(struct pfm_pmu_config *pmu);
+int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu);
+
+int pfm_sysfs_add_fmt(struct pfm_smpl_fmt *fmt);
+int pfm_sysfs_remove_fmt(struct pfm_smpl_fmt *fmt);
+
+int pfm_debugfs_add_cpu(int mycpu);
+void pfm_debugfs_del_cpu(int mycpu);
+
+void pfm_interrupt_handler(unsigned long iip, struct pt_regs *regs);
+void pfm_save_prev_context(struct pfm_context *ctxp);
+
+void pfm_reset_pmds(struct pfm_context *ctx, struct pfm_event_set *set,
+		    int num_pmds,
+		    int reset_mode);
+
+int pfm_prepare_sets(struct pfm_context *ctx, struct pfm_event_set *act_set);
+int pfm_sets_init(void);
+
+int pfm_mmap_set(struct pfm_context *ctx, struct vm_area_struct *vma,
+		 size_t size);
+
+void pfm_free_sets(struct pfm_context *ctx);
+void pfm_init_evtset(struct pfm_event_set *set);
+void pfm_switch_sets_from_intr(struct pfm_context *ctx);
+enum hrtimer_restart pfm_handle_switch_timeout(struct hrtimer *t);
+
+enum hrtimer_restart pfm_switch_sets(struct pfm_context *ctx,
+		    struct pfm_event_set *new_set,
+		    int reset_mode,
+		    int no_restart);
+
+void pfm_save_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
+int pfm_ovfl_notify_user(struct pfm_context *ctx,
+			struct pfm_event_set *set,
+			unsigned long ip);
+
+int pfm_init_fs(void);
+
+/*
+ * When adding new stats, make sure you also
+ * update the name table in perfmon_debugfs.c
+ */
+enum pfm_stats_names {
+	PFM_ST_ovfl_intr_all_count = 0,
+	PFM_ST_ovfl_intr_ns,
+	PFM_ST_ovfl_intr_p1_ns,
+	PFM_ST_ovfl_intr_p2_ns,
+	PFM_ST_ovfl_intr_p3_ns,
+	PFM_ST_ovfl_intr_spurious_count,
+	PFM_ST_ovfl_intr_replay_count,
+	PFM_ST_ovfl_intr_regular_count,
+	PFM_ST_handle_work_count,
+	PFM_ST_ovfl_notify_count,
+	PFM_ST_reset_pmds_count,
+	PFM_ST_pfm_restart_count,
+	PFM_ST_fmt_handler_calls,
+	PFM_ST_fmt_handler_ns,
+	PFM_ST_set_switch_count,
+	PFM_ST_set_switch_ns,
+	PFM_ST_ctxsw_count,
+	PFM_ST_ctxsw_ns,
+	PFM_ST_handle_timeout_count,
+	PFM_ST_ovfl_intr_nmi_count,
+	PFM_ST_LAST	/* last entry marked */
+};
+#define PFM_NUM_STATS PFM_ST_LAST
+
+struct pfm_stats {
+	u64 v[PFM_NUM_STATS];
+	struct dentry *dirs[PFM_NUM_STATS];
+	struct dentry *cpu_dir;
+	char cpu_name[8];
+};
+
+#define pfm_stats_get(x)  __get_cpu_var(pfm_stats).v[PFM_ST_##x]
+#define pfm_stats_inc(x)  __get_cpu_var(pfm_stats).v[PFM_ST_##x]++
+#define pfm_stats_add(x,y)  __get_cpu_var(pfm_stats).v[PFM_ST_##x] += (y)
+
+/*
+ * include arch-specific kernel level only definitions
+ * (split with perfmon_api.h is necessary to avoid circular
+ *  dependencies on certain data structures definitions)
+ */
+#include <asm/perfmon.h>
+
+extern const struct file_operations pfm_file_ops;
+/*
+ * max vector argument elements for local storage (no kmalloc/kfree)
+ * The PFM_ARCH_PM*_ARG should be defined in the arch specific perfmon.h
+ * file. If not, default (conservative) values are used
+ */
+
+#ifndef PFM_ARCH_PMC_STK_ARG
+#define PFM_ARCH_PMC_STK_ARG	1
+#endif
+
+#ifndef PFM_ARCH_PMD_STK_ARG
+#define PFM_ARCH_PMD_STK_ARG	1
+#endif
+
+#define PFM_PMC_STK_ARG	PFM_ARCH_PMC_STK_ARG
+#define PFM_PMD_STK_ARG	PFM_ARCH_PMD_STK_ARG
+
+#define PFM_BPL		64
+#define PFM_LBPL	6	/* log2(BPL) */
+
+/*
+ * upper limit for count in calls that take vector arguments. This is used
+ * to prevent for multiplication overflow when we compute actual storage size
+ */
+#define PFM_MAX_ARG_COUNT(m) (INT_MAX/sizeof(*(m)))
+
+/*
+ * read a single PMD register. PMD register mapping is provided by PMU
+ * description module. Virtual PMD registers have special handler.
+ */
+static inline u64 pfm_read_pmd(struct pfm_context *ctx, unsigned int cnum)
+{
+	if (unlikely(pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_V))
+		return pfm_pmu_conf->pmd_sread(ctx, cnum);
+
+	return pfm_arch_read_pmd(ctx, cnum);
+}
+
+static inline void pfm_write_pmd(struct pfm_context *ctx, unsigned int cnum, u64 value)
+{
+	/*
+	 * PMD writes are ignored for read-only registers
+	 */
+	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_RO)
+		return;
+
+	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_V) {
+		pfm_pmu_conf->pmd_swrite(ctx, cnum, value);
+		return;
+	}
+	/*
+	 * clear unimplemented bits
+	 */
+	value &= ~pfm_pmu_conf->pmd_desc[cnum].rsvd_msk;
+
+	pfm_arch_write_pmd(ctx, cnum, value);
+}
+
+#define cast_ulp(_x) ((unsigned long *)_x)
+
+#define PFM_NORMAL      0
+#define PFM_COMPAT      1
+
+void __pfm_exit_thread(struct task_struct *task);
+void __pfm_copy_thread(struct task_struct *task);
+void pfm_ctxsw(struct task_struct *prev, struct task_struct *next);
+void pfm_handle_work(struct pt_regs *regs);
+void __pfm_init_percpu (void *dummy);
+void pfm_cpu_disable(void);
+
+static inline void pfm_exit_thread(struct task_struct *task)
+{
+	if (task->pfm_context)
+		__pfm_exit_thread(task);
+}
+
+static inline void pfm_copy_thread(struct task_struct *task)
+{
+	/*
+	 * context or perfmon TIF state  is NEVER inherited
+	 * in child task. Holds for per-thread and system-wide
+	 */
+	task->pfm_context = NULL;
+	clear_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
+	clear_tsk_thread_flag(task, TIF_PERFMON_WORK);
+}
+
+static inline void pfm_init_percpu(void)
+{
+	__pfm_init_percpu(NULL);
+}
+
+#endif /* __KERNEL__ */
+
+#else /* !CONFIG_PERFMON */
+#ifdef __KERNEL__
+
+#define tsks_have_perfmon(p, n)	(0)
+#define pfm_cpu_disable()		do { } while (0)
+#define pfm_init_percpu()		do { } while (0)
+#define pfm_exit_thread(_t)  		do { } while (0)
+#define pfm_handle_work(_t)    		do { } while (0)
+#define pfm_copy_thread(_t)		do { } while (0)
+#define pfm_ctxsw(_p, _t)     		do { } while (0)
+#define	pfm_release_allcpus()		do { } while (0)
+#define	pfm_reserve_allcpus()		(0)
+#ifdef __ia64__
+#define pfm_release_dbregs(_t) 		do { } while (0)
+#define pfm_use_dbregs(_t)     		(0)
+#endif
+
+#endif /* __KERNEL__ */
+
+#endif /* CONFIG_PERFMON */
+
+#endif /* __LINUX_PERFMON_H__ */
diff -Naur linux-2.6.25-org/include/linux/perfmon_dfl_smpl.h linux-2.6.25-id/include/linux/perfmon_dfl_smpl.h
--- linux-2.6.25-org/include/linux/perfmon_dfl_smpl.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/linux/perfmon_dfl_smpl.h	2008-04-23 11:22:10.000000000 +0200
@@ -0,0 +1,78 @@
+/*
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file implements the new dfl sampling buffer format
+ * for perfmon2 subsystem.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#ifndef __PERFMON_DFL_SMPL_H__
+#define __PERFMON_DFL_SMPL_H__ 1
+
+/*
+ * format specific parameters (passed at context creation)
+ */
+struct pfm_dfl_smpl_arg {
+	__u64 buf_size;		/* size of the buffer in bytes */
+	__u32 buf_flags;	/* buffer specific flags */
+	__u32 reserved1;	/* for future use */
+	__u64 reserved[6];	/* for future use */
+};
+
+/*
+ * This header is at the beginning of the sampling buffer returned to the user.
+ * It is directly followed by the first record.
+ */
+struct pfm_dfl_smpl_hdr {
+	__u64 hdr_count;	/* how many valid entries */
+	__u64 hdr_cur_offs;	/* current offset from top of buffer */
+	__u64 hdr_overflows;	/* #overflows for buffer */
+	__u64 hdr_buf_size;	/* bytes in the buffer */
+	__u64 hdr_min_buf_space;/* minimal buffer size (internal use) */
+	__u32 hdr_version;	/* smpl format version */
+	__u32 hdr_buf_flags;	/* copy of buf_flags */
+	__u64 hdr_reserved[10];	/* for future use */
+};
+
+/*
+ * Entry header in the sampling buffer.  The header is directly followed
+ * with the values of the PMD registers of interest saved in increasing
+ * index order: PMD4, PMD5, and so on. How many PMDs are present depends
+ * on how the session was programmed.
+ *
+ * In the case where multiple counters overflow at the same time, multiple
+ * entries are written consecutively.
+ *
+ * last_reset_value member indicates the initial value of the overflowed PMD.
+ */
+struct pfm_dfl_smpl_entry {
+	__u32	pid;		/* thread id (for NPTL, this is gettid()) */
+	__u16	ovfl_pmd;	/* index of overflowed PMD for this sample */
+	__u16	reserved;	/* for future use */
+	__u64	last_reset_val;	/* initial value of overflowed PMD */
+	__u64	ip;		/* where did the overflow interrupt happened  */
+	__u64	tstamp;		/* overflow timetamp */
+	__u16	cpu;		/* cpu on which the overfow occurred */
+	__u16	set;		/* event set active when overflow ocurred   */
+	__u32	tgid;		/* thread group id (for NPTL, this is getpid())*/
+};
+
+#define PFM_DFL_SMPL_VERSION_MAJ 1U
+#define PFM_DFL_SMPL_VERSION_MIN 0U
+#define PFM_DFL_SMPL_VERSION (((PFM_DFL_SMPL_VERSION_MAJ&0xffff)<<16)|\
+				(PFM_DFL_SMPL_VERSION_MIN & 0xffff))
+
+#endif /* __PERFMON_DFL_SMPL_H__ */
diff -Naur linux-2.6.25-org/include/linux/perfmon_fmt.h linux-2.6.25-id/include/linux/perfmon_fmt.h
--- linux-2.6.25-org/include/linux/perfmon_fmt.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/linux/perfmon_fmt.h	2008-04-23 11:22:10.000000000 +0200
@@ -0,0 +1,88 @@
+/*
+ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * Interface for custom sampling buffer format modules
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#ifndef __PERFMON_FMT_H__
+#define __PERFMON_FMT_H__ 1
+
+#include <linux/kobject.h>
+
+struct pfm_ovfl_arg {
+	u16 ovfl_pmd;	/* index of overflowed PMD  */
+	u16 active_set;	/* set active at the time of the overflow */
+	u32 ovfl_ctrl;	/* control flags */
+	u64 pmd_last_reset;	/* last reset value of overflowed PMD */
+	u64 smpl_pmds_values[PFM_MAX_PMDS]; 	/* values of other PMDs */
+	u64 pmd_eventid;	/* eventid associated with PMD */
+	u16 num_smpl_pmds;	/* number of PMDS in smpl_pmd_values */
+};
+
+/*
+ * ovfl_ctrl bitmask of flags
+ */
+#define PFM_OVFL_CTRL_NOTIFY	0x1	/* notify user */
+#define PFM_OVFL_CTRL_RESET	0x2	/* reset overflowed pmds */
+#define PFM_OVFL_CTRL_MASK	0x4	/* mask monitoring */
+
+
+typedef int (*fmt_validate_t )(u32 flags, u16 npmds, void *arg);
+typedef	int (*fmt_getsize_t)(u32 flags, void *arg, size_t *size);
+typedef int (*fmt_init_t)(struct pfm_context *ctx, void *buf, u32 flags, u16 nmpds, void *arg);
+typedef int (*fmt_restart_t)(int is_active, u32 *ovfl_ctrl, void *buf);
+typedef int (*fmt_exit_t)(void *buf);
+typedef int (*fmt_handler_t)(void *buf, struct pfm_ovfl_arg *arg,
+			     unsigned long ip, u64 stamp, void *data);
+
+struct pfm_smpl_fmt {
+	char		*fmt_name;	/* name of the format (required) */
+	size_t		fmt_arg_size;	/* size of fmt args for ctx create */
+	u32		fmt_flags;	/* format specific flags */
+	u32		fmt_version;	/* format version number */
+
+	fmt_validate_t	fmt_validate;	/* validate context flags */
+	fmt_getsize_t	fmt_getsize;	/* get size for sampling buffer */
+	fmt_init_t	fmt_init;	/* initialize buffer area */
+	fmt_handler_t	fmt_handler;	/* overflow handler (required) */
+	fmt_restart_t	fmt_restart;	/* restart after notification  */
+	fmt_exit_t	fmt_exit;	/* context termination */
+
+	struct list_head fmt_list;	/* internal use only */
+
+	struct kobject	kobj;		/* sysfs internal use only */
+	struct module	*owner;		/* pointer to module owner */
+	u32		fmt_qdepth;	/* Max notify queue depth (required) */
+};
+#define to_smpl_fmt(n) container_of(n, struct pfm_smpl_fmt, kobj)
+
+#define PFM_FMTFL_IS_BUILTIN	0x1	/* fmt is compiled in */
+/*
+ * we need to know whether the format is builtin or compiled
+ * as a module
+ */
+#ifdef MODULE
+#define PFM_FMT_BUILTIN_FLAG	0	/* not built as a module */
+#else
+#define PFM_FMT_BUILTIN_FLAG	PFM_PMUFL_IS_BUILTIN /* built as a module */
+#endif
+
+int pfm_fmt_register(struct pfm_smpl_fmt *fmt);
+int pfm_fmt_unregister(struct pfm_smpl_fmt *fmt);
+void pfm_sysfs_builtin_fmt_add(void);
+
+#endif /* __PERFMON_FMT_H__ */
diff -Naur linux-2.6.25-org/include/linux/perfmon_pmu.h linux-2.6.25-id/include/linux/perfmon_pmu.h
--- linux-2.6.25-org/include/linux/perfmon_pmu.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/include/linux/perfmon_pmu.h	2008-04-23 11:22:10.000000000 +0200
@@ -0,0 +1,178 @@
+/*
+ * Copyright (c) 2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * Interface for PMU description modules
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#ifndef __PERFMON_PMU_H__
+#define __PERFMON_PMU_H__ 1
+
+/*
+ * generic information about a PMC or PMD register
+ */
+struct pfm_regmap_desc {
+	u16  type;		/* role of the register */
+	u16  reserved1;		/* for future use */
+	u32  reserved2;		/* for future use */
+	u64  dfl_val;		/* power-on default value (quiescent) */
+	u64  rsvd_msk;		/* reserved bits: 1 means reserved */
+	u64  no_emul64_msk;	/* bits to clear for PFM_REGFL_NO_EMUL64 */
+	unsigned long hw_addr;	/* HW register address or index */
+	struct kobject	kobj;	/* for internal use only */
+	char *desc;		/* HW register description string */
+};
+#define to_reg(n) container_of(n, struct pfm_regmap_desc, kobj)
+
+/*
+ * pfm_reg_desc helper macros
+ */
+#define PMC_D(t,d,v,r,n, h)   \
+	{ .type = t,          \
+	  .desc = d,          \
+	  .dfl_val = v,       \
+	  .rsvd_msk = r,      \
+	  .no_emul64_msk = n, \
+	  .hw_addr = h	      \
+	}
+
+#define PMD_D(t,d, h)         \
+	{ .type = t,          \
+	  .desc = d,          \
+	  .rsvd_msk = 0,      \
+	  .no_emul64_msk = 0, \
+	  .hw_addr = h	      \
+	}
+
+#define PMX_NA \
+	{ .type = PFM_REG_NA }
+
+/*
+ * type of a PMU register (16-bit bitmask) for use with pfm_reg_desc.type
+ */
+#define PFM_REG_NA	0x00  /* not avail. (not impl.,no access) must be 0 */
+#define PFM_REG_I	0x01  /* PMC/PMD: implemented */
+#define PFM_REG_WC	0x02  /* PMC: has write_checker */
+#define PFM_REG_C64	0x04  /* PMD: 64-bit virtualization */
+#define PFM_REG_RO	0x08  /* PMD: read-only (writes ignored) */
+#define PFM_REG_V	0x10  /* PMD: virtual reg (provided by PMU description) */
+#define PFM_REG_INTR	0x20  /* PMD: register can generate interrupt */
+#define PFM_REG_NO64	0x100 /* PMC: supports PFM_REGFL_NO_EMUL64 */
+
+/*
+ * define some shortcuts for common types
+ */
+#define PFM_REG_W	(PFM_REG_WC|PFM_REG_I)
+#define PFM_REG_W64	(PFM_REG_WC|PFM_REG_NO64|PFM_REG_I)
+#define PFM_REG_C	(PFM_REG_C64|PFM_REG_INTR|PFM_REG_I)
+#define PFM_REG_I64	(PFM_REG_NO64|PFM_REG_I)
+#define PFM_REG_IRO	(PFM_REG_I|PFM_REG_RO)
+
+typedef int (*pfm_pmc_check_t)(struct pfm_context *ctx,
+			       struct pfm_event_set *set,
+			       struct pfarg_pmc *req);
+
+typedef int (*pfm_pmd_check_t)(struct pfm_context *ctx,
+			       struct pfm_event_set *set,
+			       struct pfarg_pmd *req);
+
+
+typedef u64 (*pfm_pmd_sread_t)(struct pfm_context *ctx, unsigned int cnum);
+typedef void (*pfm_pmd_swrite_t)(struct pfm_context *ctx, unsigned int cnum, u64 val);
+
+/*
+ * registers description
+ */
+struct pfm_regdesc {
+	u64 pmcs[PFM_PMC_BV];		/* available PMC */
+	u64 pmds[PFM_PMD_BV];		/* available PMD */
+	u64 rw_pmds[PFM_PMD_BV];	/* available RW PMD */
+	u64 intr_pmds[PFM_PMD_BV];	/* PMD generating intr */
+	u64 cnt_pmds[PFM_PMD_BV];	/* PMD counters */
+	u16 max_pmc;			/* highest+1 avail PMC */
+	u16 max_pmd;			/* highest+1 avail PMD */
+	u16 max_rw_pmd;			/* highest+1 avail RW PMD */
+	u16 first_intr_pmd;		/* first intr PMD */
+	u16 max_intr_pmd;		/* highest+1 intr PMD */
+	u16 num_rw_pmd;			/* number of avail RW PMD */
+	u16 num_pmcs;			/* number of logical PMCS */
+	u16 num_pmds;			/* number of logical PMDS */
+	u16 num_counters;		/* number of counting PMD */
+};
+
+/*
+ * structure used by pmu description modules
+ *
+ * probe_pmu() routine return value:
+ * 	- 1 means recognized PMU
+ * 	- 0 means not recognized PMU
+ */
+struct pfm_pmu_config {
+	char *pmu_name;				/* PMU family name */
+	char *version;				/* config module version number */
+
+	int counter_width;			/* width of hardware counter */
+
+	struct pfm_regmap_desc	*pmc_desc;	/* PMC register descriptions */
+	struct pfm_regmap_desc	*pmd_desc;	/* PMD register descriptions */
+
+	pfm_pmc_check_t		pmc_write_check;/* PMC write checker callback (optional) */
+	pfm_pmd_check_t		pmd_write_check;/* PMD write checker callback (optional) */
+	pfm_pmd_check_t		pmd_read_check;	/* PMD read checker callback  (optional) */
+
+	pfm_pmd_sread_t		pmd_sread;	/* PMD model specific read (optional) */
+	pfm_pmd_swrite_t	pmd_swrite;	/* PMD model specific write (optional) */
+
+	int             	(*probe_pmu)(void);/* probe PMU routine */
+
+	u16			num_pmc_entries;/* number of entries in pmc_desc */
+	u16			num_pmd_entries;/* number of entries in pmd_desc */
+
+	void			*arch_info;	/* arch-specific information */
+	u32			flags;		/* set of flags */
+
+	struct module		*owner;		/* pointer to module struct */
+
+	/*
+	 * fields computed internally, do not set in module
+	 */
+	struct pfm_regdesc	regs;		/* registers currently available */
+	struct pfm_regdesc	full_regs;	/* registers presented by module */
+
+	u64			ovfl_mask;	/* overflow mask */
+	struct kobject		kobj;		/* for internal use only */
+};
+#define to_pmu(n) container_of(n, struct pfm_pmu_config, kobj)
+
+/*
+ * pfm_pmu_config flags
+ */
+#define PFM_PMUFL_IS_BUILTIN	0x1	/* pmu config is compiled in */
+
+/*
+ * we need to know whether the PMU description is builtin or compiled
+ * as a module
+ */
+#ifdef MODULE
+#define PFM_PMU_BUILTIN_FLAG	0	/* not built as a module */
+#else
+#define PFM_PMU_BUILTIN_FLAG	PFM_PMUFL_IS_BUILTIN /* built as a module */
+#endif
+
+int pfm_pmu_register(struct pfm_pmu_config *cfg);
+void pfm_pmu_unregister(struct pfm_pmu_config *cfg);
+
+#endif /* __PERFMON_PMU_H__ */
diff -Naur linux-2.6.25-org/include/linux/sched.h linux-2.6.25-id/include/linux/sched.h
--- linux-2.6.25-org/include/linux/sched.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/linux/sched.h	2008-04-23 11:22:11.000000000 +0200
@@ -97,6 +97,7 @@
 struct futex_pi_state;
 struct robust_list_head;
 struct bio;
+struct pfm_context;
 
 /*
  * List of flags we want to share for kernel threads,
diff -Naur linux-2.6.25-org/include/linux/syscalls.h linux-2.6.25-id/include/linux/syscalls.h
--- linux-2.6.25-org/include/linux/syscalls.h	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/include/linux/syscalls.h	2008-04-23 11:22:11.000000000 +0200
@@ -29,6 +29,13 @@
 struct new_utsname;
 struct nfsctl_arg;
 struct __old_kernel_stat;
+struct pfarg_ctx;
+struct pfarg_pmc;
+struct pfarg_pmd;
+struct pfarg_start;
+struct pfarg_load;
+struct pfarg_setinfo;
+struct pfarg_setdesc;
 struct pollfd;
 struct rlimit;
 struct rusage;
@@ -617,4 +624,27 @@
 
 int kernel_execve(const char *filename, char *const argv[], char *const envp[]);
 
+asmlinkage long sys_pfm_create_context(struct pfarg_ctx __user *ureq,
+				       void __user *uarg, size_t smpl_size);
+asmlinkage long sys_pfm_write_pmcs(int fd, struct pfarg_pmc __user *ureq,
+				   int count);
+asmlinkage long sys_pfm_write_pmds(int fd, struct pfarg_pmd __user *ureq,
+				   int count);
+asmlinkage long sys_pfm_read_pmds(int fd, struct pfarg_pmd __user *ureq,
+				  int count);
+asmlinkage long sys_pfm_restart(int fd);
+asmlinkage long sys_pfm_stop(int fd);
+asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *ureq);
+asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ureq);
+asmlinkage long sys_pfm_unload_context(int fd);
+asmlinkage long sys_pfm_delete_evtsets(int fd,
+				       struct pfarg_setinfo __user *ureq,
+				       int count);
+asmlinkage long sys_pfm_create_evtsets(int fd,
+				       struct pfarg_setdesc __user *ureq,
+				       int count);
+asmlinkage long sys_pfm_getinfo_evtsets(int fd,
+					struct pfarg_setinfo __user *ureq,
+					int count);
+
 #endif
diff -Naur linux-2.6.25-org/kernel/irq/manage.c linux-2.6.25-id/kernel/irq/manage.c
--- linux-2.6.25-org/kernel/irq/manage.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/kernel/irq/manage.c	2008-04-23 11:22:11.000000000 +0200
@@ -436,8 +436,13 @@
 			struct irqaction **pp = p;
 
 			p = &action->next;
-			if (action->dev_id != dev_id)
+			if (action->dev_id != dev_id) {
+				pr_debug("%s:%d: irq %u bad dev_id: request_irq(%p) != "
+					"free_irq(%p)\n" , __func__, __LINE__, irq, action->dev_id,
+					 dev_id);
+				BUG();
 				continue;
+			}
 
 			/* Found it - now remove it from the list of entries */
 			*pp = action->next;
diff -Naur linux-2.6.25-org/kernel/sys_ni.c linux-2.6.25-id/kernel/sys_ni.c
--- linux-2.6.25-org/kernel/sys_ni.c	2008-04-17 04:49:44.000000000 +0200
+++ linux-2.6.25-id/kernel/sys_ni.c	2008-04-23 11:22:11.000000000 +0200
@@ -122,6 +122,19 @@
 cond_syscall(compat_sys_ipc);
 cond_syscall(compat_sys_sysctl);
 
+cond_syscall(sys_pfm_create_context);
+cond_syscall(sys_pfm_write_pmcs);
+cond_syscall(sys_pfm_write_pmds);
+cond_syscall(sys_pfm_read_pmds);
+cond_syscall(sys_pfm_restart);
+cond_syscall(sys_pfm_start);
+cond_syscall(sys_pfm_stop);
+cond_syscall(sys_pfm_load_context);
+cond_syscall(sys_pfm_unload_context);
+cond_syscall(sys_pfm_create_evtsets);
+cond_syscall(sys_pfm_delete_evtsets);
+cond_syscall(sys_pfm_getinfo_evtsets);
+
 /* arch-specific weak syscall entries */
 cond_syscall(sys_pciconfig_read);
 cond_syscall(sys_pciconfig_write);
diff -Naur linux-2.6.25-org/patches/.gitignore linux-2.6.25-id/patches/.gitignore
--- linux-2.6.25-org/patches/.gitignore	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/.gitignore	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,4 @@
+# Normal rules
+#
+*~
+*#
diff -Naur linux-2.6.25-org/patches/other/MERGE_BLITS.diff linux-2.6.25-id/patches/other/MERGE_BLITS.diff
--- linux-2.6.25-org/patches/other/MERGE_BLITS.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/MERGE_BLITS.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,115 @@
+From: Krzysztof Helt <krzysztof.h1@wp.pl>
+
+My guess is that comparing is relatively fast comparing to preparation of the blit operation.
+One of the very first versions of the patch had a variant to merge small blits into bigger ones.
+It was done by regarding the first character after difference (string of different character) as different too.
+An advantage was fewer blitter operations but the operations got bigger at least by one character.
+
+You may look here (see MERGE_BLITS define):
+
+http://marc.info/?l=linux-fbdev-devel&m=117869435713671&w=2
+
+Some tests result on cards with real blitter:
+
+http://marc.info/?l=linux-fbdev-devel&m=117881573823606&w=2
+
+As you see, merging blits gave speed up on some cards (comparing to current method) despite it had more to compare.
+But it was usually lost at higher bpp as amount of data to move grown faster.
+
+I am interested in results of scrolling speeds with merged blits on your hardware.
+
+---------------------------------------------------------------------------
+
+                          -ga5fcaa21    -gcb32da04    -g9a79b227
+mode   rot     font      DFLT   READS  DFLT   READS  READS  +MERGE
+                                FAST          FAST   FAST    BLITS
+480p    0   default8x16   7.32   6.63   7.49   5.18   3.69   5.24
+480p    1   default8x16  11.39   5.79  11.6    7.60  10.18   8.09
+480p    2   default8x16   7.35   4.34   7.5    3.90   5.40   3.67
+480p    3   default8x16  11.39   8.85  11.61   9.31   6.80   9.92
+720p    0   default8x16  13.04  15.91  13.33   9.28   6.62   9.35
+720p    1   default8x16  22.76  12.73  23.17  18.35  23.51  19.83
+720p    2   default8x16  13.10  10.22  13.34   7.30  10.04   6.84
+720p    3   default8x16  22.60  19.65  23.03  19.56  14.48  21.11
+1080p   0   default8x16  20.76  34.25  21.09  14.98  10.78  15.01
+1080p   1   default8x16  35.13  23.82  35.78  26.27  34.15  28.11
+1080p   2   default8x16  20.69  21.51  21.12  11.57  15.79  10.73
+1080p   3   default8x16  34.98  37.03  35.68  29.23  21.47  31.43
+WUXGA   0   default8x16  24.92  46.83  25.52  18.05  12.98  18.05
+WUXGA   1   default8x16  41.19  31.08  41.98  28.61  37.90  30.99
+WUXGA   2   default8x16  25.00  29.11  25.56  13.74  18.81  12.64
+WUXGA   3   default8x16  41.20  49.49  42.02  34.79  25.66  37.84
+480p    0   lat4-19       7.20   6.57   7.35   5.12   3.63   5.18
+480p    1   lat4-19      11.19   6.02  11.36   7.86   9.97   8.31
+480p    2   lat4-19       7.21   4.32   7.36   3.83   5.31   3.62
+480p    3   lat4-19      11.12   8.90  11.33   9.08   7.01   9.63
+720p    0   lat4-19      12.69  15.57  12.94   9.09   6.52   9.13
+720p    1   lat4-19      22.43  12.92  22.86  17.46  21.68  18.89
+720p    2   lat4-19      12.71  10.04  13.08   7.19   9.83   6.70
+720p    3   lat4-19      22.50  20.02  22.89  20.21  15.92  21.84
+1080p   0   lat4-19      20.31  33.99  20.75  14.87  10.76  14.88
+1080p   1   lat4-19      34.60  24.06  35.26  25.19  31.67  27.02
+1080p   2   lat4-19      20.39  21.37  20.79  11.51  15.67  10.62
+1080p   3   lat4-19      34.50  37.45  35.24  29.13  22.61  31.30
+WUXGA   0   lat4-19      24.72  46.92  25.26  18.06  13.03  18.01
+WUXGA   1   lat4-19      40.98  32.32  41.77  30.52  38.22  33.30
+WUXGA   2   lat4-19      24.80  29.13  25.28  13.73  18.78  12.59
+WUXGA   3   lat4-19      41.25  50.26  42.25  35.50  27.82  38.63
+
+           RMS           24.50  26.47  24.99  18.60  18.87  19.74
+
+(RMS = Root Mean Squares)
+
+Two findings:
+  - MERGE_BLITS is faster for rotations 1 and 2, but slower for rotations
+    0 and 3.
+  - Todays' kernel behaves different than the one from two days ago, even
+    without changes to the console code.
+
+---
+ drivers/video/console/fbcon.c |   17 +++++++++++++++++
+ 1 files changed, 17 insertions(+)
+
+--- a/drivers/video/console/fbcon.c
++++ b/drivers/video/console/fbcon.c
+@@ -1,3 +1,4 @@
++#define MERGE_BLITS
+ /*
+  *  linux/drivers/video/fbcon.c -- Low level frame buffer based console driver
+  *
+@@ -1724,19 +1725,35 @@ static void fbcon_redraw_blit(struct vc_
+ 		unsigned short *le = advance_row(s, 1);
+ 		unsigned short c;
+ 		int x = 0;
++#ifdef MERGE_BLITS
++		int was_blit = 1;
++#endif
+ 
+ 		do {
+ 			c = scr_readw(s);
+ 
+ 			if (c == scr_readw(d)) {
+ 				if (s > start) {
++#ifdef MERGE_BLITS
++				    if (!was_blit) {
++#endif
+ 					ops->bmove(vc, info, line + ycount, x,
+ 						   line, x, 1, s-start);
+ 					x += s - start + 1;
+ 					start = s + 1;
++#ifdef MERGE_BLITS
++				    }
++				    was_blit = !was_blit;
++#endif
+ 				} else {
++#ifdef MERGE_BLITS
++				    if (was_blit) {
++#endif
+ 					x++;
+ 					start++;
++#ifdef MERGE_BLITS
++				    }
++#endif
+ 				}
+ 			}
+ 
diff -Naur linux-2.6.25-org/patches/other/boot-create-otheros-bld-in-output-dir.patch linux-2.6.25-id/patches/other/boot-create-otheros-bld-in-output-dir.patch
--- linux-2.6.25-org/patches/other/boot-create-otheros-bld-in-output-dir.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/boot-create-otheros-bld-in-output-dir.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,38 @@
+Subject: [PPC BOOT] Create otheros.bld next to specified output file, not in $object directory.
+
+From: David Woodhouse <dwmw2@infradead.org>
+
+The bootwrapper script currently generates an 'otheros.bld' file in
+addition to the file specified by the -o option, when asked to build a
+wrapper for PS3.
+
+It should do that in the same directory as the output, not the directory
+where the wrapper objects are kept (which might potentially not be
+writable when the script runs).
+
+Arguably, the 'otheros.bld' ought to be created with the filename
+specified as the -o argument. But that's a more intrusive change.
+
+Signed-off-by: David Woodhouse <dwmw2@infradead.org>
+Acked-by: Geoff Levand <geoffrey.levand@am.sony.com>
+--- a/arch/powerpc/boot/wrapper
++++ b/arch/powerpc/boot/wrapper
+@@ -287,8 +289,6 @@ ps3)
+     overlay_dest="256"
+     overlay_size="256"
+ 
+-    rm -f "$object/otheros.bld"
+-
+     ${CROSS}objcopy -O binary "$ofile" "$ofile.bin"
+ 
+     dd if="$ofile.bin" of="$ofile.bin" conv=notrunc   \
+@@ -299,6 +299,8 @@ ps3)
+         skip=$system_reset_overlay seek=$overlay_dest \
+         count=$overlay_size bs=1
+ 
+-    gzip --force -9 --stdout "$ofile.bin" > "$object/otheros.bld"
++    odir="$(dirname "$ofile.bin")"
++    rm -f "$odir/otheros.bld"
++    gzip --force -9 --stdout "$ofile.bin" > "$odir/otheros.bld"
+     ;;
+ esac
diff -Naur linux-2.6.25-org/patches/other/davem-01.patch linux-2.6.25-id/patches/other/davem-01.patch
--- linux-2.6.25-org/patches/other/davem-01.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-01.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,41 @@
+Date: 	Fri, 21 Dec 2007 20:53:13 -0800 (PST)
+Message-Id: <20071221.205313.108820486.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 1/12]: Remove inline from get_priv_size() and
+ adjust_priv_size().
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Remove inline from get_priv_size() and adjust_priv_size().
+
+The compiler inlines when appropriate.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ net/wireless/wext.c |    5 ++---
+ 1 file changed, 2 insertions(+), 3 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -514,7 +514,7 @@ static int call_commit_handler(struct ne
+ /*
+  * Calculate size of private arguments
+  */
+-static inline int get_priv_size(__u16	args)
++static int get_priv_size(__u16 args)
+ {
+ 	int	num = args & IW_PRIV_SIZE_MASK;
+ 	int	type = (args & IW_PRIV_TYPE_MASK) >> 12;
+@@ -526,8 +526,7 @@ static inline int get_priv_size(__u16	ar
+ /*
+  * Re-calculate the size of private arguments
+  */
+-static inline int adjust_priv_size(__u16		args,
+-				   union iwreq_data *	wrqu)
++static int adjust_priv_size(__u16 args, union iwreq_data *wrqu)
+ {
+ 	int	num = wrqu->data.length;
+ 	int	max = args & IW_PRIV_SIZE_MASK;
diff -Naur linux-2.6.25-org/patches/other/davem-02.patch linux-2.6.25-id/patches/other/davem-02.patch
--- linux-2.6.25-org/patches/other/davem-02.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-02.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,40 @@
+Date: 	Fri, 21 Dec 2007 20:53:42 -0800 (PST)
+Message-Id: <20071221.205342.212121038.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 2/12]: Make adjust_priv_size() take a "struct iw_point *".
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Make adjust_priv_size() take a "struct iw_point *".
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ net/wireless/wext.c |    6 +++---
+ 1 file changed, 3 insertions(+), 3 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -526,9 +526,9 @@ static int get_priv_size(__u16 args)
+ /*
+  * Re-calculate the size of private arguments
+  */
+-static int adjust_priv_size(__u16 args, union iwreq_data *wrqu)
++static int adjust_priv_size(__u16 args, struct iw_point *iwp)
+ {
+-	int	num = wrqu->data.length;
++	int	num = iwp->length;
+ 	int	max = args & IW_PRIV_SIZE_MASK;
+ 	int	type = (args & IW_PRIV_TYPE_MASK) >> 12;
+ 
+@@ -1008,7 +1008,7 @@ static int ioctl_private_call(struct net
+ 			 * avoid leaking kernel bits outside. */
+ 			if (!(descr->get_args & IW_PRIV_SIZE_FIXED)) {
+ 				extra_size = adjust_priv_size(descr->get_args,
+-							      &(iwr->u));
++							      &(iwr->u.data));
+ 			}
+ 
+ 			err = copy_to_user(iwr->u.data.pointer, extra,
diff -Naur linux-2.6.25-org/patches/other/davem-03.patch linux-2.6.25-id/patches/other/davem-03.patch
--- linux-2.6.25-org/patches/other/davem-03.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-03.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,292 @@
+Date: 	Fri, 21 Dec 2007 20:54:07 -0800 (PST)
+Message-Id: <20071221.205407.165162566.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 3/12]: Extract standard call iw_point handling into seperate
+ function.
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Extract standard call iw_point handling into seperate function.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ net/wireless/wext.c |  258 +++++++++++++++++++++++++++-------------------------
+ 1 file changed, 134 insertions(+), 124 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -726,6 +726,138 @@ void wext_proc_exit(struct net *net)
+  */
+ 
+ /* ---------------------------------------------------------------- */
++static int ioctl_standard_iw_point(struct iw_point *iwp, unsigned int cmd,
++				   const struct iw_ioctl_description *descr,
++				   iw_handler handler, struct net_device *dev,
++				   struct iw_request_info *info)
++{
++	int err, extra_size, user_length = 0, essid_compat = 0;
++	char *extra;
++
++	/* Calculate space needed by arguments. Always allocate
++	 * for max space.
++	 */
++	extra_size = descr->max_tokens * descr->token_size;
++
++	/* Check need for ESSID compatibility for WE < 21 */
++	switch (cmd) {
++	case SIOCSIWESSID:
++	case SIOCGIWESSID:
++	case SIOCSIWNICKN:
++	case SIOCGIWNICKN:
++		if (iwp->length == descr->max_tokens + 1)
++			essid_compat = 1;
++		else if (IW_IS_SET(cmd) && (iwp->length != 0)) {
++			char essid[IW_ESSID_MAX_SIZE + 1];
++
++			err = copy_from_user(essid, iwp->pointer,
++					     iwp->length *
++					     descr->token_size);
++			if (err)
++				return -EFAULT;
++
++			if (essid[iwp->length - 1] == '\0')
++				essid_compat = 1;
++		}
++		break;
++	default:
++		break;
++	}
++
++	iwp->length -= essid_compat;
++
++	/* Check what user space is giving us */
++	if (IW_IS_SET(cmd)) {
++		/* Check NULL pointer */
++		if (!iwp->pointer && iwp->length != 0)
++			return -EFAULT;
++		/* Check if number of token fits within bounds */
++		if (iwp->length > descr->max_tokens)
++			return -E2BIG;
++		if (iwp->length < descr->min_tokens)
++			return -EINVAL;
++	} else {
++		/* Check NULL pointer */
++		if (!iwp->pointer)
++			return -EFAULT;
++		/* Save user space buffer size for checking */
++		user_length = iwp->length;
++
++		/* Don't check if user_length > max to allow forward
++		 * compatibility. The test user_length < min is
++		 * implied by the test at the end.
++		 */
++
++		/* Support for very large requests */
++		if ((descr->flags & IW_DESCR_FLAG_NOMAX) &&
++		    (user_length > descr->max_tokens)) {
++			/* Allow userspace to GET more than max so
++			 * we can support any size GET requests.
++			 * There is still a limit : -ENOMEM.
++			 */
++			extra_size = user_length * descr->token_size;
++
++			/* Note : user_length is originally a __u16,
++			 * and token_size is controlled by us,
++			 * so extra_size won't get negative and
++			 * won't overflow...
++			 */
++		}
++	}
++
++	/* kzalloc() ensures NULL-termination for essid_compat. */
++	extra = kzalloc(extra_size, GFP_KERNEL);
++	if (!extra)
++		return -ENOMEM;
++
++	/* If it is a SET, get all the extra data in here */
++	if (IW_IS_SET(cmd) && (iwp->length != 0)) {
++		if (copy_from_user(extra, iwp->pointer,
++				   iwp->length *
++				   descr->token_size)) {
++			err = -EFAULT;
++			goto out;
++		}
++	}
++
++	err = handler(dev, info, (union iwreq_data *) iwp, extra);
++
++	iwp->length += essid_compat;
++
++	/* If we have something to return to the user */
++	if (!err && IW_IS_GET(cmd)) {
++		/* Check if there is enough buffer up there */
++		if (user_length < iwp->length) {
++			err = -E2BIG;
++			goto out;
++		}
++
++		if (copy_to_user(iwp->pointer, extra,
++				 iwp->length *
++				 descr->token_size)) {
++			err = -EFAULT;
++			goto out;
++		}
++	}
++
++	/* Generate an event to notify listeners of the change */
++	if ((descr->flags & IW_DESCR_FLAG_EVENT) && err == -EIWCOMMIT) {
++		union iwreq_data *data = (union iwreq_data *) iwp;
++
++		if (descr->flags & IW_DESCR_FLAG_RESTRICT)
++			/* If the event is restricted, don't
++			 * export the payload.
++			 */
++			wireless_send_event(dev, cmd, data, NULL);
++		else
++			wireless_send_event(dev, cmd, data, extra);
++	}
++
++out:
++	kfree(extra);
++	return err;
++}
++
+ /*
+  * Wrapper to call a standard Wireless Extension handler.
+  * We do various checks and also take care of moving data between
+@@ -761,130 +893,8 @@ static int ioctl_standard_call(struct ne
+ 		   ((ret == 0) || (ret == -EIWCOMMIT)))
+ 			wireless_send_event(dev, cmd, &(iwr->u), NULL);
+ 	} else {
+-		char *	extra;
+-		int	extra_size;
+-		int	user_length = 0;
+-		int	err;
+-		int	essid_compat = 0;
+-
+-		/* Calculate space needed by arguments. Always allocate
+-		 * for max space. Easier, and won't last long... */
+-		extra_size = descr->max_tokens * descr->token_size;
+-
+-		/* Check need for ESSID compatibility for WE < 21 */
+-		switch (cmd) {
+-		case SIOCSIWESSID:
+-		case SIOCGIWESSID:
+-		case SIOCSIWNICKN:
+-		case SIOCGIWNICKN:
+-			if (iwr->u.data.length == descr->max_tokens + 1)
+-				essid_compat = 1;
+-			else if (IW_IS_SET(cmd) && (iwr->u.data.length != 0)) {
+-				char essid[IW_ESSID_MAX_SIZE + 1];
+-
+-				err = copy_from_user(essid, iwr->u.data.pointer,
+-						     iwr->u.data.length *
+-						     descr->token_size);
+-				if (err)
+-					return -EFAULT;
+-
+-				if (essid[iwr->u.data.length - 1] == '\0')
+-					essid_compat = 1;
+-			}
+-			break;
+-		default:
+-			break;
+-		}
+-
+-		iwr->u.data.length -= essid_compat;
+-
+-		/* Check what user space is giving us */
+-		if (IW_IS_SET(cmd)) {
+-			/* Check NULL pointer */
+-			if ((iwr->u.data.pointer == NULL) &&
+-			   (iwr->u.data.length != 0))
+-				return -EFAULT;
+-			/* Check if number of token fits within bounds */
+-			if (iwr->u.data.length > descr->max_tokens)
+-				return -E2BIG;
+-			if (iwr->u.data.length < descr->min_tokens)
+-				return -EINVAL;
+-		} else {
+-			/* Check NULL pointer */
+-			if (iwr->u.data.pointer == NULL)
+-				return -EFAULT;
+-			/* Save user space buffer size for checking */
+-			user_length = iwr->u.data.length;
+-
+-			/* Don't check if user_length > max to allow forward
+-			 * compatibility. The test user_length < min is
+-			 * implied by the test at the end. */
+-
+-			/* Support for very large requests */
+-			if ((descr->flags & IW_DESCR_FLAG_NOMAX) &&
+-			   (user_length > descr->max_tokens)) {
+-				/* Allow userspace to GET more than max so
+-				 * we can support any size GET requests.
+-				 * There is still a limit : -ENOMEM. */
+-				extra_size = user_length * descr->token_size;
+-				/* Note : user_length is originally a __u16,
+-				 * and token_size is controlled by us,
+-				 * so extra_size won't get negative and
+-				 * won't overflow... */
+-			}
+-		}
+-
+-		/* Create the kernel buffer */
+-		/*    kzalloc ensures NULL-termination for essid_compat */
+-		extra = kzalloc(extra_size, GFP_KERNEL);
+-		if (extra == NULL)
+-			return -ENOMEM;
+-
+-		/* If it is a SET, get all the extra data in here */
+-		if (IW_IS_SET(cmd) && (iwr->u.data.length != 0)) {
+-			err = copy_from_user(extra, iwr->u.data.pointer,
+-					     iwr->u.data.length *
+-					     descr->token_size);
+-			if (err) {
+-				kfree(extra);
+-				return -EFAULT;
+-			}
+-		}
+-
+-		/* Call the handler */
+-		ret = handler(dev, &info, &(iwr->u), extra);
+-
+-		iwr->u.data.length += essid_compat;
+-
+-		/* If we have something to return to the user */
+-		if (!ret && IW_IS_GET(cmd)) {
+-			/* Check if there is enough buffer up there */
+-			if (user_length < iwr->u.data.length) {
+-				kfree(extra);
+-				return -E2BIG;
+-			}
+-
+-			err = copy_to_user(iwr->u.data.pointer, extra,
+-					   iwr->u.data.length *
+-					   descr->token_size);
+-			if (err)
+-				ret =  -EFAULT;
+-		}
+-
+-		/* Generate an event to notify listeners of the change */
+-		if ((descr->flags & IW_DESCR_FLAG_EVENT) &&
+-		   ((ret == 0) || (ret == -EIWCOMMIT))) {
+-			if (descr->flags & IW_DESCR_FLAG_RESTRICT)
+-				/* If the event is restricted, don't
+-				 * export the payload */
+-				wireless_send_event(dev, cmd, &(iwr->u), NULL);
+-			else
+-				wireless_send_event(dev, cmd, &(iwr->u),
+-						    extra);
+-		}
+-
+-		/* Cleanup - I told you it wasn't that long ;-) */
+-		kfree(extra);
++		ret = ioctl_standard_iw_point(&iwr->u.data, cmd, descr,
++					      handler, dev, &info);
+ 	}
+ 
+ 	/* Call commit handler if needed and defined */
diff -Naur linux-2.6.25-org/patches/other/davem-04.patch linux-2.6.25-id/patches/other/davem-04.patch
--- linux-2.6.25-org/patches/other/davem-04.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-04.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,194 @@
+Date: 	Fri, 21 Dec 2007 20:54:29 -0800 (PST)
+Message-Id: <20071221.205429.175484002.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 4/12]: Extract private call iw_point handling into seperate
+ functions.
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Extract private call iw_point handling into seperate functions.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ net/wireless/wext.c |  143 +++++++++++++++++++++++++++-------------------------
+ 1 file changed, 75 insertions(+), 68 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -922,25 +922,22 @@ static int ioctl_standard_call(struct ne
+  * a iw_handler but process it in your ioctl handler (i.e. use the
+  * old driver API).
+  */
+-static int ioctl_private_call(struct net_device *dev, struct ifreq *ifr,
+-			      unsigned int cmd, iw_handler handler)
++static int get_priv_descr_and_size(struct net_device *dev, unsigned int cmd,
++				   const struct iw_priv_args **descrp)
+ {
+-	struct iwreq *			iwr = (struct iwreq *) ifr;
+-	const struct iw_priv_args *	descr = NULL;
+-	struct iw_request_info		info;
+-	int				extra_size = 0;
+-	int				i;
+-	int				ret = -EINVAL;
++	const struct iw_priv_args *descr;
++	int i, extra_size;
+ 
+-	/* Get the description of the IOCTL */
+-	for (i = 0; i < dev->wireless_handlers->num_private_args; i++)
++	descr = NULL;
++	for (i = 0; i < dev->wireless_handlers->num_private_args; i++) {
+ 		if (cmd == dev->wireless_handlers->private_args[i].cmd) {
+-			descr = &(dev->wireless_handlers->private_args[i]);
++			descr = &dev->wireless_handlers->private_args[i];
+ 			break;
+ 		}
++	}
+ 
+-	/* Compute the size of the set/get arguments */
+-	if (descr != NULL) {
++	extra_size = 0;
++	if (descr) {
+ 		if (IW_IS_SET(cmd)) {
+ 			int	offset = 0;	/* For sub-ioctls */
+ 			/* Check for sub-ioctl handler */
+@@ -965,72 +962,82 @@ static int ioctl_private_call(struct net
+ 				extra_size = 0;
+ 		}
+ 	}
++	*descrp = descr;
++	return extra_size;
++}
+ 
+-	/* Prepare the call */
+-	info.cmd = cmd;
+-	info.flags = 0;
+-
+-	/* Check if we have a pointer to user space data or not. */
+-	if (extra_size == 0) {
+-		/* No extra arguments. Trivial to handle */
+-		ret = handler(dev, &info, &(iwr->u), (char *) &(iwr->u));
+-	} else {
+-		char *	extra;
+-		int	err;
++static int ioctl_private_iw_point(struct iw_point *iwp, unsigned int cmd,
++				  const struct iw_priv_args *descr,
++				  iw_handler handler, struct net_device *dev,
++				  struct iw_request_info *info, int extra_size)
++{
++	char *extra;
++	int err;
+ 
+-		/* Check what user space is giving us */
+-		if (IW_IS_SET(cmd)) {
+-			/* Check NULL pointer */
+-			if ((iwr->u.data.pointer == NULL) &&
+-			   (iwr->u.data.length != 0))
+-				return -EFAULT;
+-
+-			/* Does it fits within bounds ? */
+-			if (iwr->u.data.length > (descr->set_args &
+-						 IW_PRIV_SIZE_MASK))
+-				return -E2BIG;
+-		} else if (iwr->u.data.pointer == NULL)
++	/* Check what user space is giving us */
++	if (IW_IS_SET(cmd)) {
++		if (!iwp->pointer && iwp->length != 0)
+ 			return -EFAULT;
+ 
+-		/* Always allocate for max space. Easier, and won't last
+-		 * long... */
+-		extra = kmalloc(extra_size, GFP_KERNEL);
+-		if (extra == NULL)
+-			return -ENOMEM;
+-
+-		/* If it is a SET, get all the extra data in here */
+-		if (IW_IS_SET(cmd) && (iwr->u.data.length != 0)) {
+-			err = copy_from_user(extra, iwr->u.data.pointer,
+-					     extra_size);
+-			if (err) {
+-				kfree(extra);
+-				return -EFAULT;
+-			}
++		if (iwp->length > (descr->set_args & IW_PRIV_SIZE_MASK))
++			return -E2BIG;
++	} else if (!iwp->pointer)
++		return -EFAULT;
++
++	extra = kmalloc(extra_size, GFP_KERNEL);
++	if (!extra)
++		return -ENOMEM;
++
++	/* If it is a SET, get all the extra data in here */
++	if (IW_IS_SET(cmd) && (iwp->length != 0)) {
++		if (copy_from_user(extra, iwp->pointer, extra_size)) {
++			err = -EFAULT;
++			goto out;
+ 		}
++	}
+ 
+-		/* Call the handler */
+-		ret = handler(dev, &info, &(iwr->u), extra);
++	/* Call the handler */
++	err = handler(dev, info, (union iwreq_data *) iwp, extra);
+ 
+-		/* If we have something to return to the user */
+-		if (!ret && IW_IS_GET(cmd)) {
++	/* If we have something to return to the user */
++	if (!err && IW_IS_GET(cmd)) {
++		/* Adjust for the actual length if it's variable,
++		 * avoid leaking kernel bits outside.
++		 */
++		if (!(descr->get_args & IW_PRIV_SIZE_FIXED))
++			extra_size = adjust_priv_size(descr->get_args, iwp);
+ 
+-			/* Adjust for the actual length if it's variable,
+-			 * avoid leaking kernel bits outside. */
+-			if (!(descr->get_args & IW_PRIV_SIZE_FIXED)) {
+-				extra_size = adjust_priv_size(descr->get_args,
+-							      &(iwr->u.data));
+-			}
++		if (copy_to_user(iwp->pointer, extra, extra_size))
++			err =  -EFAULT;
++	}
+ 
+-			err = copy_to_user(iwr->u.data.pointer, extra,
+-					   extra_size);
+-			if (err)
+-				ret =  -EFAULT;
+-		}
++out:
++	kfree(extra);
++	return err;
++}
+ 
+-		/* Cleanup - I told you it wasn't that long ;-) */
+-		kfree(extra);
+-	}
++static int ioctl_private_call(struct net_device *dev, struct ifreq *ifr,
++			      unsigned int cmd, iw_handler handler)
++{
++	struct iwreq *iwr = (struct iwreq *) ifr;
++	int extra_size = 0, ret = -EINVAL;
++	const struct iw_priv_args *descr;
++	struct iw_request_info info;
++
++	extra_size = get_priv_descr_and_size(dev, cmd, &descr);
+ 
++	/* Prepare the call */
++	info.cmd = cmd;
++	info.flags = 0;
++
++	/* Check if we have a pointer to user space data or not. */
++	if (extra_size == 0) {
++		/* No extra arguments. Trivial to handle */
++		ret = handler(dev, &info, &(iwr->u), (char *) &(iwr->u));
++	} else {
++		ret = ioctl_private_iw_point(&iwr->u.data, cmd, descr,
++					     handler, dev, &info, extra_size);
++	}
+ 
+ 	/* Call commit handler if needed and defined */
+ 	if (ret == -EIWCOMMIT)
diff -Naur linux-2.6.25-org/patches/other/davem-05.patch linux-2.6.25-id/patches/other/davem-05.patch
--- linux-2.6.25-org/patches/other/davem-05.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-05.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,54 @@
+Date: 	Fri, 21 Dec 2007 20:54:56 -0800 (PST)
+Message-Id: <20071221.205456.259648428.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 5/12]: Pull ioctl permission checking out into helper
+ function.
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Pull ioctl permission checking out into helper function.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ net/wireless/wext.c |   22 +++++++++++++++-------
+ 1 file changed, 15 insertions(+), 7 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -1093,18 +1093,26 @@ static int wireless_process_ioctl(struct
+ 	return -EOPNOTSUPP;
+ }
+ 
++/* If command is `set a parameter', or `get the encoding parameters',
++ * check if the user has the right to do it.
++ */
++static int wext_permission_check(unsigned int cmd)
++{
++	if ((IW_IS_SET(cmd) || cmd == SIOCGIWENCODE || cmd == SIOCGIWENCODEEXT)
++	    && !capable(CAP_NET_ADMIN))
++		return -EPERM;
++
++	return 0;
++}
++
+ /* entry point from dev ioctl */
+ int wext_handle_ioctl(struct net *net, struct ifreq *ifr, unsigned int cmd,
+ 		      void __user *arg)
+ {
+-	int ret;
++	int ret = wext_permission_check(cmd);
+ 
+-	/* If command is `set a parameter', or
+-	 * `get the encoding parameters', check if
+-	 * the user has the right to do it */
+-	if ((IW_IS_SET(cmd) || cmd == SIOCGIWENCODE || cmd == SIOCGIWENCODEEXT)
+-	    && !capable(CAP_NET_ADMIN))
+-		return -EPERM;
++	if (ret)
++		return ret;
+ 
+ 	dev_load(net, ifr->ifr_name);
+ 	rtnl_lock();
diff -Naur linux-2.6.25-org/patches/other/davem-06.patch linux-2.6.25-id/patches/other/davem-06.patch
--- linux-2.6.25-org/patches/other/davem-06.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-06.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,81 @@
+Date: 	Fri, 21 Dec 2007 20:55:28 -0800 (PST)
+Message-Id: <20071221.205528.53416957.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 6/12]: Parameterize the standard/private handlers.
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Parameterize the standard/private handlers.
+
+The WEXT standard and private handlers to use are now
+arguments to wireless_process_ioctl().
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ net/wireless/wext.c |   24 ++++++++++++++++--------
+ 1 file changed, 16 insertions(+), 8 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -1047,11 +1047,17 @@ static int ioctl_private_call(struct net
+ }
+ 
+ /* ---------------------------------------------------------------- */
++typedef int (*wext_ioctl_func)(struct net_device *, struct ifreq *,
++			       unsigned int, iw_handler);
++
+ /*
+  * Main IOCTl dispatcher.
+  * Check the type of IOCTL and call the appropriate wrapper...
+  */
+-static int wireless_process_ioctl(struct net *net, struct ifreq *ifr, unsigned int cmd)
++static int wireless_process_ioctl(struct net *net, struct ifreq *ifr,
++				  unsigned int cmd,
++				  wext_ioctl_func standard,
++				  wext_ioctl_func private)
+ {
+ 	struct net_device *dev;
+ 	iw_handler	handler;
+@@ -1067,12 +1073,12 @@ static int wireless_process_ioctl(struct
+ 	 * Note that 'cmd' is already filtered in dev_ioctl() with
+ 	 * (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST) */
+ 	if (cmd == SIOCGIWSTATS)
+-		return ioctl_standard_call(dev, ifr, cmd,
+-					   &iw_handler_get_iwstats);
++		return standard(dev, ifr, cmd,
++				&iw_handler_get_iwstats);
+ 
+ 	if (cmd == SIOCGIWPRIV && dev->wireless_handlers)
+-		return ioctl_standard_call(dev, ifr, cmd,
+-					   &iw_handler_get_private);
++		return standard(dev, ifr, cmd,
++				&iw_handler_get_private);
+ 
+ 	/* Basic check */
+ 	if (!netif_device_present(dev))
+@@ -1083,9 +1089,9 @@ static int wireless_process_ioctl(struct
+ 	if (handler) {
+ 		/* Standard and private are not the same */
+ 		if (cmd < SIOCIWFIRSTPRIV)
+-			return ioctl_standard_call(dev, ifr, cmd, handler);
++			return standard(dev, ifr, cmd, handler);
+ 		else
+-			return ioctl_private_call(dev, ifr, cmd, handler);
++			return private(dev, ifr, cmd, handler);
+ 	}
+ 	/* Old driver API : call driver ioctl handler */
+ 	if (dev->do_ioctl)
+@@ -1116,7 +1122,9 @@ int wext_handle_ioctl(struct net *net, s
+ 
+ 	dev_load(net, ifr->ifr_name);
+ 	rtnl_lock();
+-	ret = wireless_process_ioctl(net, ifr, cmd);
++	ret = wireless_process_ioctl(net, ifr, cmd,
++				     ioctl_standard_call,
++				     ioctl_private_call);
+ 	rtnl_unlock();
+ 	if (IW_IS_GET(cmd) && copy_to_user(arg, ifr, sizeof(struct iwreq)))
+ 		return -EFAULT;
diff -Naur linux-2.6.25-org/patches/other/davem-07.patch linux-2.6.25-id/patches/other/davem-07.patch
--- linux-2.6.25-org/patches/other/davem-07.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-07.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,90 @@
+Date: 	Fri, 21 Dec 2007 20:55:54 -0800 (PST)
+Message-Id: <20071221.205554.40677101.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 7/12]: Pass iwreq pointer down into standard/private
+ handlers.
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Pass iwreq pointer down into standard/private handlers.
+
+They have no need to see the object as an ifreq.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ net/wireless/wext.c |   17 ++++++++---------
+ 1 file changed, 8 insertions(+), 9 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -864,11 +864,10 @@ out:
+  * user space and kernel space.
+  */
+ static int ioctl_standard_call(struct net_device *	dev,
+-			       struct ifreq *		ifr,
++			       struct iwreq *		iwr,
+ 			       unsigned int		cmd,
+ 			       iw_handler		handler)
+ {
+-	struct iwreq *				iwr = (struct iwreq *) ifr;
+ 	const struct iw_ioctl_description *	descr;
+ 	struct iw_request_info			info;
+ 	int					ret = -EINVAL;
+@@ -1016,10 +1015,9 @@ out:
+ 	return err;
+ }
+ 
+-static int ioctl_private_call(struct net_device *dev, struct ifreq *ifr,
++static int ioctl_private_call(struct net_device *dev, struct iwreq *iwr,
+ 			      unsigned int cmd, iw_handler handler)
+ {
+-	struct iwreq *iwr = (struct iwreq *) ifr;
+ 	int extra_size = 0, ret = -EINVAL;
+ 	const struct iw_priv_args *descr;
+ 	struct iw_request_info info;
+@@ -1047,7 +1045,7 @@ static int ioctl_private_call(struct net
+ }
+ 
+ /* ---------------------------------------------------------------- */
+-typedef int (*wext_ioctl_func)(struct net_device *, struct ifreq *,
++typedef int (*wext_ioctl_func)(struct net_device *, struct iwreq *,
+ 			       unsigned int, iw_handler);
+ 
+ /*
+@@ -1059,6 +1057,7 @@ static int wireless_process_ioctl(struct
+ 				  wext_ioctl_func standard,
+ 				  wext_ioctl_func private)
+ {
++	struct iwreq *iwr = (struct iwreq *) ifr;
+ 	struct net_device *dev;
+ 	iw_handler	handler;
+ 
+@@ -1073,11 +1072,11 @@ static int wireless_process_ioctl(struct
+ 	 * Note that 'cmd' is already filtered in dev_ioctl() with
+ 	 * (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST) */
+ 	if (cmd == SIOCGIWSTATS)
+-		return standard(dev, ifr, cmd,
++		return standard(dev, iwr, cmd,
+ 				&iw_handler_get_iwstats);
+ 
+ 	if (cmd == SIOCGIWPRIV && dev->wireless_handlers)
+-		return standard(dev, ifr, cmd,
++		return standard(dev, iwr, cmd,
+ 				&iw_handler_get_private);
+ 
+ 	/* Basic check */
+@@ -1089,9 +1088,9 @@ static int wireless_process_ioctl(struct
+ 	if (handler) {
+ 		/* Standard and private are not the same */
+ 		if (cmd < SIOCIWFIRSTPRIV)
+-			return standard(dev, ifr, cmd, handler);
++			return standard(dev, iwr, cmd, handler);
+ 		else
+-			return private(dev, ifr, cmd, handler);
++			return private(dev, iwr, cmd, handler);
+ 	}
+ 	/* Old driver API : call driver ioctl handler */
+ 	if (dev->do_ioctl)
diff -Naur linux-2.6.25-org/patches/other/davem-08-fix.patch linux-2.6.25-id/patches/other/davem-08-fix.patch
--- linux-2.6.25-org/patches/other/davem-08-fix.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-08-fix.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,15 @@
+---
+ net/wireless/wext.c |    2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -1136,7 +1136,7 @@ int wext_handle_ioctl(struct net *net, s
+ 				      ioctl_standard_call,
+ 				      ioctl_private_call);
+ 
+-	if (ret > 0 &&
++	if (ret >= 0 &&
+ 	    IW_IS_GET(cmd) &&
+ 	    copy_to_user(arg, ifr, sizeof(struct iwreq)))
+ 		return -EFAULT;
diff -Naur linux-2.6.25-org/patches/other/davem-08.patch linux-2.6.25-id/patches/other/davem-08.patch
--- linux-2.6.25-org/patches/other/davem-08.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-08.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,62 @@
+Date: 	Fri, 21 Dec 2007 20:56:23 -0800 (PST)
+Message-Id: <20071221.205623.141035851.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 8/12]: Pull top-level ioctl dispatch logic into helper
+ function.
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Pull top-level ioctl dispatch logic into helper function.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ net/wireless/wext.c |   26 ++++++++++++++++++++------
+ 1 file changed, 20 insertions(+), 6 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -1111,8 +1111,10 @@ static int wext_permission_check(unsigne
+ }
+ 
+ /* entry point from dev ioctl */
+-int wext_handle_ioctl(struct net *net, struct ifreq *ifr, unsigned int cmd,
+-		      void __user *arg)
++static int wext_ioctl_dispatch(struct net *net, struct ifreq *ifr,
++			       unsigned int cmd,
++			       wext_ioctl_func standard,
++			       wext_ioctl_func private)
+ {
+ 	int ret = wext_permission_check(cmd);
+ 
+@@ -1121,12 +1123,24 @@ int wext_handle_ioctl(struct net *net, s
+ 
+ 	dev_load(net, ifr->ifr_name);
+ 	rtnl_lock();
+-	ret = wireless_process_ioctl(net, ifr, cmd,
+-				     ioctl_standard_call,
+-				     ioctl_private_call);
++	ret = wireless_process_ioctl(net, ifr, cmd, standard, private);
+ 	rtnl_unlock();
+-	if (IW_IS_GET(cmd) && copy_to_user(arg, ifr, sizeof(struct iwreq)))
++
++	return ret;
++}
++
++int wext_handle_ioctl(struct net *net, struct ifreq *ifr, unsigned int cmd,
++		      void __user *arg)
++{
++	int ret = wext_ioctl_dispatch(net, ifr, cmd,
++				      ioctl_standard_call,
++				      ioctl_private_call);
++
++	if (ret > 0 &&
++	    IW_IS_GET(cmd) &&
++	    copy_to_user(arg, ifr, sizeof(struct iwreq)))
+ 		return -EFAULT;
++
+ 	return ret;
+ }
+ 
diff -Naur linux-2.6.25-org/patches/other/davem-09-fix.patch linux-2.6.25-id/patches/other/davem-09-fix.patch
--- linux-2.6.25-org/patches/other/davem-09-fix.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-09-fix.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,15 @@
+---
+ net/wireless/wext.c |    2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -1239,7 +1239,7 @@ int compat_wext_handle_ioctl(struct net 
+ 				  compat_standard_call,
+ 				  compat_private_call);
+ 
+-	if (ret > 0 &&
++	if (ret >= 0 &&
+ 	    IW_IS_GET(cmd) &&
+ 	    copy_to_user(argp, &iwr, sizeof(struct iwreq)))
+ 		return -EFAULT;
diff -Naur linux-2.6.25-org/patches/other/davem-09.patch linux-2.6.25-id/patches/other/davem-09.patch
--- linux-2.6.25-org/patches/other/davem-09.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-09.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,233 @@
+Date: 	Fri, 21 Dec 2007 20:56:48 -0800 (PST)
+Message-Id: <20071221.205648.29936075.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 9/12]: Dispatch and handle compat ioctls entirely in
+ net/wireless/wext.c
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Dispatch and handle compat ioctls entirely in net/wireless/wext.c
+
+Next we can kill the hacks in fs/compat_ioctl.c and also
+dispatch compat ioctls down into the driver and 80211 protocol
+helper layers in order to handle iw_point objects embedded in
+stream replies which need to be translated.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ fs/compat_ioctl.c        |    6 --
+ include/linux/wireless.h |    9 ++++
+ include/net/wext.h       |    7 +++
+ net/socket.c             |   10 ++++
+ net/wireless/wext.c      |  104 +++++++++++++++++++++++++++++++++++++++++++++++
+ 5 files changed, 130 insertions(+), 6 deletions(-)
+
+--- a/fs/compat_ioctl.c
++++ b/fs/compat_ioctl.c
+@@ -1756,12 +1756,6 @@ static int do_i2c_smbus_ioctl(unsigned i
+ 	return sys_ioctl(fd, cmd, (unsigned long)tdata);
+ }
+ 
+-struct compat_iw_point {
+-	compat_caddr_t pointer;
+-	__u16 length;
+-	__u16 flags;
+-};
+-
+ static int do_wireless_ioctl(unsigned int fd, unsigned int cmd, unsigned long arg)
+ {
+ 	struct iwreq __user *iwr;
+--- a/include/linux/wireless.h
++++ b/include/linux/wireless.h
+@@ -76,6 +76,7 @@
+ #include <linux/types.h>		/* for "caddr_t" et al		*/
+ #include <linux/socket.h>		/* for "struct sockaddr" et al	*/
+ #include <linux/if.h>			/* for IFNAMSIZ and co... */
++#include <linux/compat.h>
+ #endif	/* __KERNEL__ */
+ 
+ /***************************** VERSION *****************************/
+@@ -669,6 +670,14 @@ struct	iw_point
+   __u16		flags;		/* Optional params */
+ };
+ 
++#if defined(__KERNEL__) && defined(CONFIG_COMPAT)
++struct compat_iw_point {
++	compat_caddr_t pointer;
++	__u16 length;
++	__u16 flags;
++};
++#endif
++
+ /*
+  *	A frequency
+  *	For numbers lower than 10^9, we encode the number in 'm' and
+--- a/include/net/wext.h
++++ b/include/net/wext.h
+@@ -12,6 +12,8 @@ extern int wext_proc_init(struct net *ne
+ extern void wext_proc_exit(struct net *net);
+ extern int wext_handle_ioctl(struct net *net, struct ifreq *ifr, unsigned int cmd,
+ 			     void __user *arg);
++extern int compat_wext_handle_ioctl(struct net *net, unsigned int cmd,
++				    unsigned long arg);
+ #else
+ static inline int wext_proc_init(struct net *net)
+ {
+@@ -26,6 +28,11 @@ static inline int wext_handle_ioctl(stru
+ {
+ 	return -EINVAL;
+ }
++static inline int compat_wext_handle_ioctl(struct net *net, unsigned int cmd,
++					   unsigned long arg)
++{
++	return -EINVAL;
++}
+ #endif
+ 
+ #endif /* __NET_WEXT_H */
+--- a/net/socket.c
++++ b/net/socket.c
+@@ -90,6 +90,7 @@
+ #include <asm/unistd.h>
+ 
+ #include <net/compat.h>
++#include <net/wext.h>
+ 
+ #include <net/sock.h>
+ #include <linux/netfilter.h>
+@@ -2207,10 +2208,19 @@ static long compat_sock_ioctl(struct fil
+ {
+ 	struct socket *sock = file->private_data;
+ 	int ret = -ENOIOCTLCMD;
++	struct sock *sk;
++	struct net *net;
++
++	sk = sock->sk;
++	net = sk->sk_net;
+ 
+ 	if (sock->ops->compat_ioctl)
+ 		ret = sock->ops->compat_ioctl(sock, cmd, arg);
+ 
++	if (ret == -ENOIOCTLCMD &&
++	    (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST))
++		ret = compat_wext_handle_ioctl(net, cmd, arg);
++
+ 	return ret;
+ }
+ #endif
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -1144,6 +1144,110 @@ int wext_handle_ioctl(struct net *net, s
+ 	return ret;
+ }
+ 
++#ifdef CONFIG_COMPAT
++static int compat_standard_call(struct net_device *	dev,
++				struct iwreq *		iwr,
++				unsigned int		cmd,
++				iw_handler		handler)
++{
++	const struct iw_ioctl_description *descr;
++	struct compat_iw_point *iwp_compat;
++	struct iw_request_info info;
++	struct iw_point iwp;
++	int err;
++
++	descr = standard_ioctl + (cmd - SIOCIWFIRST);
++
++	if (descr->header_type != IW_HEADER_TYPE_POINT)
++		return ioctl_standard_call(dev, iwr, cmd, handler);
++
++	iwp_compat = (struct compat_iw_point *) &iwr->u.data;
++	iwp.pointer = compat_ptr(iwp_compat->pointer);
++	iwp.length = iwp_compat->length;
++	iwp.flags = iwp_compat->flags;
++
++	info.cmd = cmd;
++	info.flags = 0;
++
++	err = ioctl_standard_iw_point(&iwp, cmd, descr, handler, dev, &info);
++
++	iwp_compat->pointer = ptr_to_compat(iwp.pointer);
++	iwp_compat->length = iwp.length;
++	iwp_compat->flags = iwp.flags;
++
++	return err;
++}
++
++static int compat_private_call(struct net_device *dev, struct iwreq *iwr,
++			       unsigned int cmd, iw_handler handler)
++{
++	const struct iw_priv_args *descr;
++	struct iw_request_info info;
++	int ret, extra_size;
++
++	extra_size = get_priv_descr_and_size(dev, cmd, &descr);
++
++	/* Prepare the call */
++	info.cmd = cmd;
++	info.flags = 0;
++
++	/* Check if we have a pointer to user space data or not. */
++	if (extra_size == 0) {
++		/* No extra arguments. Trivial to handle */
++		ret = handler(dev, &info, &(iwr->u), (char *) &(iwr->u));
++	} else {
++		struct compat_iw_point *iwp_compat;
++		struct iw_point iwp;
++
++		iwp_compat = (struct compat_iw_point *) &iwr->u.data;
++		iwp.pointer = compat_ptr(iwp_compat->pointer);
++		iwp.length = iwp_compat->length;
++		iwp.flags = iwp_compat->flags;
++
++		ret = ioctl_private_iw_point(&iwp, cmd, descr,
++					     handler, dev, &info, extra_size);
++
++		iwp_compat->pointer = ptr_to_compat(iwp.pointer);
++		iwp_compat->length = iwp.length;
++		iwp_compat->flags = iwp.flags;
++	}
++
++	/* Call commit handler if needed and defined */
++	if (ret == -EIWCOMMIT)
++		ret = call_commit_handler(dev);
++
++	return ret;
++}
++
++int compat_wext_handle_ioctl(struct net *net, unsigned int cmd,
++			     unsigned long arg)
++{
++	void __user *argp = (void __user *)arg;
++	struct iwreq iwr;
++	char *colon;
++	int ret;
++
++	if (copy_from_user(&iwr, argp, sizeof(struct iwreq)))
++		return -EFAULT;
++
++	iwr.ifr_name[IFNAMSIZ-1] = 0;
++	colon = strchr(iwr.ifr_name, ':');
++	if (colon)
++		*colon = 0;
++
++	ret = wext_ioctl_dispatch(net, (struct ifreq *) &iwr, cmd,
++				  compat_standard_call,
++				  compat_private_call);
++
++	if (ret > 0 &&
++	    IW_IS_GET(cmd) &&
++	    copy_to_user(argp, &iwr, sizeof(struct iwreq)))
++		return -EFAULT;
++
++	return ret;
++}
++#endif
++
+ /************************* EVENT PROCESSING *************************/
+ /*
+  * Process events generated by the wireless layer or the driver.
diff -Naur linux-2.6.25-org/patches/other/davem-10.patch linux-2.6.25-id/patches/other/davem-10.patch
--- linux-2.6.25-org/patches/other/davem-10.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-10.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,156 @@
+Date: 	Fri, 21 Dec 2007 20:57:17 -0800 (PST)
+Message-Id: <20071221.205717.179449993.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 10/12]: Remove compat handling from fs/compat_ioctl.c
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Remove compat handling from fs/compat_ioctl.c
+
+No longer used.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ fs/compat_ioctl.c |  107 ------------------------------------------------------
+ 1 file changed, 1 insertion(+), 106 deletions(-)
+
+--- a/fs/compat_ioctl.c
++++ b/fs/compat_ioctl.c
+@@ -56,7 +56,6 @@
+ #include <linux/syscalls.h>
+ #include <linux/i2c.h>
+ #include <linux/i2c-dev.h>
+-#include <linux/wireless.h>
+ #include <linux/atalk.h>
+ #include <linux/loop.h>
+ 
+@@ -1756,58 +1755,6 @@ static int do_i2c_smbus_ioctl(unsigned i
+ 	return sys_ioctl(fd, cmd, (unsigned long)tdata);
+ }
+ 
+-static int do_wireless_ioctl(unsigned int fd, unsigned int cmd, unsigned long arg)
+-{
+-	struct iwreq __user *iwr;
+-	struct iwreq __user *iwr_u;
+-	struct iw_point __user *iwp;
+-	struct compat_iw_point __user *iwp_u;
+-	compat_caddr_t pointer_u;
+-	void __user *pointer;
+-	__u16 length, flags;
+-	int ret;
+-
+-	iwr_u = compat_ptr(arg);
+-	iwp_u = (struct compat_iw_point __user *) &iwr_u->u.data;
+-	iwr = compat_alloc_user_space(sizeof(*iwr));
+-	if (iwr == NULL)
+-		return -ENOMEM;
+-
+-	iwp = &iwr->u.data;
+-
+-	if (!access_ok(VERIFY_WRITE, iwr, sizeof(*iwr)))
+-		return -EFAULT;
+-
+-	if (__copy_in_user(&iwr->ifr_ifrn.ifrn_name[0],
+-			   &iwr_u->ifr_ifrn.ifrn_name[0],
+-			   sizeof(iwr->ifr_ifrn.ifrn_name)))
+-		return -EFAULT;
+-
+-	if (__get_user(pointer_u, &iwp_u->pointer) ||
+-	    __get_user(length, &iwp_u->length) ||
+-	    __get_user(flags, &iwp_u->flags))
+-		return -EFAULT;
+-
+-	if (__put_user(compat_ptr(pointer_u), &iwp->pointer) ||
+-	    __put_user(length, &iwp->length) ||
+-	    __put_user(flags, &iwp->flags))
+-		return -EFAULT;
+-
+-	ret = sys_ioctl(fd, cmd, (unsigned long) iwr);
+-
+-	if (__get_user(pointer, &iwp->pointer) ||
+-	    __get_user(length, &iwp->length) ||
+-	    __get_user(flags, &iwp->flags))
+-		return -EFAULT;
+-
+-	if (__put_user(ptr_to_compat(pointer), &iwp_u->pointer) ||
+-	    __put_user(length, &iwp_u->length) ||
+-	    __put_user(flags, &iwp_u->flags))
+-		return -EFAULT;
+-
+-	return ret;
+-}
+-
+ /* Since old style bridge ioctl's endup using SIOCDEVPRIVATE
+  * for some operations; this forces use of the newer bridge-utils that
+  * use compatiable ioctls
+@@ -2521,36 +2468,6 @@ COMPATIBLE_IOCTL(I2C_TENBIT)
+ COMPATIBLE_IOCTL(I2C_PEC)
+ COMPATIBLE_IOCTL(I2C_RETRIES)
+ COMPATIBLE_IOCTL(I2C_TIMEOUT)
+-/* wireless */
+-COMPATIBLE_IOCTL(SIOCSIWCOMMIT)
+-COMPATIBLE_IOCTL(SIOCGIWNAME)
+-COMPATIBLE_IOCTL(SIOCSIWNWID)
+-COMPATIBLE_IOCTL(SIOCGIWNWID)
+-COMPATIBLE_IOCTL(SIOCSIWFREQ)
+-COMPATIBLE_IOCTL(SIOCGIWFREQ)
+-COMPATIBLE_IOCTL(SIOCSIWMODE)
+-COMPATIBLE_IOCTL(SIOCGIWMODE)
+-COMPATIBLE_IOCTL(SIOCSIWSENS)
+-COMPATIBLE_IOCTL(SIOCGIWSENS)
+-COMPATIBLE_IOCTL(SIOCSIWRANGE)
+-COMPATIBLE_IOCTL(SIOCSIWPRIV)
+-COMPATIBLE_IOCTL(SIOCSIWSTATS)
+-COMPATIBLE_IOCTL(SIOCSIWAP)
+-COMPATIBLE_IOCTL(SIOCGIWAP)
+-COMPATIBLE_IOCTL(SIOCSIWRATE)
+-COMPATIBLE_IOCTL(SIOCGIWRATE)
+-COMPATIBLE_IOCTL(SIOCSIWRTS)
+-COMPATIBLE_IOCTL(SIOCGIWRTS)
+-COMPATIBLE_IOCTL(SIOCSIWFRAG)
+-COMPATIBLE_IOCTL(SIOCGIWFRAG)
+-COMPATIBLE_IOCTL(SIOCSIWTXPOW)
+-COMPATIBLE_IOCTL(SIOCGIWTXPOW)
+-COMPATIBLE_IOCTL(SIOCSIWRETRY)
+-COMPATIBLE_IOCTL(SIOCGIWRETRY)
+-COMPATIBLE_IOCTL(SIOCSIWPOWER)
+-COMPATIBLE_IOCTL(SIOCGIWPOWER)
+-COMPATIBLE_IOCTL(SIOCSIWAUTH)
+-COMPATIBLE_IOCTL(SIOCGIWAUTH)
+ /* hiddev */
+ COMPATIBLE_IOCTL(HIDIOCGVERSION)
+ COMPATIBLE_IOCTL(HIDIOCAPPLICATION)
+@@ -2775,29 +2692,7 @@ COMPATIBLE_IOCTL(USBDEVFS_IOCTL32)
+ HANDLE_IOCTL(I2C_FUNCS, w_long)
+ HANDLE_IOCTL(I2C_RDWR, do_i2c_rdwr_ioctl)
+ HANDLE_IOCTL(I2C_SMBUS, do_i2c_smbus_ioctl)
+-/* wireless */
+-HANDLE_IOCTL(SIOCGIWRANGE, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWPRIV, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWSTATS, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWSPY, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWSPY, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWTHRSPY, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWTHRSPY, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWMLME, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWAPLIST, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWSCAN, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWSCAN, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWESSID, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWESSID, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWNICKN, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWNICKN, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWENCODE, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWENCODE, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWGENIE, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWGENIE, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWENCODEEXT, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCGIWENCODEEXT, do_wireless_ioctl)
+-HANDLE_IOCTL(SIOCSIWPMKSA, do_wireless_ioctl)
++/* bridge */
+ HANDLE_IOCTL(SIOCSIFBR, old_bridge_ioctl)
+ HANDLE_IOCTL(SIOCGIFBR, old_bridge_ioctl)
+ /* Not implemented in the native kernel */
diff -Naur linux-2.6.25-org/patches/other/davem-11.patch linux-2.6.25-id/patches/other/davem-11.patch
--- linux-2.6.25-org/patches/other/davem-11.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-11.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,267 @@
+Date: 	Fri, 21 Dec 2007 20:57:40 -0800 (PST)
+Message-Id: <20071221.205740.156308612.davem@davemloft.net>
+To: linux-wireless@vger.kernel.org
+CC: netdev@vger.kernel.org
+Subject: [WEXT 11/12]: Create IW_REQUEST_FLAG_COMPAT and set it as needed.
+From: David Miller <davem@davemloft.net>
+Content-Type: Text/Plain; charset=us-ascii
+Content-Transfer-Encoding: 7bit
+
+
+[WEXT]: Create IW_REQUEST_FLAG_COMPAT and set it as needed.
+
+Now low-level WEXT ioctl handlers can do compat handling
+when necessary.
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ include/net/iw_handler.h |    2 -
+ net/wireless/wext.c      |   74 +++++++++++++++++++++--------------------------
+ 2 files changed, 35 insertions(+), 41 deletions(-)
+
+--- a/include/net/iw_handler.h
++++ b/include/net/iw_handler.h
+@@ -256,7 +256,7 @@
+ #define EIWCOMMIT	EINPROGRESS
+ 
+ /* Flags available in struct iw_request_info */
+-#define IW_REQUEST_FLAG_NONE	0x0000	/* No flag so far */
++#define IW_REQUEST_FLAG_COMPAT	0x0001	/* Compat ioctl call */
+ 
+ /* Type of headers we know about (basically union iwreq_data) */
+ #define IW_HEADER_TYPE_NULL	0	/* Not available */
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -866,10 +866,10 @@ out:
+ static int ioctl_standard_call(struct net_device *	dev,
+ 			       struct iwreq *		iwr,
+ 			       unsigned int		cmd,
++			       struct iw_request_info	*info,
+ 			       iw_handler		handler)
+ {
+ 	const struct iw_ioctl_description *	descr;
+-	struct iw_request_info			info;
+ 	int					ret = -EINVAL;
+ 
+ 	/* Get the description of the IOCTL */
+@@ -877,15 +877,11 @@ static int ioctl_standard_call(struct ne
+ 		return -EOPNOTSUPP;
+ 	descr = &(standard_ioctl[cmd - SIOCIWFIRST]);
+ 
+-	/* Prepare the call */
+-	info.cmd = cmd;
+-	info.flags = 0;
+-
+ 	/* Check if we have a pointer to user space data or not */
+ 	if (descr->header_type != IW_HEADER_TYPE_POINT) {
+ 
+ 		/* No extra arguments. Trivial to handle */
+-		ret = handler(dev, &info, &(iwr->u), NULL);
++		ret = handler(dev, info, &(iwr->u), NULL);
+ 
+ 		/* Generate an event to notify listeners of the change */
+ 		if ((descr->flags & IW_DESCR_FLAG_EVENT) &&
+@@ -893,7 +889,7 @@ static int ioctl_standard_call(struct ne
+ 			wireless_send_event(dev, cmd, &(iwr->u), NULL);
+ 	} else {
+ 		ret = ioctl_standard_iw_point(&iwr->u.data, cmd, descr,
+-					      handler, dev, &info);
++					      handler, dev, info);
+ 	}
+ 
+ 	/* Call commit handler if needed and defined */
+@@ -1016,25 +1012,21 @@ out:
+ }
+ 
+ static int ioctl_private_call(struct net_device *dev, struct iwreq *iwr,
+-			      unsigned int cmd, iw_handler handler)
++			      unsigned int cmd, struct iw_request_info *info,
++			      iw_handler handler)
+ {
+ 	int extra_size = 0, ret = -EINVAL;
+ 	const struct iw_priv_args *descr;
+-	struct iw_request_info info;
+ 
+ 	extra_size = get_priv_descr_and_size(dev, cmd, &descr);
+ 
+-	/* Prepare the call */
+-	info.cmd = cmd;
+-	info.flags = 0;
+-
+ 	/* Check if we have a pointer to user space data or not. */
+ 	if (extra_size == 0) {
+ 		/* No extra arguments. Trivial to handle */
+-		ret = handler(dev, &info, &(iwr->u), (char *) &(iwr->u));
++		ret = handler(dev, info, &(iwr->u), (char *) &(iwr->u));
+ 	} else {
+ 		ret = ioctl_private_iw_point(&iwr->u.data, cmd, descr,
+-					     handler, dev, &info, extra_size);
++					     handler, dev, info, extra_size);
+ 	}
+ 
+ 	/* Call commit handler if needed and defined */
+@@ -1046,7 +1038,8 @@ static int ioctl_private_call(struct net
+ 
+ /* ---------------------------------------------------------------- */
+ typedef int (*wext_ioctl_func)(struct net_device *, struct iwreq *,
+-			       unsigned int, iw_handler);
++			       unsigned int, struct iw_request_info *,
++			       iw_handler);
+ 
+ /*
+  * Main IOCTl dispatcher.
+@@ -1054,6 +1047,7 @@ typedef int (*wext_ioctl_func)(struct ne
+  */
+ static int wireless_process_ioctl(struct net *net, struct ifreq *ifr,
+ 				  unsigned int cmd,
++				  struct iw_request_info *info,
+ 				  wext_ioctl_func standard,
+ 				  wext_ioctl_func private)
+ {
+@@ -1072,11 +1066,11 @@ static int wireless_process_ioctl(struct
+ 	 * Note that 'cmd' is already filtered in dev_ioctl() with
+ 	 * (cmd >= SIOCIWFIRST && cmd <= SIOCIWLAST) */
+ 	if (cmd == SIOCGIWSTATS)
+-		return standard(dev, iwr, cmd,
++		return standard(dev, iwr, cmd, info,
+ 				&iw_handler_get_iwstats);
+ 
+ 	if (cmd == SIOCGIWPRIV && dev->wireless_handlers)
+-		return standard(dev, iwr, cmd,
++		return standard(dev, iwr, cmd, info,
+ 				&iw_handler_get_private);
+ 
+ 	/* Basic check */
+@@ -1088,9 +1082,9 @@ static int wireless_process_ioctl(struct
+ 	if (handler) {
+ 		/* Standard and private are not the same */
+ 		if (cmd < SIOCIWFIRSTPRIV)
+-			return standard(dev, iwr, cmd, handler);
++			return standard(dev, iwr, cmd, info, handler);
+ 		else
+-			return private(dev, iwr, cmd, handler);
++			return private(dev, iwr, cmd, info, handler);
+ 	}
+ 	/* Old driver API : call driver ioctl handler */
+ 	if (dev->do_ioctl)
+@@ -1112,7 +1106,7 @@ static int wext_permission_check(unsigne
+ 
+ /* entry point from dev ioctl */
+ static int wext_ioctl_dispatch(struct net *net, struct ifreq *ifr,
+-			       unsigned int cmd,
++			       unsigned int cmd, struct iw_request_info *info,
+ 			       wext_ioctl_func standard,
+ 			       wext_ioctl_func private)
+ {
+@@ -1123,7 +1117,7 @@ static int wext_ioctl_dispatch(struct ne
+ 
+ 	dev_load(net, ifr->ifr_name);
+ 	rtnl_lock();
+-	ret = wireless_process_ioctl(net, ifr, cmd, standard, private);
++	ret = wireless_process_ioctl(net, ifr, cmd, info, standard, private);
+ 	rtnl_unlock();
+ 
+ 	return ret;
+@@ -1132,9 +1126,12 @@ static int wext_ioctl_dispatch(struct ne
+ int wext_handle_ioctl(struct net *net, struct ifreq *ifr, unsigned int cmd,
+ 		      void __user *arg)
+ {
+-	int ret = wext_ioctl_dispatch(net, ifr, cmd,
+-				      ioctl_standard_call,
+-				      ioctl_private_call);
++ 	struct iw_request_info info = { .cmd = cmd, .flags = 0 };
++ 	int ret;
++
++	ret = wext_ioctl_dispatch(net, ifr, cmd, &info,
++				  ioctl_standard_call,
++				  ioctl_private_call);
+ 
+ 	if (ret >= 0 &&
+ 	    IW_IS_GET(cmd) &&
+@@ -1148,28 +1145,25 @@ int wext_handle_ioctl(struct net *net, s
+ static int compat_standard_call(struct net_device *	dev,
+ 				struct iwreq *		iwr,
+ 				unsigned int		cmd,
++				struct iw_request_info	*info,
+ 				iw_handler		handler)
+ {
+ 	const struct iw_ioctl_description *descr;
+ 	struct compat_iw_point *iwp_compat;
+-	struct iw_request_info info;
+ 	struct iw_point iwp;
+ 	int err;
+ 
+ 	descr = standard_ioctl + (cmd - SIOCIWFIRST);
+ 
+ 	if (descr->header_type != IW_HEADER_TYPE_POINT)
+-		return ioctl_standard_call(dev, iwr, cmd, handler);
++		return ioctl_standard_call(dev, iwr, cmd, info, handler);
+ 
+ 	iwp_compat = (struct compat_iw_point *) &iwr->u.data;
+ 	iwp.pointer = compat_ptr(iwp_compat->pointer);
+ 	iwp.length = iwp_compat->length;
+ 	iwp.flags = iwp_compat->flags;
+ 
+-	info.cmd = cmd;
+-	info.flags = 0;
+-
+-	err = ioctl_standard_iw_point(&iwp, cmd, descr, handler, dev, &info);
++	err = ioctl_standard_iw_point(&iwp, cmd, descr, handler, dev, info);
+ 
+ 	iwp_compat->pointer = ptr_to_compat(iwp.pointer);
+ 	iwp_compat->length = iwp.length;
+@@ -1179,22 +1173,18 @@ static int compat_standard_call(struct n
+ }
+ 
+ static int compat_private_call(struct net_device *dev, struct iwreq *iwr,
+-			       unsigned int cmd, iw_handler handler)
++			       unsigned int cmd, struct iw_request_info *info,
++			       iw_handler handler)
+ {
+ 	const struct iw_priv_args *descr;
+-	struct iw_request_info info;
+ 	int ret, extra_size;
+ 
+ 	extra_size = get_priv_descr_and_size(dev, cmd, &descr);
+ 
+-	/* Prepare the call */
+-	info.cmd = cmd;
+-	info.flags = 0;
+-
+ 	/* Check if we have a pointer to user space data or not. */
+ 	if (extra_size == 0) {
+ 		/* No extra arguments. Trivial to handle */
+-		ret = handler(dev, &info, &(iwr->u), (char *) &(iwr->u));
++		ret = handler(dev, info, &(iwr->u), (char *) &(iwr->u));
+ 	} else {
+ 		struct compat_iw_point *iwp_compat;
+ 		struct iw_point iwp;
+@@ -1205,7 +1195,7 @@ static int compat_private_call(struct ne
+ 		iwp.flags = iwp_compat->flags;
+ 
+ 		ret = ioctl_private_iw_point(&iwp, cmd, descr,
+-					     handler, dev, &info, extra_size);
++					     handler, dev, info, extra_size);
+ 
+ 		iwp_compat->pointer = ptr_to_compat(iwp.pointer);
+ 		iwp_compat->length = iwp.length;
+@@ -1223,6 +1213,7 @@ int compat_wext_handle_ioctl(struct net 
+ 			     unsigned long arg)
+ {
+ 	void __user *argp = (void __user *)arg;
++	struct iw_request_info info;
+ 	struct iwreq iwr;
+ 	char *colon;
+ 	int ret;
+@@ -1235,7 +1226,10 @@ int compat_wext_handle_ioctl(struct net 
+ 	if (colon)
+ 		*colon = 0;
+ 
+-	ret = wext_ioctl_dispatch(net, (struct ifreq *) &iwr, cmd,
++	info.cmd = cmd;
++	info.flags = IW_REQUEST_FLAG_COMPAT;
++
++	ret = wext_ioctl_dispatch(net, (struct ifreq *) &iwr, cmd, &info,
+ 				  compat_standard_call,
+ 				  compat_private_call);
+ 
diff -Naur linux-2.6.25-org/patches/other/davem-12-fix.patch linux-2.6.25-id/patches/other/davem-12-fix.patch
--- linux-2.6.25-org/patches/other/davem-12-fix.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-12-fix.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,232 @@
+---
+ .git/ORIG_HEAD                             |    2 +-
+ drivers/net/wireless/airo.c                |    7 +++++++
+ drivers/net/wireless/hostap/hostap_ioctl.c |    7 +++++++
+ drivers/net/wireless/libertas/scan.c       |    7 +++++++
+ drivers/net/wireless/orinoco.c             |   11 +++++++++--
+ drivers/net/wireless/prism54/isl_ioctl.c   |   12 ++++++++++--
+ include/linux/wireless.h                   |    9 +++------
+ include/net/iw_handler.h                   |   22 ++++++++++++++--------
+ net/ieee80211/ieee80211_wx.c               |    7 +++++++
+ 9 files changed, 65 insertions(+), 19 deletions(-)
+
+--- a/.git/ORIG_HEAD
++++ b/.git/ORIG_HEAD
+@@ -1 +1 @@
+-a7da60f41551abb3c520b03d42ec05dd7decfc7f
++d8c89eb3a12f0da96d049bd515c7fa3702e511c5
+--- a/drivers/net/wireless/airo.c
++++ b/drivers/net/wireless/airo.c
+@@ -7318,7 +7318,14 @@ static inline char *airo_translate_scan(
+ 
+ 	/* Rate : stuffing multiple values in a single event require a bit
+ 	 * more of magic - Jean II */
++#ifdef CONFIG_COMPAT
++	if (info->flags & IW_REQUEST_FLAG_COMPAT)
++		current_val = current_ev + IW_EV_COMPAT_LCP_LEN;
++	else
++		current_val = current_ev + IW_EV_LCP_LEN;
++#else
+ 	current_val = current_ev + IW_EV_LCP_LEN;
++#endif
+ 
+ 	iwe.cmd = SIOCGIWRATE;
+ 	/* Those two flags are ignored... */
+--- a/drivers/net/wireless/hostap/hostap_ioctl.c
++++ b/drivers/net/wireless/hostap/hostap_ioctl.c
+@@ -1904,7 +1904,14 @@ static char * __prism2_translate_scan(lo
+ 	if (scan) {
+ 		memset(&iwe, 0, sizeof(iwe));
+ 		iwe.cmd = SIOCGIWRATE;
++#ifdef CONFIG_COMPAT
++		if (info->flags & IW_REQUEST_FLAG_COMPAT)
++			current_val = current_ev + IW_EV_COMPAT_LCP_LEN;
++		else
++			current_val = current_ev + IW_EV_LCP_LEN;
++#else
+ 		current_val = current_ev + IW_EV_LCP_LEN;
++#endif
+ 		pos = scan->sup_rates;
+ 		for (i = 0; i < sizeof(scan->sup_rates); i++) {
+ 			if (pos[i] == 0)
+--- a/drivers/net/wireless/libertas/scan.c
++++ b/drivers/net/wireless/libertas/scan.c
+@@ -1536,7 +1536,14 @@ static inline char *libertas_translate_s
+ 	iwe.u.data.length = 0;
+ 	start = iwe_stream_add_point(info, start, stop, &iwe, bss->ssid);
+ 
++#ifdef CONFIG_COMPAT
++	if (info->flags & IW_REQUEST_FLAG_COMPAT)
++		current_val = start + IW_EV_COMPAT_LCP_LEN;
++	else
++		current_val = start + IW_EV_LCP_LEN;
++#else
+ 	current_val = start + IW_EV_LCP_LEN;
++#endif
+ 
+ 	iwe.cmd = SIOCGIWRATE;
+ 	iwe.u.bitrate.fixed = 0;
+--- a/drivers/net/wireless/orinoco.c
++++ b/drivers/net/wireless/orinoco.c
+@@ -4044,10 +4044,17 @@ static inline int orinoco_translate_scan
+ 
+ 		/* Bit rate is not available in Lucent/Agere firmwares */
+ 		if (priv->firmware_type != FIRMWARE_TYPE_AGERE) {
+-			char *	current_val = current_ev + IW_EV_LCP_LEN;
++			char *current_val;
+ 			int	i;
+ 			int	step;
+-
++#ifdef CONFIG_COMPAT
++			if (info->flags & IW_REQUEST_FLAG_COMPAT)
++				current_val = current_ev + IW_EV_COMPAT_LCP_LEN;
++			else
++				current_val = current_ev + IW_EV_LCP_LEN;
++#else
++			current_val = current_ev + IW_EV_LCP_LEN;
++#endif
+ 			if (priv->firmware_type == FIRMWARE_TYPE_SYMBOL)
+ 				step = 2;
+ 			else
+--- a/drivers/net/wireless/prism54/isl_ioctl.c
++++ b/drivers/net/wireless/prism54/isl_ioctl.c
+@@ -651,11 +651,19 @@ prism54_translate_bss(struct net_device 
+ 	}
+ 	/* Do the bitrates */
+ 	{
+-		char *	current_val = current_ev + IW_EV_LCP_LEN;
++		char *current_val;
+ 		int i;
+ 		int mask;
+ 
+ 		iwe.cmd = SIOCGIWRATE;
++#ifdef CONFIG_COMPAT
++		if (info->flags & IW_REQUEST_FLAG_COMPAT)
++			current_val = current_ev + IW_EV_COMPAT_LCP_LEN;
++		else
++			current_val = current_ev + IW_EV_LCP_LEN;
++#else
++		current_val = current_ev + IW_EV_LCP_LEN;
++#endif
+ 		/* Those two flags are ignored... */
+ 		iwe.u.bitrate.fixed = iwe.u.bitrate.disabled = 0;
+ 
+@@ -2735,7 +2743,7 @@ prism2_ioctl_scan_req(struct net_device 
+ 
+ 	/* ok now, scan the list and translate its info */
+ 	for (i = 0; i < min(IW_MAX_AP, (int) bsslist->nr); i++)
+-		current_ev = prism54_translate_bss(ndev, current_ev, &info,
++		current_ev = prism54_translate_bss(ndev, &info, current_ev,
+ 						   extra + IW_SCAN_MAX_DATA,
+ 						   &(bsslist->bsslist[i]),
+ 						   noise);
+--- a/include/linux/wireless.h
++++ b/include/linux/wireless.h
+@@ -1100,15 +1100,12 @@ struct iw_event
+ 
+ #if defined(__KERNEL__) && defined(CONFIG_COMPAT)
+ struct __compat_iw_event {
+-	__u16		len;			/* Real lenght of this stuff */
++	__u16		len;			/* Real length of this stuff */
+ 	__u16		cmd;			/* Wireless IOCTL */
+ 	compat_caddr_t	pointer;
+ };
+-#define IW_EV_COMPAT_LCP_LEN \
+-	(sizeof(struct __compat_iw_event) - sizeof(compat_caddr_t))
+-#define IW_EV_COMPAT_POINT_OFF (((char *) \
+-			  &(((struct compat_iw_point *) NULL)->length)) - \
+-			  (char *) NULL)
++#define IW_EV_COMPAT_LCP_LEN offsetof(struct __compat_iw_event, pointer)
++#define IW_EV_COMPAT_POINT_OFF offsetof(struct compat_iw_point, length)
+ #define IW_EV_COMPAT_POINT_LEN	\
+ 	(IW_EV_COMPAT_LCP_LEN + sizeof(struct compat_iw_point) - \
+ 	 IW_EV_COMPAT_POINT_OFF)
+--- a/include/net/iw_handler.h
++++ b/include/net/iw_handler.h
+@@ -487,6 +487,7 @@ iwe_stream_add_event(struct iw_request_i
+ 		     struct iw_event *iwe, int event_len)
+ {
+ 	int lcp_len = IW_EV_LCP_LEN;
++	int org_event_len = event_len;
+ 
+ #ifdef CONFIG_COMPAT
+ 	if (info->flags & IW_REQUEST_FLAG_COMPAT) {
+@@ -500,9 +501,8 @@ iwe_stream_add_event(struct iw_request_i
+ 		iwe->len = event_len;
+ 		/* Beware of alignement issues on 64 bits */
+ 		memcpy(stream, (char *) iwe, IW_EV_LCP_PK_LEN);
+-		memcpy(stream + lcp_len,
+-		       ((char *) iwe) + lcp_len,
+-		       event_len - lcp_len);
++		memcpy(stream + lcp_len, &iwe->u,
++		       org_event_len - IW_EV_LCP_LEN);
+ 		stream += event_len;
+ 	}
+ 	return stream;
+@@ -520,14 +520,12 @@ iwe_stream_add_point(struct iw_request_i
+ 	int event_len = IW_EV_POINT_LEN + iwe->u.data.length;
+ 	int point_len = IW_EV_POINT_LEN;
+ 	int lcp_len = IW_EV_LCP_LEN;
+-	int point_off = IW_EV_POINT_OFF;
+ 
+ #ifdef CONFIG_COMPAT
+ 	if (info->flags & IW_REQUEST_FLAG_COMPAT) {
+ 		event_len = IW_EV_COMPAT_POINT_LEN + iwe->u.data.length;
+ 		point_len = IW_EV_COMPAT_POINT_LEN;
+ 		lcp_len = IW_EV_COMPAT_LCP_LEN;
+-		point_off = IW_EV_COMPAT_POINT_OFF;
+ 	}
+ #endif
+ 
+@@ -536,7 +534,7 @@ iwe_stream_add_point(struct iw_request_i
+ 		iwe->len = event_len;
+ 		memcpy(stream, (char *) iwe, IW_EV_LCP_PK_LEN);
+ 		memcpy(stream + lcp_len,
+-		       ((char *) iwe) + lcp_len + point_off,
++		       &iwe->u.data.length,
+ 		       IW_EV_POINT_PK_LEN - IW_EV_LCP_PK_LEN);
+ 		memcpy(stream + point_len, extra, iwe->u.data.length);
+ 		stream += event_len;
+@@ -548,7 +546,15 @@ iwe_stream_add_point(struct iw_request_i
+ /*
+  * Wrapper to add a value to a Wireless Event in a stream of events.
+  * Be careful, this one is tricky to use properly :
+- * At the first run, you need to have (value = event + IW_EV_LCP_LEN).
++ * At the first run, you need to have the code like:
++ * #ifdef CONFIG_COMPAT
++ * if (info->flags & IW_REQUEST_FLAG_COMPAT)
++ * 	value = ev + IW_EV_COMPAT_LCP_LEN;
++ * else
++ * 	value = ev + IW_EV_COMPAT_LEN;
++ * #else
++ * value = ev + IW_EV_COMPAT_LEN;
++ * #endif
+  */
+ static inline char *
+ iwe_stream_add_value(struct iw_request_info *info, char *event, char *value,
+@@ -566,7 +572,7 @@ iwe_stream_add_value(struct iw_request_i
+ 	/* Check if it's possible */
+ 	if(likely((value + event_len) < ends)) {
+ 		/* Add new value */
+-		memcpy(value, (char *) iwe + lcp_len, event_len);
++		memcpy(value, &iwe->u, event_len);
+ 		value += event_len;
+ 		/* Patch LCP */
+ 		iwe->len = value - event;
+--- a/net/ieee80211/ieee80211_wx.c
++++ b/net/ieee80211/ieee80211_wx.c
+@@ -114,7 +114,14 @@ static char *ieee80211_translate_scan(st
+ 	/* Add basic and extended rates */
+ 	/* Rate : stuffing multiple values in a single event require a bit
+ 	 * more of magic - Jean II */
++#ifdef CONFIG_COMPAT
++	if (info->flags & IW_REQUEST_FLAG_COMPAT)
++		current_val = start + IW_EV_COMPAT_LCP_LEN;
++	else
++		current_val = start + IW_EV_LCP_LEN;
++#else
+ 	current_val = start + IW_EV_LCP_LEN;
++#endif
+ 	iwe.cmd = SIOCGIWRATE;
+ 	/* Those two flags are ignored... */
+ 	iwe.u.bitrate.fixed = iwe.u.bitrate.disabled = 0;
diff -Naur linux-2.6.25-org/patches/other/davem-12.patch linux-2.6.25-id/patches/other/davem-12.patch
--- linux-2.6.25-org/patches/other/davem-12.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/davem-12.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,1580 @@
+Here is a new version of the last patch (#12), it should handle
+all of these cases properly now.
+
+Let me know if you spot any more errors.
+
+Thanks!
+
+[WEXT]: Emit event stream entries correctly when compat.
+
+Three major portions to this change:
+
+1) Add IW_EV_COMPAT_LCP_LEN, IW_EV_COMPAT_POINT_OFF,
+   and IW_EV_COMPAT_POINT_LEN helper defines.
+
+2) Delete iw_stream_check_add_*(), they are unused.
+
+3) Add iw_request_info argument to iwe_stream_add_*(), and use it to
+   size the event and pointer lengths correctly depending upon whether
+   IW_REQUEST_FLAG_COMPAT is set or not.
+
+4) The mechanical transformations to the drivers and wireless stack
+   bits to get the iw_request_info passed down into the routines
+   modified in #3.
+
+With help from Masakazu Mokuno
+
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ drivers/net/wireless/airo.c                |   39 +++++---
+ drivers/net/wireless/atmel.c               |   24 ++++-
+ drivers/net/wireless/hostap/hostap.h       |    3 +-
+ drivers/net/wireless/hostap/hostap_ap.c    |   32 +++---
+ drivers/net/wireless/hostap/hostap_ioctl.c |   54 ++++++-----
+ drivers/net/wireless/libertas/scan.c       |   35 ++++---
+ drivers/net/wireless/orinoco.c             |   30 ++++--
+ drivers/net/wireless/prism54/isl_ioctl.c   |   45 +++++----
+ drivers/net/wireless/wl3501_cs.c           |   10 +-
+ drivers/net/wireless/zd1201.c              |   21 +++--
+ include/linux/wireless.h                   |   16 +++
+ include/net/iw_handler.h                   |  150 ++++++++--------------------
+ net/ieee80211/ieee80211_wx.c               |   44 +++++----
+ net/mac80211/ieee80211_i.h                 |    5 +-
+ net/mac80211/ieee80211_ioctl.c             |    2 +-
+ net/mac80211/ieee80211_sta.c               |   59 ++++++-----
+ 16 files changed, 293 insertions(+), 276 deletions(-)
+
+diff --git a/drivers/net/wireless/airo.c b/drivers/net/wireless/airo.c
+index 074055e..ad4f140 100644
+--- a/drivers/net/wireless/airo.c
++++ b/drivers/net/wireless/airo.c
+@@ -7234,6 +7234,7 @@ out:
+  * format that the Wireless Tools will understand - Jean II
+  */
+ static inline char *airo_translate_scan(struct net_device *dev,
++					struct iw_request_info *info,
+ 					char *current_ev,
+ 					char *end_buf,
+ 					BSSListRid *bss)
+@@ -7249,7 +7250,8 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 	iwe.cmd = SIOCGIWAP;
+ 	iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 	memcpy(iwe.u.ap_addr.sa_data, bss->bssid, ETH_ALEN);
+-	current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_ADDR_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_ADDR_LEN);
+ 
+ 	/* Other entries will be displayed in the order we give them */
+ 
+@@ -7259,7 +7261,8 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 		iwe.u.data.length = 32;
+ 	iwe.cmd = SIOCGIWESSID;
+ 	iwe.u.data.flags = 1;
+-	current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, bss->ssid);
++	current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++					  &iwe, bss->ssid);
+ 
+ 	/* Add mode */
+ 	iwe.cmd = SIOCGIWMODE;
+@@ -7269,7 +7272,8 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 			iwe.u.mode = IW_MODE_MASTER;
+ 		else
+ 			iwe.u.mode = IW_MODE_ADHOC;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_UINT_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_UINT_LEN);
+ 	}
+ 
+ 	/* Add frequency */
+@@ -7280,7 +7284,8 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 	 */
+ 	iwe.u.freq.m = frequency_list[iwe.u.freq.m - 1] * 100000;
+ 	iwe.u.freq.e = 1;
+-	current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_FREQ_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_FREQ_LEN);
+ 
+ 	/* Add quality statistics */
+ 	iwe.cmd = IWEVQUAL;
+@@ -7298,7 +7303,8 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 				| IW_QUAL_DBM;
+ 	}
+ 	iwe.u.qual.noise = ai->wstats.qual.noise;
+-	current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_QUAL_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_QUAL_LEN);
+ 
+ 	/* Add encryption capability */
+ 	iwe.cmd = SIOCGIWENCODE;
+@@ -7307,7 +7313,8 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 	else
+ 		iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 	iwe.u.data.length = 0;
+-	current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, bss->ssid);
++	current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++					  &iwe, bss->ssid);
+ 
+ 	/* Rate : stuffing multiple values in a single event require a bit
+ 	 * more of magic - Jean II */
+@@ -7324,7 +7331,9 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 		/* Bit rate given in 500 kb/s units (+ 0x80) */
+ 		iwe.u.bitrate.value = ((bss->rates[i] & 0x7f) * 500000);
+ 		/* Add new value to event */
+-		current_val = iwe_stream_add_value(current_ev, current_val, end_buf, &iwe, IW_EV_PARAM_LEN);
++		current_val = iwe_stream_add_value(info, current_ev,
++						   current_val, end_buf,
++						   &iwe, IW_EV_PARAM_LEN);
+ 	}
+ 	/* Check if we added any event */
+ 	if((current_val - current_ev) > IW_EV_LCP_LEN)
+@@ -7336,7 +7345,8 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 		iwe.cmd = IWEVCUSTOM;
+ 		sprintf(buf, "bcn_int=%d", bss->beaconInterval);
+ 		iwe.u.data.length = strlen(buf);
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, buf);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, buf);
+ 		kfree(buf);
+ 	}
+ 
+@@ -7370,8 +7380,10 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 					iwe.cmd = IWEVGENIE;
+ 					iwe.u.data.length = min(info_element->len + 2,
+ 								  MAX_WPA_IE_LEN);
+-					current_ev = iwe_stream_add_point(current_ev, end_buf,
+-							&iwe, (char *) info_element);
++					current_ev = iwe_stream_add_point(
++							info, current_ev,
++							end_buf, &iwe,
++							(char *) info_element);
+ 				}
+ 				break;
+ 
+@@ -7379,8 +7391,9 @@ static inline char *airo_translate_scan(struct net_device *dev,
+ 				iwe.cmd = IWEVGENIE;
+ 				iwe.u.data.length = min(info_element->len + 2,
+ 							  MAX_WPA_IE_LEN);
+-				current_ev = iwe_stream_add_point(current_ev, end_buf,
+-						&iwe, (char *) info_element);
++				current_ev = iwe_stream_add_point(
++					info, current_ev, end_buf,
++					&iwe, (char *) info_element);
+ 				break;
+ 
+ 			default:
+@@ -7419,7 +7432,7 @@ static int airo_get_scan(struct net_device *dev,
+ 
+ 	list_for_each_entry (net, &ai->network_list, list) {
+ 		/* Translate to WE format this entry */
+-		current_ev = airo_translate_scan(dev, current_ev,
++		current_ev = airo_translate_scan(dev, info, current_ev,
+ 						 extra + dwrq->length,
+ 						 &net->bss);
+ 
+diff --git a/drivers/net/wireless/atmel.c b/drivers/net/wireless/atmel.c
+index 059ce3f..985068e 100644
+--- a/drivers/net/wireless/atmel.c
++++ b/drivers/net/wireless/atmel.c
+@@ -2320,30 +2320,40 @@ static int atmel_get_scan(struct net_device *dev,
+ 		iwe.cmd = SIOCGIWAP;
+ 		iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 		memcpy(iwe.u.ap_addr.sa_data, priv->BSSinfo[i].BSSID, 6);
+-		current_ev = iwe_stream_add_event(current_ev, extra + IW_SCAN_MAX_DATA, &iwe, IW_EV_ADDR_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev,
++						  extra + IW_SCAN_MAX_DATA,
++						  &iwe, IW_EV_ADDR_LEN);
+ 
+ 		iwe.u.data.length =  priv->BSSinfo[i].SSIDsize;
+ 		if (iwe.u.data.length > 32)
+ 			iwe.u.data.length = 32;
+ 		iwe.cmd = SIOCGIWESSID;
+ 		iwe.u.data.flags = 1;
+-		current_ev = iwe_stream_add_point(current_ev, extra + IW_SCAN_MAX_DATA, &iwe, priv->BSSinfo[i].SSID);
++		current_ev = iwe_stream_add_point(info, current_ev,
++						  extra + IW_SCAN_MAX_DATA,
++						  &iwe, priv->BSSinfo[i].SSID);
+ 
+ 		iwe.cmd = SIOCGIWMODE;
+ 		iwe.u.mode = priv->BSSinfo[i].BSStype;
+-		current_ev = iwe_stream_add_event(current_ev, extra + IW_SCAN_MAX_DATA, &iwe, IW_EV_UINT_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev,
++						  extra + IW_SCAN_MAX_DATA,
++						  &iwe, IW_EV_UINT_LEN);
+ 
+ 		iwe.cmd = SIOCGIWFREQ;
+ 		iwe.u.freq.m = priv->BSSinfo[i].channel;
+ 		iwe.u.freq.e = 0;
+-		current_ev = iwe_stream_add_event(current_ev, extra + IW_SCAN_MAX_DATA, &iwe, IW_EV_FREQ_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev,
++						  extra + IW_SCAN_MAX_DATA,
++						  &iwe, IW_EV_FREQ_LEN);
+ 
+ 		/* Add quality statistics */
+ 		iwe.cmd = IWEVQUAL;
+ 		iwe.u.qual.level = priv->BSSinfo[i].RSSI;
+ 		iwe.u.qual.qual  = iwe.u.qual.level;
+ 		/* iwe.u.qual.noise  = SOMETHING */
+-		current_ev = iwe_stream_add_event(current_ev, extra + IW_SCAN_MAX_DATA , &iwe, IW_EV_QUAL_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev,
++						  extra + IW_SCAN_MAX_DATA,
++						  &iwe, IW_EV_QUAL_LEN);
+ 
+ 
+ 		iwe.cmd = SIOCGIWENCODE;
+@@ -2352,7 +2362,9 @@ static int atmel_get_scan(struct net_device *dev,
+ 		else
+ 			iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 		iwe.u.data.length = 0;
+-		current_ev = iwe_stream_add_point(current_ev, extra + IW_SCAN_MAX_DATA, &iwe, NULL);
++		current_ev = iwe_stream_add_point(info, current_ev,
++						  extra + IW_SCAN_MAX_DATA,
++						  &iwe, NULL);
+ 	}
+ 
+ 	/* Length of data */
+diff --git a/drivers/net/wireless/hostap/hostap.h b/drivers/net/wireless/hostap/hostap.h
+index 547ba84..3a386a6 100644
+--- a/drivers/net/wireless/hostap/hostap.h
++++ b/drivers/net/wireless/hostap/hostap.h
+@@ -67,7 +67,8 @@ void * ap_crypt_get_ptrs(struct ap_data *ap, u8 *addr, int permanent,
+ int prism2_ap_get_sta_qual(local_info_t *local, struct sockaddr addr[],
+ 			   struct iw_quality qual[], int buf_size,
+ 			   int aplist);
+-int prism2_ap_translate_scan(struct net_device *dev, char *buffer);
++int prism2_ap_translate_scan(struct net_device *dev,
++			     struct iw_request_info *info, char *buffer);
+ int prism2_hostapd(struct ap_data *ap, struct prism2_hostapd_param *param);
+ 
+ 
+diff --git a/drivers/net/wireless/hostap/hostap_ap.c b/drivers/net/wireless/hostap/hostap_ap.c
+index 6bbdb76..8131e84 100644
+--- a/drivers/net/wireless/hostap/hostap_ap.c
++++ b/drivers/net/wireless/hostap/hostap_ap.c
+@@ -2381,7 +2381,8 @@ int prism2_ap_get_sta_qual(local_info_t *local, struct sockaddr addr[],
+ 
+ /* Translate our list of Access Points & Stations to a card independant
+  * format that the Wireless Tools will understand - Jean II */
+-int prism2_ap_translate_scan(struct net_device *dev, char *buffer)
++int prism2_ap_translate_scan(struct net_device *dev,
++			     struct iw_request_info *info, char *buffer)
+ {
+ 	struct hostap_interface *iface;
+ 	local_info_t *local;
+@@ -2410,8 +2411,8 @@ int prism2_ap_translate_scan(struct net_device *dev, char *buffer)
+ 		iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 		memcpy(iwe.u.ap_addr.sa_data, sta->addr, ETH_ALEN);
+ 		iwe.len = IW_EV_ADDR_LEN;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-						  IW_EV_ADDR_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_ADDR_LEN);
+ 
+ 		/* Use the mode to indicate if it's a station or
+ 		 * an Access Point */
+@@ -2422,8 +2423,8 @@ int prism2_ap_translate_scan(struct net_device *dev, char *buffer)
+ 		else
+ 			iwe.u.mode = IW_MODE_INFRA;
+ 		iwe.len = IW_EV_UINT_LEN;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-						  IW_EV_UINT_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_UINT_LEN);
+ 
+ 		/* Some quality */
+ 		memset(&iwe, 0, sizeof(iwe));
+@@ -2438,8 +2439,8 @@ int prism2_ap_translate_scan(struct net_device *dev, char *buffer)
+ 		iwe.u.qual.noise = HFA384X_LEVEL_TO_dBm(sta->last_rx_silence);
+ 		iwe.u.qual.updated = sta->last_rx_updated;
+ 		iwe.len = IW_EV_QUAL_LEN;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-						  IW_EV_QUAL_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_QUAL_LEN);
+ 
+ #ifndef PRISM2_NO_KERNEL_IEEE80211_MGMT
+ 		if (sta->ap) {
+@@ -2447,8 +2448,8 @@ int prism2_ap_translate_scan(struct net_device *dev, char *buffer)
+ 			iwe.cmd = SIOCGIWESSID;
+ 			iwe.u.data.length = sta->u.ap.ssid_len;
+ 			iwe.u.data.flags = 1;
+-			current_ev = iwe_stream_add_point(current_ev, end_buf,
+-							  &iwe,
++			current_ev = iwe_stream_add_point(info, current_ev,
++							  end_buf, &iwe,
+ 							  sta->u.ap.ssid);
+ 
+ 			memset(&iwe, 0, sizeof(iwe));
+@@ -2458,10 +2459,9 @@ int prism2_ap_translate_scan(struct net_device *dev, char *buffer)
+ 					IW_ENCODE_ENABLED | IW_ENCODE_NOKEY;
+ 			else
+ 				iwe.u.data.flags = IW_ENCODE_DISABLED;
+-			current_ev = iwe_stream_add_point(current_ev, end_buf,
+-							  &iwe,
+-							  sta->u.ap.ssid
+-							  /* 0 byte memcpy */);
++			current_ev = iwe_stream_add_point(info, current_ev,
++							  end_buf, &iwe,
++							  sta->u.ap.ssid);
+ 
+ 			if (sta->u.ap.channel > 0 &&
+ 			    sta->u.ap.channel <= FREQ_COUNT) {
+@@ -2471,7 +2471,7 @@ int prism2_ap_translate_scan(struct net_device *dev, char *buffer)
+ 					* 100000;
+ 				iwe.u.freq.e = 1;
+ 				current_ev = iwe_stream_add_event(
+-					current_ev, end_buf, &iwe,
++					info, current_ev, end_buf, &iwe,
+ 					IW_EV_FREQ_LEN);
+ 			}
+ 
+@@ -2480,8 +2480,8 @@ int prism2_ap_translate_scan(struct net_device *dev, char *buffer)
+ 			sprintf(buf, "beacon_interval=%d",
+ 				sta->listen_interval);
+ 			iwe.u.data.length = strlen(buf);
+-			current_ev = iwe_stream_add_point(current_ev, end_buf,
+-							  &iwe, buf);
++			current_ev = iwe_stream_add_point(info, current_ev,
++							  end_buf, &iwe, buf);
+ 		}
+ #endif /* PRISM2_NO_KERNEL_IEEE80211_MGMT */
+ 
+diff --git a/drivers/net/wireless/hostap/hostap_ioctl.c b/drivers/net/wireless/hostap/hostap_ioctl.c
+index d8f5efc..5b04054 100644
+--- a/drivers/net/wireless/hostap/hostap_ioctl.c
++++ b/drivers/net/wireless/hostap/hostap_ioctl.c
+@@ -1794,6 +1794,7 @@ static int prism2_ioctl_siwscan(struct net_device *dev,
+ 
+ #ifndef PRISM2_NO_STATION_MODES
+ static char * __prism2_translate_scan(local_info_t *local,
++				      struct iw_request_info *info,
+ 				      struct hfa384x_hostscan_result *scan,
+ 				      struct hostap_bss_info *bss,
+ 				      char *current_ev, char *end_buf)
+@@ -1824,7 +1825,7 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 	iwe.cmd = SIOCGIWAP;
+ 	iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 	memcpy(iwe.u.ap_addr.sa_data, bssid, ETH_ALEN);
+-	current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf, &iwe,
+ 					  IW_EV_ADDR_LEN);
+ 
+ 	/* Other entries will be displayed in the order we give them */
+@@ -1833,7 +1834,8 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 	iwe.cmd = SIOCGIWESSID;
+ 	iwe.u.data.length = ssid_len;
+ 	iwe.u.data.flags = 1;
+-	current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, ssid);
++	current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++					  &iwe, ssid);
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+ 	iwe.cmd = SIOCGIWMODE;
+@@ -1848,8 +1850,8 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 			iwe.u.mode = IW_MODE_MASTER;
+ 		else
+ 			iwe.u.mode = IW_MODE_ADHOC;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-						  IW_EV_UINT_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_UINT_LEN);
+ 	}
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+@@ -1865,8 +1867,8 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 	if (chan > 0) {
+ 		iwe.u.freq.m = freq_list[chan - 1] * 100000;
+ 		iwe.u.freq.e = 1;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-						  IW_EV_FREQ_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_FREQ_LEN);
+ 	}
+ 
+ 	if (scan) {
+@@ -1885,8 +1887,8 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 			| IW_QUAL_NOISE_UPDATED
+ 			| IW_QUAL_QUAL_INVALID
+ 			| IW_QUAL_DBM;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-						  IW_EV_QUAL_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_QUAL_LEN);
+ 	}
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+@@ -1896,7 +1898,7 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 	else
+ 		iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 	iwe.u.data.length = 0;
+-	current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, "");
++	current_ev = iwe_stream_add_point(info, current_ev, end_buf, &iwe, "");
+ 
+ 	/* TODO: add SuppRates into BSS table */
+ 	if (scan) {
+@@ -1910,7 +1912,7 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 			/* Bit rate given in 500 kb/s units (+ 0x80) */
+ 			iwe.u.bitrate.value = ((pos[i] & 0x7f) * 500000);
+ 			current_val = iwe_stream_add_value(
+-				current_ev, current_val, end_buf, &iwe,
++				info, current_ev, current_val, end_buf, &iwe,
+ 				IW_EV_PARAM_LEN);
+ 		}
+ 		/* Check if we added any event */
+@@ -1925,15 +1927,15 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 		iwe.cmd = IWEVCUSTOM;
+ 		sprintf(buf, "bcn_int=%d", le16_to_cpu(scan->beacon_interval));
+ 		iwe.u.data.length = strlen(buf);
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe,
+-						  buf);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, buf);
+ 
+ 		memset(&iwe, 0, sizeof(iwe));
+ 		iwe.cmd = IWEVCUSTOM;
+ 		sprintf(buf, "resp_rate=%d", le16_to_cpu(scan->rate));
+ 		iwe.u.data.length = strlen(buf);
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe,
+-						  buf);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, buf);
+ 
+ 		if (local->last_scan_type == PRISM2_HOSTSCAN &&
+ 		    (capabilities & WLAN_CAPABILITY_IBSS)) {
+@@ -1941,8 +1943,8 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 			iwe.cmd = IWEVCUSTOM;
+ 			sprintf(buf, "atim=%d", le16_to_cpu(scan->atim));
+ 			iwe.u.data.length = strlen(buf);
+-			current_ev = iwe_stream_add_point(current_ev, end_buf,
+-							  &iwe, buf);
++			current_ev = iwe_stream_add_point(info, current_ev,
++							  end_buf, &iwe, buf);
+ 		}
+ 	}
+ 	kfree(buf);
+@@ -1951,16 +1953,16 @@ static char * __prism2_translate_scan(local_info_t *local,
+ 		memset(&iwe, 0, sizeof(iwe));
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = bss->wpa_ie_len;
+-		current_ev = iwe_stream_add_point(
+-			current_ev, end_buf, &iwe, bss->wpa_ie);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, bss->wpa_ie);
+ 	}
+ 
+ 	if (bss && bss->rsn_ie_len > 0 && bss->rsn_ie_len <= MAX_WPA_IE_LEN) {
+ 		memset(&iwe, 0, sizeof(iwe));
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = bss->rsn_ie_len;
+-		current_ev = iwe_stream_add_point(
+-			current_ev, end_buf, &iwe, bss->rsn_ie);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, bss->rsn_ie);
+ 	}
+ 
+ 	return current_ev;
+@@ -1970,6 +1972,7 @@ static char * __prism2_translate_scan(local_info_t *local,
+ /* Translate scan data returned from the card to a card independant
+  * format that the Wireless Tools will understand - Jean II */
+ static inline int prism2_translate_scan(local_info_t *local,
++					struct iw_request_info *info,
+ 					char *buffer, int buflen)
+ {
+ 	struct hfa384x_hostscan_result *scan;
+@@ -2000,13 +2003,14 @@ static inline int prism2_translate_scan(local_info_t *local,
+ 			if (memcmp(bss->bssid, scan->bssid, ETH_ALEN) == 0) {
+ 				bss->included = 1;
+ 				current_ev = __prism2_translate_scan(
+-					local, scan, bss, current_ev, end_buf);
++					local, info, scan, bss, current_ev,
++					end_buf);
+ 				found++;
+ 			}
+ 		}
+ 		if (!found) {
+ 			current_ev = __prism2_translate_scan(
+-				local, scan, NULL, current_ev, end_buf);
++				local, info, scan, NULL, current_ev, end_buf);
+ 		}
+ 		/* Check if there is space for one more entry */
+ 		if ((end_buf - current_ev) <= IW_EV_ADDR_LEN) {
+@@ -2024,7 +2028,7 @@ static inline int prism2_translate_scan(local_info_t *local,
+ 		bss = list_entry(ptr, struct hostap_bss_info, list);
+ 		if (bss->included)
+ 			continue;
+-		current_ev = __prism2_translate_scan(local, NULL, bss,
++		current_ev = __prism2_translate_scan(local, info, NULL, bss,
+ 						     current_ev, end_buf);
+ 		/* Check if there is space for one more entry */
+ 		if ((end_buf - current_ev) <= IW_EV_ADDR_LEN) {
+@@ -2071,7 +2075,7 @@ static inline int prism2_ioctl_giwscan_sta(struct net_device *dev,
+ 	}
+ 	local->scan_timestamp = 0;
+ 
+-	res = prism2_translate_scan(local, extra, data->length);
++	res = prism2_translate_scan(local, info, extra, data->length);
+ 
+ 	if (res >= 0) {
+ 		data->length = res;
+@@ -2104,7 +2108,7 @@ static int prism2_ioctl_giwscan(struct net_device *dev,
+ 		 * Jean II */
+ 
+ 		/* Translate to WE format */
+-		res = prism2_ap_translate_scan(dev, extra);
++		res = prism2_ap_translate_scan(dev, info, extra);
+ 		if (res >= 0) {
+ 			printk(KERN_DEBUG "Scan result translation succeeded "
+ 			       "(length=%d)\n", res);
+diff --git a/drivers/net/wireless/libertas/scan.c b/drivers/net/wireless/libertas/scan.c
+index ad1e67d..9dbd932 100644
+--- a/drivers/net/wireless/libertas/scan.c
++++ b/drivers/net/wireless/libertas/scan.c
+@@ -1444,8 +1444,9 @@ out:
+ #define MAX_CUSTOM_LEN 64
+ 
+ static inline char *libertas_translate_scan(wlan_private *priv,
+-					char *start, char *stop,
+-					struct bss_descriptor *bss)
++					    struct iw_request_info *info,
++					    char *start, char *stop,
++					    struct bss_descriptor *bss)
+ {
+ 	wlan_adapter *adapter = priv->adapter;
+ 	struct chan_freq_power *cfp;
+@@ -1470,24 +1471,24 @@ static inline char *libertas_translate_scan(wlan_private *priv,
+ 	iwe.cmd = SIOCGIWAP;
+ 	iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 	memcpy(iwe.u.ap_addr.sa_data, &bss->bssid, ETH_ALEN);
+-	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_ADDR_LEN);
++	start = iwe_stream_add_event(info, start, stop, &iwe, IW_EV_ADDR_LEN);
+ 
+ 	/* SSID */
+ 	iwe.cmd = SIOCGIWESSID;
+ 	iwe.u.data.flags = 1;
+ 	iwe.u.data.length = min((u32) bss->ssid_len, (u32) IW_ESSID_MAX_SIZE);
+-	start = iwe_stream_add_point(start, stop, &iwe, bss->ssid);
++	start = iwe_stream_add_point(info, start, stop, &iwe, bss->ssid);
+ 
+ 	/* Mode */
+ 	iwe.cmd = SIOCGIWMODE;
+ 	iwe.u.mode = bss->mode;
+-	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_UINT_LEN);
++	start = iwe_stream_add_event(info, start, stop, &iwe, IW_EV_UINT_LEN);
+ 
+ 	/* Frequency */
+ 	iwe.cmd = SIOCGIWFREQ;
+ 	iwe.u.freq.m = (long)cfp->freq * 100000;
+ 	iwe.u.freq.e = 1;
+-	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_FREQ_LEN);
++	start = iwe_stream_add_event(info, start, stop, &iwe, IW_EV_FREQ_LEN);
+ 
+ 	/* Add quality statistics */
+ 	iwe.cmd = IWEVQUAL;
+@@ -1523,7 +1524,7 @@ static inline char *libertas_translate_scan(wlan_private *priv,
+ 		nf = adapter->NF[TYPE_RXPD][TYPE_AVG] / AVG_SCALE;
+ 		iwe.u.qual.level = CAL_RSSI(snr, nf);
+ 	}
+-	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_QUAL_LEN);
++	start = iwe_stream_add_event(info, start, stop, &iwe, IW_EV_QUAL_LEN);
+ 
+ 	/* Add encryption capability */
+ 	iwe.cmd = SIOCGIWENCODE;
+@@ -1533,7 +1534,7 @@ static inline char *libertas_translate_scan(wlan_private *priv,
+ 		iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 	}
+ 	iwe.u.data.length = 0;
+-	start = iwe_stream_add_point(start, stop, &iwe, bss->ssid);
++	start = iwe_stream_add_point(info, start, stop, &iwe, bss->ssid);
+ 
+ 	current_val = start + IW_EV_LCP_LEN;
+ 
+@@ -1545,8 +1546,8 @@ static inline char *libertas_translate_scan(wlan_private *priv,
+ 	for (j = 0; bss->rates[j] && (j < sizeof(bss->rates)); j++) {
+ 		/* Bit rate given in 500 kb/s units */
+ 		iwe.u.bitrate.value = bss->rates[j] * 500000;
+-		current_val = iwe_stream_add_value(start, current_val,
+-					 stop, &iwe, IW_EV_PARAM_LEN);
++		current_val = iwe_stream_add_value(info, start, current_val,
++						   stop, &iwe, IW_EV_PARAM_LEN);
+ 	}
+ 	if ((bss->mode == IW_MODE_ADHOC)
+ 	    && !libertas_ssid_cmp(adapter->curbssparams.ssid,
+@@ -1554,8 +1555,8 @@ static inline char *libertas_translate_scan(wlan_private *priv,
+ 	                          bss->ssid, bss->ssid_len)
+ 	    && adapter->adhoccreate) {
+ 		iwe.u.bitrate.value = 22 * 500000;
+-		current_val = iwe_stream_add_value(start, current_val,
+-					 stop, &iwe, IW_EV_PARAM_LEN);
++		current_val = iwe_stream_add_value(info, start, current_val,
++						   stop, &iwe, IW_EV_PARAM_LEN);
+ 	}
+ 	/* Check if we added any event */
+ 	if((current_val - start) > IW_EV_LCP_LEN)
+@@ -1567,7 +1568,7 @@ static inline char *libertas_translate_scan(wlan_private *priv,
+ 		memcpy(buf, bss->wpa_ie, bss->wpa_ie_len);
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = bss->wpa_ie_len;
+-		start = iwe_stream_add_point(start, stop, &iwe, buf);
++		start = iwe_stream_add_point(info, start, stop, &iwe, buf);
+ 	}
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+@@ -1576,7 +1577,7 @@ static inline char *libertas_translate_scan(wlan_private *priv,
+ 		memcpy(buf, bss->rsn_ie, bss->rsn_ie_len);
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = bss->rsn_ie_len;
+-		start = iwe_stream_add_point(start, stop, &iwe, buf);
++		start = iwe_stream_add_point(info, start, stop, &iwe, buf);
+ 	}
+ 
+ 	if (bss->mesh) {
+@@ -1588,7 +1589,8 @@ static inline char *libertas_translate_scan(wlan_private *priv,
+ 		              "mesh-type: olpc");
+ 		iwe.u.data.length = p - custom;
+ 		if (iwe.u.data.length)
+-			start = iwe_stream_add_point(start, stop, &iwe, custom);
++			start = iwe_stream_add_point(info, start, stop,
++						     &iwe, custom);
+ 	}
+ 
+ out:
+@@ -1650,7 +1652,8 @@ int libertas_get_scan(struct net_device *dev, struct iw_request_info *info,
+ 		}
+ 
+ 		/* Translate to WE format this entry */
+-		next_ev = libertas_translate_scan(priv, ev, stop, iter_bss);
++		next_ev = libertas_translate_scan(priv, info, ev, stop,
++						  iter_bss);
+ 		if (next_ev == NULL)
+ 			continue;
+ 		ev = next_ev;
+diff --git a/drivers/net/wireless/orinoco.c b/drivers/net/wireless/orinoco.c
+index ca6c2da..9141b8f 100644
+--- a/drivers/net/wireless/orinoco.c
++++ b/drivers/net/wireless/orinoco.c
+@@ -3909,6 +3909,7 @@ static int orinoco_ioctl_setscan(struct net_device *dev,
+  * format that the Wireless Tools will understand - Jean II
+  * Return message length or -errno for fatal errors */
+ static inline int orinoco_translate_scan(struct net_device *dev,
++					 struct iw_request_info *info,
+ 					 char *buffer,
+ 					 char *scan,
+ 					 int scan_len)
+@@ -3979,7 +3980,8 @@ static inline int orinoco_translate_scan(struct net_device *dev,
+ 		iwe.cmd = SIOCGIWAP;
+ 		iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 		memcpy(iwe.u.ap_addr.sa_data, atom->a.bssid, ETH_ALEN);
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_ADDR_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_ADDR_LEN);
+ 
+ 		/* Other entries will be displayed in the order we give them */
+ 
+@@ -3989,7 +3991,8 @@ static inline int orinoco_translate_scan(struct net_device *dev,
+ 			iwe.u.data.length = 32;
+ 		iwe.cmd = SIOCGIWESSID;
+ 		iwe.u.data.flags = 1;
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, atom->a.essid);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, atom->a.essid);
+ 
+ 		/* Add mode */
+ 		iwe.cmd = SIOCGIWMODE;
+@@ -3999,7 +4002,9 @@ static inline int orinoco_translate_scan(struct net_device *dev,
+ 				iwe.u.mode = IW_MODE_MASTER;
+ 			else
+ 				iwe.u.mode = IW_MODE_ADHOC;
+-			current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_UINT_LEN);
++			current_ev = iwe_stream_add_event(info, current_ev,
++							  end_buf, &iwe,
++							  IW_EV_UINT_LEN);
+ 		}
+ 
+ 		channel = atom->s.channel;
+@@ -4008,8 +4013,9 @@ static inline int orinoco_translate_scan(struct net_device *dev,
+ 			iwe.cmd = SIOCGIWFREQ;
+ 			iwe.u.freq.m = channel_frequency[channel-1] * 100000;
+ 			iwe.u.freq.e = 1;
+-			current_ev = iwe_stream_add_event(current_ev, end_buf,
+-							  &iwe, IW_EV_FREQ_LEN);
++			current_ev = iwe_stream_add_event(info, current_ev,
++							  end_buf, &iwe,
++							  IW_EV_FREQ_LEN);
+ 		}
+ 
+ 		/* Add quality statistics */
+@@ -4023,7 +4029,8 @@ static inline int orinoco_translate_scan(struct net_device *dev,
+ 			iwe.u.qual.qual = iwe.u.qual.level - iwe.u.qual.noise;
+ 		else
+ 			iwe.u.qual.qual = 0;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_QUAL_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_QUAL_LEN);
+ 
+ 		/* Add encryption capability */
+ 		iwe.cmd = SIOCGIWENCODE;
+@@ -4032,7 +4039,8 @@ static inline int orinoco_translate_scan(struct net_device *dev,
+ 		else
+ 			iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 		iwe.u.data.length = 0;
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, atom->a.essid);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, atom->a.essid);
+ 
+ 		/* Bit rate is not available in Lucent/Agere firmwares */
+ 		if (priv->firmware_type != FIRMWARE_TYPE_AGERE) {
+@@ -4055,9 +4063,9 @@ static inline int orinoco_translate_scan(struct net_device *dev,
+ 					break;
+ 				/* Bit rate given in 500 kb/s units (+ 0x80) */
+ 				iwe.u.bitrate.value = ((atom->p.rates[i] & 0x7f) * 500000);
+-				current_val = iwe_stream_add_value(current_ev, current_val,
+-								   end_buf, &iwe,
+-								   IW_EV_PARAM_LEN);
++				current_val = iwe_stream_add_value(
++					info, current_ev, current_val,
++					end_buf, &iwe, IW_EV_PARAM_LEN);
+ 			}
+ 			/* Check if we added any event */
+ 			if ((current_val - current_ev) > IW_EV_LCP_LEN)
+@@ -4102,7 +4110,7 @@ static int orinoco_ioctl_getscan(struct net_device *dev,
+ 		/* We have some results to push back to user space */
+ 
+ 		/* Translate to WE format */
+-		int ret = orinoco_translate_scan(dev, extra,
++		int ret = orinoco_translate_scan(dev, info, extra,
+ 						 priv->scan_result,
+ 						 priv->scan_len);
+ 
+diff --git a/drivers/net/wireless/prism54/isl_ioctl.c b/drivers/net/wireless/prism54/isl_ioctl.c
+index 6d80ca4..4dc0b5e 100644
+--- a/drivers/net/wireless/prism54/isl_ioctl.c
++++ b/drivers/net/wireless/prism54/isl_ioctl.c
+@@ -572,8 +572,9 @@ prism54_set_scan(struct net_device *dev, struct iw_request_info *info,
+  */
+ 
+ static char *
+-prism54_translate_bss(struct net_device *ndev, char *current_ev,
+-		      char *end_buf, struct obj_bss *bss, char noise)
++prism54_translate_bss(struct net_device *ndev, struct iw_request_info *info,
++		      char *current_ev, char *end_buf, struct obj_bss *bss,
++		      char noise)
+ {
+ 	struct iw_event iwe;	/* Temporary buffer */
+ 	short cap;
+@@ -585,8 +586,8 @@ prism54_translate_bss(struct net_device *ndev, char *current_ev,
+ 	memcpy(iwe.u.ap_addr.sa_data, bss->address, 6);
+ 	iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 	iwe.cmd = SIOCGIWAP;
+-	current_ev =
+-	    iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_ADDR_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_ADDR_LEN);
+ 
+ 	/* The following entries will be displayed in the same order we give them */
+ 
+@@ -594,7 +595,7 @@ prism54_translate_bss(struct net_device *ndev, char *current_ev,
+ 	iwe.u.data.length = bss->ssid.length;
+ 	iwe.u.data.flags = 1;
+ 	iwe.cmd = SIOCGIWESSID;
+-	current_ev = iwe_stream_add_point(current_ev, end_buf,
++	current_ev = iwe_stream_add_point(info, current_ev, end_buf,
+ 					  &iwe, bss->ssid.octets);
+ 
+ 	/* Capabilities */
+@@ -611,9 +612,8 @@ prism54_translate_bss(struct net_device *ndev, char *current_ev,
+ 		iwe.u.mode = IW_MODE_ADHOC;
+ 	iwe.cmd = SIOCGIWMODE;
+ 	if (iwe.u.mode)
+-		current_ev =
+-		    iwe_stream_add_event(current_ev, end_buf, &iwe,
+-					 IW_EV_UINT_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_UINT_LEN);
+ 
+ 	/* Encryption capability */
+ 	if (cap & CAP_CRYPT)
+@@ -622,14 +622,15 @@ prism54_translate_bss(struct net_device *ndev, char *current_ev,
+ 		iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 	iwe.u.data.length = 0;
+ 	iwe.cmd = SIOCGIWENCODE;
+-	current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, NULL);
++	current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++					  &iwe, NULL);
+ 
+ 	/* Add frequency. (short) bss->channel is the frequency in MHz */
+ 	iwe.u.freq.m = bss->channel;
+ 	iwe.u.freq.e = 6;
+ 	iwe.cmd = SIOCGIWFREQ;
+-	current_ev =
+-	    iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_FREQ_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_FREQ_LEN);
+ 
+ 	/* Add quality statistics */
+ 	iwe.u.qual.level = bss->rssi;
+@@ -637,16 +638,16 @@ prism54_translate_bss(struct net_device *ndev, char *current_ev,
+ 	/* do a simple SNR for quality */
+ 	iwe.u.qual.qual = bss->rssi - noise;
+ 	iwe.cmd = IWEVQUAL;
+-	current_ev =
+-	    iwe_stream_add_event(current_ev, end_buf, &iwe, IW_EV_QUAL_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_QUAL_LEN);
+ 
+ 	/* Add WPA/RSN Information Element, if any */
+ 	wpa_ie_len = prism54_wpa_bss_ie_get(priv, bss->address, wpa_ie);
+ 	if (wpa_ie_len > 0) {
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = min(wpa_ie_len, (size_t)MAX_WPA_IE_LEN);
+-		current_ev = iwe_stream_add_point(current_ev, end_buf,
+-				&iwe, wpa_ie);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, wpa_ie);
+ 	}
+ 	/* Do the bitrates */
+ 	{
+@@ -663,9 +664,9 @@ prism54_translate_bss(struct net_device *ndev, char *current_ev,
+ 		for(i = 0; i < sizeof(scan_rate_list); i++) {
+ 			if(bss->rates & mask) {
+ 				iwe.u.bitrate.value = (scan_rate_list[i] * 500000);
+-				current_val = iwe_stream_add_value(current_ev, current_val,
+-								   end_buf, &iwe,
+-								   IW_EV_PARAM_LEN);
++				current_val = iwe_stream_add_value(
++					info, current_ev, current_val,
++					end_buf, &iwe, IW_EV_PARAM_LEN);
+ 			}
+ 			mask <<= 1;
+ 		}
+@@ -711,7 +712,7 @@ prism54_get_scan(struct net_device *ndev, struct iw_request_info *info,
+ 
+ 	/* ok now, scan the list and translate its info */
+ 	for (i = 0; i < (int) bsslist->nr; i++) {
+-		current_ev = prism54_translate_bss(ndev, current_ev,
++		current_ev = prism54_translate_bss(ndev, info, current_ev,
+ 						   extra + dwrq->length,
+ 						   &(bsslist->bsslist[i]),
+ 						   noise);
+@@ -2705,6 +2706,7 @@ prism2_ioctl_scan_req(struct net_device *ndev,
+                      struct prism2_hostapd_param *param)
+ {
+ 	islpci_private *priv = netdev_priv(ndev);
++	struct iw_request_info info;
+ 	int i, rvalue;
+ 	struct obj_bsslist *bsslist;
+ 	u32 noise = 0;
+@@ -2728,9 +2730,12 @@ prism2_ioctl_scan_req(struct net_device *ndev,
+ 	rvalue |= mgt_get_request(priv, DOT11_OID_BSSLIST, 0, NULL, &r);
+ 	bsslist = r.ptr;
+ 
++	info.cmd = PRISM54_HOSTAPD;
++	info.flags = 0;
++
+ 	/* ok now, scan the list and translate its info */
+ 	for (i = 0; i < min(IW_MAX_AP, (int) bsslist->nr); i++)
+-		current_ev = prism54_translate_bss(ndev, current_ev,
++		current_ev = prism54_translate_bss(ndev, current_ev, &info,
+ 						   extra + IW_SCAN_MAX_DATA,
+ 						   &(bsslist->bsslist[i]),
+ 						   noise);
+diff --git a/drivers/net/wireless/wl3501_cs.c b/drivers/net/wireless/wl3501_cs.c
+index 42a36b3..3771419 100644
+--- a/drivers/net/wireless/wl3501_cs.c
++++ b/drivers/net/wireless/wl3501_cs.c
+@@ -1624,25 +1624,25 @@ static int wl3501_get_scan(struct net_device *dev, struct iw_request_info *info,
+ 		iwe.cmd			= SIOCGIWAP;
+ 		iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 		memcpy(iwe.u.ap_addr.sa_data, this->bss_set[i].bssid, ETH_ALEN);
+-		current_ev = iwe_stream_add_event(current_ev,
++		current_ev = iwe_stream_add_event(info, current_ev,
+ 						  extra + IW_SCAN_MAX_DATA,
+ 						  &iwe, IW_EV_ADDR_LEN);
+ 		iwe.cmd		  = SIOCGIWESSID;
+ 		iwe.u.data.flags  = 1;
+ 		iwe.u.data.length = this->bss_set[i].ssid.el.len;
+-		current_ev = iwe_stream_add_point(current_ev,
++		current_ev = iwe_stream_add_point(info, current_ev,
+ 						  extra + IW_SCAN_MAX_DATA,
+ 						  &iwe,
+ 						  this->bss_set[i].ssid.essid);
+ 		iwe.cmd	   = SIOCGIWMODE;
+ 		iwe.u.mode = this->bss_set[i].bss_type;
+-		current_ev = iwe_stream_add_event(current_ev,
++		current_ev = iwe_stream_add_event(info, current_ev,
+ 						  extra + IW_SCAN_MAX_DATA,
+ 						  &iwe, IW_EV_UINT_LEN);
+ 		iwe.cmd = SIOCGIWFREQ;
+ 		iwe.u.freq.m = this->bss_set[i].ds_pset.chan;
+ 		iwe.u.freq.e = 0;
+-		current_ev = iwe_stream_add_event(current_ev,
++		current_ev = iwe_stream_add_event(info, current_ev,
+ 						  extra + IW_SCAN_MAX_DATA,
+ 						  &iwe, IW_EV_FREQ_LEN);
+ 		iwe.cmd = SIOCGIWENCODE;
+@@ -1651,7 +1651,7 @@ static int wl3501_get_scan(struct net_device *dev, struct iw_request_info *info,
+ 		else
+ 			iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 		iwe.u.data.length = 0;
+-		current_ev = iwe_stream_add_point(current_ev,
++		current_ev = iwe_stream_add_point(info, current_ev,
+ 						  extra + IW_SCAN_MAX_DATA,
+ 						  &iwe, NULL);
+ 	}
+diff --git a/drivers/net/wireless/zd1201.c b/drivers/net/wireless/zd1201.c
+index d5c0c66..10c8642 100644
+--- a/drivers/net/wireless/zd1201.c
++++ b/drivers/net/wireless/zd1201.c
+@@ -1152,32 +1152,36 @@ static int zd1201_get_scan(struct net_device *dev,
+ 		iwe.cmd = SIOCGIWAP;
+ 		iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 		memcpy(iwe.u.ap_addr.sa_data, zd->rxdata+i+6, 6);
+-		cev = iwe_stream_add_event(cev, end_buf, &iwe, IW_EV_ADDR_LEN);
++		cev = iwe_stream_add_event(info, cev, end_buf,
++					   &iwe, IW_EV_ADDR_LEN);
+ 
+ 		iwe.cmd = SIOCGIWESSID;
+ 		iwe.u.data.length = zd->rxdata[i+16];
+ 		iwe.u.data.flags = 1;
+-		cev = iwe_stream_add_point(cev, end_buf, &iwe, zd->rxdata+i+18);
++		cev = iwe_stream_add_point(info, cev, end_buf,
++					   &iwe, zd->rxdata+i+18);
+ 
+ 		iwe.cmd = SIOCGIWMODE;
+ 		if (zd->rxdata[i+14]&0x01)
+ 			iwe.u.mode = IW_MODE_MASTER;
+ 		else
+ 			iwe.u.mode = IW_MODE_ADHOC;
+-		cev = iwe_stream_add_event(cev, end_buf, &iwe, IW_EV_UINT_LEN);
++		cev = iwe_stream_add_event(info, cev, end_buf,
++					   &iwe, IW_EV_UINT_LEN);
+ 		
+ 		iwe.cmd = SIOCGIWFREQ;
+ 		iwe.u.freq.m = zd->rxdata[i+0];
+ 		iwe.u.freq.e = 0;
+-		cev = iwe_stream_add_event(cev, end_buf, &iwe, IW_EV_FREQ_LEN);
++		cev = iwe_stream_add_event(info, cev, end_buf,
++					   &iwe, IW_EV_FREQ_LEN);
+ 		
+ 		iwe.cmd = SIOCGIWRATE;
+ 		iwe.u.bitrate.fixed = 0;
+ 		iwe.u.bitrate.disabled = 0;
+ 		for (j=0; j<10; j++) if (zd->rxdata[i+50+j]) {
+ 			iwe.u.bitrate.value = (zd->rxdata[i+50+j]&0x7f)*500000;
+-			cev=iwe_stream_add_event(cev, end_buf, &iwe,
+-			    IW_EV_PARAM_LEN);
++			cev=iwe_stream_add_event(info, cev, end_buf,
++						 &iwe, IW_EV_PARAM_LEN);
+ 		}
+ 		
+ 		iwe.cmd = SIOCGIWENCODE;
+@@ -1186,14 +1190,15 @@ static int zd1201_get_scan(struct net_device *dev,
+ 			iwe.u.data.flags = IW_ENCODE_ENABLED;
+ 		else
+ 			iwe.u.data.flags = IW_ENCODE_DISABLED;
+-		cev = iwe_stream_add_point(cev, end_buf, &iwe, NULL);
++		cev = iwe_stream_add_point(info, cev, end_buf, &iwe, NULL);
+ 		
+ 		iwe.cmd = IWEVQUAL;
+ 		iwe.u.qual.qual = zd->rxdata[i+4];
+ 		iwe.u.qual.noise= zd->rxdata[i+2]/10-100;
+ 		iwe.u.qual.level = (256+zd->rxdata[i+4]*100)/255-100;
+ 		iwe.u.qual.updated = 7;
+-		cev = iwe_stream_add_event(cev, end_buf, &iwe, IW_EV_QUAL_LEN);
++		cev = iwe_stream_add_event(info, cev, end_buf,
++					   &iwe, IW_EV_QUAL_LEN);
+ 	}
+ 
+ 	if (!enabled_save)
+diff --git a/include/linux/wireless.h b/include/linux/wireless.h
+index 2088524..bec73df 100644
+--- a/include/linux/wireless.h
++++ b/include/linux/wireless.h
+@@ -1098,6 +1098,22 @@ struct iw_event
+ #define IW_EV_POINT_LEN	(IW_EV_LCP_LEN + sizeof(struct iw_point) - \
+ 			 IW_EV_POINT_OFF)
+ 
++#if defined(__KERNEL__) && defined(CONFIG_COMPAT)
++struct __compat_iw_event {
++	__u16		len;			/* Real lenght of this stuff */
++	__u16		cmd;			/* Wireless IOCTL */
++	compat_caddr_t	pointer;
++};
++#define IW_EV_COMPAT_LCP_LEN \
++	(sizeof(struct __compat_iw_event) - sizeof(compat_caddr_t))
++#define IW_EV_COMPAT_POINT_OFF (((char *) \
++			  &(((struct compat_iw_point *) NULL)->length)) - \
++			  (char *) NULL)
++#define IW_EV_COMPAT_POINT_LEN	\
++	(IW_EV_COMPAT_LCP_LEN + sizeof(struct compat_iw_point) - \
++	 IW_EV_COMPAT_POINT_OFF)
++#endif
++
+ /* Size of the Event prefix when packed in stream */
+ #define IW_EV_LCP_PK_LEN	(4)
+ /* Size of the various events when packed in stream */
+diff --git a/include/net/iw_handler.h b/include/net/iw_handler.h
+index c99a8ee..d6f0c51 100644
+--- a/include/net/iw_handler.h
++++ b/include/net/iw_handler.h
+@@ -483,19 +483,26 @@ extern void wireless_spy_update(struct net_device *	dev,
+  * Wrapper to add an Wireless Event to a stream of events.
+  */
+ static inline char *
+-iwe_stream_add_event(char *	stream,		/* Stream of events */
+-		     char *	ends,		/* End of stream */
+-		     struct iw_event *iwe,	/* Payload */
+-		     int	event_len)	/* Real size of payload */
++iwe_stream_add_event(struct iw_request_info *info, char *stream, char *ends,
++		     struct iw_event *iwe, int event_len)
+ {
++	int lcp_len = IW_EV_LCP_LEN;
++
++#ifdef CONFIG_COMPAT
++	if (info->flags & IW_REQUEST_FLAG_COMPAT) {
++		event_len -= IW_EV_LCP_LEN;
++		event_len += IW_EV_COMPAT_LCP_LEN;
++		lcp_len = IW_EV_COMPAT_LCP_LEN;
++	}
++#endif
+ 	/* Check if it's possible */
+ 	if(likely((stream + event_len) < ends)) {
+ 		iwe->len = event_len;
+ 		/* Beware of alignement issues on 64 bits */
+ 		memcpy(stream, (char *) iwe, IW_EV_LCP_PK_LEN);
+-		memcpy(stream + IW_EV_LCP_LEN,
+-		       ((char *) iwe) + IW_EV_LCP_LEN,
+-		       event_len - IW_EV_LCP_LEN);
++		memcpy(stream + lcp_len,
++		       ((char *) iwe) + lcp_len,
++		       event_len - lcp_len);
+ 		stream += event_len;
+ 	}
+ 	return stream;
+@@ -507,104 +514,33 @@ iwe_stream_add_event(char *	stream,		/* Stream of events */
+  * stream of events.
+  */
+ static inline char *
+-iwe_stream_add_point(char *	stream,		/* Stream of events */
+-		     char *	ends,		/* End of stream */
+-		     struct iw_event *iwe,	/* Payload length + flags */
+-		     char *	extra)		/* More payload */
++iwe_stream_add_point(struct iw_request_info *info, char *stream, char *ends,
++		     struct iw_event *iwe, char *extra)
+ {
+-	int	event_len = IW_EV_POINT_LEN + iwe->u.data.length;
+-	/* Check if it's possible */
+-	if(likely((stream + event_len) < ends)) {
+-		iwe->len = event_len;
+-		memcpy(stream, (char *) iwe, IW_EV_LCP_PK_LEN);
+-		memcpy(stream + IW_EV_LCP_LEN,
+-		       ((char *) iwe) + IW_EV_LCP_LEN + IW_EV_POINT_OFF,
+-		       IW_EV_POINT_PK_LEN - IW_EV_LCP_PK_LEN);
+-		memcpy(stream + IW_EV_POINT_LEN, extra, iwe->u.data.length);
+-		stream += event_len;
++	int event_len = IW_EV_POINT_LEN + iwe->u.data.length;
++	int point_len = IW_EV_POINT_LEN;
++	int lcp_len = IW_EV_LCP_LEN;
++	int point_off = IW_EV_POINT_OFF;
++
++#ifdef CONFIG_COMPAT
++	if (info->flags & IW_REQUEST_FLAG_COMPAT) {
++		event_len = IW_EV_COMPAT_POINT_LEN + iwe->u.data.length;
++		point_len = IW_EV_COMPAT_POINT_LEN;
++		lcp_len = IW_EV_COMPAT_LCP_LEN;
++		point_off = IW_EV_COMPAT_POINT_OFF;
+ 	}
+-	return stream;
+-}
+-
+-/*------------------------------------------------------------------*/
+-/*
+- * Wrapper to add a value to a Wireless Event in a stream of events.
+- * Be careful, this one is tricky to use properly :
+- * At the first run, you need to have (value = event + IW_EV_LCP_LEN).
+- */
+-static inline char *
+-iwe_stream_add_value(char *	event,		/* Event in the stream */
+-		     char *	value,		/* Value in event */
+-		     char *	ends,		/* End of stream */
+-		     struct iw_event *iwe,	/* Payload */
+-		     int	event_len)	/* Real size of payload */
+-{
+-	/* Don't duplicate LCP */
+-	event_len -= IW_EV_LCP_LEN;
++#endif
+ 
+ 	/* Check if it's possible */
+-	if(likely((value + event_len) < ends)) {
+-		/* Add new value */
+-		memcpy(value, (char *) iwe + IW_EV_LCP_LEN, event_len);
+-		value += event_len;
+-		/* Patch LCP */
+-		iwe->len = value - event;
+-		memcpy(event, (char *) iwe, IW_EV_LCP_LEN);
+-	}
+-	return value;
+-}
+-
+-/*------------------------------------------------------------------*/
+-/*
+- * Wrapper to add an Wireless Event to a stream of events.
+- * Same as above, with explicit error check...
+- */
+-static inline char *
+-iwe_stream_check_add_event(char *	stream,		/* Stream of events */
+-			   char *	ends,		/* End of stream */
+-			   struct iw_event *iwe,	/* Payload */
+-			   int		event_len,	/* Size of payload */
+-			   int *	perr)		/* Error report */
+-{
+-	/* Check if it's possible, set error if not */
+ 	if(likely((stream + event_len) < ends)) {
+ 		iwe->len = event_len;
+-		/* Beware of alignement issues on 64 bits */
+ 		memcpy(stream, (char *) iwe, IW_EV_LCP_PK_LEN);
+-		memcpy(stream + IW_EV_LCP_LEN,
+-		       ((char *) iwe) + IW_EV_LCP_LEN,
+-		       event_len - IW_EV_LCP_LEN);
+-		stream += event_len;
+-	} else
+-		*perr = -E2BIG;
+-	return stream;
+-}
+-
+-/*------------------------------------------------------------------*/
+-/*
+- * Wrapper to add an short Wireless Event containing a pointer to a
+- * stream of events.
+- * Same as above, with explicit error check...
+- */
+-static inline char *
+-iwe_stream_check_add_point(char *	stream,		/* Stream of events */
+-			   char *	ends,		/* End of stream */
+-			   struct iw_event *iwe,	/* Payload length + flags */
+-			   char *	extra,		/* More payload */
+-			   int *	perr)		/* Error report */
+-{
+-	int	event_len = IW_EV_POINT_LEN + iwe->u.data.length;
+-	/* Check if it's possible */
+-	if(likely((stream + event_len) < ends)) {
+-		iwe->len = event_len;
+-		memcpy(stream, (char *) iwe, IW_EV_LCP_PK_LEN);
+-		memcpy(stream + IW_EV_LCP_LEN,
+-		       ((char *) iwe) + IW_EV_LCP_LEN + IW_EV_POINT_OFF,
++		memcpy(stream + lcp_len,
++		       ((char *) iwe) + lcp_len + point_off,
+ 		       IW_EV_POINT_PK_LEN - IW_EV_LCP_PK_LEN);
+-		memcpy(stream + IW_EV_POINT_LEN, extra, iwe->u.data.length);
++		memcpy(stream + point_len, extra, iwe->u.data.length);
+ 		stream += event_len;
+-	} else
+-		*perr = -E2BIG;
++	}
+ 	return stream;
+ }
+ 
+@@ -613,29 +549,29 @@ iwe_stream_check_add_point(char *	stream,		/* Stream of events */
+  * Wrapper to add a value to a Wireless Event in a stream of events.
+  * Be careful, this one is tricky to use properly :
+  * At the first run, you need to have (value = event + IW_EV_LCP_LEN).
+- * Same as above, with explicit error check...
+  */
+ static inline char *
+-iwe_stream_check_add_value(char *	event,		/* Event in the stream */
+-			   char *	value,		/* Value in event */
+-			   char *	ends,		/* End of stream */
+-			   struct iw_event *iwe,	/* Payload */
+-			   int		event_len,	/* Size of payload */
+-			   int *	perr)		/* Error report */
++iwe_stream_add_value(struct iw_request_info *info, char *event, char *value,
++		     char *ends, struct iw_event *iwe, int event_len)
+ {
++	int lcp_len = IW_EV_LCP_LEN;
++
++#ifdef CONFIG_COMPAT
++	if (info->flags & IW_REQUEST_FLAG_COMPAT)
++		lcp_len = IW_EV_COMPAT_LCP_LEN;
++#endif
+ 	/* Don't duplicate LCP */
+ 	event_len -= IW_EV_LCP_LEN;
+ 
+ 	/* Check if it's possible */
+ 	if(likely((value + event_len) < ends)) {
+ 		/* Add new value */
+-		memcpy(value, (char *) iwe + IW_EV_LCP_LEN, event_len);
++		memcpy(value, (char *) iwe + lcp_len, event_len);
+ 		value += event_len;
+ 		/* Patch LCP */
+ 		iwe->len = value - event;
+-		memcpy(event, (char *) iwe, IW_EV_LCP_LEN);
+-	} else
+-		*perr = -E2BIG;
++		memcpy(event, (char *) iwe, lcp_len);
++	}
+ 	return value;
+ }
+ 
+diff --git a/net/ieee80211/ieee80211_wx.c b/net/ieee80211/ieee80211_wx.c
+index d309e8f..cba556c 100644
+--- a/net/ieee80211/ieee80211_wx.c
++++ b/net/ieee80211/ieee80211_wx.c
+@@ -43,8 +43,9 @@ static const char *ieee80211_modes[] = {
+ 
+ #define MAX_CUSTOM_LEN 64
+ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+-					   char *start, char *stop,
+-					   struct ieee80211_network *network)
++				      char *start, char *stop,
++				      struct ieee80211_network *network,
++				      struct iw_request_info *info)
+ {
+ 	char custom[MAX_CUSTOM_LEN];
+ 	char *p;
+@@ -57,7 +58,7 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 	iwe.cmd = SIOCGIWAP;
+ 	iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 	memcpy(iwe.u.ap_addr.sa_data, network->bssid, ETH_ALEN);
+-	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_ADDR_LEN);
++	start = iwe_stream_add_event(info, start, stop, &iwe, IW_EV_ADDR_LEN);
+ 
+ 	/* Remaining entries will be displayed in the order we provide them */
+ 
+@@ -66,17 +67,19 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 	iwe.u.data.flags = 1;
+ 	if (network->flags & NETWORK_EMPTY_ESSID) {
+ 		iwe.u.data.length = sizeof("<hidden>");
+-		start = iwe_stream_add_point(start, stop, &iwe, "<hidden>");
++		start = iwe_stream_add_point(info, start, stop,
++					     &iwe, "<hidden>");
+ 	} else {
+ 		iwe.u.data.length = min(network->ssid_len, (u8) 32);
+-		start = iwe_stream_add_point(start, stop, &iwe, network->ssid);
++		start = iwe_stream_add_point(info, start, stop,
++					     &iwe, network->ssid);
+ 	}
+ 
+ 	/* Add the protocol name */
+ 	iwe.cmd = SIOCGIWNAME;
+ 	snprintf(iwe.u.name, IFNAMSIZ, "IEEE 802.11%s",
+ 		 ieee80211_modes[network->mode]);
+-	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_CHAR_LEN);
++	start = iwe_stream_add_event(info, start, stop, &iwe, IW_EV_CHAR_LEN);
+ 
+ 	/* Add mode */
+ 	iwe.cmd = SIOCGIWMODE;
+@@ -86,7 +89,8 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 		else
+ 			iwe.u.mode = IW_MODE_ADHOC;
+ 
+-		start = iwe_stream_add_event(start, stop, &iwe, IW_EV_UINT_LEN);
++		start = iwe_stream_add_event(info, start, stop,
++					     &iwe, IW_EV_UINT_LEN);
+ 	}
+ 
+ 	/* Add channel and frequency */
+@@ -95,7 +99,7 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 	iwe.u.freq.m = ieee80211_channel_to_freq(ieee, network->channel);
+ 	iwe.u.freq.e = 6;
+ 	iwe.u.freq.i = 0;
+-	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_FREQ_LEN);
++	start = iwe_stream_add_event(info, start, stop, &iwe, IW_EV_FREQ_LEN);
+ 
+ 	/* Add encryption capability */
+ 	iwe.cmd = SIOCGIWENCODE;
+@@ -104,7 +108,8 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 	else
+ 		iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 	iwe.u.data.length = 0;
+-	start = iwe_stream_add_point(start, stop, &iwe, network->ssid);
++	start = iwe_stream_add_point(info, start, stop,
++				     &iwe, network->ssid);
+ 
+ 	/* Add basic and extended rates */
+ 	/* Rate : stuffing multiple values in a single event require a bit
+@@ -124,14 +129,16 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 		/* Bit rate given in 500 kb/s units (+ 0x80) */
+ 		iwe.u.bitrate.value = ((rate & 0x7f) * 500000);
+ 		/* Add new value to event */
+-		current_val = iwe_stream_add_value(start, current_val, stop, &iwe, IW_EV_PARAM_LEN);
++		current_val = iwe_stream_add_value(info, start, current_val,
++						   stop, &iwe, IW_EV_PARAM_LEN);
+ 	}
+ 	for (; j < network->rates_ex_len; j++) {
+ 		rate = network->rates_ex[j] & 0x7F;
+ 		/* Bit rate given in 500 kb/s units (+ 0x80) */
+ 		iwe.u.bitrate.value = ((rate & 0x7f) * 500000);
+ 		/* Add new value to event */
+-		current_val = iwe_stream_add_value(start, current_val, stop, &iwe, IW_EV_PARAM_LEN);
++		current_val = iwe_stream_add_value(info, start, current_val,
++						   stop, &iwe, IW_EV_PARAM_LEN);
+ 	}
+ 	/* Check if we added any rate */
+ 	if((current_val - start) > IW_EV_LCP_LEN)
+@@ -181,14 +188,14 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 		iwe.u.qual.level = network->stats.signal;
+ 	}
+ 
+-	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_QUAL_LEN);
++	start = iwe_stream_add_event(info, start, stop, &iwe, IW_EV_QUAL_LEN);
+ 
+ 	iwe.cmd = IWEVCUSTOM;
+ 	p = custom;
+ 
+ 	iwe.u.data.length = p - custom;
+ 	if (iwe.u.data.length)
+-		start = iwe_stream_add_point(start, stop, &iwe, custom);
++		start = iwe_stream_add_point(info, start, stop, &iwe, custom);
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+ 	if (network->wpa_ie_len) {
+@@ -196,7 +203,7 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 		memcpy(buf, network->wpa_ie, network->wpa_ie_len);
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = network->wpa_ie_len;
+-		start = iwe_stream_add_point(start, stop, &iwe, buf);
++		start = iwe_stream_add_point(info, start, stop, &iwe, buf);
+ 	}
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+@@ -205,7 +212,7 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 		memcpy(buf, network->rsn_ie, network->rsn_ie_len);
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = network->rsn_ie_len;
+-		start = iwe_stream_add_point(start, stop, &iwe, buf);
++		start = iwe_stream_add_point(info, start, stop, &iwe, buf);
+ 	}
+ 
+ 	/* Add EXTRA: Age to display seconds since last beacon/probe response
+@@ -217,7 +224,7 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 		      jiffies_to_msecs(jiffies - network->last_scanned));
+ 	iwe.u.data.length = p - custom;
+ 	if (iwe.u.data.length)
+-		start = iwe_stream_add_point(start, stop, &iwe, custom);
++		start = iwe_stream_add_point(info, start, stop, &iwe, custom);
+ 
+ 	/* Add spectrum management information */
+ 	iwe.cmd = -1;
+@@ -238,7 +245,7 @@ static char *ieee80211_translate_scan(struct ieee80211_device *ieee,
+ 
+ 	if (iwe.cmd == IWEVCUSTOM) {
+ 		iwe.u.data.length = p - custom;
+-		start = iwe_stream_add_point(start, stop, &iwe, custom);
++		start = iwe_stream_add_point(info, start, stop, &iwe, custom);
+ 	}
+ 
+ 	return start;
+@@ -272,7 +279,8 @@ int ieee80211_wx_get_scan(struct ieee80211_device *ieee,
+ 
+ 		if (ieee->scan_age == 0 ||
+ 		    time_after(network->last_scanned + ieee->scan_age, jiffies))
+-			ev = ieee80211_translate_scan(ieee, ev, stop, network);
++			ev = ieee80211_translate_scan(ieee, ev, stop, network,
++						      info);
+ 		else
+ 			IEEE80211_DEBUG_SCAN("Not showing network '%s ("
+ 					     "%s)' due to age (%dms).\n",
+diff --git a/net/mac80211/ieee80211_i.h b/net/mac80211/ieee80211_i.h
+index 72e1c93..b8306aa 100644
+--- a/net/mac80211/ieee80211_i.h
++++ b/net/mac80211/ieee80211_i.h
+@@ -23,6 +23,7 @@
+ #include <linux/spinlock.h>
+ #include <linux/etherdevice.h>
+ #include <net/wireless.h>
++#include <net/iw_handler.h>
+ #include "ieee80211_key.h"
+ #include "sta_info.h"
+ 
+@@ -748,7 +749,9 @@ int ieee80211_sta_set_bssid(struct net_device *dev, u8 *bssid);
+ int ieee80211_sta_req_scan(struct net_device *dev, u8 *ssid, size_t ssid_len);
+ void ieee80211_sta_req_auth(struct net_device *dev,
+ 			    struct ieee80211_if_sta *ifsta);
+-int ieee80211_sta_scan_results(struct net_device *dev, char *buf, size_t len);
++int ieee80211_sta_scan_results(struct net_device *dev,
++			       struct iw_request_info *info,
++			       char *buf, size_t len);
+ void ieee80211_sta_rx_scan(struct net_device *dev, struct sk_buff *skb,
+ 			   struct ieee80211_rx_status *rx_status);
+ void ieee80211_rx_bss_list_init(struct net_device *dev);
+diff --git a/net/mac80211/ieee80211_ioctl.c b/net/mac80211/ieee80211_ioctl.c
+index 7027eed..0f686f1 100644
+--- a/net/mac80211/ieee80211_ioctl.c
++++ b/net/mac80211/ieee80211_ioctl.c
+@@ -560,7 +560,7 @@ static int ieee80211_ioctl_giwscan(struct net_device *dev,
+ 	struct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);
+ 	if (local->sta_scanning)
+ 		return -EAGAIN;
+-	res = ieee80211_sta_scan_results(dev, extra, data->length);
++	res = ieee80211_sta_scan_results(dev, info, extra, data->length);
+ 	if (res >= 0) {
+ 		data->length = res;
+ 		return 0;
+diff --git a/net/mac80211/ieee80211_sta.c b/net/mac80211/ieee80211_sta.c
+index bee8080..b176e13 100644
+--- a/net/mac80211/ieee80211_sta.c
++++ b/net/mac80211/ieee80211_sta.c
+@@ -2881,6 +2881,7 @@ int ieee80211_sta_req_scan(struct net_device *dev, u8 *ssid, size_t ssid_len)
+ 
+ static char *
+ ieee80211_sta_scan_result(struct net_device *dev,
++			  struct iw_request_info *info,
+ 			  struct ieee80211_sta_bss *bss,
+ 			  char *current_ev, char *end_buf)
+ {
+@@ -2907,15 +2908,15 @@ ieee80211_sta_scan_result(struct net_device *dev,
+ 	iwe.cmd = SIOCGIWAP;
+ 	iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
+ 	memcpy(iwe.u.ap_addr.sa_data, bss->bssid, ETH_ALEN);
+-	current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-					  IW_EV_ADDR_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_ADDR_LEN);
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+ 	iwe.cmd = SIOCGIWESSID;
+ 	iwe.u.data.length = bss->ssid_len;
+ 	iwe.u.data.flags = 1;
+-	current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe,
+-					  bss->ssid);
++	current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++					  &iwe, bss->ssid);
+ 
+ 	if (bss->capability & (WLAN_CAPABILITY_ESS | WLAN_CAPABILITY_IBSS)) {
+ 		memset(&iwe, 0, sizeof(iwe));
+@@ -2924,20 +2925,20 @@ ieee80211_sta_scan_result(struct net_device *dev,
+ 			iwe.u.mode = IW_MODE_MASTER;
+ 		else
+ 			iwe.u.mode = IW_MODE_ADHOC;
+-		current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-						  IW_EV_UINT_LEN);
++		current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++						  &iwe, IW_EV_UINT_LEN);
+ 	}
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+ 	iwe.cmd = SIOCGIWFREQ;
+ 	iwe.u.freq.m = bss->channel;
+ 	iwe.u.freq.e = 0;
+-	current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-					  IW_EV_FREQ_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_FREQ_LEN);
+ 	iwe.u.freq.m = bss->freq * 100000;
+ 	iwe.u.freq.e = 1;
+-	current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-					  IW_EV_FREQ_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_FREQ_LEN);
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+ 	iwe.cmd = IWEVQUAL;
+@@ -2945,8 +2946,8 @@ ieee80211_sta_scan_result(struct net_device *dev,
+ 	iwe.u.qual.level = bss->rssi;
+ 	iwe.u.qual.noise = bss->noise;
+ 	iwe.u.qual.updated = local->wstats_flags;
+-	current_ev = iwe_stream_add_event(current_ev, end_buf, &iwe,
+-					  IW_EV_QUAL_LEN);
++	current_ev = iwe_stream_add_event(info, current_ev, end_buf,
++					  &iwe, IW_EV_QUAL_LEN);
+ 
+ 	memset(&iwe, 0, sizeof(iwe));
+ 	iwe.cmd = SIOCGIWENCODE;
+@@ -2955,22 +2956,22 @@ ieee80211_sta_scan_result(struct net_device *dev,
+ 	else
+ 		iwe.u.data.flags = IW_ENCODE_DISABLED;
+ 	iwe.u.data.length = 0;
+-	current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe, "");
++	current_ev = iwe_stream_add_point(info, current_ev, end_buf, &iwe, "");
+ 
+ 	if (bss && bss->wpa_ie) {
+ 		memset(&iwe, 0, sizeof(iwe));
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = bss->wpa_ie_len;
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe,
+-						  bss->wpa_ie);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, bss->wpa_ie);
+ 	}
+ 
+ 	if (bss && bss->rsn_ie) {
+ 		memset(&iwe, 0, sizeof(iwe));
+ 		iwe.cmd = IWEVGENIE;
+ 		iwe.u.data.length = bss->rsn_ie_len;
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe,
+-						  bss->rsn_ie);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, bss->rsn_ie);
+ 	}
+ 
+ 	if (bss && bss->supp_rates_len > 0) {
+@@ -2986,8 +2987,8 @@ ieee80211_sta_scan_result(struct net_device *dev,
+ 		for (i = 0; i < bss->supp_rates_len; i++) {
+ 			iwe.u.bitrate.value = ((bss->supp_rates[i] &
+ 							0x7f) * 500000);
+-			p = iwe_stream_add_value(current_ev, p,
+-					end_buf, &iwe, IW_EV_PARAM_LEN);
++			p = iwe_stream_add_value(info, current_ev, p, end_buf,
++						 &iwe, IW_EV_PARAM_LEN);
+ 		}
+ 		current_ev = p;
+ 	}
+@@ -3000,8 +3001,8 @@ ieee80211_sta_scan_result(struct net_device *dev,
+ 			iwe.cmd = IWEVCUSTOM;
+ 			sprintf(buf, "tsf=%016llx", (unsigned long long)(bss->timestamp));
+ 			iwe.u.data.length = strlen(buf);
+-			current_ev = iwe_stream_add_point(current_ev, end_buf,
+-							  &iwe, buf);
++			current_ev = iwe_stream_add_point(info, current_ev,
++							  end_buf, &iwe, buf);
+ 			kfree(buf);
+ 		}
+ 	}
+@@ -3020,15 +3021,15 @@ ieee80211_sta_scan_result(struct net_device *dev,
+ 		iwe.cmd = IWEVCUSTOM;
+ 		sprintf(buf, "bcn_int=%d", bss->beacon_int);
+ 		iwe.u.data.length = strlen(buf);
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe,
+-						  buf);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, buf);
+ 
+ 		memset(&iwe, 0, sizeof(iwe));
+ 		iwe.cmd = IWEVCUSTOM;
+ 		sprintf(buf, "capab=0x%04x", bss->capability);
+ 		iwe.u.data.length = strlen(buf);
+-		current_ev = iwe_stream_add_point(current_ev, end_buf, &iwe,
+-						  buf);
++		current_ev = iwe_stream_add_point(info, current_ev, end_buf,
++						  &iwe, buf);
+ 
+ 		kfree(buf);
+ 		break;
+@@ -3038,7 +3039,9 @@ ieee80211_sta_scan_result(struct net_device *dev,
+ }
+ 
+ 
+-int ieee80211_sta_scan_results(struct net_device *dev, char *buf, size_t len)
++int ieee80211_sta_scan_results(struct net_device *dev,
++			       struct iw_request_info *info,
++			       char *buf, size_t len)
+ {
+ 	struct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);
+ 	char *current_ev = buf;
+@@ -3051,8 +3054,8 @@ int ieee80211_sta_scan_results(struct net_device *dev, char *buf, size_t len)
+ 			spin_unlock_bh(&local->sta_bss_lock);
+ 			return -E2BIG;
+ 		}
+-		current_ev = ieee80211_sta_scan_result(dev, bss, current_ev,
+-						       end_buf);
++		current_ev = ieee80211_sta_scan_result(dev, info, bss,
++						       current_ev, end_buf);
+ 	}
+ 	spin_unlock_bh(&local->sta_bss_lock);
+ 	return current_ev - buf;
+-- 
+1.5.4.rc2.84.gf85fd
+
+-
+To unsubscribe from this list: send the line "unsubscribe linux-wireless" in
+the body of a message to majordomo@vger.kernel.org
+More majordomo info at  http://vger.kernel.org/majordomo-info.html
+
diff -Naur linux-2.6.25-org/patches/other/kbuild-includecheck-versioncheck-src-prefix.diff linux-2.6.25-id/patches/other/kbuild-includecheck-versioncheck-src-prefix.diff
--- linux-2.6.25-org/patches/other/kbuild-includecheck-versioncheck-src-prefix.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/kbuild-includecheck-versioncheck-src-prefix.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,27 @@
+Subject: kbuild: Add missing srctree prefix for includecheck and versioncheck
+
+Add missing $(srctree) prefix for scripts used by the includecheck and
+versioncheck make targets
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ Makefile |    4 ++--
+ 1 files changed, 2 insertions(+), 2 deletions(-)
+
+--- a/Makefile
++++ b/Makefile
+@@ -1414,12 +1414,12 @@ tags: FORCE
+ includecheck:
+ 	find * $(RCS_FIND_IGNORE) \
+ 		-name '*.[hcS]' -type f -print | sort \
+-		| xargs $(PERL) -w scripts/checkincludes.pl
++		| xargs $(PERL) -w $(srctree)/scripts/checkincludes.pl
+ 
+ versioncheck:
+ 	find * $(RCS_FIND_IGNORE) \
+ 		-name '*.[hcS]' -type f -print | sort \
+-		| xargs $(PERL) -w scripts/checkversion.pl
++		| xargs $(PERL) -w $(srctree)/scripts/checkversion.pl
+ 
+ namespacecheck:
+ 	$(PERL) $(srctree)/scripts/namespace.pl
diff -Naur linux-2.6.25-org/patches/other/powerpc-add-cell-srpn-bkmk.patch linux-2.6.25-id/patches/other/powerpc-add-cell-srpn-bkmk.patch
--- linux-2.6.25-org/patches/other/powerpc-add-cell-srpn-bkmk.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/powerpc-add-cell-srpn-bkmk.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,21 @@
+Subject: POWERPC: Add Cell SPRN bookmark register
+
+Add a definition for the Cell SPRN bookmark register
+to asm-powerpc/regs.h
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+Acked-by: Arnd Bergmann <arnd@arndb.de>
+---
+ include/asm-powerpc/reg.h |    1 +
+ 1 file changed, 1 insertion(+)
+
+--- a/include/asm-powerpc/reg.h
++++ b/include/asm-powerpc/reg.h
+@@ -553,6 +553,7 @@
+ #define SPRN_PA6T_BTCR	978	/* Breakpoint and Tagging Control Register */
+ #define SPRN_PA6T_IMAAT	979	/* Instruction Match Array Action Table */
+ #define SPRN_PA6T_PCCR	1019	/* Power Counter Control Register */
++#define SPRN_BKMK	1020	/* Cell Bookmark Register */
+ #define SPRN_PA6T_RPCCR	1021	/* Retire PC Trace Control Register */
+ 
+ 
diff -Naur linux-2.6.25-org/patches/other/powerpc-asm-mmu-hash64-sparse.diff linux-2.6.25-id/patches/other/powerpc-asm-mmu-hash64-sparse.diff
--- linux-2.6.25-org/patches/other/powerpc-asm-mmu-hash64-sparse.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/powerpc-asm-mmu-hash64-sparse.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,24 @@
+Subject: POWERPC: Kill sparse warning in HPTE_V_COMPARE()
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+Fix sparse warning in asm-powerpc/mmu-hash64.h:
+  constant 0xffffffffffffff80 is so big it is unsigned long
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ include/asm-powerpc/mmu-hash64.h |    2 +-
+ 1 files changed, 1 insertion(+), 1 deletion(-)
+
+--- a/include/asm-powerpc/mmu-hash64.h
++++ b/include/asm-powerpc/mmu-hash64.h
+@@ -80,7 +80,7 @@ extern char initial_stab[];
+ #define HPTE_V_AVPN_SHIFT	7
+ #define HPTE_V_AVPN		ASM_CONST(0x3fffffffffffff80)
+ #define HPTE_V_AVPN_VAL(x)	(((x) & HPTE_V_AVPN) >> HPTE_V_AVPN_SHIFT)
+-#define HPTE_V_COMPARE(x,y)	(!(((x) ^ (y)) & 0xffffffffffffff80))
++#define HPTE_V_COMPARE(x,y)	(!(((x) ^ (y)) & 0xffffffffffffff80UL))
+ #define HPTE_V_BOLTED		ASM_CONST(0x0000000000000010)
+ #define HPTE_V_LOCK		ASM_CONST(0x0000000000000008)
+ #define HPTE_V_LARGE		ASM_CONST(0x0000000000000004)
diff -Naur linux-2.6.25-org/patches/other/powerpc-fix-slb-debug-warnings.diff linux-2.6.25-id/patches/other/powerpc-fix-slb-debug-warnings.diff
--- linux-2.6.25-org/patches/other/powerpc-fix-slb-debug-warnings.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/powerpc-fix-slb-debug-warnings.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,37 @@
+Subject: POWERPC: Fix slb.c compile warnings
+
+Arrange for a syntax check to always be done on the powerpc/mm/slb.c
+DBG() macro by defining it to pr_debug() for non-debug builds. 
+
+Also, fix these related compile warnings:
+
+  slb.c:273: warning: format '%04x' expects type 'unsigned int', but argument 2 has type 'long unsigned int
+  slb.c:274: warning: format '%04x' expects type 'unsigned int', but argument 2 has type 'long unsigned int'
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/mm/slb.c |    6 +++---
+ 1 file changed, 3 insertions(+), 3 deletions(-)
+
+--- a/arch/powerpc/mm/slb.c
++++ b/arch/powerpc/mm/slb.c
+@@ -30,7 +30,7 @@
+ #ifdef DEBUG
+ #define DBG(fmt...) udbg_printf(fmt)
+ #else
+-#define DBG(fmt...)
++#define DBG(fmt...) pr_debug(fmt)
+ #endif
+ 
+ extern void slb_allocate_realmode(unsigned long ea);
+@@ -270,8 +270,8 @@ void slb_initialize(void)
+ 		patch_slb_encoding(slb_miss_kernel_load_io,
+ 				   SLB_VSID_KERNEL | io_llp);
+ 
+-		DBG("SLB: linear  LLP = %04x\n", linear_llp);
+-		DBG("SLB: io      LLP = %04x\n", io_llp);
++		DBG("SLB: linear  LLP = %04lx\n", linear_llp);
++		DBG("SLB: io      LLP = %04lx\n", io_llp);
+ 	}
+ 
+ 	get_paca()->stab_rr = SLB_NUM_BOLTED;
diff -Naur linux-2.6.25-org/patches/other/powerpc-warn-on-emulation.diff linux-2.6.25-id/patches/other/powerpc-warn-on-emulation.diff
--- linux-2.6.25-org/patches/other/powerpc-warn-on-emulation.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/powerpc-warn-on-emulation.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,358 @@
+Subject: powerpc: Keep track of emulated instructions
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+powerpc: Keep track of emulated instructions
+
+Counters for the various classes of emulated instructions are available under
+/sys/devices/system/cpu/cpu*/emulated/.
+Optionally (controlled by /proc/sys/kernel/cpu_emulation_warnings),
+rate-limited warnings can be printed to the console when instructions are
+emulated.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ arch/powerpc/kernel/align.c        |   17 ++++--
+ arch/powerpc/kernel/sysfs.c        |   99 ++++++++++++++++++++++++++++++++++++-
+ arch/powerpc/kernel/traps.c        |   24 ++++++++
+ include/asm-powerpc/emulated_ops.h |   52 +++++++++++++++++++
+ 4 files changed, 185 insertions(+), 7 deletions(-)
+
+--- a/arch/powerpc/kernel/align.c
++++ b/arch/powerpc/kernel/align.c
+@@ -24,6 +24,7 @@
+ #include <asm/system.h>
+ #include <asm/cache.h>
+ #include <asm/cputable.h>
++#include <asm/emulated_ops.h>
+ 
+ struct aligninfo {
+ 	unsigned char len;
+@@ -696,8 +697,10 @@ int fix_alignment(struct pt_regs *regs)
+ 	areg = dsisr & 0x1f;		/* register to update */
+ 
+ #ifdef CONFIG_SPE
+-	if ((instr >> 26) == 0x4)
++	if ((instr >> 26) == 0x4) {
++		WARN_EMULATE(spe);
+ 		return emulate_spe(regs, reg, instr);
++	}
+ #endif
+ 
+ 	instr = (dsisr >> 10) & 0x7f;
+@@ -731,17 +734,21 @@ int fix_alignment(struct pt_regs *regs)
+ 	/* A size of 0 indicates an instruction we don't support, with
+ 	 * the exception of DCBZ which is handled as a special case here
+ 	 */
+-	if (instr == DCBZ)
++	if (instr == DCBZ) {
++		WARN_EMULATE(dcbz);
+ 		return emulate_dcbz(regs, addr);
++	}
+ 	if (unlikely(nb == 0))
+ 		return 0;
+ 
+ 	/* Load/Store Multiple instructions are handled in their own
+ 	 * function
+ 	 */
+-	if (flags & M)
++	if (flags & M) {
++		WARN_EMULATE(multiple);
+ 		return emulate_multiple(regs, addr, reg, nb,
+ 					flags, instr, swiz);
++	}
+ 
+ 	/* Verify the address of the operand */
+ 	if (unlikely(user_mode(regs) &&
+@@ -758,8 +765,10 @@ int fix_alignment(struct pt_regs *regs)
+ 	}
+ 
+ 	/* Special case for 16-byte FP loads and stores */
+-	if (nb == 16)
++	if (nb == 16) {
++		WARN_EMULATE(fp_pair);
+ 		return emulate_fp_pair(regs, addr, reg, flags);
++	}
+ 
+ 	/* If we are loading, get the data from user space, else
+ 	 * get it from register values
+--- a/arch/powerpc/kernel/sysfs.c
++++ b/arch/powerpc/kernel/sysfs.c
+@@ -8,6 +8,7 @@
+ #include <linux/nodemask.h>
+ #include <linux/cpumask.h>
+ #include <linux/notifier.h>
++#include <linux/sysctl.h>
+ 
+ #include <asm/current.h>
+ #include <asm/processor.h>
+@@ -19,6 +20,7 @@
+ #include <asm/lppaca.h>
+ #include <asm/machdep.h>
+ #include <asm/smp.h>
++#include <asm/emulated_ops.h>
+ 
+ static DEFINE_PER_CPU(struct cpu, cpu_devices);
+ 
+@@ -291,12 +293,100 @@ static struct sysdev_attribute pa6t_attr
+ };
+ 
+ 
++#define SYSFS_EMULATED_SETUP(type)					\
++DEFINE_PER_CPU(atomic_long_t, emulated_ ## type);			\
++static ssize_t show_emulated_ ## type (struct sys_device *dev,		\
++				       char *buf)			\
++{									\
++	struct cpu *cpu = container_of(dev, struct cpu, sysdev);	\
++									\
++	return sprintf(buf, "%lu\n",					\
++		       atomic_long_read(&per_cpu(emulated_ ## type,	\
++					cpu->sysdev.id)));		\
++}									\
++									\
++static struct sysdev_attribute emulated_ ## type ## _attr = {		\
++	.attr = { .name = #type, .mode = 0400 },			\
++	.show = show_emulated_ ## type,					\
++};
++
++SYSFS_EMULATED_SETUP(dcba);
++SYSFS_EMULATED_SETUP(dcbz);
++SYSFS_EMULATED_SETUP(fp_pair);
++SYSFS_EMULATED_SETUP(mcrxr);
++SYSFS_EMULATED_SETUP(mfpvr);
++SYSFS_EMULATED_SETUP(multiple);
++SYSFS_EMULATED_SETUP(popcntb);
++SYSFS_EMULATED_SETUP(spe);
++SYSFS_EMULATED_SETUP(string);
++#ifdef CONFIG_MATH_EMULATION
++SYSFS_EMULATED_SETUP(math);
++#elif defined(CONFIG_8XX_MINIMAL_FPEMU)
++SYSFS_EMULATED_SETUP(8xx);
++#endif
++
++static struct attribute *emulated_attrs[] = {
++	&emulated_dcba_attr.attr,
++	&emulated_dcbz_attr.attr,
++	&emulated_fp_pair_attr.attr,
++	&emulated_mcrxr_attr.attr,
++	&emulated_mfpvr_attr.attr,
++	&emulated_multiple_attr.attr,
++	&emulated_popcntb_attr.attr,
++	&emulated_spe_attr.attr,
++	&emulated_string_attr.attr,
++#ifdef CONFIG_MATH_EMULATION
++	&emulated_math_attr.attr,
++#elif defined(CONFIG_8XX_MINIMAL_FPEMU)
++	&emulated_8xx_attr.attr,
++#endif
++	NULL
++};
++
++static struct attribute_group emulated_attr_group = {
++	.attrs = emulated_attrs,
++	.name = "emulated"
++};
++
++int sysctl_warn_emulated;
++
++#ifdef CONFIG_SYSCTL
++static ctl_table warn_emulated_ctl_table[]={
++	{
++		.procname	= "cpu_emulation_warnings",
++		.data		= &sysctl_warn_emulated,
++		.maxlen		= sizeof(int),
++		.mode		= 0644,
++		.proc_handler	= &proc_dointvec,
++	},
++	{}
++};
++
++static ctl_table warn_emulated_sysctl_root[] = {
++	{
++		.ctl_name	= CTL_KERN,
++		.procname	= "kernel",
++		.mode		= 0555,
++		.child		= warn_emulated_ctl_table,
++	},
++	{}
++};
++
++static inline void warn_emulated_sysctl_register(void)
++{
++	register_sysctl_table(warn_emulated_sysctl_root);
++}
++#else /* !CONFIG_SYSCTL */
++static inline void warn_emulated_sysctl_register(void) {}
++#endif /* !CONFIG_SYSCTL */
++
++
+ static void register_cpu_online(unsigned int cpu)
+ {
+ 	struct cpu *c = &per_cpu(cpu_devices, cpu);
+ 	struct sys_device *s = &c->sysdev;
+ 	struct sysdev_attribute *attrs, *pmc_attrs;
+-	int i, nattrs;
++	int i, nattrs, res;
+ 
+ 	if (!firmware_has_feature(FW_FEATURE_ISERIES) &&
+ 			cpu_has_feature(CPU_FTR_SMT))
+@@ -339,6 +429,11 @@ static void register_cpu_online(unsigned
+ 
+ 	if (cpu_has_feature(CPU_FTR_DSCR))
+ 		sysdev_create_file(s, &attr_dscr);
++
++	res = sysfs_create_group(&s->kobj, &emulated_attr_group);
++	if (res)
++		pr_warning("Cannot create emulated sysfs group for cpu %u\n",
++			   cpu);
+ }
+ 
+ #ifdef CONFIG_HOTPLUG_CPU
+@@ -560,6 +655,8 @@ static int __init topology_init(void)
+ 			register_cpu_online(cpu);
+ 	}
+ 
++	warn_emulated_sysctl_register();
++
+ 	return 0;
+ }
+ subsys_initcall(topology_init);
+--- a/arch/powerpc/kernel/traps.c
++++ b/arch/powerpc/kernel/traps.c
+@@ -53,6 +53,7 @@
+ #include <asm/processor.h>
+ #endif
+ #include <asm/kexec.h>
++#include <asm/emulated_ops.h>
+ 
+ #ifdef CONFIG_DEBUGGER
+ int (*__debugger)(struct pt_regs *regs);
+@@ -707,6 +708,13 @@ static int emulate_popcntb_inst(struct p
+ 	return 0;
+ }
+ 
++void do_warn_emulate(const char *type)
++{
++	if (printk_ratelimit())
++		pr_warning("%s used emulated %s instruction\n", current->comm,
++			   type);
++}
++
+ static int emulate_instruction(struct pt_regs *regs)
+ {
+ 	u32 instword;
+@@ -721,31 +729,38 @@ static int emulate_instruction(struct pt
+ 
+ 	/* Emulate the mfspr rD, PVR. */
+ 	if ((instword & INST_MFSPR_PVR_MASK) == INST_MFSPR_PVR) {
++		WARN_EMULATE(mfpvr);
+ 		rd = (instword >> 21) & 0x1f;
+ 		regs->gpr[rd] = mfspr(SPRN_PVR);
+ 		return 0;
+ 	}
+ 
+ 	/* Emulating the dcba insn is just a no-op.  */
+-	if ((instword & INST_DCBA_MASK) == INST_DCBA)
++	if ((instword & INST_DCBA_MASK) == INST_DCBA) {
++		WARN_EMULATE(dcba);
+ 		return 0;
++	}
+ 
+ 	/* Emulate the mcrxr insn.  */
+ 	if ((instword & INST_MCRXR_MASK) == INST_MCRXR) {
+ 		int shift = (instword >> 21) & 0x1c;
+ 		unsigned long msk = 0xf0000000UL >> shift;
+ 
++		WARN_EMULATE(mcrxr);
+ 		regs->ccr = (regs->ccr & ~msk) | ((regs->xer >> shift) & msk);
+ 		regs->xer &= ~0xf0000000UL;
+ 		return 0;
+ 	}
+ 
+ 	/* Emulate load/store string insn. */
+-	if ((instword & INST_STRING_GEN_MASK) == INST_STRING)
++	if ((instword & INST_STRING_GEN_MASK) == INST_STRING) {
++		WARN_EMULATE(string);
+ 		return emulate_string_inst(regs, instword);
++	}
+ 
+ 	/* Emulate the popcntb (Population Count Bytes) instruction. */
+ 	if ((instword & INST_POPCNTB_MASK) == INST_POPCNTB) {
++		WARN_EMULATE(popcntb);
+ 		return emulate_popcntb_inst(regs, instword);
+ 	}
+ 
+@@ -929,6 +944,8 @@ void SoftwareEmulation(struct pt_regs *r
+ 
+ #ifdef CONFIG_MATH_EMULATION
+ 	errcode = do_mathemu(regs);
++	if (errcode >= 0)
++		WARN_EMULATE(math);
+ 
+ 	switch (errcode) {
+ 	case 0:
+@@ -950,6 +967,9 @@ void SoftwareEmulation(struct pt_regs *r
+ 
+ #elif defined(CONFIG_8XX_MINIMAL_FPEMU)
+ 	errcode = Soft_emulate_8xx(regs);
++	if (errcode >= 0)
++		WARN_EMULATE(8xx);
++
+ 	switch (errcode) {
+ 	case 0:
+ 		emulate_single_step(regs);
+--- /dev/null
++++ b/include/asm-powerpc/emulated_ops.h
+@@ -0,0 +1,52 @@
++/*
++ *  Copyright 2007 Sony Corp.
++ *
++ *  This program is free software; you can redistribute it and/or modify
++ *  it under the terms of the GNU General Public License as published by
++ *  the Free Software Foundation; version 2 of the License.
++ *
++ *  This program is distributed in the hope that it will be useful,
++ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
++ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ *  GNU General Public License for more details.
++ *
++ *  You should have received a copy of the GNU General Public License
++ *  along with this program; if not, write to the Free Software
++ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
++ */
++
++#ifndef _ASM_POWERPC_EMULATED_OPS_H
++#define _ASM_POWERPC_EMULATED_OPS_H
++
++#include <linux/percpu.h>
++
++#include <asm/atomic.h>
++
++DECLARE_PER_CPU(atomic_long_t, emulated_dcba);
++DECLARE_PER_CPU(atomic_long_t, emulated_dcbz);
++DECLARE_PER_CPU(atomic_long_t, emulated_fp_pair);
++DECLARE_PER_CPU(atomic_long_t, emulated_mcrxr);
++DECLARE_PER_CPU(atomic_long_t, emulated_mfpvr);
++DECLARE_PER_CPU(atomic_long_t, emulated_multiple);
++DECLARE_PER_CPU(atomic_long_t, emulated_popcntb);
++DECLARE_PER_CPU(atomic_long_t, emulated_spe);
++DECLARE_PER_CPU(atomic_long_t, emulated_string);
++#ifdef CONFIG_MATH_EMULATION
++DECLARE_PER_CPU(atomic_long_t, emulated_math);
++#elif defined(CONFIG_8XX_MINIMAL_FPEMU)
++DECLARE_PER_CPU(atomic_long_t, emulated_8xx);
++#endif
++
++extern int sysctl_warn_emulated;
++extern void do_warn_emulate(const char *type);
++
++#define WARN_EMULATE(type)						\
++	do {								\
++		atomic_long_inc(&per_cpu(emulated_ ## type,		\
++					 raw_smp_processor_id()));	\
++		if (sysctl_warn_emulated)				\
++			do_warn_emulate(#type);				\
++	} while (0)
++
++
++#endif /* _ASM_POWERPC_EMULATED_OPS_H */
diff -Naur linux-2.6.25-org/patches/other/usb-fix-ehci-iso-transfer.patch linux-2.6.25-id/patches/other/usb-fix-ehci-iso-transfer.patch
--- linux-2.6.25-org/patches/other/usb-fix-ehci-iso-transfer.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/usb-fix-ehci-iso-transfer.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,164 @@
+
+From: David Brownell <dbrownell@users.sourceforge.net>
+
+This adds a workaround for an issue reported with ISO transfers
+on some EHCI controllers, most recently with VIA KT800 and PS3
+EHCI silicon.
+
+The issue is that the silicon doesn't necessarily seem to be done
+using ISO DMA descriptors (itd, sitd) when it marks them inactive.
+(One theory is that the ill-defined mechanism where hardware caches
+periodic transfer descriptors isn't invalidating their state...)
+With such silicon, quick re-use of those descriptors makes trouble.
+Waiting until the next frame seems to be a sufficient workaround.
+
+This patch ensures that the relevant descriptors aren't available
+for immediate re-use.  It does so by not recycling them until after
+issuing the completion callback which would reuse them by enqueueing
+an URB and thus (re)allocating ISO DMA descriptors.
+
+Signed-off-by: David Brownell <dbrownell@users.sourceforge.net>
+---
+ drivers/usb/host/ehci-sched.c |   58 ++++++++++++++++++++++++++++++------------
+ 1 file changed, 42 insertions(+), 16 deletions(-)
+
+--- g26.orig/drivers/usb/host/ehci-sched.c	2008-01-09 01:27:25.000000000 -0800
++++ g26/drivers/usb/host/ehci-sched.c	2008-01-09 04:18:39.000000000 -0800
+@@ -1565,6 +1565,16 @@ itd_link_urb (
+ 
+ #define	ISO_ERRS (EHCI_ISOC_BUF_ERR | EHCI_ISOC_BABBLE | EHCI_ISOC_XACTERR)
+ 
++/* Process and recycle a completed ITD.  Return true iff its urb completed,
++ * and hence its completion callback probably added things to the hardware
++ * schedule.
++ *
++ * Note that we carefully avoid recycling this descriptor until after any
++ * completion callback runs, so that it won't be reused quickly.  That is,
++ * assuming (a) no more than two urbs per frame on this endpoint, and also
++ * (b) only this endpoint's completions submit URBs.  It seems some silicon
++ * corrupts things if you reuse completed descriptors very quickly...
++ */
+ static unsigned
+ itd_complete (
+ 	struct ehci_hcd	*ehci,
+@@ -1577,6 +1587,7 @@ itd_complete (
+ 	int					urb_index = -1;
+ 	struct ehci_iso_stream			*stream = itd->stream;
+ 	struct usb_device			*dev;
++	unsigned				retval = false;
+ 
+ 	/* for each uframe with a packet */
+ 	for (uframe = 0; uframe < 8; uframe++) {
+@@ -1610,15 +1621,9 @@ itd_complete (
+ 		}
+ 	}
+ 
+-	usb_put_urb (urb);
+-	itd->urb = NULL;
+-	itd->stream = NULL;
+-	list_move (&itd->itd_list, &stream->free_list);
+-	iso_stream_put (ehci, stream);
+-
+ 	/* handle completion now? */
+ 	if (likely ((urb_index + 1) != urb->number_of_packets))
+-		return 0;
++		goto done;
+ 
+ 	/* ASSERT: it's really the last itd for this urb
+ 	list_for_each_entry (itd, &stream->td_list, itd_list)
+@@ -1628,6 +1633,7 @@ itd_complete (
+ 	/* give urb back to the driver; completion often (re)submits */
+ 	dev = urb->dev;
+ 	ehci_urb_done(ehci, urb, 0);
++	retval = true;
+ 	urb = NULL;
+ 	ehci->periodic_sched--;
+ 	ehci_to_hcd(ehci)->self.bandwidth_isoc_reqs--;
+@@ -1641,8 +1647,15 @@ itd_complete (
+ 			(stream->bEndpointAddress & USB_DIR_IN) ? "in" : "out");
+ 	}
+ 	iso_stream_put (ehci, stream);
++	/* OK to recycle this ITD now that its completion callback ran. */
++done:
++	usb_put_urb(urb);
++	itd->urb = NULL;
++	itd->stream = NULL;
++	list_move(&itd->itd_list, &stream->free_list);
++	iso_stream_put(ehci, stream);
+ 
+-	return 1;
++	return retval;
+ }
+ 
+ /*-------------------------------------------------------------------------*/
+@@ -1944,6 +1957,16 @@ sitd_link_urb (
+ #define	SITD_ERRS (SITD_STS_ERR | SITD_STS_DBE | SITD_STS_BABBLE \
+ 				| SITD_STS_XACT | SITD_STS_MMF)
+ 
++/* Process and recycle a completed SITD.  Return true iff its urb completed,
++ * and hence its completion callback probably added things to the hardware
++ * schedule.
++ *
++ * Note that we carefully avoid recycling this descriptor until after any
++ * completion callback runs, so that it won't be reused quickly.  That is,
++ * assuming (a) no more than two urbs per frame on this endpoint, and also
++ * (b) only this endpoint's completions submit URBs.  It seems some silicon
++ * corrupts things if you reuse completed descriptors very quickly...
++ */
+ static unsigned
+ sitd_complete (
+ 	struct ehci_hcd		*ehci,
+@@ -1955,6 +1978,7 @@ sitd_complete (
+ 	int					urb_index = -1;
+ 	struct ehci_iso_stream			*stream = sitd->stream;
+ 	struct usb_device			*dev;
++	unsigned				retval = false;
+ 
+ 	urb_index = sitd->index;
+ 	desc = &urb->iso_frame_desc [urb_index];
+@@ -1975,17 +1999,11 @@ sitd_complete (
+ 		desc->status = 0;
+ 		desc->actual_length = desc->length - SITD_LENGTH (t);
+ 	}
+-
+-	usb_put_urb (urb);
+-	sitd->urb = NULL;
+-	sitd->stream = NULL;
+-	list_move (&sitd->sitd_list, &stream->free_list);
+ 	stream->depth -= stream->interval << 3;
+-	iso_stream_put (ehci, stream);
+ 
+ 	/* handle completion now? */
+ 	if ((urb_index + 1) != urb->number_of_packets)
+-		return 0;
++		goto done;
+ 
+ 	/* ASSERT: it's really the last sitd for this urb
+ 	list_for_each_entry (sitd, &stream->td_list, sitd_list)
+@@ -1995,6 +2013,7 @@ sitd_complete (
+ 	/* give urb back to the driver; completion often (re)submits */
+ 	dev = urb->dev;
+ 	ehci_urb_done(ehci, urb, 0);
++	retval = true;
+ 	urb = NULL;
+ 	ehci->periodic_sched--;
+ 	ehci_to_hcd(ehci)->self.bandwidth_isoc_reqs--;
+@@ -2008,8 +2027,15 @@ sitd_complete (
+ 			(stream->bEndpointAddress & USB_DIR_IN) ? "in" : "out");
+ 	}
+ 	iso_stream_put (ehci, stream);
++	/* OK to recycle this SITD now that its completion callback ran. */
++done:
++	usb_put_urb(urb);
++	sitd->urb = NULL;
++	sitd->stream = NULL;
++	list_move(&sitd->sitd_list, &stream->free_list);
++	iso_stream_put(ehci, stream);
+ 
+-	return 1;
++	return retval;
+ }
+ 
+ 
+
+
diff -Naur linux-2.6.25-org/patches/other/wext-c99-style-initializer.patch linux-2.6.25-id/patches/other/wext-c99-style-initializer.patch
--- linux-2.6.25-org/patches/other/wext-c99-style-initializer.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/wext-c99-style-initializer.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,52 @@
+---
+ net/wireless/wext.c |   35 +++++++++++++++--------------------
+ 1 file changed, 15 insertions(+), 20 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -391,30 +391,25 @@ static const struct iw_ioctl_description
+ static const unsigned standard_event_num = ARRAY_SIZE(standard_event);
+ 
+ /* Size (in bytes) of the various private data types */
+-static const char iw_priv_type_size[] = {
+-	0,				/* IW_PRIV_TYPE_NONE */
+-	1,				/* IW_PRIV_TYPE_BYTE */
+-	1,				/* IW_PRIV_TYPE_CHAR */
+-	0,				/* Not defined */
+-	sizeof(__u32),			/* IW_PRIV_TYPE_INT */
+-	sizeof(struct iw_freq),		/* IW_PRIV_TYPE_FLOAT */
+-	sizeof(struct sockaddr),	/* IW_PRIV_TYPE_ADDR */
+-	0,				/* Not defined */
++static const char iw_priv_type_size[(IW_PRIV_TYPE_MASK >> 12) + 1] = {
++	[IW_PRIV_TYPE_NONE >> 12]	= 0,
++	[IW_PRIV_TYPE_BYTE >> 12]	= 1,
++	[IW_PRIV_TYPE_CHAR >> 12]	= 1,
++	[IW_PRIV_TYPE_INT >> 12]	= sizeof(__u32),
++	[IW_PRIV_TYPE_FLOAT >> 12]	= sizeof(struct iw_freq),
++	[IW_PRIV_TYPE_ADDR >> 12]	= sizeof(struct sockaddr)
+ };
+ 
+ /* Size (in bytes) of various events */
+ static const int event_type_size[] = {
+-	IW_EV_LCP_LEN,			/* IW_HEADER_TYPE_NULL */
+-	0,
+-	IW_EV_CHAR_LEN,			/* IW_HEADER_TYPE_CHAR */
+-	0,
+-	IW_EV_UINT_LEN,			/* IW_HEADER_TYPE_UINT */
+-	IW_EV_FREQ_LEN,			/* IW_HEADER_TYPE_FREQ */
+-	IW_EV_ADDR_LEN,			/* IW_HEADER_TYPE_ADDR */
+-	0,
+-	IW_EV_POINT_LEN,		/* Without variable payload */
+-	IW_EV_PARAM_LEN,		/* IW_HEADER_TYPE_PARAM */
+-	IW_EV_QUAL_LEN,			/* IW_HEADER_TYPE_QUAL */
++	[IW_HEADER_TYPE_NULL]	= IW_EV_LCP_LEN,
++	[IW_HEADER_TYPE_CHAR]	= IW_EV_CHAR_LEN,
++	[IW_HEADER_TYPE_UINT]	= IW_EV_UINT_LEN,
++	[IW_HEADER_TYPE_FREQ]	= IW_EV_FREQ_LEN,
++	[IW_HEADER_TYPE_ADDR]	= IW_EV_ADDR_LEN,
++	[IW_HEADER_TYPE_POINT]	= IW_EV_POINT_LEN, /* Without variable payload */
++	[IW_HEADER_TYPE_PARAM]	= IW_EV_PARAM_LEN,
++	[IW_HEADER_TYPE_QUAL]	= IW_EV_QUAL_LEN
+ };
+ 
+ 
diff -Naur linux-2.6.25-org/patches/other/wext-remove-unused-var.patch linux-2.6.25-id/patches/other/wext-remove-unused-var.patch
--- linux-2.6.25-org/patches/other/wext-remove-unused-var.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/other/wext-remove-unused-var.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,27 @@
+---
+ net/wireless/wext.c |   14 --------------
+ 1 file changed, 14 deletions(-)
+
+--- a/net/wireless/wext.c
++++ b/net/wireless/wext.c
+@@ -417,20 +417,6 @@ static const int event_type_size[] = {
+ 	IW_EV_QUAL_LEN,			/* IW_HEADER_TYPE_QUAL */
+ };
+ 
+-/* Size (in bytes) of various events, as packed */
+-static const int event_type_pk_size[] = {
+-	IW_EV_LCP_PK_LEN,		/* IW_HEADER_TYPE_NULL */
+-	0,
+-	IW_EV_CHAR_PK_LEN,		/* IW_HEADER_TYPE_CHAR */
+-	0,
+-	IW_EV_UINT_PK_LEN,		/* IW_HEADER_TYPE_UINT */
+-	IW_EV_FREQ_PK_LEN,		/* IW_HEADER_TYPE_FREQ */
+-	IW_EV_ADDR_PK_LEN,		/* IW_HEADER_TYPE_ADDR */
+-	0,
+-	IW_EV_POINT_PK_LEN,		/* Without variable payload */
+-	IW_EV_PARAM_PK_LEN,		/* IW_HEADER_TYPE_PARAM */
+-	IW_EV_QUAL_PK_LEN,		/* IW_HEADER_TYPE_QUAL */
+-};
+ 
+ /************************ COMMON SUBROUTINES ************************/
+ /*
diff -Naur linux-2.6.25-org/patches/perfmon/perfmon-cell-Initial-PS3-perfmon2-support.patch linux-2.6.25-id/patches/perfmon/perfmon-cell-Initial-PS3-perfmon2-support.patch
--- linux-2.6.25-org/patches/perfmon/perfmon-cell-Initial-PS3-perfmon2-support.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/perfmon/perfmon-cell-Initial-PS3-perfmon2-support.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,476 @@
+From 387b6abfd291107fe9f71293427a98afb0025b2d Mon Sep 17 00:00:00 2001
+From: Takashi Yamamoto <TakashiA.Yamamoto at jp.sony.com>
+Date: Wed, 31 Oct 2007 09:51:34 -0700
+Subject: [PATCH] perfmon(cell): Initial PS3 perfmon2 support
+
+Changelog:
+	- On PS3, the PS3 PMU(arch/powerpc/platform/ps3/pmu.c) is used
+	  instead of the Cell PMU(arch/powerpc/platform/cell/pmu.c) to
+	  control the HW PMU. This patch basically replaces cbe_***()
+	  with ps3_***() to use the PS3 PMU. To reduce platform dependency,
+	  this patch adds the pfm_cell_platform_pmu_info structure which
+	  has PMU function pointers for each Cell platform.
+
+	- add aquire_pmu()/release_pmu() to pfm_arch_pmu_info structure.
+	  They are needed to create/delete the PS3 PMU at PFM PMU
+	  acquisition/ release.
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto at jp.sony.com>
+Signed-off-by: Stephane Eranian <eranian@hpl.hp.com>
+---
+ arch/powerpc/perfmon/perfmon_cell.c |  216 +++++++++++++++++++++++++++++++---
+ include/asm-powerpc/perfmon.h       |   17 +++
+ 2 files changed, 214 insertions(+), 19 deletions(-)
+
+diff --git a/arch/powerpc/perfmon/perfmon_cell.c b/arch/powerpc/perfmon/perfmon_cell.c
+index 8dae6b6..a55a442 100644
+--- a/arch/powerpc/perfmon/perfmon_cell.c
++++ b/arch/powerpc/perfmon/perfmon_cell.c
+@@ -31,12 +31,30 @@
+ #include <asm/io.h>
+ #include <asm/machdep.h>
+ #include <asm/rtas.h>
++#include <asm/ps3.h>
+ 
+ MODULE_AUTHOR("Kevin Corry <kevcorry@us.ibm.com>, "
+ 	      "Carl Love <carll@us.ibm.com>");
+ MODULE_DESCRIPTION("Cell PMU description table");
+ MODULE_LICENSE("GPL");
+ 
++struct pfm_cell_platform_pmu_info {
++	u32  (*read_ctr)(u32 cpu, u32 ctr);
++	void (*write_ctr)(u32 cpu, u32 ctr, u32 val);
++	void (*write_pm07_control)(u32 cpu, u32 ctr, u32 val);
++	void (*write_pm)(u32 cpu, enum pm_reg_name reg, u32 val);
++	void (*enable_pm)(u32 cpu);
++	void (*disable_pm)(u32 cpu);
++	void (*enable_pm_interrupts)(u32 cpu, u32 thread, u32 mask);
++	u32  (*get_and_clear_pm_interrupts)(u32 cpu);
++	u32  (*get_hw_thread_id)(int cpu);
++	struct cbe_ppe_priv_regs __iomem *(*get_cpu_ppe_priv_regs)(int cpu);
++	struct cbe_pmd_regs __iomem *(*get_cpu_pmd_regs)(int cpu);
++	struct cbe_mic_tm_regs __iomem *(*get_cpu_mic_tm_regs)(int cpu);
++	int (*rtas_token)(const char *service);
++	int (*rtas_call)(int token, int param1, int param2, int *param3, ...);
++};
++
+ /*
+  * Mapping from Perfmon logical control registers to Cell hardware registers.
+  */
+@@ -154,10 +172,13 @@ static int rtas_reset_signals(u32 cpu)
+ 	struct cell_rtas_arg signal;
+ 	u64 real_addr = virt_to_phys(&signal);
+ 	int rc;
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+ 	memset(&signal, 0, sizeof(signal));
+ 	signal.cpu = RTAS_CPU(cpu);
+-	rc = rtas_call(rtas_token("ibm,cbe-perftools"),
++	rc = info->rtas_call(info->rtas_token("ibm,cbe-perftools"),
+ 		       5, 1, NULL,
+ 		       subfunc_RESET,
+ 		       passthru_DISABLE,
+@@ -179,8 +200,11 @@ static int rtas_activate_signals(struct cell_rtas_arg *signals,
+ {
+ 	u64 real_addr = virt_to_phys(signals);
+ 	int rc;
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+-	rc = rtas_call(rtas_token("ibm,cbe-perftools"),
++	rc = info->rtas_call(info->rtas_token("ibm,cbe-perftools"),
+ 		       5, 1, NULL,
+ 		       subfunc_ACTIVATE,
+ 		       passthru_ENABLE,
+@@ -267,10 +291,13 @@ static int passthru(u32 cpu, u64 enable)
+ 	struct cbe_ppe_priv_regs __iomem *ppe_priv_regs;
+ 	struct cbe_pmd_regs __iomem *pmd_regs;
+ 	struct cbe_mic_tm_regs __iomem *mic_tm_regs;
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+-	ppe_priv_regs = cbe_get_cpu_ppe_priv_regs(cpu);
+-	pmd_regs = cbe_get_cpu_pmd_regs(cpu);
+-	mic_tm_regs = cbe_get_cpu_mic_tm_regs(cpu);
++	ppe_priv_regs = info->get_cpu_ppe_priv_regs(cpu);
++	pmd_regs = info->get_cpu_pmd_regs(cpu);
++	mic_tm_regs = info->get_cpu_mic_tm_regs(cpu);
+ 
+ 	if (!ppe_priv_regs || !pmd_regs || !mic_tm_regs) {
+ 		PFM_ERR("Error getting Cell PPE, PMD, and MIC "
+@@ -402,6 +429,39 @@ static int celleb_activate_signals(struct cell_rtas_arg *signals,
+ }
+ 
+ /**
++ * ps3_reset_signals
++ *
++ * ps3 version of resetting the debug-bus signals.
++ **/
++static int ps3_reset_signals(u32 cpu)
++{
++#ifdef CONFIG_PPC_PS3
++	return ps3_set_signal(0, 0, 0, 0);
++#else
++	return 0;
++#endif
++}
++
++/**
++ * ps3_activate_signals
++ *
++ * ps3 version of activating the debug-bus signals.
++ **/
++static int ps3_activate_signals(struct cell_rtas_arg *signals,
++				int num_signals)
++{
++#ifdef CONFIG_PPC_PS3
++	int i;
++
++	for (i = 0; i < num_signals; i++)
++		ps3_set_signal(signals[i].signal_group, signals[i].bit,
++			       signals[i].sub_unit, signals[i].bus_word);
++#endif
++	return 0;
++}
++
++
++/**
+  * reset_signals
+  *
+  * Call to the firmware (if available) to reset the debug-bus signals.
+@@ -413,6 +473,8 @@ int reset_signals(u32 cpu)
+ 
+ 	if (machine_is(celleb))
+ 		rc = celleb_reset_signals(cpu);
++	else if (machine_is(ps3))
++		rc = ps3_reset_signals(cpu);
+ 	else
+ 		rc = rtas_reset_signals(cpu);
+ 
+@@ -431,6 +493,8 @@ int activate_signals(struct cell_rtas_arg *signals, int num_signals)
+ 
+ 	if (machine_is(celleb))
+ 		rc = celleb_activate_signals(signals, num_signals);
++	else if (machine_is(ps3))
++		rc = ps3_activate_signals(signals, num_signals);
+ 	else
+ 		rc = rtas_activate_signals(signals, num_signals);
+ 
+@@ -539,9 +603,12 @@ static int pfm_cell_probe_pmu(void)
+ static void pfm_cell_write_pmc(unsigned int cnum, u64 value)
+ {
+ 	int cpu = smp_processor_id();
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+ 	if (cnum < NR_CTRS) {
+-		cbe_write_pm07_control(cpu, cnum, value);
++		info->write_pm07_control(cpu, cnum, value);
+ 
+ 	} else if (cnum < NR_CTRS * 2) {
+ 		write_pm07_event(cpu, cnum - NR_CTRS, value);
+@@ -552,10 +619,11 @@ static void pfm_cell_write_pmc(unsigned int cnum, u64 value)
+ 		 * the interrupts are routed to the correct CPU, as well
+ 		 * as writing the desired value to the pm_status register.
+ 		 */
+-		cbe_enable_pm_interrupts(cpu, cbe_get_hw_thread_id(cpu), value);
++		info->enable_pm_interrupts(cpu, info->get_hw_thread_id(cpu),
++					   value);
+ 
+ 	} else if (cnum < PFM_PM_NUM_PMCS) {
+-		cbe_write_pm(cpu, cnum - (NR_CTRS * 2), value);
++		info->write_pm(cpu, cnum - (NR_CTRS * 2), value);
+ 	}
+ }
+ 
+@@ -565,9 +633,12 @@ static void pfm_cell_write_pmc(unsigned int cnum, u64 value)
+ static void pfm_cell_write_pmd(unsigned int cnum, u64 value)
+ {
+ 	int cpu = smp_processor_id();
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+ 	if (cnum < NR_CTRS) {
+-		cbe_write_ctr(cpu, cnum, value);
++		info->write_ctr(cpu, cnum, value);
+ 	}
+ }
+ 
+@@ -577,9 +648,12 @@ static void pfm_cell_write_pmd(unsigned int cnum, u64 value)
+ static u64 pfm_cell_read_pmd(unsigned int cnum)
+ {
+ 	int cpu = smp_processor_id();
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+ 	if (cnum < NR_CTRS) {
+-		return cbe_read_ctr(cpu, cnum);
++		return info->read_ctr(cpu, cnum);
+ 	}
+ 
+ 	return -EINVAL;
+@@ -593,7 +667,11 @@ static u64 pfm_cell_read_pmd(unsigned int cnum)
+ static void pfm_cell_enable_counters(struct pfm_context *ctx,
+ 				     struct pfm_event_set *set)
+ {
+-	cbe_enable_pm(smp_processor_id());
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
++
++	info->enable_pm(smp_processor_id());
+ }
+ 
+ /**
+@@ -604,7 +682,13 @@ static void pfm_cell_enable_counters(struct pfm_context *ctx,
+ static void pfm_cell_disable_counters(struct pfm_context *ctx,
+ 				      struct pfm_event_set *set)
+ {
+-	cbe_disable_pm(smp_processor_id());
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
++
++	info->disable_pm(smp_processor_id());
++	if (machine_is(ps3))
++		reset_signals(smp_processor_id());
+ }
+ 
+ /**
+@@ -622,6 +706,9 @@ void pfm_cell_restore_pmcs(struct pfm_event_set *set)
+ 	u64 value, *used_pmcs = set->used_pmcs;
+ 	int i, rc, num_used = 0, cpu = smp_processor_id();
+ 	s32 signal_number;
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+ 	memset(signals, 0, sizeof(signals));
+ 
+@@ -630,7 +717,7 @@ void pfm_cell_restore_pmcs(struct pfm_event_set *set)
+ 		 * in use, then it will simply clear the register, which will
+ 		 * disable the associated counter.
+ 		 */
+-		cbe_write_pm07_control(cpu, i, set->pmcs[i]);
++		info->write_pm07_control(cpu, i, set->pmcs[i]);
+ 
+ 		/* Set up the next RTAS array entry for this counter. Only
+ 		 * include pm07_event registers that are in use by this set
+@@ -720,6 +807,9 @@ static void pfm_cell_get_ovfl_pmds(struct pfm_context *ctx,
+ 	u32 pm_status, ovfl_ctrs;
+ 	u64 povfl_pmds = 0;
+ 	int i;
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+ 	if (!ctx_arch->last_read_updated)
+ 		/* This routine was not called via the interrupt handler.
+@@ -727,7 +817,7 @@ static void pfm_cell_get_ovfl_pmds(struct pfm_context *ctx,
+ 		 * last_read_pm_status.
+ 		 */
+ 		ctx_arch->last_read_pm_status =
+-			cbe_get_and_clear_pm_interrupts(smp_processor_id());
++ info->get_and_clear_pm_interrupts(smp_processor_id());
+ 
+ 	/* Reset the flag that the interrupt handler last read pm_status. */
+ 	ctx_arch->last_read_updated = 0;
+@@ -753,6 +843,45 @@ static void pfm_cell_get_ovfl_pmds(struct pfm_context *ctx,
+ }
+ 
+ /**
++ * pfm_cell_acquire_pmu
++ *
++ * acquire PMU resource.
++ * This acquisition is done when the first context is created.
++ **/
++int pfm_cell_acquire_pmu(void)
++{
++#ifdef CONFIG_PPC_PS3
++	int ret;
++
++	if (machine_is(ps3)) {
++		PFM_DBG("");
++		ret = ps3_create_lpm(1, 0, 0, 1);
++		if (ret) {
++			PFM_ERR("Can't create PS3 lpm. error:%d", ret);
++			return -EFAULT;
++		}
++	}
++#endif
++	return 0;
++}
++
++/**
++ * pfm_cell_release_pmu
++ *
++ * release PMU resource.
++ * actual release happens when last context is destroyed
++ **/
++void pfm_cell_release_pmu(void)
++{
++#ifdef CONFIG_PPC_PS3
++	if (machine_is(ps3)) {
++		if (ps3_delete_lpm())
++			PFM_ERR("Can't delete PS3 lpm.");
++	}
++#endif
++}
++
++/**
+  * handle_trace_buffer_interrupts
+  *
+  * This routine is for processing just the interval timer and trace buffer
+@@ -780,12 +909,15 @@ static void pfm_cell_irq_handler(struct pt_regs *regs, struct pfm_context *ctx)
+ 	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
+ 	u32 last_read_pm_status;
+ 	int cpu = smp_processor_id();
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+ 	/* Need to disable and reenable the performance counters to get the
+ 	 * desired behavior from the hardware. This is specific to the Cell
+ 	 * PMU hardware.
+ 	 */
+-	cbe_disable_pm(cpu);
++	info->disable_pm(cpu);
+ 
+ 	/* Read the pm_status register to get the interrupt bits. If a
+ 	 * perfmormance counter overflow interrupt occurred, call the core
+@@ -807,7 +939,7 @@ static void pfm_cell_irq_handler(struct pt_regs *regs, struct pfm_context *ctx)
+ 	 * - The pmd0 bit is the MSB of the 32 bit register.
+ 	 */
+ 	ctx_arch->last_read_pm_status = last_read_pm_status =
+-					cbe_get_and_clear_pm_interrupts(cpu);
++			info->get_and_clear_pm_interrupts(cpu);
+ 
+ 	/* Set flag for pfm_cell_get_ovfl_pmds() routine so it knows
+ 	 * last_read_pm_status was updated by the interrupt handler.
+@@ -829,8 +961,8 @@ static void pfm_cell_irq_handler(struct pt_regs *regs, struct pfm_context *ctx)
+ 	 * register. It is saved in the context when the register is
+ 	 * written.
+ 	 */
+-	cbe_enable_pm_interrupts(cpu, cbe_get_hw_thread_id(cpu),
+-				 ctx->active_set->pmcs[CELL_PMC_PM_STATUS]);
++	info->enable_pm_interrupts(cpu, info->get_hw_thread_id(cpu),
++	ctx->active_set->pmcs[CELL_PMC_PM_STATUS]);
+ 
+ 	/* The writes to the various performance counters only writes to a
+ 	 * latch. The new values (interrupt setting bits, reset counter value
+@@ -839,11 +971,52 @@ static void pfm_cell_irq_handler(struct pt_regs *regs, struct pfm_context *ctx)
+ 	 * permormance monitor needs to be disabled while writting to the
+ 	 * latches. This is a HW design issue.
+ 	 */
+-	cbe_enable_pm(cpu);
++	info->enable_pm(cpu);
+ }
+ 
++
++static struct pfm_cell_platform_pmu_info ps3_platform_pmu_info = {
++#ifdef CONFIG_PPC_PS3
++	.read_ctr                    = ps3_read_ctr,
++	.write_ctr                   = ps3_write_ctr,
++	.write_pm07_control          = ps3_write_pm07_control,
++	.write_pm                    = ps3_write_pm,
++	.enable_pm                   = ps3_enable_pm,
++	.disable_pm                  = ps3_disable_pm,
++	.enable_pm_interrupts        = ps3_enable_pm_interrupts,
++	.get_and_clear_pm_interrupts = ps3_get_and_clear_pm_interrupts,
++	.get_hw_thread_id            = ps3_get_hw_thread_id,
++	.get_cpu_ppe_priv_regs       = NULL,
++	.get_cpu_pmd_regs            = NULL,
++	.get_cpu_mic_tm_regs         = NULL,
++	.rtas_token                  = NULL,
++	.rtas_call                   = NULL,
++#endif
++};
++
++static struct pfm_cell_platform_pmu_info native_platform_pmu_info = {
++#ifdef CONFIG_PPC_CELL_NATIVE
++	.read_ctr                    = cbe_read_ctr,
++	.write_ctr                   = cbe_write_ctr,
++	.write_pm07_control          = cbe_write_pm07_control,
++	.write_pm                    = cbe_write_pm,
++	.enable_pm                   = cbe_enable_pm,
++	.disable_pm                  = cbe_disable_pm,
++	.enable_pm_interrupts        = cbe_enable_pm_interrupts,
++	.get_and_clear_pm_interrupts = cbe_get_and_clear_pm_interrupts,
++	.get_hw_thread_id            = cbe_get_hw_thread_id,
++	.get_cpu_ppe_priv_regs       = cbe_get_cpu_ppe_priv_regs,
++	.get_cpu_pmd_regs            = cbe_get_cpu_pmd_regs,
++	.get_cpu_mic_tm_regs         = cbe_get_cpu_mic_tm_regs,
++	.rtas_token                  = rtas_token,
++	.rtas_call                   = rtas_call,
++#endif
++};
++
+ static struct pfm_arch_pmu_info pfm_cell_pmu_info = {
+ 	.pmu_style        = PFM_POWERPC_PMU_CELL,
++	.acquire_pmu      = pfm_cell_acquire_pmu,
++	.release_pmu      = pfm_cell_release_pmu,
+ 	.write_pmc        = pfm_cell_write_pmc,
+ 	.write_pmd        = pfm_cell_write_pmd,
+ 	.read_pmd         = pfm_cell_read_pmd,
+@@ -884,6 +1057,11 @@ static void pfm_cell_platform_probe(void)
+ 		for (cnum = NR_CTRS; cnum < (NR_CTRS * 2); cnum++)
+ 			pfm_cell_pmc_desc[cnum].type |= PFM_REG_WC;
+ 	}
++
++	if (machine_is(ps3))
++		pfm_cell_pmu_info.platform_info = &ps3_platform_pmu_info;
++	else
++		pfm_cell_pmu_info.platform_info = &native_platform_pmu_info;
+ }
+ 
+ static int __init pfm_cell_pmu_init_module(void)
+diff --git a/include/asm-powerpc/perfmon.h b/include/asm-powerpc/perfmon.h
+index 7dae5ac..a38de85 100644
+--- a/include/asm-powerpc/perfmon.h
++++ b/include/asm-powerpc/perfmon.h
+@@ -74,6 +74,9 @@ struct pfm_arch_pmu_info {
+ 			     struct task_struct *task);
+ 	int  (*unload_context)(struct pfm_context *ctx,
+ 			       struct task_struct *task);
++	int  (*acquire_pmu)(void);
++	void (*release_pmu)(void);
++	void *platform_info;
+ };
+ 
+ #ifdef CONFIG_PPC32
+@@ -340,11 +343,25 @@ static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+ 
+ static inline int pfm_arch_pmu_acquire(void)
+ {
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	int rc = 0;
++
++	if (arch_info->acquire_pmu) {
++		rc = arch_info->acquire_pmu();
++		if (rc)
++			return rc;
++	}
++
+ 	return reserve_pmc_hardware(powerpc_irq_handler);
+ }
+ 
+ static inline void pfm_arch_pmu_release(void)
+ {
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	if (arch_info->release_pmu)
++		arch_info->release_pmu();
++
+ 	release_pmc_hardware();
+ }
+ 
+-- 
+1.5.2.2
+
diff -Naur linux-2.6.25-org/patches/perfmon/perfmon-core.diff linux-2.6.25-id/patches/perfmon/perfmon-core.diff
--- linux-2.6.25-org/patches/perfmon/perfmon-core.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/perfmon/perfmon-core.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,37582 @@
+---
+ Documentation/ABI/testing/sysfs-perfmon       |   82 
+ Documentation/ABI/testing/sysfs-perfmon-fmt   |   18 
+ Documentation/ABI/testing/sysfs-perfmon-pmu   |   46 
+ Documentation/ABI/testing/sysfs-perfmon-stats |  100 
+ Documentation/perfmon2.txt                    |  208 
+ MAINTAINERS                                   |    8 
+ Makefile                                      |    2 
+ arch/ia64/Kconfig                             |   10 
+ arch/ia64/Makefile                            |    1 
+ arch/ia64/defconfig                           |   11 
+ arch/ia64/kernel/Makefile                     |    3 
+ arch/ia64/kernel/entry.S                      |   12 
+ arch/ia64/kernel/irq_ia64.c                   |    7 
+ arch/ia64/kernel/perfmon.c                    | 6875 --------------------------
+ arch/ia64/kernel/perfmon_default_smpl.c       |  296 -
+ arch/ia64/kernel/perfmon_generic.h            |   45 
+ arch/ia64/kernel/perfmon_itanium.h            |  115 
+ arch/ia64/kernel/perfmon_mckinley.h           |  187 
+ arch/ia64/kernel/perfmon_montecito.h          |  269 -
+ arch/ia64/kernel/process.c                    |   90 
+ arch/ia64/kernel/ptrace.c                     |    9 
+ arch/ia64/kernel/setup.c                      |    3 
+ arch/ia64/kernel/smpboot.c                    |   10 
+ arch/ia64/kernel/sys_ia64.c                   |    7 
+ arch/ia64/lib/Makefile                        |    1 
+ arch/ia64/oprofile/init.c                     |    8 
+ arch/ia64/oprofile/perfmon.c                  |   32 
+ arch/ia64/perfmon/Kconfig                     |   58 
+ arch/ia64/perfmon/Makefile                    |   11 
+ arch/ia64/perfmon/perfmon.c                   |  949 +++
+ arch/ia64/perfmon/perfmon_compat.c            | 1247 ++++
+ arch/ia64/perfmon/perfmon_default_smpl.c      |  268 +
+ arch/ia64/perfmon/perfmon_generic.c           |  148 
+ arch/ia64/perfmon/perfmon_itanium.c           |  229 
+ arch/ia64/perfmon/perfmon_mckinley.c          |  285 +
+ arch/ia64/perfmon/perfmon_montecito.c         |  404 +
+ arch/mips/Kconfig                             |    2 
+ arch/mips/Makefile                            |    6 
+ arch/mips/kernel/process.c                    |    4 
+ arch/mips/kernel/scall32-o32.S                |   13 
+ arch/mips/kernel/scall64-64.S                 |   13 
+ arch/mips/kernel/scall64-n32.S                |   15 
+ arch/mips/kernel/scall64-o32.S                |   12 
+ arch/mips/kernel/signal.c                     |    4 
+ arch/mips/mips-boards/generic/time.c          |    1 
+ arch/mips/perfmon/Kconfig                     |   45 
+ arch/mips/perfmon/Makefile                    |    2 
+ arch/mips/perfmon/perfmon.c                   |  304 +
+ arch/mips/perfmon/perfmon_mips64.c            |  224 
+ arch/powerpc/Kconfig                          |    2 
+ arch/powerpc/Makefile                         |    1 
+ arch/powerpc/kernel/entry_32.S                |    2 
+ arch/powerpc/kernel/entry_64.S                |    4 
+ arch/powerpc/kernel/process.c                 |    6 
+ arch/powerpc/perfmon/Kconfig                  |   57 
+ arch/powerpc/perfmon/Makefile                 |    6 
+ arch/powerpc/perfmon/perfmon.c                |  286 +
+ arch/powerpc/perfmon/perfmon_cell.c           |  901 +++
+ arch/powerpc/perfmon/perfmon_power4.c         |  293 +
+ arch/powerpc/perfmon/perfmon_power5.c         |  292 +
+ arch/powerpc/perfmon/perfmon_power6.c         |  495 +
+ arch/powerpc/perfmon/perfmon_ppc32.c          |  340 +
+ arch/powerpc/platforms/cell/cbe_regs.c        |   27 
+ arch/sparc/kernel/systbls.S                   |   10 
+ arch/sparc64/Kconfig                          |    2 
+ arch/sparc64/Makefile                         |    2 
+ arch/sparc64/kernel/cpu.c                     |   65 
+ arch/sparc64/kernel/entry.S                   |   40 
+ arch/sparc64/kernel/hvapi.c                   |    1 
+ arch/sparc64/kernel/irq.c                     |   64 
+ arch/sparc64/kernel/process.c                 |   26 
+ arch/sparc64/kernel/rtrap.S                   |   55 
+ arch/sparc64/kernel/setup.c                   |    5 
+ arch/sparc64/kernel/signal.c                  |    4 
+ arch/sparc64/kernel/smp.c                     |   22 
+ arch/sparc64/kernel/sys_sparc.c               |  101 
+ arch/sparc64/kernel/systbls.S                 |   17 
+ arch/sparc64/kernel/traps.c                   |  164 
+ arch/sparc64/kernel/ttable.S                  |    2 
+ arch/sparc64/perfmon/Kconfig                  |   16 
+ arch/sparc64/perfmon/Makefile                 |    1 
+ arch/sparc64/perfmon/perfmon.c                |  423 +
+ arch/x86/Kconfig                              |    2 
+ arch/x86/Makefile_32                          |    1 
+ arch/x86/Makefile_64                          |    1 
+ arch/x86/ia32/ia32entry.S                     |   13 
+ arch/x86/kernel/apic_32.c                     |   39 
+ arch/x86/kernel/apic_64.c                     |   42 
+ arch/x86/kernel/cpu/common.c                  |    3 
+ arch/x86/kernel/cpu/mcheck/mce_amd_64.c       |   10 
+ arch/x86/kernel/entry_32.S                    |    2 
+ arch/x86/kernel/entry_64.S                    |   14 
+ arch/x86/kernel/i8259_64.c                    |    4 
+ arch/x86/kernel/io_apic_64.c                  |    2 
+ arch/x86/kernel/process_32.c                  |    8 
+ arch/x86/kernel/process_64.c                  |    9 
+ arch/x86/kernel/setup64.c                     |    3 
+ arch/x86/kernel/signal_32.c                   |    5 
+ arch/x86/kernel/signal_64.c                   |    5 
+ arch/x86/kernel/smpboot_32.c                  |    2 
+ arch/x86/kernel/smpboot_64.c                  |    6 
+ arch/x86/kernel/syscall_table_32.S            |   12 
+ arch/x86/oprofile/nmi_int.c                   |    6 
+ arch/x86/pci/common.c                         |    1 
+ arch/x86/perfmon/Kconfig                      |   72 
+ arch/x86/perfmon/Makefile                     |   12 
+ arch/x86/perfmon/perfmon.c                    | 1456 +++++
+ arch/x86/perfmon/perfmon_amd64.c              |  641 ++
+ arch/x86/perfmon/perfmon_intel_arch.c         |  387 +
+ arch/x86/perfmon/perfmon_intel_core.c         |  248 
+ arch/x86/perfmon/perfmon_p4.c                 |  411 +
+ arch/x86/perfmon/perfmon_p6.c                 |  164 
+ arch/x86/perfmon/perfmon_pebs_core_smpl.c     |  252 
+ arch/x86/perfmon/perfmon_pebs_p4_smpl.c       |  252 
+ drivers/oprofile/oprofile_files.c             |    2 
+ include/asm-ia64/hw_irq.h                     |    2 
+ include/asm-ia64/perfmon.h                    |  518 +
+ include/asm-ia64/perfmon_compat.h             |  168 
+ include/asm-ia64/perfmon_const.h              |   52 
+ include/asm-ia64/perfmon_default_smpl.h       |  123 
+ include/asm-ia64/processor.h                  |   10 
+ include/asm-ia64/system.h                     |   16 
+ include/asm-ia64/thread_info.h                |   11 
+ include/asm-ia64/unistd.h                     |   14 
+ include/asm-mips/perfmon.h                    |  421 +
+ include/asm-mips/perfmon_const.h              |   30 
+ include/asm-mips/system.h                     |    3 
+ include/asm-mips/thread_info.h                |    4 
+ include/asm-mips/unistd.h                     |   46 
+ include/asm-powerpc/cell-pmu.h                |    5 
+ include/asm-powerpc/cell-regs.h               |   30 
+ include/asm-powerpc/perfmon.h                 |  374 +
+ include/asm-powerpc/perfmon_const.h           |   30 
+ include/asm-powerpc/reg.h                     |    1 
+ include/asm-powerpc/systbl.h                  |   12 
+ include/asm-powerpc/thread_info.h             |    4 
+ include/asm-powerpc/unistd.h                  |   14 
+ include/asm-sparc/Kbuild                      |    1 
+ include/asm-sparc/perfctr.h                   |  173 
+ include/asm-sparc/unistd.h                    |   14 
+ include/asm-sparc64/Kbuild                    |    1 
+ include/asm-sparc64/hypervisor.h              |   25 
+ include/asm-sparc64/irq.h                     |    3 
+ include/asm-sparc64/perfctr.h                 |  173 
+ include/asm-sparc64/perfmon.h                 |  298 +
+ include/asm-sparc64/perfmon_const.h           |    7 
+ include/asm-sparc64/system.h                  |   34 
+ include/asm-sparc64/thread_info.h             |   28 
+ include/asm-sparc64/unistd.h                  |   14 
+ include/asm-x86/apic.h                        |  139 
+ include/asm-x86/apic_32.h                     |  127 
+ include/asm-x86/apic_64.h                     |  102 
+ include/asm-x86/apicdef.h                     |  412 +
+ include/asm-x86/apicdef_32.h                  |  375 -
+ include/asm-x86/apicdef_64.h                  |  392 -
+ include/asm-x86/cpufeature_32.h               |    1 
+ include/asm-x86/hw_irq_64.h                   |    3 
+ include/asm-x86/irq_64.h                      |    2 
+ include/asm-x86/mach-default/entry_arch.h     |    4 
+ include/asm-x86/mach-default/irq_vectors.h    |    3 
+ include/asm-x86/msr-index.h                   |    1 
+ include/asm-x86/paravirt.h                    |    4 
+ include/asm-x86/perfmon.h                     |  520 +
+ include/asm-x86/perfmon_const.h               |   30 
+ include/asm-x86/perfmon_pebs_core_smpl.h      |  164 
+ include/asm-x86/perfmon_pebs_p4_smpl.h        |  193 
+ include/asm-x86/thread_info_32.h              |    8 
+ include/asm-x86/thread_info_64.h              |    6 
+ include/asm-x86/unistd_32.h                   |   14 
+ include/asm-x86/unistd_64.h                   |   24 
+ include/linux/pci_ids.h                       |    2 
+ include/linux/perfmon.h                       |  748 ++
+ include/linux/perfmon_dfl_smpl.h              |   78 
+ include/linux/perfmon_fmt.h                   |   88 
+ include/linux/perfmon_pmu.h                   |  178 
+ include/linux/sched.h                         |    4 
+ include/linux/syscalls.h                      |   30 
+ kernel/sched.c                                |    1 
+ kernel/sys_ni.c                               |   13 
+ perfmon/Makefile                              |    8 
+ perfmon/perfmon.c                             | 1801 ++++++
+ perfmon/perfmon_ctxsw.c                       |  371 +
+ perfmon/perfmon_debugfs.c                     |  186 
+ perfmon/perfmon_dfl_smpl.c                    |  294 +
+ perfmon/perfmon_file.c                        |  796 +++
+ perfmon/perfmon_fmt.c                         |  218 
+ perfmon/perfmon_intr.c                        |  577 ++
+ perfmon/perfmon_pmu.c                         |  599 ++
+ perfmon/perfmon_res.c                         |  419 +
+ perfmon/perfmon_rw.c                          |  591 ++
+ perfmon/perfmon_sets.c                        |  773 ++
+ perfmon/perfmon_syscalls.c                    | 1061 ++++
+ perfmon/perfmon_sysfs.c                       |  677 ++
+ 193 files changed, 25276 insertions(+), 9938 deletions(-)
+
+--- /dev/null
++++ b/Documentation/ABI/testing/sysfs-perfmon
+@@ -0,0 +1,82 @@
++What:		/sys/kernel/perfmon
++Date:		Nov 2007
++KernelVersion:	2.6.24
++Contact:	eranian@hpl.hp.com
++
++Description:	provide the configuration interface for the perfmon2 subsystems.
++	        The tree contains information about the detected hardware, current
++		state of the subsystem as well as some configuration parameters.
++	
++		The tree consists of the following entries:
++
++	/sys/kernel/perfmon/debug (read-write):
++
++		Enable perfmon2 debugging output via klogd. Debug messages produced during
++		PMU interrupt handling are not controlled by this entry. The traces a rate-limited
++		to avoid flooding of the console. It is possible to change the throttling
++	        via /proc/sys/kernel/printk_ratelimit
++
++	/sys/kernel/perfmon/debug_ovfl (read-write):
++	
++		Enable perfmon2 overflow interrupt debugging. It is complementary to the
++		previous entry. It is also rate-limited and can be controlled using the
++		same /proc/sys/kernel/printk_ratelimit entry.
++
++
++	/sys/kernel/perfmon/pmc_max_fast_arg (read-only):
++	
++		Number of perfmon2 syscall arguments copied directly onto the
++   		stack (copy_from_user) for pfm_write_pmcs(). Copying to the stack avoids
++		having to allocate a buffer. The unit is the number of pfarg_pmc_t
++		structures.
++
++	/sys/kernel/perfmon/pmd_max_fast_arg (read-only):
++
++		Number of perfmon2 syscall arguments copied directly onto the
++   		stack (copy_from_user) for pfm_write_pmds()/pfm_read_pmds(). Copying
++		to the stack avoids having to allocate a buffer. The unit is the number
++		of pfarg_pmd_t structures.
++
++
++	/sys/kernel/perfmon/reset_stats (write-only):
++
++		Reset the statistics collected by perfmon2. Stats are available
++		per-cpu via debugfs.
++   	
++	/sys/kernel/perfmon/smpl_buffer_mem_cur (read-only):
++	
++		Reports the amount of memory currently dedicated to sampling
++   		buffers by the kernel. The unit is byte.
++
++   	/sys/kernel/perfmon/smpl_buffer_mem_max (read-write):
++
++		Maximum amount of kernel memory usable for sampling buffers. -1 means
++		everything that is available. Unit is byte.
++
++   	/sys/kernel/perfmon/smpl_buffer_mem_cur (read-only):
++
++		Current utilization of kernel memory in bytes.
++
++   	/sys/kernel/perfmon/sys_group (read-write):
++	
++		Users group allowed to create a system-wide perfmon2 context (session).
++   		-1 means any group. This control will be kept until we find a package
++		able to control capabilities via PAM.
++
++	/sys/kernel/perfmon/task_group (read-write):
++	
++		Users group allowed to create a per-thread context (session).
++   		-1 means any group. This control will be kept until we find a
++		package able to control capabilities via PAM.
++
++	/sys/kernel/perfmon/sys_sessions_count (read-only):
++
++		Number of system-wide contexts currently attached to CPUs.
++
++	/sys/kernel/perfmon/task_sessions_count (read-only):
++
++		Number of per-thread contexts currently attached to threads.
++
++   	/sys/kernel/perfmon/version (read-only):
++	
++		Perfmon2 interface revision number.
+--- /dev/null
++++ b/Documentation/ABI/testing/sysfs-perfmon-fmt
+@@ -0,0 +1,18 @@
++What:		/sys/kernel/perfmon/formats
++Date:		2007
++KernelVersion:	2.6.24
++Contact:	eranian@hpl.hp.com
++
++Description:	provide description of available perfmon2 custom sampling buffer formats
++		which are implemented as independent kernel modules. Each formats gets
++		a subdir which a few entries.
++
++		The name of the subdir is the name of the sampling format. The same name
++		must be passed to pfm_create_context() to use the format.
++
++		Each subdir XX contains the following entries:
++
++	/sys/kernel/perfmon/formats/XX/version (read-only):
++
++		Version number of the format in clear text and null terminated.
++
+--- /dev/null
++++ b/Documentation/ABI/testing/sysfs-perfmon-pmu
+@@ -0,0 +1,46 @@
++What:		/sys/kernel/perfmon/pmu
++Date:		Nov 2007
++KernelVersion:	2.6.24
++Contact:	eranian@hpl.hp.com
++
++Description:	provide information about the currently loaded PMU description module.
++		The module contains the mapping of the actual performance counter registers
++		onto the logical PMU exposed by perfmon.  There is at most one PMU description
++		module loaded at any time.
++
++		The sysfs PMU tree provides a description of the mapping for each register.
++		There is one subdir per config and data registers along an entry for the
++		name of the PMU model.
++	
++		The model entry is as follows:
++
++	/sys/kernel/perfmon/pmu_desc/model (read-only):
++
++		Name of the PMU model is clear text and zero terminated.
++   	
++		Then for each logical PMU register, XX, gets a subtree with the following entries:
++
++	/sys/kernel/perfmon/pmu_desc/pm*XX/addr (read-only):
++	
++		The physical address or index of the actual underlying hardware register.
++		On Itanium, it corresponds to the index. But on X86 processor, this is
++		the actual MSR address.
++		
++	/sys/kernel/perfmon/pmu_desc/pm*XX/dfl_val (read-only):
++
++		The default value of the register in hexadecimal.
++
++	/sys/kernel/perfmon/pmu_desc/pm*XX/name (read-only):
++
++		The name of the hardware register.
++
++	/sys/kernel/perfmon/pmu_desc/pm*XX/rsvd_msk (read-only):
++
++		The bitmask of reserved bits, i.e., bits which cannot be changed by
++		applications. When a bit is set, it means the corresponding bit in the
++		actual register is reserved.
++
++	/sys/kernel/perfmon/pmu_desc/pm*XX/width (read-only):
++
++		the width in bits of the registers. This field is only relevant for counter
++		registers.
+--- /dev/null
++++ b/Documentation/ABI/testing/sysfs-perfmon-stats
+@@ -0,0 +1,100 @@
++What:		/sys/devices/system/cpu/cpuXX/perfmon
++Date:		Nov 2007
++KernelVersion:	2.6.24
++Contact:	eranian@hpl.hp.com
++
++Description:	exports internal perfmon2 statistics to user. Mostly used for debugging
++		and performance analysis of the perfmon2 subsystem. Statistics can be
++		reset using the /sys/kernel/perfmon/reset_stats entry.
++	
++		There is one subdir per online CPU. Each subdir contains the following
++		entries:
++
++	/sys/devices/system/cpu/cpuXX/perfmon/ctxsw_count (read-only):
++	
++		Number of PMU context switches (switch-out, switch-in or both).
++
++	/sys/devices/system/cpu/cpuXX/perfmon/ctxsw_ns (read-only):
++
++		Number of nanoseconds spent in the PMU context switch routine.
++		Dividing this number by the value of ctxsw_count, yields average
++		cost of the PMU context switch.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/fmt_handler_calls (read-only):
++
++		Number of calls to the sampling format routine that handles
++		PMU interrupts, i.e., typically the routine that records a sample.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/fmt_handler_ns (read-only):
++
++		Number of nanoseconds spent in the routine that handle PMU
++		interrupt in the sampling format. Dividing this number by
++		the number of calls provided by fmt_handler_calls, yields
++		average time spent in this routine.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_all_count (read-only):
++
++		Number of PMU interrupts received by the kernel.
++
++
++	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_nmi_count (read-only):
++
++		Number of Non Maskeable Interrupts (NMI) received by the kernel
++		for perfmon. This is relevant only on X86 hardware.
++	
++	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_ns (read-only):
++
++		Number of nanoseconds spent in the perfmon2 PMU interrupt
++		handler routine. Dividing this number of ovfl_intr_all_count
++		yields the average time to handle one PMU interrupt.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_regular_count (read-only):
++
++		Number of PMU interrupts which are actually processed by 
++		the perfmon interrupt handler. There may be spurious or replay
++		interrupts.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_replay_count (read-only):
++
++		Number of PMU interrupts which were replayed on context switch in or
++		on event set switching. Interrupts get replayed when they were in
++		flight at the time monitoring had to be stopped.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_intr_spurious_count (read-only):
++
++		Number of PMU interrupts which were dropped because there was no
++		active context (session).
++
++	/sys/devices/system/cpu/cpuXX/perfmon/ovfl_notify_count (read-only):
++
++		Number of user level notification sent. Notification are appended
++		as messages to the context queue. Notifications may be sent on
++		PMU interrupts.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/pfm_restart_count (read-only):
++
++		Number of times pfm_restart() is called.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/reset_pmds_count (read-only):
++
++		Number of times pfm_reset_pmds() is called.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/set_switch_count (read-only):
++
++		Number of event set switches.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/set_switch_ns (read-only):
++
++		Number of nanoseconds spent in the set switching routine.
++		Dividing this number by set_switch_count yields the average
++		cost of switching sets.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/handle_timeout_count (read-only):
++
++		Number of times the pfm_handle_timeout() routine is called. It is
++		used for timeout-based set switching.
++
++	/sys/devices/system/cpu/cpuXX/perfmon/handle_work_count (read-only):
++
++		Number of times pfm_handle_work() is called. The routine handles
++		asynchronous perfmon2 work for per-thread contexts (sessions).
+--- /dev/null
++++ b/Documentation/perfmon2.txt
+@@ -0,0 +1,208 @@
++              The perfmon2 hardware monitoring interface
++              ------------------------------------------
++			     Stephane Eranian
++			<stephane.eranian@hp.com>
++
++I/ Introduction
++
++   The perfmon2 interface provides access to the hardware performance counters of
++   major processors. Nowadays, all processors implement some flavors of performance
++   counters which capture micro-architectural level information such as the number
++   of elapsed cycles, number of cache misses, and so on.
++
++   The interface is implemented as a set of new system calls and a set of config files
++   in /sys.
++
++   It is possible to monitoring a single thread or a CPU. In either mode, applications
++   can count or collect samples. System-wide monitoring is supported by running a
++   monitoring session on each CPU. The interface support event-based sampling where the
++   sampling period is expressed as the number of occurrences of event, instead of just a
++   timeout.  This approach provides a much better granularity and flexibility.
++
++   For performance reason, it is possible to use a kernel-level sampling buffer to minimize
++   the overhead incurred by sampling. The format of the buffer, i.e., what is recorded, how
++   it is recorded, and how it is exported to user-land is controlled by a kernel module called
++   a custom sampling format. The current implementation comes with a default format but
++   it is possible to create additional formats. There is an in-kernel registration
++   interface for formats. Each format is identified by a simple string which a tool
++   can pass when a monitoring session is created.
++
++   The interface also provides support for event set and multiplexing to work around
++   hardware limitations in the number of available counters or in how events can be 
++   combined. Each set defines as many counters as the hardware can support. The kernel
++   then multiplexes the sets. The interface supports time-base switching but also
++   overflow based switching, i.e., after n overflows of designated counters.
++
++   Applications never manipulates the actual performance counter registers. Instead they see
++   a logical Performance Monitoring Unit (PMU) composed of a set of config register (PMC)
++   and a set of data registers (PMD). Note that PMD are not necessarily counters, they
++   can be buffers. The logical PMU is then mapped onto the actual PMU using a mapping
++   table which is implemented as a kernel module. The mapping is chosen once for each
++   new processor. It is visible in /sys/kernel/perfmon/pmu_desc. The kernel module
++   is automatically loaded on first use.
++
++   A monitoring session, or context, is uniquely identified by a file descriptor
++   obtained when the context is created. File sharing semantics apply to access
++   the context inside a process. A context is never inherited across fork. The file
++   descriptor can be used to received counter overflow notifications or when the
++   sampling buffer is full. It is possible to use poll/select on the descriptor 
++   to wait for notifications from multiplex contexts. Similarly, the descriptor
++   supports asynchronous notification via SIGIO.
++
++   Counters are always exported as being 64-bit wide regardless of what the underlying
++   hardware implements.
++
++II/ Kernel compilation
++
++    To enable perfmon2, you need to enable CONFIG_PERFMON
++
++III/ OProfile interactions
++
++    The set of features offered by perfmon2 is rich enough to support migrating
++    Oprofile on top of it. That means that PMU programming and low-level interrupt
++    handling could be done by perfmon2. The Oprofile sampling buffer management code
++    in the kernel as well as how samples are exported to users could remain through
++    the use of a custom sampling buffer format. This is how Oprofile work on Itanium.
++
++    The current interactions with Oprofile are:
++	- on X86: Both subsystems can be compiled into the same kernel. There is enforced
++	          mutual exclusion between the two subsystems. When there is an Oprofile
++		  session, no perfmon2 session can exist and vice-versa. Perfmon2 session
++		  encapsulates both per-thread and system-wide sessions here.
++
++	- On IA-64: Oprofile works on top of perfmon2. Oprofile being a system-wide monitoring
++		    tool, the regular per-thread vs. system-wide session restrictions apply.
++
++	- on PPC: no integration yet. You need to enable/disble one of the two subsystems
++	- on MIPS: no integration yet. You need to enable/disble one of the two subsystems
++
++IV/ User tools
++
++    We have released a simple monitoring tool to demonstrate the feature of the
++    interface. The tool is called pfmon and it comes with a simple helper library
++    called libpfm. The library comes with a set of examples to show how to use the
++    kernel perfmon2 interface. Visit http://perfmon2.sf.net for details.
++
++    There maybe other tools available for perfmon2.
++
++V/ How to program?
++
++   The best way to learn how to program perfmon2, is to take a look at the source
++   code for the examples in libpfm. The source code is available from:
++		http://perfmon2.sf.net
++
++VI/ System calls overview
++
++   The interface is implemented by the following system calls:
++
++   * int pfm_create_context(pfarg_ctx_t *ctx, char *fmt, void *arg, size_t arg_size)
++
++      This function create a perfmon2 context. The type of context is per-thread by
++      default unless PFM_FL_SYSTEM_WIDE is passed in ctx. The sampling format name
++      is passed in fmt. Arguments to the format are passed in arg which is of size
++      arg_size. Upon successful return, the file descriptor identifying the context
++      is returned.
++
++   * int pfm_write_pmds(int fd, pfarg_pmd_t *pmds, int n)
++   
++      This function is used to program the PMD registers. It is possible to pass
++      vectors of PMDs.
++
++   * int pfm_write_pmcs(int fd, pfarg_pmc_t *pmds, int n)
++   
++      This function is used to program the PMC registers. It is possible to pass
++      vectors of PMDs.
++
++   * int pfm_read_pmds(int fd, pfarg_pmd_t *pmds, int n)
++   
++      This function is used to read the PMD registers. It is possible to pass
++      vectors of PMDs.
++
++   * int pfm_load_context(int fd, pfarg_load_t *load)
++   
++      This function is used to attach the context to a thread or CPU.
++      Thread means kernel-visible thread (NPTL). The thread identification
++      as obtained by gettid must be passed to load->load_target.
++
++      To operate on another thread (not self), it is mandatory that the thread
++      be stopped via ptrace().
++
++      To attach to a CPU, the CPU number must be specified in load->load_target
++      AND the call must be issued on that CPU. To monitor a CPU, a thread MUST
++      be pinned on that CPU.
++
++      Until the context is attached, the actual counters are not accessed.
++
++   * int pfm_unload_context(int fd)
++
++     The context is detached for the thread or CPU is was attached to.
++     As a consequence monitoring is stopped.
++
++     When monitoring another thread, the thread MUST be stopped via ptrace()
++     for this function to succeed.
++
++   * int pfm_start(int fd, pfarg_start_t *st)
++
++     Start monitoring. The context must be attached for this function to succeed.
++     Optionally, it is possible to specify the event set on which to start using the
++     st argument, otherwise just pass NULL.
++
++     When monitoring another thread, the thread MUST be stopped via ptrace()
++     for this function to succeed.
++
++   * int pfm_stop(int fd)
++
++     Stop monitoring. The context must be attached for this function to succeed.
++
++     When monitoring another thread, the thread MUST be stopped via ptrace()
++     for this function to succeed.
++
++
++   * int pfm_create_evtsets(int fd, pfarg_setdesc_t *sets, int n)
++
++     This function is used to create or change event sets. By default set 0 exists.
++     It is possible to create/change multiple sets in one call.
++
++     The context must be detached for this call to succeed.
++
++     Sets are identified by a 16-bit integer. They are sorted based on this
++     set and switching occurs in a round-robin fashion.
++
++   * int pfm_delete_evtsets(int fd, pfarg_setdesc_t *sets, int n)
++   
++     Delete event sets. The context must be detached for this call to succeed.
++
++
++   * int pfm_getinfo_evtsets(int fd, pfarg_setinfo_t *sets, int n)
++
++     Retrieve information about event sets. In particular it is possible
++     to get the number of activation of a set. It is possible to retrieve
++     information about multiple sets in one call.
++
++
++   * int pfm_restart(int fd)
++
++     Indicate to the kernel that the application is done processing an overflow
++     notification. A consequence of this call could be that monitoring resumes.
++
++   * int read(fd, pfm_msg_t *msg, sizeof(pfm_msg_t))
++
++   the regular read() system call can be used with the context file descriptor to
++   receive overflow notification messages. Non-blocking read() is supported.
++
++   Each message carry information about the overflow such as which counter overflowed
++   and where the program was (interrupted instruction pointer).
++
++   * int close(int fd)
++
++   To destroy a context, the regular close() system call is used.
++
++
++VII/ /sys interface overview
++
++   Refer to Documentation/ABI/testing/sysfs-perfmon-* for a detailed description
++   of the sysfs interface of perfmon2.
++
++VIII/ Documentation
++
++   Visit http://perfmon2.sf.net
+--- a/MAINTAINERS
++++ b/MAINTAINERS
+@@ -2974,6 +2974,14 @@ M:	nagar@watson.ibm.com
+ L:	linux-kernel@vger.kernel.org
+ S:	Maintained
+ 
++PERFMON SUBSYSTEM
++P:	Stephane Eranian
++M:	eranian@hpl.hp.com
++L:	perfmon@linux.hpl.hp.com
++W:	http://perfmon2.sf.net
++T:	git kernel.org:/pub/scm/linux/kernel/git/eranian/linux-2.6
++S:	Maintained
++
+ PERSONALITY HANDLING
+ P:	Christoph Hellwig
+ M:	hch@infradead.org
+--- a/Makefile
++++ b/Makefile
+@@ -597,7 +597,7 @@ export mod_strip_cmd
+ 
+ 
+ ifeq ($(KBUILD_EXTMOD),)
+-core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/
++core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/ perfmon/
+ 
+ vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
+ 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
+--- a/arch/ia64/Kconfig
++++ b/arch/ia64/Kconfig
+@@ -430,14 +430,6 @@ config COMPAT_FOR_U64_ALIGNMENT
+ config IA64_MCA_RECOVERY
+ 	tristate "MCA recovery from errors other than TLB."
+ 
+-config PERFMON
+-	bool "Performance monitor support"
+-	help
+-	  Selects whether support for the IA-64 performance monitor hardware
+-	  is included in the kernel.  This makes some kernel data-structures a
+-	  little bigger and slows down execution a bit, but it is generally
+-	  a good idea to turn this on.  If you're unsure, say Y.
+-
+ config IA64_PALINFO
+ 	tristate "/proc/pal support"
+ 	help
+@@ -509,6 +501,8 @@ source "drivers/firmware/Kconfig"
+ 
+ source "fs/Kconfig.binfmt"
+ 
++source "arch/ia64/perfmon/Kconfig"
++
+ endmenu
+ 
+ menu "Power management and ACPI"
+--- a/arch/ia64/Makefile
++++ b/arch/ia64/Makefile
+@@ -55,6 +55,7 @@ core-$(CONFIG_IA64_GENERIC) 	+= arch/ia6
+ core-$(CONFIG_IA64_HP_ZX1)	+= arch/ia64/dig/
+ core-$(CONFIG_IA64_HP_ZX1_SWIOTLB) += arch/ia64/dig/
+ core-$(CONFIG_IA64_SGI_SN2)	+= arch/ia64/sn/
++core-$(CONFIG_PERFMON)		+= arch/ia64/perfmon/
+ 
+ drivers-$(CONFIG_PCI)		+= arch/ia64/pci/
+ drivers-$(CONFIG_IA64_HP_SIM)	+= arch/ia64/hp/sim/
+--- a/arch/ia64/defconfig
++++ b/arch/ia64/defconfig
+@@ -163,7 +163,6 @@ CONFIG_HAVE_ARCH_NODEDATA_EXTENSION=y
+ CONFIG_IA32_SUPPORT=y
+ CONFIG_COMPAT=y
+ CONFIG_IA64_MCA_RECOVERY=y
+-CONFIG_PERFMON=y
+ CONFIG_IA64_PALINFO=y
+ # CONFIG_IA64_MC_ERR_INJECT is not set
+ CONFIG_SGI_SN=y
+@@ -186,6 +185,16 @@ CONFIG_BINFMT_ELF=y
+ CONFIG_BINFMT_MISC=m
+ 
+ #
++# Hardware Performance Monitoring support
++#
++CONFIG_PERFMON=y
++CONFIG_IA64_PERFMON_COMPAT=y
++CONFIG_IA64_PERFMON_GENERIC=m
++CONFIG_IA64_PERFMON_ITANIUM=y
++CONFIG_IA64_PERFMON_MCKINLEY=y
++CONFIG_IA64_PERFMON_MONTECITO=y
++
++#
+ # Power management and ACPI
+ #
+ CONFIG_PM=y
+--- a/arch/ia64/kernel/Makefile
++++ b/arch/ia64/kernel/Makefile
+@@ -5,7 +5,7 @@
+ extra-y	:= head.o init_task.o vmlinux.lds
+ 
+ obj-y := acpi.o entry.o efi.o efi_stub.o gate-data.o fsys.o ia64_ksyms.o irq.o irq_ia64.o	\
+-	 irq_lsapic.o ivt.o machvec.o pal.o patch.o process.o perfmon.o ptrace.o sal.o		\
++	 irq_lsapic.o ivt.o machvec.o pal.o patch.o process.o ptrace.o sal.o		\
+ 	 salinfo.o semaphore.o setup.o signal.o sys_ia64.o time.o traps.o unaligned.o \
+ 	 unwind.o mca.o mca_asm.o topology.o
+ 
+@@ -23,7 +23,6 @@ obj-$(CONFIG_IOSAPIC)		+= iosapic.o
+ obj-$(CONFIG_MODULES)		+= module.o
+ obj-$(CONFIG_SMP)		+= smp.o smpboot.o
+ obj-$(CONFIG_NUMA)		+= numa.o
+-obj-$(CONFIG_PERFMON)		+= perfmon_default_smpl.o
+ obj-$(CONFIG_IA64_CYCLONE)	+= cyclone.o
+ obj-$(CONFIG_CPU_FREQ)		+= cpufreq/
+ obj-$(CONFIG_IA64_MCA_RECOVERY)	+= mca_recovery.o
+--- a/arch/ia64/kernel/entry.S
++++ b/arch/ia64/kernel/entry.S
+@@ -1588,5 +1588,17 @@ sys_call_table:
+ 	data8 sys_signalfd
+ 	data8 sys_timerfd
+ 	data8 sys_eventfd
++	data8 sys_pfm_create_context		// 1310
++	data8 sys_pfm_write_pmcs
++	data8 sys_pfm_write_pmds
++	data8 sys_pfm_read_pmds
++	data8 sys_pfm_load_context
++	data8 sys_pfm_start			// 1315
++	data8 sys_pfm_stop
++	data8 sys_pfm_restart
++	data8 sys_pfm_create_evtsets
++	data8 sys_pfm_getinfo_evtsets
++	data8 sys_pfm_delete_evtsets		// 1320
++	data8 sys_pfm_unload_context
+ 
+ 	.org sys_call_table + 8*NR_syscalls	// guard against failures to increase NR_syscalls
+--- a/arch/ia64/kernel/irq_ia64.c
++++ b/arch/ia64/kernel/irq_ia64.c
+@@ -40,10 +40,6 @@
+ #include <asm/system.h>
+ #include <asm/tlbflush.h>
+ 
+-#ifdef CONFIG_PERFMON
+-# include <asm/perfmon.h>
+-#endif
+-
+ #define IRQ_DEBUG	0
+ 
+ #define IRQ_VECTOR_UNASSIGNED	(0)
+@@ -579,9 +575,6 @@ init_IRQ (void)
+ 	register_percpu_irq(IA64_IPI_RESCHEDULE, &resched_irqaction);
+ 	register_percpu_irq(IA64_IPI_LOCAL_TLB_FLUSH, &tlb_irqaction);
+ #endif
+-#ifdef CONFIG_PERFMON
+-	pfm_init_percpu();
+-#endif
+ 	platform_irq_init();
+ }
+ 
+--- a/arch/ia64/kernel/perfmon.c
++++ /dev/null
+@@ -1,6875 +0,0 @@
+-/*
+- * This file implements the perfmon-2 subsystem which is used
+- * to program the IA-64 Performance Monitoring Unit (PMU).
+- *
+- * The initial version of perfmon.c was written by
+- * Ganesh Venkitachalam, IBM Corp.
+- *
+- * Then it was modified for perfmon-1.x by Stephane Eranian and
+- * David Mosberger, Hewlett Packard Co.
+- *
+- * Version Perfmon-2.x is a rewrite of perfmon-1.x
+- * by Stephane Eranian, Hewlett Packard Co.
+- *
+- * Copyright (C) 1999-2005  Hewlett Packard Co
+- *               Stephane Eranian <eranian@hpl.hp.com>
+- *               David Mosberger-Tang <davidm@hpl.hp.com>
+- *
+- * More information about perfmon available at:
+- * 	http://www.hpl.hp.com/research/linux/perfmon
+- */
+-
+-#include <linux/module.h>
+-#include <linux/kernel.h>
+-#include <linux/sched.h>
+-#include <linux/interrupt.h>
+-#include <linux/proc_fs.h>
+-#include <linux/seq_file.h>
+-#include <linux/init.h>
+-#include <linux/vmalloc.h>
+-#include <linux/mm.h>
+-#include <linux/sysctl.h>
+-#include <linux/list.h>
+-#include <linux/file.h>
+-#include <linux/poll.h>
+-#include <linux/vfs.h>
+-#include <linux/smp.h>
+-#include <linux/pagemap.h>
+-#include <linux/mount.h>
+-#include <linux/bitops.h>
+-#include <linux/capability.h>
+-#include <linux/rcupdate.h>
+-#include <linux/completion.h>
+-
+-#include <asm/errno.h>
+-#include <asm/intrinsics.h>
+-#include <asm/page.h>
+-#include <asm/perfmon.h>
+-#include <asm/processor.h>
+-#include <asm/signal.h>
+-#include <asm/system.h>
+-#include <asm/uaccess.h>
+-#include <asm/delay.h>
+-
+-#ifdef CONFIG_PERFMON
+-/*
+- * perfmon context state
+- */
+-#define PFM_CTX_UNLOADED	1	/* context is not loaded onto any task */
+-#define PFM_CTX_LOADED		2	/* context is loaded onto a task */
+-#define PFM_CTX_MASKED		3	/* context is loaded but monitoring is masked due to overflow */
+-#define PFM_CTX_ZOMBIE		4	/* owner of the context is closing it */
+-
+-#define PFM_INVALID_ACTIVATION	(~0UL)
+-
+-#define PFM_NUM_PMC_REGS	64	/* PMC save area for ctxsw */
+-#define PFM_NUM_PMD_REGS	64	/* PMD save area for ctxsw */
+-
+-/*
+- * depth of message queue
+- */
+-#define PFM_MAX_MSGS		32
+-#define PFM_CTXQ_EMPTY(g)	((g)->ctx_msgq_head == (g)->ctx_msgq_tail)
+-
+-/*
+- * type of a PMU register (bitmask).
+- * bitmask structure:
+- * 	bit0   : register implemented
+- * 	bit1   : end marker
+- * 	bit2-3 : reserved
+- * 	bit4   : pmc has pmc.pm
+- * 	bit5   : pmc controls a counter (has pmc.oi), pmd is used as counter
+- * 	bit6-7 : register type
+- * 	bit8-31: reserved
+- */
+-#define PFM_REG_NOTIMPL		0x0 /* not implemented at all */
+-#define PFM_REG_IMPL		0x1 /* register implemented */
+-#define PFM_REG_END		0x2 /* end marker */
+-#define PFM_REG_MONITOR		(0x1<<4|PFM_REG_IMPL) /* a PMC with a pmc.pm field only */
+-#define PFM_REG_COUNTING	(0x2<<4|PFM_REG_MONITOR) /* a monitor + pmc.oi+ PMD used as a counter */
+-#define PFM_REG_CONTROL		(0x4<<4|PFM_REG_IMPL) /* PMU control register */
+-#define	PFM_REG_CONFIG		(0x8<<4|PFM_REG_IMPL) /* configuration register */
+-#define PFM_REG_BUFFER	 	(0xc<<4|PFM_REG_IMPL) /* PMD used as buffer */
+-
+-#define PMC_IS_LAST(i)	(pmu_conf->pmc_desc[i].type & PFM_REG_END)
+-#define PMD_IS_LAST(i)	(pmu_conf->pmd_desc[i].type & PFM_REG_END)
+-
+-#define PMC_OVFL_NOTIFY(ctx, i)	((ctx)->ctx_pmds[i].flags &  PFM_REGFL_OVFL_NOTIFY)
+-
+-/* i assumed unsigned */
+-#define PMC_IS_IMPL(i)	  (i< PMU_MAX_PMCS && (pmu_conf->pmc_desc[i].type & PFM_REG_IMPL))
+-#define PMD_IS_IMPL(i)	  (i< PMU_MAX_PMDS && (pmu_conf->pmd_desc[i].type & PFM_REG_IMPL))
+-
+-/* XXX: these assume that register i is implemented */
+-#define PMD_IS_COUNTING(i) ((pmu_conf->pmd_desc[i].type & PFM_REG_COUNTING) == PFM_REG_COUNTING)
+-#define PMC_IS_COUNTING(i) ((pmu_conf->pmc_desc[i].type & PFM_REG_COUNTING) == PFM_REG_COUNTING)
+-#define PMC_IS_MONITOR(i)  ((pmu_conf->pmc_desc[i].type & PFM_REG_MONITOR)  == PFM_REG_MONITOR)
+-#define PMC_IS_CONTROL(i)  ((pmu_conf->pmc_desc[i].type & PFM_REG_CONTROL)  == PFM_REG_CONTROL)
+-
+-#define PMC_DFL_VAL(i)     pmu_conf->pmc_desc[i].default_value
+-#define PMC_RSVD_MASK(i)   pmu_conf->pmc_desc[i].reserved_mask
+-#define PMD_PMD_DEP(i)	   pmu_conf->pmd_desc[i].dep_pmd[0]
+-#define PMC_PMD_DEP(i)	   pmu_conf->pmc_desc[i].dep_pmd[0]
+-
+-#define PFM_NUM_IBRS	  IA64_NUM_DBG_REGS
+-#define PFM_NUM_DBRS	  IA64_NUM_DBG_REGS
+-
+-#define CTX_OVFL_NOBLOCK(c)	((c)->ctx_fl_block == 0)
+-#define CTX_HAS_SMPL(c)		((c)->ctx_fl_is_sampling)
+-#define PFM_CTX_TASK(h)		(h)->ctx_task
+-
+-#define PMU_PMC_OI		5 /* position of pmc.oi bit */
+-
+-/* XXX: does not support more than 64 PMDs */
+-#define CTX_USED_PMD(ctx, mask) (ctx)->ctx_used_pmds[0] |= (mask)
+-#define CTX_IS_USED_PMD(ctx, c) (((ctx)->ctx_used_pmds[0] & (1UL << (c))) != 0UL)
+-
+-#define CTX_USED_MONITOR(ctx, mask) (ctx)->ctx_used_monitors[0] |= (mask)
+-
+-#define CTX_USED_IBR(ctx,n) 	(ctx)->ctx_used_ibrs[(n)>>6] |= 1UL<< ((n) % 64)
+-#define CTX_USED_DBR(ctx,n) 	(ctx)->ctx_used_dbrs[(n)>>6] |= 1UL<< ((n) % 64)
+-#define CTX_USES_DBREGS(ctx)	(((pfm_context_t *)(ctx))->ctx_fl_using_dbreg==1)
+-#define PFM_CODE_RR	0	/* requesting code range restriction */
+-#define PFM_DATA_RR	1	/* requestion data range restriction */
+-
+-#define PFM_CPUINFO_CLEAR(v)	pfm_get_cpu_var(pfm_syst_info) &= ~(v)
+-#define PFM_CPUINFO_SET(v)	pfm_get_cpu_var(pfm_syst_info) |= (v)
+-#define PFM_CPUINFO_GET()	pfm_get_cpu_var(pfm_syst_info)
+-
+-#define RDEP(x)	(1UL<<(x))
+-
+-/*
+- * context protection macros
+- * in SMP:
+- * 	- we need to protect against CPU concurrency (spin_lock)
+- * 	- we need to protect against PMU overflow interrupts (local_irq_disable)
+- * in UP:
+- * 	- we need to protect against PMU overflow interrupts (local_irq_disable)
+- *
+- * spin_lock_irqsave()/spin_unlock_irqrestore():
+- * 	in SMP: local_irq_disable + spin_lock
+- * 	in UP : local_irq_disable
+- *
+- * spin_lock()/spin_lock():
+- * 	in UP : removed automatically
+- * 	in SMP: protect against context accesses from other CPU. interrupts
+- * 	        are not masked. This is useful for the PMU interrupt handler
+- * 	        because we know we will not get PMU concurrency in that code.
+- */
+-#define PROTECT_CTX(c, f) \
+-	do {  \
+-		DPRINT(("spinlock_irq_save ctx %p by [%d]\n", c, task_pid_nr(current))); \
+-		spin_lock_irqsave(&(c)->ctx_lock, f); \
+-		DPRINT(("spinlocked ctx %p  by [%d]\n", c, task_pid_nr(current))); \
+-	} while(0)
+-
+-#define UNPROTECT_CTX(c, f) \
+-	do { \
+-		DPRINT(("spinlock_irq_restore ctx %p by [%d]\n", c, task_pid_nr(current))); \
+-		spin_unlock_irqrestore(&(c)->ctx_lock, f); \
+-	} while(0)
+-
+-#define PROTECT_CTX_NOPRINT(c, f) \
+-	do {  \
+-		spin_lock_irqsave(&(c)->ctx_lock, f); \
+-	} while(0)
+-
+-
+-#define UNPROTECT_CTX_NOPRINT(c, f) \
+-	do { \
+-		spin_unlock_irqrestore(&(c)->ctx_lock, f); \
+-	} while(0)
+-
+-
+-#define PROTECT_CTX_NOIRQ(c) \
+-	do {  \
+-		spin_lock(&(c)->ctx_lock); \
+-	} while(0)
+-
+-#define UNPROTECT_CTX_NOIRQ(c) \
+-	do { \
+-		spin_unlock(&(c)->ctx_lock); \
+-	} while(0)
+-
+-
+-#ifdef CONFIG_SMP
+-
+-#define GET_ACTIVATION()	pfm_get_cpu_var(pmu_activation_number)
+-#define INC_ACTIVATION()	pfm_get_cpu_var(pmu_activation_number)++
+-#define SET_ACTIVATION(c)	(c)->ctx_last_activation = GET_ACTIVATION()
+-
+-#else /* !CONFIG_SMP */
+-#define SET_ACTIVATION(t) 	do {} while(0)
+-#define GET_ACTIVATION(t) 	do {} while(0)
+-#define INC_ACTIVATION(t) 	do {} while(0)
+-#endif /* CONFIG_SMP */
+-
+-#define SET_PMU_OWNER(t, c)	do { pfm_get_cpu_var(pmu_owner) = (t); pfm_get_cpu_var(pmu_ctx) = (c); } while(0)
+-#define GET_PMU_OWNER()		pfm_get_cpu_var(pmu_owner)
+-#define GET_PMU_CTX()		pfm_get_cpu_var(pmu_ctx)
+-
+-#define LOCK_PFS(g)	    	spin_lock_irqsave(&pfm_sessions.pfs_lock, g)
+-#define UNLOCK_PFS(g)	    	spin_unlock_irqrestore(&pfm_sessions.pfs_lock, g)
+-
+-#define PFM_REG_RETFLAG_SET(flags, val)	do { flags &= ~PFM_REG_RETFL_MASK; flags |= (val); } while(0)
+-
+-/*
+- * cmp0 must be the value of pmc0
+- */
+-#define PMC0_HAS_OVFL(cmp0)  (cmp0 & ~0x1UL)
+-
+-#define PFMFS_MAGIC 0xa0b4d889
+-
+-/*
+- * debugging
+- */
+-#define PFM_DEBUGGING 1
+-#ifdef PFM_DEBUGGING
+-#define DPRINT(a) \
+-	do { \
+-		if (unlikely(pfm_sysctl.debug >0)) { printk("%s.%d: CPU%d [%d] ", __FUNCTION__, __LINE__, smp_processor_id(), task_pid_nr(current)); printk a; } \
+-	} while (0)
+-
+-#define DPRINT_ovfl(a) \
+-	do { \
+-		if (unlikely(pfm_sysctl.debug > 0 && pfm_sysctl.debug_ovfl >0)) { printk("%s.%d: CPU%d [%d] ", __FUNCTION__, __LINE__, smp_processor_id(), task_pid_nr(current)); printk a; } \
+-	} while (0)
+-#endif
+-
+-/*
+- * 64-bit software counter structure
+- *
+- * the next_reset_type is applied to the next call to pfm_reset_regs()
+- */
+-typedef struct {
+-	unsigned long	val;		/* virtual 64bit counter value */
+-	unsigned long	lval;		/* last reset value */
+-	unsigned long	long_reset;	/* reset value on sampling overflow */
+-	unsigned long	short_reset;    /* reset value on overflow */
+-	unsigned long	reset_pmds[4];  /* which other pmds to reset when this counter overflows */
+-	unsigned long	smpl_pmds[4];   /* which pmds are accessed when counter overflow */
+-	unsigned long	seed;		/* seed for random-number generator */
+-	unsigned long	mask;		/* mask for random-number generator */
+-	unsigned int 	flags;		/* notify/do not notify */
+-	unsigned long	eventid;	/* overflow event identifier */
+-} pfm_counter_t;
+-
+-/*
+- * context flags
+- */
+-typedef struct {
+-	unsigned int block:1;		/* when 1, task will blocked on user notifications */
+-	unsigned int system:1;		/* do system wide monitoring */
+-	unsigned int using_dbreg:1;	/* using range restrictions (debug registers) */
+-	unsigned int is_sampling:1;	/* true if using a custom format */
+-	unsigned int excl_idle:1;	/* exclude idle task in system wide session */
+-	unsigned int going_zombie:1;	/* context is zombie (MASKED+blocking) */
+-	unsigned int trap_reason:2;	/* reason for going into pfm_handle_work() */
+-	unsigned int no_msg:1;		/* no message sent on overflow */
+-	unsigned int can_restart:1;	/* allowed to issue a PFM_RESTART */
+-	unsigned int reserved:22;
+-} pfm_context_flags_t;
+-
+-#define PFM_TRAP_REASON_NONE		0x0	/* default value */
+-#define PFM_TRAP_REASON_BLOCK		0x1	/* we need to block on overflow */
+-#define PFM_TRAP_REASON_RESET		0x2	/* we need to reset PMDs */
+-
+-
+-/*
+- * perfmon context: encapsulates all the state of a monitoring session
+- */
+-
+-typedef struct pfm_context {
+-	spinlock_t		ctx_lock;		/* context protection */
+-
+-	pfm_context_flags_t	ctx_flags;		/* bitmask of flags  (block reason incl.) */
+-	unsigned int		ctx_state;		/* state: active/inactive (no bitfield) */
+-
+-	struct task_struct 	*ctx_task;		/* task to which context is attached */
+-
+-	unsigned long		ctx_ovfl_regs[4];	/* which registers overflowed (notification) */
+-
+-	struct completion	ctx_restart_done;  	/* use for blocking notification mode */
+-
+-	unsigned long		ctx_used_pmds[4];	/* bitmask of PMD used            */
+-	unsigned long		ctx_all_pmds[4];	/* bitmask of all accessible PMDs */
+-	unsigned long		ctx_reload_pmds[4];	/* bitmask of force reload PMD on ctxsw in */
+-
+-	unsigned long		ctx_all_pmcs[4];	/* bitmask of all accessible PMCs */
+-	unsigned long		ctx_reload_pmcs[4];	/* bitmask of force reload PMC on ctxsw in */
+-	unsigned long		ctx_used_monitors[4];	/* bitmask of monitor PMC being used */
+-
+-	unsigned long		ctx_pmcs[PFM_NUM_PMC_REGS];	/*  saved copies of PMC values */
+-
+-	unsigned int		ctx_used_ibrs[1];		/* bitmask of used IBR (speedup ctxsw in) */
+-	unsigned int		ctx_used_dbrs[1];		/* bitmask of used DBR (speedup ctxsw in) */
+-	unsigned long		ctx_dbrs[IA64_NUM_DBG_REGS];	/* DBR values (cache) when not loaded */
+-	unsigned long		ctx_ibrs[IA64_NUM_DBG_REGS];	/* IBR values (cache) when not loaded */
+-
+-	pfm_counter_t		ctx_pmds[PFM_NUM_PMD_REGS]; /* software state for PMDS */
+-
+-	unsigned long		th_pmcs[PFM_NUM_PMC_REGS];	/* PMC thread save state */
+-	unsigned long		th_pmds[PFM_NUM_PMD_REGS];	/* PMD thread save state */
+-
+-	u64			ctx_saved_psr_up;	/* only contains psr.up value */
+-
+-	unsigned long		ctx_last_activation;	/* context last activation number for last_cpu */
+-	unsigned int		ctx_last_cpu;		/* CPU id of current or last CPU used (SMP only) */
+-	unsigned int		ctx_cpu;		/* cpu to which perfmon is applied (system wide) */
+-
+-	int			ctx_fd;			/* file descriptor used my this context */
+-	pfm_ovfl_arg_t		ctx_ovfl_arg;		/* argument to custom buffer format handler */
+-
+-	pfm_buffer_fmt_t	*ctx_buf_fmt;		/* buffer format callbacks */
+-	void			*ctx_smpl_hdr;		/* points to sampling buffer header kernel vaddr */
+-	unsigned long		ctx_smpl_size;		/* size of sampling buffer */
+-	void			*ctx_smpl_vaddr;	/* user level virtual address of smpl buffer */
+-
+-	wait_queue_head_t 	ctx_msgq_wait;
+-	pfm_msg_t		ctx_msgq[PFM_MAX_MSGS];
+-	int			ctx_msgq_head;
+-	int			ctx_msgq_tail;
+-	struct fasync_struct	*ctx_async_queue;
+-
+-	wait_queue_head_t 	ctx_zombieq;		/* termination cleanup wait queue */
+-} pfm_context_t;
+-
+-/*
+- * magic number used to verify that structure is really
+- * a perfmon context
+- */
+-#define PFM_IS_FILE(f)		((f)->f_op == &pfm_file_ops)
+-
+-#define PFM_GET_CTX(t)	 	((pfm_context_t *)(t)->thread.pfm_context)
+-
+-#ifdef CONFIG_SMP
+-#define SET_LAST_CPU(ctx, v)	(ctx)->ctx_last_cpu = (v)
+-#define GET_LAST_CPU(ctx)	(ctx)->ctx_last_cpu
+-#else
+-#define SET_LAST_CPU(ctx, v)	do {} while(0)
+-#define GET_LAST_CPU(ctx)	do {} while(0)
+-#endif
+-
+-
+-#define ctx_fl_block		ctx_flags.block
+-#define ctx_fl_system		ctx_flags.system
+-#define ctx_fl_using_dbreg	ctx_flags.using_dbreg
+-#define ctx_fl_is_sampling	ctx_flags.is_sampling
+-#define ctx_fl_excl_idle	ctx_flags.excl_idle
+-#define ctx_fl_going_zombie	ctx_flags.going_zombie
+-#define ctx_fl_trap_reason	ctx_flags.trap_reason
+-#define ctx_fl_no_msg		ctx_flags.no_msg
+-#define ctx_fl_can_restart	ctx_flags.can_restart
+-
+-#define PFM_SET_WORK_PENDING(t, v)	do { (t)->thread.pfm_needs_checking = v; } while(0);
+-#define PFM_GET_WORK_PENDING(t)		(t)->thread.pfm_needs_checking
+-
+-/*
+- * global information about all sessions
+- * mostly used to synchronize between system wide and per-process
+- */
+-typedef struct {
+-	spinlock_t		pfs_lock;		   /* lock the structure */
+-
+-	unsigned int		pfs_task_sessions;	   /* number of per task sessions */
+-	unsigned int		pfs_sys_sessions;	   /* number of per system wide sessions */
+-	unsigned int		pfs_sys_use_dbregs;	   /* incremented when a system wide session uses debug regs */
+-	unsigned int		pfs_ptrace_use_dbregs;	   /* incremented when a process uses debug regs */
+-	struct task_struct	*pfs_sys_session[NR_CPUS]; /* point to task owning a system-wide session */
+-} pfm_session_t;
+-
+-/*
+- * information about a PMC or PMD.
+- * dep_pmd[]: a bitmask of dependent PMD registers
+- * dep_pmc[]: a bitmask of dependent PMC registers
+- */
+-typedef int (*pfm_reg_check_t)(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
+-typedef struct {
+-	unsigned int		type;
+-	int			pm_pos;
+-	unsigned long		default_value;	/* power-on default value */
+-	unsigned long		reserved_mask;	/* bitmask of reserved bits */
+-	pfm_reg_check_t		read_check;
+-	pfm_reg_check_t		write_check;
+-	unsigned long		dep_pmd[4];
+-	unsigned long		dep_pmc[4];
+-} pfm_reg_desc_t;
+-
+-/* assume cnum is a valid monitor */
+-#define PMC_PM(cnum, val)	(((val) >> (pmu_conf->pmc_desc[cnum].pm_pos)) & 0x1)
+-
+-/*
+- * This structure is initialized at boot time and contains
+- * a description of the PMU main characteristics.
+- *
+- * If the probe function is defined, detection is based
+- * on its return value: 
+- * 	- 0 means recognized PMU
+- * 	- anything else means not supported
+- * When the probe function is not defined, then the pmu_family field
+- * is used and it must match the host CPU family such that:
+- * 	- cpu->family & config->pmu_family != 0
+- */
+-typedef struct {
+-	unsigned long  ovfl_val;	/* overflow value for counters */
+-
+-	pfm_reg_desc_t *pmc_desc;	/* detailed PMC register dependencies descriptions */
+-	pfm_reg_desc_t *pmd_desc;	/* detailed PMD register dependencies descriptions */
+-
+-	unsigned int   num_pmcs;	/* number of PMCS: computed at init time */
+-	unsigned int   num_pmds;	/* number of PMDS: computed at init time */
+-	unsigned long  impl_pmcs[4];	/* bitmask of implemented PMCS */
+-	unsigned long  impl_pmds[4];	/* bitmask of implemented PMDS */
+-
+-	char	      *pmu_name;	/* PMU family name */
+-	unsigned int  pmu_family;	/* cpuid family pattern used to identify pmu */
+-	unsigned int  flags;		/* pmu specific flags */
+-	unsigned int  num_ibrs;		/* number of IBRS: computed at init time */
+-	unsigned int  num_dbrs;		/* number of DBRS: computed at init time */
+-	unsigned int  num_counters;	/* PMC/PMD counting pairs : computed at init time */
+-	int           (*probe)(void);   /* customized probe routine */
+-	unsigned int  use_rr_dbregs:1;	/* set if debug registers used for range restriction */
+-} pmu_config_t;
+-/*
+- * PMU specific flags
+- */
+-#define PFM_PMU_IRQ_RESEND	1	/* PMU needs explicit IRQ resend */
+-
+-/*
+- * debug register related type definitions
+- */
+-typedef struct {
+-	unsigned long ibr_mask:56;
+-	unsigned long ibr_plm:4;
+-	unsigned long ibr_ig:3;
+-	unsigned long ibr_x:1;
+-} ibr_mask_reg_t;
+-
+-typedef struct {
+-	unsigned long dbr_mask:56;
+-	unsigned long dbr_plm:4;
+-	unsigned long dbr_ig:2;
+-	unsigned long dbr_w:1;
+-	unsigned long dbr_r:1;
+-} dbr_mask_reg_t;
+-
+-typedef union {
+-	unsigned long  val;
+-	ibr_mask_reg_t ibr;
+-	dbr_mask_reg_t dbr;
+-} dbreg_t;
+-
+-
+-/*
+- * perfmon command descriptions
+- */
+-typedef struct {
+-	int		(*cmd_func)(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);
+-	char		*cmd_name;
+-	int		cmd_flags;
+-	unsigned int	cmd_narg;
+-	size_t		cmd_argsize;
+-	int		(*cmd_getsize)(void *arg, size_t *sz);
+-} pfm_cmd_desc_t;
+-
+-#define PFM_CMD_FD		0x01	/* command requires a file descriptor */
+-#define PFM_CMD_ARG_READ	0x02	/* command must read argument(s) */
+-#define PFM_CMD_ARG_RW		0x04	/* command must read/write argument(s) */
+-#define PFM_CMD_STOP		0x08	/* command does not work on zombie context */
+-
+-
+-#define PFM_CMD_NAME(cmd)	pfm_cmd_tab[(cmd)].cmd_name
+-#define PFM_CMD_READ_ARG(cmd)	(pfm_cmd_tab[(cmd)].cmd_flags & PFM_CMD_ARG_READ)
+-#define PFM_CMD_RW_ARG(cmd)	(pfm_cmd_tab[(cmd)].cmd_flags & PFM_CMD_ARG_RW)
+-#define PFM_CMD_USE_FD(cmd)	(pfm_cmd_tab[(cmd)].cmd_flags & PFM_CMD_FD)
+-#define PFM_CMD_STOPPED(cmd)	(pfm_cmd_tab[(cmd)].cmd_flags & PFM_CMD_STOP)
+-
+-#define PFM_CMD_ARG_MANY	-1 /* cannot be zero */
+-
+-typedef struct {
+-	unsigned long pfm_spurious_ovfl_intr_count;	/* keep track of spurious ovfl interrupts */
+-	unsigned long pfm_replay_ovfl_intr_count;	/* keep track of replayed ovfl interrupts */
+-	unsigned long pfm_ovfl_intr_count; 		/* keep track of ovfl interrupts */
+-	unsigned long pfm_ovfl_intr_cycles;		/* cycles spent processing ovfl interrupts */
+-	unsigned long pfm_ovfl_intr_cycles_min;		/* min cycles spent processing ovfl interrupts */
+-	unsigned long pfm_ovfl_intr_cycles_max;		/* max cycles spent processing ovfl interrupts */
+-	unsigned long pfm_smpl_handler_calls;
+-	unsigned long pfm_smpl_handler_cycles;
+-	char pad[SMP_CACHE_BYTES] ____cacheline_aligned;
+-} pfm_stats_t;
+-
+-/*
+- * perfmon internal variables
+- */
+-static pfm_stats_t		pfm_stats[NR_CPUS];
+-static pfm_session_t		pfm_sessions;	/* global sessions information */
+-
+-static DEFINE_SPINLOCK(pfm_alt_install_check);
+-static pfm_intr_handler_desc_t  *pfm_alt_intr_handler;
+-
+-static struct proc_dir_entry 	*perfmon_dir;
+-static pfm_uuid_t		pfm_null_uuid = {0,};
+-
+-static spinlock_t		pfm_buffer_fmt_lock;
+-static LIST_HEAD(pfm_buffer_fmt_list);
+-
+-static pmu_config_t		*pmu_conf;
+-
+-/* sysctl() controls */
+-pfm_sysctl_t pfm_sysctl;
+-EXPORT_SYMBOL(pfm_sysctl);
+-
+-static ctl_table pfm_ctl_table[]={
+-	{
+-		.ctl_name	= CTL_UNNUMBERED,
+-		.procname	= "debug",
+-		.data		= &pfm_sysctl.debug,
+-		.maxlen		= sizeof(int),
+-		.mode		= 0666,
+-		.proc_handler	= &proc_dointvec,
+-	},
+-	{
+-		.ctl_name	= CTL_UNNUMBERED,
+-		.procname	= "debug_ovfl",
+-		.data		= &pfm_sysctl.debug_ovfl,
+-		.maxlen		= sizeof(int),
+-		.mode		= 0666,
+-		.proc_handler	= &proc_dointvec,
+-	},
+-	{
+-		.ctl_name	= CTL_UNNUMBERED,
+-		.procname	= "fastctxsw",
+-		.data		= &pfm_sysctl.fastctxsw,
+-		.maxlen		= sizeof(int),
+-		.mode		= 0600,
+-		.proc_handler	=  &proc_dointvec,
+-	},
+-	{
+-		.ctl_name	= CTL_UNNUMBERED,
+-		.procname	= "expert_mode",
+-		.data		= &pfm_sysctl.expert_mode,
+-		.maxlen		= sizeof(int),
+-		.mode		= 0600,
+-		.proc_handler	= &proc_dointvec,
+-	},
+-	{}
+-};
+-static ctl_table pfm_sysctl_dir[] = {
+-	{
+-		.ctl_name	= CTL_UNNUMBERED,
+-		.procname	= "perfmon",
+-		.mode		= 0555,
+-		.child		= pfm_ctl_table,
+-	},
+- 	{}
+-};
+-static ctl_table pfm_sysctl_root[] = {
+-	{
+-		.ctl_name	= CTL_KERN,
+-		.procname	= "kernel",
+-		.mode		= 0555,
+-		.child		= pfm_sysctl_dir,
+-	},
+- 	{}
+-};
+-static struct ctl_table_header *pfm_sysctl_header;
+-
+-static int pfm_context_unload(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);
+-
+-#define pfm_get_cpu_var(v)		__ia64_per_cpu_var(v)
+-#define pfm_get_cpu_data(a,b)		per_cpu(a, b)
+-
+-static inline void
+-pfm_put_task(struct task_struct *task)
+-{
+-	if (task != current) put_task_struct(task);
+-}
+-
+-static inline void
+-pfm_set_task_notify(struct task_struct *task)
+-{
+-	struct thread_info *info;
+-
+-	info = (struct thread_info *) ((char *) task + IA64_TASK_SIZE);
+-	set_bit(TIF_PERFMON_WORK, &info->flags);
+-}
+-
+-static inline void
+-pfm_clear_task_notify(void)
+-{
+-	clear_thread_flag(TIF_PERFMON_WORK);
+-}
+-
+-static inline void
+-pfm_reserve_page(unsigned long a)
+-{
+-	SetPageReserved(vmalloc_to_page((void *)a));
+-}
+-static inline void
+-pfm_unreserve_page(unsigned long a)
+-{
+-	ClearPageReserved(vmalloc_to_page((void*)a));
+-}
+-
+-static inline unsigned long
+-pfm_protect_ctx_ctxsw(pfm_context_t *x)
+-{
+-	spin_lock(&(x)->ctx_lock);
+-	return 0UL;
+-}
+-
+-static inline void
+-pfm_unprotect_ctx_ctxsw(pfm_context_t *x, unsigned long f)
+-{
+-	spin_unlock(&(x)->ctx_lock);
+-}
+-
+-static inline unsigned int
+-pfm_do_munmap(struct mm_struct *mm, unsigned long addr, size_t len, int acct)
+-{
+-	return do_munmap(mm, addr, len);
+-}
+-
+-static inline unsigned long 
+-pfm_get_unmapped_area(struct file *file, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags, unsigned long exec)
+-{
+-	return get_unmapped_area(file, addr, len, pgoff, flags);
+-}
+-
+-
+-static int
+-pfmfs_get_sb(struct file_system_type *fs_type, int flags, const char *dev_name, void *data,
+-	     struct vfsmount *mnt)
+-{
+-	return get_sb_pseudo(fs_type, "pfm:", NULL, PFMFS_MAGIC, mnt);
+-}
+-
+-static struct file_system_type pfm_fs_type = {
+-	.name     = "pfmfs",
+-	.get_sb   = pfmfs_get_sb,
+-	.kill_sb  = kill_anon_super,
+-};
+-
+-DEFINE_PER_CPU(unsigned long, pfm_syst_info);
+-DEFINE_PER_CPU(struct task_struct *, pmu_owner);
+-DEFINE_PER_CPU(pfm_context_t  *, pmu_ctx);
+-DEFINE_PER_CPU(unsigned long, pmu_activation_number);
+-EXPORT_PER_CPU_SYMBOL_GPL(pfm_syst_info);
+-
+-
+-/* forward declaration */
+-static const struct file_operations pfm_file_ops;
+-
+-/*
+- * forward declarations
+- */
+-#ifndef CONFIG_SMP
+-static void pfm_lazy_save_regs (struct task_struct *ta);
+-#endif
+-
+-void dump_pmu_state(const char *);
+-static int pfm_write_ibr_dbr(int mode, pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);
+-
+-#include "perfmon_itanium.h"
+-#include "perfmon_mckinley.h"
+-#include "perfmon_montecito.h"
+-#include "perfmon_generic.h"
+-
+-static pmu_config_t *pmu_confs[]={
+-	&pmu_conf_mont,
+-	&pmu_conf_mck,
+-	&pmu_conf_ita,
+-	&pmu_conf_gen, /* must be last */
+-	NULL
+-};
+-
+-
+-static int pfm_end_notify_user(pfm_context_t *ctx);
+-
+-static inline void
+-pfm_clear_psr_pp(void)
+-{
+-	ia64_rsm(IA64_PSR_PP);
+-	ia64_srlz_i();
+-}
+-
+-static inline void
+-pfm_set_psr_pp(void)
+-{
+-	ia64_ssm(IA64_PSR_PP);
+-	ia64_srlz_i();
+-}
+-
+-static inline void
+-pfm_clear_psr_up(void)
+-{
+-	ia64_rsm(IA64_PSR_UP);
+-	ia64_srlz_i();
+-}
+-
+-static inline void
+-pfm_set_psr_up(void)
+-{
+-	ia64_ssm(IA64_PSR_UP);
+-	ia64_srlz_i();
+-}
+-
+-static inline unsigned long
+-pfm_get_psr(void)
+-{
+-	unsigned long tmp;
+-	tmp = ia64_getreg(_IA64_REG_PSR);
+-	ia64_srlz_i();
+-	return tmp;
+-}
+-
+-static inline void
+-pfm_set_psr_l(unsigned long val)
+-{
+-	ia64_setreg(_IA64_REG_PSR_L, val);
+-	ia64_srlz_i();
+-}
+-
+-static inline void
+-pfm_freeze_pmu(void)
+-{
+-	ia64_set_pmc(0,1UL);
+-	ia64_srlz_d();
+-}
+-
+-static inline void
+-pfm_unfreeze_pmu(void)
+-{
+-	ia64_set_pmc(0,0UL);
+-	ia64_srlz_d();
+-}
+-
+-static inline void
+-pfm_restore_ibrs(unsigned long *ibrs, unsigned int nibrs)
+-{
+-	int i;
+-
+-	for (i=0; i < nibrs; i++) {
+-		ia64_set_ibr(i, ibrs[i]);
+-		ia64_dv_serialize_instruction();
+-	}
+-	ia64_srlz_i();
+-}
+-
+-static inline void
+-pfm_restore_dbrs(unsigned long *dbrs, unsigned int ndbrs)
+-{
+-	int i;
+-
+-	for (i=0; i < ndbrs; i++) {
+-		ia64_set_dbr(i, dbrs[i]);
+-		ia64_dv_serialize_data();
+-	}
+-	ia64_srlz_d();
+-}
+-
+-/*
+- * PMD[i] must be a counter. no check is made
+- */
+-static inline unsigned long
+-pfm_read_soft_counter(pfm_context_t *ctx, int i)
+-{
+-	return ctx->ctx_pmds[i].val + (ia64_get_pmd(i) & pmu_conf->ovfl_val);
+-}
+-
+-/*
+- * PMD[i] must be a counter. no check is made
+- */
+-static inline void
+-pfm_write_soft_counter(pfm_context_t *ctx, int i, unsigned long val)
+-{
+-	unsigned long ovfl_val = pmu_conf->ovfl_val;
+-
+-	ctx->ctx_pmds[i].val = val  & ~ovfl_val;
+-	/*
+-	 * writing to unimplemented part is ignore, so we do not need to
+-	 * mask off top part
+-	 */
+-	ia64_set_pmd(i, val & ovfl_val);
+-}
+-
+-static pfm_msg_t *
+-pfm_get_new_msg(pfm_context_t *ctx)
+-{
+-	int idx, next;
+-
+-	next = (ctx->ctx_msgq_tail+1) % PFM_MAX_MSGS;
+-
+-	DPRINT(("ctx_fd=%p head=%d tail=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail));
+-	if (next == ctx->ctx_msgq_head) return NULL;
+-
+- 	idx = 	ctx->ctx_msgq_tail;
+-	ctx->ctx_msgq_tail = next;
+-
+-	DPRINT(("ctx=%p head=%d tail=%d msg=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail, idx));
+-
+-	return ctx->ctx_msgq+idx;
+-}
+-
+-static pfm_msg_t *
+-pfm_get_next_msg(pfm_context_t *ctx)
+-{
+-	pfm_msg_t *msg;
+-
+-	DPRINT(("ctx=%p head=%d tail=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail));
+-
+-	if (PFM_CTXQ_EMPTY(ctx)) return NULL;
+-
+-	/*
+-	 * get oldest message
+-	 */
+-	msg = ctx->ctx_msgq+ctx->ctx_msgq_head;
+-
+-	/*
+-	 * and move forward
+-	 */
+-	ctx->ctx_msgq_head = (ctx->ctx_msgq_head+1) % PFM_MAX_MSGS;
+-
+-	DPRINT(("ctx=%p head=%d tail=%d type=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail, msg->pfm_gen_msg.msg_type));
+-
+-	return msg;
+-}
+-
+-static void
+-pfm_reset_msgq(pfm_context_t *ctx)
+-{
+-	ctx->ctx_msgq_head = ctx->ctx_msgq_tail = 0;
+-	DPRINT(("ctx=%p msgq reset\n", ctx));
+-}
+-
+-static void *
+-pfm_rvmalloc(unsigned long size)
+-{
+-	void *mem;
+-	unsigned long addr;
+-
+-	size = PAGE_ALIGN(size);
+-	mem  = vmalloc(size);
+-	if (mem) {
+-		//printk("perfmon: CPU%d pfm_rvmalloc(%ld)=%p\n", smp_processor_id(), size, mem);
+-		memset(mem, 0, size);
+-		addr = (unsigned long)mem;
+-		while (size > 0) {
+-			pfm_reserve_page(addr);
+-			addr+=PAGE_SIZE;
+-			size-=PAGE_SIZE;
+-		}
+-	}
+-	return mem;
+-}
+-
+-static void
+-pfm_rvfree(void *mem, unsigned long size)
+-{
+-	unsigned long addr;
+-
+-	if (mem) {
+-		DPRINT(("freeing physical buffer @%p size=%lu\n", mem, size));
+-		addr = (unsigned long) mem;
+-		while ((long) size > 0) {
+-			pfm_unreserve_page(addr);
+-			addr+=PAGE_SIZE;
+-			size-=PAGE_SIZE;
+-		}
+-		vfree(mem);
+-	}
+-	return;
+-}
+-
+-static pfm_context_t *
+-pfm_context_alloc(void)
+-{
+-	pfm_context_t *ctx;
+-
+-	/* 
+-	 * allocate context descriptor 
+-	 * must be able to free with interrupts disabled
+-	 */
+-	ctx = kzalloc(sizeof(pfm_context_t), GFP_KERNEL);
+-	if (ctx) {
+-		DPRINT(("alloc ctx @%p\n", ctx));
+-	}
+-	return ctx;
+-}
+-
+-static void
+-pfm_context_free(pfm_context_t *ctx)
+-{
+-	if (ctx) {
+-		DPRINT(("free ctx @%p\n", ctx));
+-		kfree(ctx);
+-	}
+-}
+-
+-static void
+-pfm_mask_monitoring(struct task_struct *task)
+-{
+-	pfm_context_t *ctx = PFM_GET_CTX(task);
+-	unsigned long mask, val, ovfl_mask;
+-	int i;
+-
+-	DPRINT_ovfl(("masking monitoring for [%d]\n", task_pid_nr(task)));
+-
+-	ovfl_mask = pmu_conf->ovfl_val;
+-	/*
+-	 * monitoring can only be masked as a result of a valid
+-	 * counter overflow. In UP, it means that the PMU still
+-	 * has an owner. Note that the owner can be different
+-	 * from the current task. However the PMU state belongs
+-	 * to the owner.
+-	 * In SMP, a valid overflow only happens when task is
+-	 * current. Therefore if we come here, we know that
+-	 * the PMU state belongs to the current task, therefore
+-	 * we can access the live registers.
+-	 *
+-	 * So in both cases, the live register contains the owner's
+-	 * state. We can ONLY touch the PMU registers and NOT the PSR.
+-	 *
+-	 * As a consequence to this call, the ctx->th_pmds[] array
+-	 * contains stale information which must be ignored
+-	 * when context is reloaded AND monitoring is active (see
+-	 * pfm_restart).
+-	 */
+-	mask = ctx->ctx_used_pmds[0];
+-	for (i = 0; mask; i++, mask>>=1) {
+-		/* skip non used pmds */
+-		if ((mask & 0x1) == 0) continue;
+-		val = ia64_get_pmd(i);
+-
+-		if (PMD_IS_COUNTING(i)) {
+-			/*
+-		 	 * we rebuild the full 64 bit value of the counter
+-		 	 */
+-			ctx->ctx_pmds[i].val += (val & ovfl_mask);
+-		} else {
+-			ctx->ctx_pmds[i].val = val;
+-		}
+-		DPRINT_ovfl(("pmd[%d]=0x%lx hw_pmd=0x%lx\n",
+-			i,
+-			ctx->ctx_pmds[i].val,
+-			val & ovfl_mask));
+-	}
+-	/*
+-	 * mask monitoring by setting the privilege level to 0
+-	 * we cannot use psr.pp/psr.up for this, it is controlled by
+-	 * the user
+-	 *
+-	 * if task is current, modify actual registers, otherwise modify
+-	 * thread save state, i.e., what will be restored in pfm_load_regs()
+-	 */
+-	mask = ctx->ctx_used_monitors[0] >> PMU_FIRST_COUNTER;
+-	for(i= PMU_FIRST_COUNTER; mask; i++, mask>>=1) {
+-		if ((mask & 0x1) == 0UL) continue;
+-		ia64_set_pmc(i, ctx->th_pmcs[i] & ~0xfUL);
+-		ctx->th_pmcs[i] &= ~0xfUL;
+-		DPRINT_ovfl(("pmc[%d]=0x%lx\n", i, ctx->th_pmcs[i]));
+-	}
+-	/*
+-	 * make all of this visible
+-	 */
+-	ia64_srlz_d();
+-}
+-
+-/*
+- * must always be done with task == current
+- *
+- * context must be in MASKED state when calling
+- */
+-static void
+-pfm_restore_monitoring(struct task_struct *task)
+-{
+-	pfm_context_t *ctx = PFM_GET_CTX(task);
+-	unsigned long mask, ovfl_mask;
+-	unsigned long psr, val;
+-	int i, is_system;
+-
+-	is_system = ctx->ctx_fl_system;
+-	ovfl_mask = pmu_conf->ovfl_val;
+-
+-	if (task != current) {
+-		printk(KERN_ERR "perfmon.%d: invalid task[%d] current[%d]\n", __LINE__, task_pid_nr(task), task_pid_nr(current));
+-		return;
+-	}
+-	if (ctx->ctx_state != PFM_CTX_MASKED) {
+-		printk(KERN_ERR "perfmon.%d: task[%d] current[%d] invalid state=%d\n", __LINE__,
+-			task_pid_nr(task), task_pid_nr(current), ctx->ctx_state);
+-		return;
+-	}
+-	psr = pfm_get_psr();
+-	/*
+-	 * monitoring is masked via the PMC.
+-	 * As we restore their value, we do not want each counter to
+-	 * restart right away. We stop monitoring using the PSR,
+-	 * restore the PMC (and PMD) and then re-establish the psr
+-	 * as it was. Note that there can be no pending overflow at
+-	 * this point, because monitoring was MASKED.
+-	 *
+-	 * system-wide session are pinned and self-monitoring
+-	 */
+-	if (is_system && (PFM_CPUINFO_GET() & PFM_CPUINFO_DCR_PP)) {
+-		/* disable dcr pp */
+-		ia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) & ~IA64_DCR_PP);
+-		pfm_clear_psr_pp();
+-	} else {
+-		pfm_clear_psr_up();
+-	}
+-	/*
+-	 * first, we restore the PMD
+-	 */
+-	mask = ctx->ctx_used_pmds[0];
+-	for (i = 0; mask; i++, mask>>=1) {
+-		/* skip non used pmds */
+-		if ((mask & 0x1) == 0) continue;
+-
+-		if (PMD_IS_COUNTING(i)) {
+-			/*
+-			 * we split the 64bit value according to
+-			 * counter width
+-			 */
+-			val = ctx->ctx_pmds[i].val & ovfl_mask;
+-			ctx->ctx_pmds[i].val &= ~ovfl_mask;
+-		} else {
+-			val = ctx->ctx_pmds[i].val;
+-		}
+-		ia64_set_pmd(i, val);
+-
+-		DPRINT(("pmd[%d]=0x%lx hw_pmd=0x%lx\n",
+-			i,
+-			ctx->ctx_pmds[i].val,
+-			val));
+-	}
+-	/*
+-	 * restore the PMCs
+-	 */
+-	mask = ctx->ctx_used_monitors[0] >> PMU_FIRST_COUNTER;
+-	for(i= PMU_FIRST_COUNTER; mask; i++, mask>>=1) {
+-		if ((mask & 0x1) == 0UL) continue;
+-		ctx->th_pmcs[i] = ctx->ctx_pmcs[i];
+-		ia64_set_pmc(i, ctx->th_pmcs[i]);
+-		DPRINT(("[%d] pmc[%d]=0x%lx\n",
+-					task_pid_nr(task), i, ctx->th_pmcs[i]));
+-	}
+-	ia64_srlz_d();
+-
+-	/*
+-	 * must restore DBR/IBR because could be modified while masked
+-	 * XXX: need to optimize 
+-	 */
+-	if (ctx->ctx_fl_using_dbreg) {
+-		pfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);
+-		pfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);
+-	}
+-
+-	/*
+-	 * now restore PSR
+-	 */
+-	if (is_system && (PFM_CPUINFO_GET() & PFM_CPUINFO_DCR_PP)) {
+-		/* enable dcr pp */
+-		ia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) | IA64_DCR_PP);
+-		ia64_srlz_i();
+-	}
+-	pfm_set_psr_l(psr);
+-}
+-
+-static inline void
+-pfm_save_pmds(unsigned long *pmds, unsigned long mask)
+-{
+-	int i;
+-
+-	ia64_srlz_d();
+-
+-	for (i=0; mask; i++, mask>>=1) {
+-		if (mask & 0x1) pmds[i] = ia64_get_pmd(i);
+-	}
+-}
+-
+-/*
+- * reload from thread state (used for ctxw only)
+- */
+-static inline void
+-pfm_restore_pmds(unsigned long *pmds, unsigned long mask)
+-{
+-	int i;
+-	unsigned long val, ovfl_val = pmu_conf->ovfl_val;
+-
+-	for (i=0; mask; i++, mask>>=1) {
+-		if ((mask & 0x1) == 0) continue;
+-		val = PMD_IS_COUNTING(i) ? pmds[i] & ovfl_val : pmds[i];
+-		ia64_set_pmd(i, val);
+-	}
+-	ia64_srlz_d();
+-}
+-
+-/*
+- * propagate PMD from context to thread-state
+- */
+-static inline void
+-pfm_copy_pmds(struct task_struct *task, pfm_context_t *ctx)
+-{
+-	unsigned long ovfl_val = pmu_conf->ovfl_val;
+-	unsigned long mask = ctx->ctx_all_pmds[0];
+-	unsigned long val;
+-	int i;
+-
+-	DPRINT(("mask=0x%lx\n", mask));
+-
+-	for (i=0; mask; i++, mask>>=1) {
+-
+-		val = ctx->ctx_pmds[i].val;
+-
+-		/*
+-		 * We break up the 64 bit value into 2 pieces
+-		 * the lower bits go to the machine state in the
+-		 * thread (will be reloaded on ctxsw in).
+-		 * The upper part stays in the soft-counter.
+-		 */
+-		if (PMD_IS_COUNTING(i)) {
+-			ctx->ctx_pmds[i].val = val & ~ovfl_val;
+-			 val &= ovfl_val;
+-		}
+-		ctx->th_pmds[i] = val;
+-
+-		DPRINT(("pmd[%d]=0x%lx soft_val=0x%lx\n",
+-			i,
+-			ctx->th_pmds[i],
+-			ctx->ctx_pmds[i].val));
+-	}
+-}
+-
+-/*
+- * propagate PMC from context to thread-state
+- */
+-static inline void
+-pfm_copy_pmcs(struct task_struct *task, pfm_context_t *ctx)
+-{
+-	unsigned long mask = ctx->ctx_all_pmcs[0];
+-	int i;
+-
+-	DPRINT(("mask=0x%lx\n", mask));
+-
+-	for (i=0; mask; i++, mask>>=1) {
+-		/* masking 0 with ovfl_val yields 0 */
+-		ctx->th_pmcs[i] = ctx->ctx_pmcs[i];
+-		DPRINT(("pmc[%d]=0x%lx\n", i, ctx->th_pmcs[i]));
+-	}
+-}
+-
+-
+-
+-static inline void
+-pfm_restore_pmcs(unsigned long *pmcs, unsigned long mask)
+-{
+-	int i;
+-
+-	for (i=0; mask; i++, mask>>=1) {
+-		if ((mask & 0x1) == 0) continue;
+-		ia64_set_pmc(i, pmcs[i]);
+-	}
+-	ia64_srlz_d();
+-}
+-
+-static inline int
+-pfm_uuid_cmp(pfm_uuid_t a, pfm_uuid_t b)
+-{
+-	return memcmp(a, b, sizeof(pfm_uuid_t));
+-}
+-
+-static inline int
+-pfm_buf_fmt_exit(pfm_buffer_fmt_t *fmt, struct task_struct *task, void *buf, struct pt_regs *regs)
+-{
+-	int ret = 0;
+-	if (fmt->fmt_exit) ret = (*fmt->fmt_exit)(task, buf, regs);
+-	return ret;
+-}
+-
+-static inline int
+-pfm_buf_fmt_getsize(pfm_buffer_fmt_t *fmt, struct task_struct *task, unsigned int flags, int cpu, void *arg, unsigned long *size)
+-{
+-	int ret = 0;
+-	if (fmt->fmt_getsize) ret = (*fmt->fmt_getsize)(task, flags, cpu, arg, size);
+-	return ret;
+-}
+-
+-
+-static inline int
+-pfm_buf_fmt_validate(pfm_buffer_fmt_t *fmt, struct task_struct *task, unsigned int flags,
+-		     int cpu, void *arg)
+-{
+-	int ret = 0;
+-	if (fmt->fmt_validate) ret = (*fmt->fmt_validate)(task, flags, cpu, arg);
+-	return ret;
+-}
+-
+-static inline int
+-pfm_buf_fmt_init(pfm_buffer_fmt_t *fmt, struct task_struct *task, void *buf, unsigned int flags,
+-		     int cpu, void *arg)
+-{
+-	int ret = 0;
+-	if (fmt->fmt_init) ret = (*fmt->fmt_init)(task, buf, flags, cpu, arg);
+-	return ret;
+-}
+-
+-static inline int
+-pfm_buf_fmt_restart(pfm_buffer_fmt_t *fmt, struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs)
+-{
+-	int ret = 0;
+-	if (fmt->fmt_restart) ret = (*fmt->fmt_restart)(task, ctrl, buf, regs);
+-	return ret;
+-}
+-
+-static inline int
+-pfm_buf_fmt_restart_active(pfm_buffer_fmt_t *fmt, struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs)
+-{
+-	int ret = 0;
+-	if (fmt->fmt_restart_active) ret = (*fmt->fmt_restart_active)(task, ctrl, buf, regs);
+-	return ret;
+-}
+-
+-static pfm_buffer_fmt_t *
+-__pfm_find_buffer_fmt(pfm_uuid_t uuid)
+-{
+-	struct list_head * pos;
+-	pfm_buffer_fmt_t * entry;
+-
+-	list_for_each(pos, &pfm_buffer_fmt_list) {
+-		entry = list_entry(pos, pfm_buffer_fmt_t, fmt_list);
+-		if (pfm_uuid_cmp(uuid, entry->fmt_uuid) == 0)
+-			return entry;
+-	}
+-	return NULL;
+-}
+- 
+-/*
+- * find a buffer format based on its uuid
+- */
+-static pfm_buffer_fmt_t *
+-pfm_find_buffer_fmt(pfm_uuid_t uuid)
+-{
+-	pfm_buffer_fmt_t * fmt;
+-	spin_lock(&pfm_buffer_fmt_lock);
+-	fmt = __pfm_find_buffer_fmt(uuid);
+-	spin_unlock(&pfm_buffer_fmt_lock);
+-	return fmt;
+-}
+- 
+-int
+-pfm_register_buffer_fmt(pfm_buffer_fmt_t *fmt)
+-{
+-	int ret = 0;
+-
+-	/* some sanity checks */
+-	if (fmt == NULL || fmt->fmt_name == NULL) return -EINVAL;
+-
+-	/* we need at least a handler */
+-	if (fmt->fmt_handler == NULL) return -EINVAL;
+-
+-	/*
+-	 * XXX: need check validity of fmt_arg_size
+-	 */
+-
+-	spin_lock(&pfm_buffer_fmt_lock);
+-
+-	if (__pfm_find_buffer_fmt(fmt->fmt_uuid)) {
+-		printk(KERN_ERR "perfmon: duplicate sampling format: %s\n", fmt->fmt_name);
+-		ret = -EBUSY;
+-		goto out;
+-	} 
+-	list_add(&fmt->fmt_list, &pfm_buffer_fmt_list);
+-	printk(KERN_INFO "perfmon: added sampling format %s\n", fmt->fmt_name);
+-
+-out:
+-	spin_unlock(&pfm_buffer_fmt_lock);
+- 	return ret;
+-}
+-EXPORT_SYMBOL(pfm_register_buffer_fmt);
+-
+-int
+-pfm_unregister_buffer_fmt(pfm_uuid_t uuid)
+-{
+-	pfm_buffer_fmt_t *fmt;
+-	int ret = 0;
+-
+-	spin_lock(&pfm_buffer_fmt_lock);
+-
+-	fmt = __pfm_find_buffer_fmt(uuid);
+-	if (!fmt) {
+-		printk(KERN_ERR "perfmon: cannot unregister format, not found\n");
+-		ret = -EINVAL;
+-		goto out;
+-	}
+-	list_del_init(&fmt->fmt_list);
+-	printk(KERN_INFO "perfmon: removed sampling format: %s\n", fmt->fmt_name);
+-
+-out:
+-	spin_unlock(&pfm_buffer_fmt_lock);
+-	return ret;
+-
+-}
+-EXPORT_SYMBOL(pfm_unregister_buffer_fmt);
+-
+-extern void update_pal_halt_status(int);
+-
+-static int
+-pfm_reserve_session(struct task_struct *task, int is_syswide, unsigned int cpu)
+-{
+-	unsigned long flags;
+-	/*
+-	 * validity checks on cpu_mask have been done upstream
+-	 */
+-	LOCK_PFS(flags);
+-
+-	DPRINT(("in sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",
+-		pfm_sessions.pfs_sys_sessions,
+-		pfm_sessions.pfs_task_sessions,
+-		pfm_sessions.pfs_sys_use_dbregs,
+-		is_syswide,
+-		cpu));
+-
+-	if (is_syswide) {
+-		/*
+-		 * cannot mix system wide and per-task sessions
+-		 */
+-		if (pfm_sessions.pfs_task_sessions > 0UL) {
+-			DPRINT(("system wide not possible, %u conflicting task_sessions\n",
+-			  	pfm_sessions.pfs_task_sessions));
+-			goto abort;
+-		}
+-
+-		if (pfm_sessions.pfs_sys_session[cpu]) goto error_conflict;
+-
+-		DPRINT(("reserving system wide session on CPU%u currently on CPU%u\n", cpu, smp_processor_id()));
+-
+-		pfm_sessions.pfs_sys_session[cpu] = task;
+-
+-		pfm_sessions.pfs_sys_sessions++ ;
+-
+-	} else {
+-		if (pfm_sessions.pfs_sys_sessions) goto abort;
+-		pfm_sessions.pfs_task_sessions++;
+-	}
+-
+-	DPRINT(("out sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",
+-		pfm_sessions.pfs_sys_sessions,
+-		pfm_sessions.pfs_task_sessions,
+-		pfm_sessions.pfs_sys_use_dbregs,
+-		is_syswide,
+-		cpu));
+-
+-	/*
+-	 * disable default_idle() to go to PAL_HALT
+-	 */
+-	update_pal_halt_status(0);
+-
+-	UNLOCK_PFS(flags);
+-
+-	return 0;
+-
+-error_conflict:
+-	DPRINT(("system wide not possible, conflicting session [%d] on CPU%d\n",
+-  		task_pid_nr(pfm_sessions.pfs_sys_session[cpu]),
+-		cpu));
+-abort:
+-	UNLOCK_PFS(flags);
+-
+-	return -EBUSY;
+-
+-}
+-
+-static int
+-pfm_unreserve_session(pfm_context_t *ctx, int is_syswide, unsigned int cpu)
+-{
+-	unsigned long flags;
+-	/*
+-	 * validity checks on cpu_mask have been done upstream
+-	 */
+-	LOCK_PFS(flags);
+-
+-	DPRINT(("in sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",
+-		pfm_sessions.pfs_sys_sessions,
+-		pfm_sessions.pfs_task_sessions,
+-		pfm_sessions.pfs_sys_use_dbregs,
+-		is_syswide,
+-		cpu));
+-
+-
+-	if (is_syswide) {
+-		pfm_sessions.pfs_sys_session[cpu] = NULL;
+-		/*
+-		 * would not work with perfmon+more than one bit in cpu_mask
+-		 */
+-		if (ctx && ctx->ctx_fl_using_dbreg) {
+-			if (pfm_sessions.pfs_sys_use_dbregs == 0) {
+-				printk(KERN_ERR "perfmon: invalid release for ctx %p sys_use_dbregs=0\n", ctx);
+-			} else {
+-				pfm_sessions.pfs_sys_use_dbregs--;
+-			}
+-		}
+-		pfm_sessions.pfs_sys_sessions--;
+-	} else {
+-		pfm_sessions.pfs_task_sessions--;
+-	}
+-	DPRINT(("out sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",
+-		pfm_sessions.pfs_sys_sessions,
+-		pfm_sessions.pfs_task_sessions,
+-		pfm_sessions.pfs_sys_use_dbregs,
+-		is_syswide,
+-		cpu));
+-
+-	/*
+-	 * if possible, enable default_idle() to go into PAL_HALT
+-	 */
+-	if (pfm_sessions.pfs_task_sessions == 0 && pfm_sessions.pfs_sys_sessions == 0)
+-		update_pal_halt_status(1);
+-
+-	UNLOCK_PFS(flags);
+-
+-	return 0;
+-}
+-
+-/*
+- * removes virtual mapping of the sampling buffer.
+- * IMPORTANT: cannot be called with interrupts disable, e.g. inside
+- * a PROTECT_CTX() section.
+- */
+-static int
+-pfm_remove_smpl_mapping(struct task_struct *task, void *vaddr, unsigned long size)
+-{
+-	int r;
+-
+-	/* sanity checks */
+-	if (task->mm == NULL || size == 0UL || vaddr == NULL) {
+-		printk(KERN_ERR "perfmon: pfm_remove_smpl_mapping [%d] invalid context mm=%p\n", task_pid_nr(task), task->mm);
+-		return -EINVAL;
+-	}
+-
+-	DPRINT(("smpl_vaddr=%p size=%lu\n", vaddr, size));
+-
+-	/*
+-	 * does the actual unmapping
+-	 */
+-	down_write(&task->mm->mmap_sem);
+-
+-	DPRINT(("down_write done smpl_vaddr=%p size=%lu\n", vaddr, size));
+-
+-	r = pfm_do_munmap(task->mm, (unsigned long)vaddr, size, 0);
+-
+-	up_write(&task->mm->mmap_sem);
+-	if (r !=0) {
+-		printk(KERN_ERR "perfmon: [%d] unable to unmap sampling buffer @%p size=%lu\n", task_pid_nr(task), vaddr, size);
+-	}
+-
+-	DPRINT(("do_unmap(%p, %lu)=%d\n", vaddr, size, r));
+-
+-	return 0;
+-}
+-
+-/*
+- * free actual physical storage used by sampling buffer
+- */
+-#if 0
+-static int
+-pfm_free_smpl_buffer(pfm_context_t *ctx)
+-{
+-	pfm_buffer_fmt_t *fmt;
+-
+-	if (ctx->ctx_smpl_hdr == NULL) goto invalid_free;
+-
+-	/*
+-	 * we won't use the buffer format anymore
+-	 */
+-	fmt = ctx->ctx_buf_fmt;
+-
+-	DPRINT(("sampling buffer @%p size %lu vaddr=%p\n",
+-		ctx->ctx_smpl_hdr,
+-		ctx->ctx_smpl_size,
+-		ctx->ctx_smpl_vaddr));
+-
+-	pfm_buf_fmt_exit(fmt, current, NULL, NULL);
+-
+-	/*
+-	 * free the buffer
+-	 */
+-	pfm_rvfree(ctx->ctx_smpl_hdr, ctx->ctx_smpl_size);
+-
+-	ctx->ctx_smpl_hdr  = NULL;
+-	ctx->ctx_smpl_size = 0UL;
+-
+-	return 0;
+-
+-invalid_free:
+-	printk(KERN_ERR "perfmon: pfm_free_smpl_buffer [%d] no buffer\n", task_pid_nr(current));
+-	return -EINVAL;
+-}
+-#endif
+-
+-static inline void
+-pfm_exit_smpl_buffer(pfm_buffer_fmt_t *fmt)
+-{
+-	if (fmt == NULL) return;
+-
+-	pfm_buf_fmt_exit(fmt, current, NULL, NULL);
+-
+-}
+-
+-/*
+- * pfmfs should _never_ be mounted by userland - too much of security hassle,
+- * no real gain from having the whole whorehouse mounted. So we don't need
+- * any operations on the root directory. However, we need a non-trivial
+- * d_name - pfm: will go nicely and kill the special-casing in procfs.
+- */
+-static struct vfsmount *pfmfs_mnt;
+-
+-static int __init
+-init_pfm_fs(void)
+-{
+-	int err = register_filesystem(&pfm_fs_type);
+-	if (!err) {
+-		pfmfs_mnt = kern_mount(&pfm_fs_type);
+-		err = PTR_ERR(pfmfs_mnt);
+-		if (IS_ERR(pfmfs_mnt))
+-			unregister_filesystem(&pfm_fs_type);
+-		else
+-			err = 0;
+-	}
+-	return err;
+-}
+-
+-static ssize_t
+-pfm_read(struct file *filp, char __user *buf, size_t size, loff_t *ppos)
+-{
+-	pfm_context_t *ctx;
+-	pfm_msg_t *msg;
+-	ssize_t ret;
+-	unsigned long flags;
+-  	DECLARE_WAITQUEUE(wait, current);
+-	if (PFM_IS_FILE(filp) == 0) {
+-		printk(KERN_ERR "perfmon: pfm_poll: bad magic [%d]\n", task_pid_nr(current));
+-		return -EINVAL;
+-	}
+-
+-	ctx = (pfm_context_t *)filp->private_data;
+-	if (ctx == NULL) {
+-		printk(KERN_ERR "perfmon: pfm_read: NULL ctx [%d]\n", task_pid_nr(current));
+-		return -EINVAL;
+-	}
+-
+-	/*
+-	 * check even when there is no message
+-	 */
+-	if (size < sizeof(pfm_msg_t)) {
+-		DPRINT(("message is too small ctx=%p (>=%ld)\n", ctx, sizeof(pfm_msg_t)));
+-		return -EINVAL;
+-	}
+-
+-	PROTECT_CTX(ctx, flags);
+-
+-  	/*
+-	 * put ourselves on the wait queue
+-	 */
+-  	add_wait_queue(&ctx->ctx_msgq_wait, &wait);
+-
+-
+-  	for(;;) {
+-		/*
+-		 * check wait queue
+-		 */
+-
+-  		set_current_state(TASK_INTERRUPTIBLE);
+-
+-		DPRINT(("head=%d tail=%d\n", ctx->ctx_msgq_head, ctx->ctx_msgq_tail));
+-
+-		ret = 0;
+-		if(PFM_CTXQ_EMPTY(ctx) == 0) break;
+-
+-		UNPROTECT_CTX(ctx, flags);
+-
+-		/*
+-		 * check non-blocking read
+-		 */
+-      		ret = -EAGAIN;
+-		if(filp->f_flags & O_NONBLOCK) break;
+-
+-		/*
+-		 * check pending signals
+-		 */
+-		if(signal_pending(current)) {
+-			ret = -EINTR;
+-			break;
+-		}
+-      		/*
+-		 * no message, so wait
+-		 */
+-      		schedule();
+-
+-		PROTECT_CTX(ctx, flags);
+-	}
+-	DPRINT(("[%d] back to running ret=%ld\n", task_pid_nr(current), ret));
+-  	set_current_state(TASK_RUNNING);
+-	remove_wait_queue(&ctx->ctx_msgq_wait, &wait);
+-
+-	if (ret < 0) goto abort;
+-
+-	ret = -EINVAL;
+-	msg = pfm_get_next_msg(ctx);
+-	if (msg == NULL) {
+-		printk(KERN_ERR "perfmon: pfm_read no msg for ctx=%p [%d]\n", ctx, task_pid_nr(current));
+-		goto abort_locked;
+-	}
+-
+-	DPRINT(("fd=%d type=%d\n", msg->pfm_gen_msg.msg_ctx_fd, msg->pfm_gen_msg.msg_type));
+-
+-	ret = -EFAULT;
+-  	if(copy_to_user(buf, msg, sizeof(pfm_msg_t)) == 0) ret = sizeof(pfm_msg_t);
+-
+-abort_locked:
+-	UNPROTECT_CTX(ctx, flags);
+-abort:
+-	return ret;
+-}
+-
+-static ssize_t
+-pfm_write(struct file *file, const char __user *ubuf,
+-			  size_t size, loff_t *ppos)
+-{
+-	DPRINT(("pfm_write called\n"));
+-	return -EINVAL;
+-}
+-
+-static unsigned int
+-pfm_poll(struct file *filp, poll_table * wait)
+-{
+-	pfm_context_t *ctx;
+-	unsigned long flags;
+-	unsigned int mask = 0;
+-
+-	if (PFM_IS_FILE(filp) == 0) {
+-		printk(KERN_ERR "perfmon: pfm_poll: bad magic [%d]\n", task_pid_nr(current));
+-		return 0;
+-	}
+-
+-	ctx = (pfm_context_t *)filp->private_data;
+-	if (ctx == NULL) {
+-		printk(KERN_ERR "perfmon: pfm_poll: NULL ctx [%d]\n", task_pid_nr(current));
+-		return 0;
+-	}
+-
+-
+-	DPRINT(("pfm_poll ctx_fd=%d before poll_wait\n", ctx->ctx_fd));
+-
+-	poll_wait(filp, &ctx->ctx_msgq_wait, wait);
+-
+-	PROTECT_CTX(ctx, flags);
+-
+-	if (PFM_CTXQ_EMPTY(ctx) == 0)
+-		mask =  POLLIN | POLLRDNORM;
+-
+-	UNPROTECT_CTX(ctx, flags);
+-
+-	DPRINT(("pfm_poll ctx_fd=%d mask=0x%x\n", ctx->ctx_fd, mask));
+-
+-	return mask;
+-}
+-
+-static int
+-pfm_ioctl(struct inode *inode, struct file *file, unsigned int cmd, unsigned long arg)
+-{
+-	DPRINT(("pfm_ioctl called\n"));
+-	return -EINVAL;
+-}
+-
+-/*
+- * interrupt cannot be masked when coming here
+- */
+-static inline int
+-pfm_do_fasync(int fd, struct file *filp, pfm_context_t *ctx, int on)
+-{
+-	int ret;
+-
+-	ret = fasync_helper (fd, filp, on, &ctx->ctx_async_queue);
+-
+-	DPRINT(("pfm_fasync called by [%d] on ctx_fd=%d on=%d async_queue=%p ret=%d\n",
+-		task_pid_nr(current),
+-		fd,
+-		on,
+-		ctx->ctx_async_queue, ret));
+-
+-	return ret;
+-}
+-
+-static int
+-pfm_fasync(int fd, struct file *filp, int on)
+-{
+-	pfm_context_t *ctx;
+-	int ret;
+-
+-	if (PFM_IS_FILE(filp) == 0) {
+-		printk(KERN_ERR "perfmon: pfm_fasync bad magic [%d]\n", task_pid_nr(current));
+-		return -EBADF;
+-	}
+-
+-	ctx = (pfm_context_t *)filp->private_data;
+-	if (ctx == NULL) {
+-		printk(KERN_ERR "perfmon: pfm_fasync NULL ctx [%d]\n", task_pid_nr(current));
+-		return -EBADF;
+-	}
+-	/*
+-	 * we cannot mask interrupts during this call because this may
+-	 * may go to sleep if memory is not readily avalaible.
+-	 *
+-	 * We are protected from the conetxt disappearing by the get_fd()/put_fd()
+-	 * done in caller. Serialization of this function is ensured by caller.
+-	 */
+-	ret = pfm_do_fasync(fd, filp, ctx, on);
+-
+-
+-	DPRINT(("pfm_fasync called on ctx_fd=%d on=%d async_queue=%p ret=%d\n",
+-		fd,
+-		on,
+-		ctx->ctx_async_queue, ret));
+-
+-	return ret;
+-}
+-
+-#ifdef CONFIG_SMP
+-/*
+- * this function is exclusively called from pfm_close().
+- * The context is not protected at that time, nor are interrupts
+- * on the remote CPU. That's necessary to avoid deadlocks.
+- */
+-static void
+-pfm_syswide_force_stop(void *info)
+-{
+-	pfm_context_t   *ctx = (pfm_context_t *)info;
+-	struct pt_regs *regs = task_pt_regs(current);
+-	struct task_struct *owner;
+-	unsigned long flags;
+-	int ret;
+-
+-	if (ctx->ctx_cpu != smp_processor_id()) {
+-		printk(KERN_ERR "perfmon: pfm_syswide_force_stop for CPU%d  but on CPU%d\n",
+-			ctx->ctx_cpu,
+-			smp_processor_id());
+-		return;
+-	}
+-	owner = GET_PMU_OWNER();
+-	if (owner != ctx->ctx_task) {
+-		printk(KERN_ERR "perfmon: pfm_syswide_force_stop CPU%d unexpected owner [%d] instead of [%d]\n",
+-			smp_processor_id(),
+-			task_pid_nr(owner), task_pid_nr(ctx->ctx_task));
+-		return;
+-	}
+-	if (GET_PMU_CTX() != ctx) {
+-		printk(KERN_ERR "perfmon: pfm_syswide_force_stop CPU%d unexpected ctx %p instead of %p\n",
+-			smp_processor_id(),
+-			GET_PMU_CTX(), ctx);
+-		return;
+-	}
+-
+-	DPRINT(("on CPU%d forcing system wide stop for [%d]\n", smp_processor_id(), task_pid_nr(ctx->ctx_task)));
+-	/*
+-	 * the context is already protected in pfm_close(), we simply
+-	 * need to mask interrupts to avoid a PMU interrupt race on
+-	 * this CPU
+-	 */
+-	local_irq_save(flags);
+-
+-	ret = pfm_context_unload(ctx, NULL, 0, regs);
+-	if (ret) {
+-		DPRINT(("context_unload returned %d\n", ret));
+-	}
+-
+-	/*
+-	 * unmask interrupts, PMU interrupts are now spurious here
+-	 */
+-	local_irq_restore(flags);
+-}
+-
+-static void
+-pfm_syswide_cleanup_other_cpu(pfm_context_t *ctx)
+-{
+-	int ret;
+-
+-	DPRINT(("calling CPU%d for cleanup\n", ctx->ctx_cpu));
+-	ret = smp_call_function_single(ctx->ctx_cpu, pfm_syswide_force_stop, ctx, 0, 1);
+-	DPRINT(("called CPU%d for cleanup ret=%d\n", ctx->ctx_cpu, ret));
+-}
+-#endif /* CONFIG_SMP */
+-
+-/*
+- * called for each close(). Partially free resources.
+- * When caller is self-monitoring, the context is unloaded.
+- */
+-static int
+-pfm_flush(struct file *filp, fl_owner_t id)
+-{
+-	pfm_context_t *ctx;
+-	struct task_struct *task;
+-	struct pt_regs *regs;
+-	unsigned long flags;
+-	unsigned long smpl_buf_size = 0UL;
+-	void *smpl_buf_vaddr = NULL;
+-	int state, is_system;
+-
+-	if (PFM_IS_FILE(filp) == 0) {
+-		DPRINT(("bad magic for\n"));
+-		return -EBADF;
+-	}
+-
+-	ctx = (pfm_context_t *)filp->private_data;
+-	if (ctx == NULL) {
+-		printk(KERN_ERR "perfmon: pfm_flush: NULL ctx [%d]\n", task_pid_nr(current));
+-		return -EBADF;
+-	}
+-
+-	/*
+-	 * remove our file from the async queue, if we use this mode.
+-	 * This can be done without the context being protected. We come
+-	 * here when the context has become unreachable by other tasks.
+-	 *
+-	 * We may still have active monitoring at this point and we may
+-	 * end up in pfm_overflow_handler(). However, fasync_helper()
+-	 * operates with interrupts disabled and it cleans up the
+-	 * queue. If the PMU handler is called prior to entering
+-	 * fasync_helper() then it will send a signal. If it is
+-	 * invoked after, it will find an empty queue and no
+-	 * signal will be sent. In both case, we are safe
+-	 */
+-	if (filp->f_flags & FASYNC) {
+-		DPRINT(("cleaning up async_queue=%p\n", ctx->ctx_async_queue));
+-		pfm_do_fasync (-1, filp, ctx, 0);
+-	}
+-
+-	PROTECT_CTX(ctx, flags);
+-
+-	state     = ctx->ctx_state;
+-	is_system = ctx->ctx_fl_system;
+-
+-	task = PFM_CTX_TASK(ctx);
+-	regs = task_pt_regs(task);
+-
+-	DPRINT(("ctx_state=%d is_current=%d\n",
+-		state,
+-		task == current ? 1 : 0));
+-
+-	/*
+-	 * if state == UNLOADED, then task is NULL
+-	 */
+-
+-	/*
+-	 * we must stop and unload because we are losing access to the context.
+-	 */
+-	if (task == current) {
+-#ifdef CONFIG_SMP
+-		/*
+-		 * the task IS the owner but it migrated to another CPU: that's bad
+-		 * but we must handle this cleanly. Unfortunately, the kernel does
+-		 * not provide a mechanism to block migration (while the context is loaded).
+-		 *
+-		 * We need to release the resource on the ORIGINAL cpu.
+-		 */
+-		if (is_system && ctx->ctx_cpu != smp_processor_id()) {
+-
+-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
+-			/*
+-			 * keep context protected but unmask interrupt for IPI
+-			 */
+-			local_irq_restore(flags);
+-
+-			pfm_syswide_cleanup_other_cpu(ctx);
+-
+-			/*
+-			 * restore interrupt masking
+-			 */
+-			local_irq_save(flags);
+-
+-			/*
+-			 * context is unloaded at this point
+-			 */
+-		} else
+-#endif /* CONFIG_SMP */
+-		{
+-
+-			DPRINT(("forcing unload\n"));
+-			/*
+-		 	* stop and unload, returning with state UNLOADED
+-		 	* and session unreserved.
+-		 	*/
+-			pfm_context_unload(ctx, NULL, 0, regs);
+-
+-			DPRINT(("ctx_state=%d\n", ctx->ctx_state));
+-		}
+-	}
+-
+-	/*
+-	 * remove virtual mapping, if any, for the calling task.
+-	 * cannot reset ctx field until last user is calling close().
+-	 *
+-	 * ctx_smpl_vaddr must never be cleared because it is needed
+-	 * by every task with access to the context
+-	 *
+-	 * When called from do_exit(), the mm context is gone already, therefore
+-	 * mm is NULL, i.e., the VMA is already gone  and we do not have to
+-	 * do anything here
+-	 */
+-	if (ctx->ctx_smpl_vaddr && current->mm) {
+-		smpl_buf_vaddr = ctx->ctx_smpl_vaddr;
+-		smpl_buf_size  = ctx->ctx_smpl_size;
+-	}
+-
+-	UNPROTECT_CTX(ctx, flags);
+-
+-	/*
+-	 * if there was a mapping, then we systematically remove it
+-	 * at this point. Cannot be done inside critical section
+-	 * because some VM function reenables interrupts.
+-	 *
+-	 */
+-	if (smpl_buf_vaddr) pfm_remove_smpl_mapping(current, smpl_buf_vaddr, smpl_buf_size);
+-
+-	return 0;
+-}
+-/*
+- * called either on explicit close() or from exit_files(). 
+- * Only the LAST user of the file gets to this point, i.e., it is
+- * called only ONCE.
+- *
+- * IMPORTANT: we get called ONLY when the refcnt on the file gets to zero 
+- * (fput()),i.e, last task to access the file. Nobody else can access the 
+- * file at this point.
+- *
+- * When called from exit_files(), the VMA has been freed because exit_mm()
+- * is executed before exit_files().
+- *
+- * When called from exit_files(), the current task is not yet ZOMBIE but we
+- * flush the PMU state to the context. 
+- */
+-static int
+-pfm_close(struct inode *inode, struct file *filp)
+-{
+-	pfm_context_t *ctx;
+-	struct task_struct *task;
+-	struct pt_regs *regs;
+-  	DECLARE_WAITQUEUE(wait, current);
+-	unsigned long flags;
+-	unsigned long smpl_buf_size = 0UL;
+-	void *smpl_buf_addr = NULL;
+-	int free_possible = 1;
+-	int state, is_system;
+-
+-	DPRINT(("pfm_close called private=%p\n", filp->private_data));
+-
+-	if (PFM_IS_FILE(filp) == 0) {
+-		DPRINT(("bad magic\n"));
+-		return -EBADF;
+-	}
+-	
+-	ctx = (pfm_context_t *)filp->private_data;
+-	if (ctx == NULL) {
+-		printk(KERN_ERR "perfmon: pfm_close: NULL ctx [%d]\n", task_pid_nr(current));
+-		return -EBADF;
+-	}
+-
+-	PROTECT_CTX(ctx, flags);
+-
+-	state     = ctx->ctx_state;
+-	is_system = ctx->ctx_fl_system;
+-
+-	task = PFM_CTX_TASK(ctx);
+-	regs = task_pt_regs(task);
+-
+-	DPRINT(("ctx_state=%d is_current=%d\n", 
+-		state,
+-		task == current ? 1 : 0));
+-
+-	/*
+-	 * if task == current, then pfm_flush() unloaded the context
+-	 */
+-	if (state == PFM_CTX_UNLOADED) goto doit;
+-
+-	/*
+-	 * context is loaded/masked and task != current, we need to
+-	 * either force an unload or go zombie
+-	 */
+-
+-	/*
+-	 * The task is currently blocked or will block after an overflow.
+-	 * we must force it to wakeup to get out of the
+-	 * MASKED state and transition to the unloaded state by itself.
+-	 *
+-	 * This situation is only possible for per-task mode
+-	 */
+-	if (state == PFM_CTX_MASKED && CTX_OVFL_NOBLOCK(ctx) == 0) {
+-
+-		/*
+-		 * set a "partial" zombie state to be checked
+-		 * upon return from down() in pfm_handle_work().
+-		 *
+-		 * We cannot use the ZOMBIE state, because it is checked
+-		 * by pfm_load_regs() which is called upon wakeup from down().
+-		 * In such case, it would free the context and then we would
+-		 * return to pfm_handle_work() which would access the
+-		 * stale context. Instead, we set a flag invisible to pfm_load_regs()
+-		 * but visible to pfm_handle_work().
+-		 *
+-		 * For some window of time, we have a zombie context with
+-		 * ctx_state = MASKED  and not ZOMBIE
+-		 */
+-		ctx->ctx_fl_going_zombie = 1;
+-
+-		/*
+-		 * force task to wake up from MASKED state
+-		 */
+-		complete(&ctx->ctx_restart_done);
+-
+-		DPRINT(("waking up ctx_state=%d\n", state));
+-
+-		/*
+-		 * put ourself to sleep waiting for the other
+-		 * task to report completion
+-		 *
+-		 * the context is protected by mutex, therefore there
+-		 * is no risk of being notified of completion before
+-		 * begin actually on the waitq.
+-		 */
+-  		set_current_state(TASK_INTERRUPTIBLE);
+-  		add_wait_queue(&ctx->ctx_zombieq, &wait);
+-
+-		UNPROTECT_CTX(ctx, flags);
+-
+-		/*
+-		 * XXX: check for signals :
+-		 * 	- ok for explicit close
+-		 * 	- not ok when coming from exit_files()
+-		 */
+-      		schedule();
+-
+-
+-		PROTECT_CTX(ctx, flags);
+-
+-
+-		remove_wait_queue(&ctx->ctx_zombieq, &wait);
+-  		set_current_state(TASK_RUNNING);
+-
+-		/*
+-		 * context is unloaded at this point
+-		 */
+-		DPRINT(("after zombie wakeup ctx_state=%d for\n", state));
+-	}
+-	else if (task != current) {
+-#ifdef CONFIG_SMP
+-		/*
+-	 	 * switch context to zombie state
+-	 	 */
+-		ctx->ctx_state = PFM_CTX_ZOMBIE;
+-
+-		DPRINT(("zombie ctx for [%d]\n", task_pid_nr(task)));
+-		/*
+-		 * cannot free the context on the spot. deferred until
+-		 * the task notices the ZOMBIE state
+-		 */
+-		free_possible = 0;
+-#else
+-		pfm_context_unload(ctx, NULL, 0, regs);
+-#endif
+-	}
+-
+-doit:
+-	/* reload state, may have changed during  opening of critical section */
+-	state = ctx->ctx_state;
+-
+-	/*
+-	 * the context is still attached to a task (possibly current)
+-	 * we cannot destroy it right now
+-	 */
+-
+-	/*
+-	 * we must free the sampling buffer right here because
+-	 * we cannot rely on it being cleaned up later by the
+-	 * monitored task. It is not possible to free vmalloc'ed
+-	 * memory in pfm_load_regs(). Instead, we remove the buffer
+-	 * now. should there be subsequent PMU overflow originally
+-	 * meant for sampling, the will be converted to spurious
+-	 * and that's fine because the monitoring tools is gone anyway.
+-	 */
+-	if (ctx->ctx_smpl_hdr) {
+-		smpl_buf_addr = ctx->ctx_smpl_hdr;
+-		smpl_buf_size = ctx->ctx_smpl_size;
+-		/* no more sampling */
+-		ctx->ctx_smpl_hdr = NULL;
+-		ctx->ctx_fl_is_sampling = 0;
+-	}
+-
+-	DPRINT(("ctx_state=%d free_possible=%d addr=%p size=%lu\n",
+-		state,
+-		free_possible,
+-		smpl_buf_addr,
+-		smpl_buf_size));
+-
+-	if (smpl_buf_addr) pfm_exit_smpl_buffer(ctx->ctx_buf_fmt);
+-
+-	/*
+-	 * UNLOADED that the session has already been unreserved.
+-	 */
+-	if (state == PFM_CTX_ZOMBIE) {
+-		pfm_unreserve_session(ctx, ctx->ctx_fl_system , ctx->ctx_cpu);
+-	}
+-
+-	/*
+-	 * disconnect file descriptor from context must be done
+-	 * before we unlock.
+-	 */
+-	filp->private_data = NULL;
+-
+-	/*
+-	 * if we free on the spot, the context is now completely unreachable
+-	 * from the callers side. The monitored task side is also cut, so we
+-	 * can freely cut.
+-	 *
+-	 * If we have a deferred free, only the caller side is disconnected.
+-	 */
+-	UNPROTECT_CTX(ctx, flags);
+-
+-	/*
+-	 * All memory free operations (especially for vmalloc'ed memory)
+-	 * MUST be done with interrupts ENABLED.
+-	 */
+-	if (smpl_buf_addr)  pfm_rvfree(smpl_buf_addr, smpl_buf_size);
+-
+-	/*
+-	 * return the memory used by the context
+-	 */
+-	if (free_possible) pfm_context_free(ctx);
+-
+-	return 0;
+-}
+-
+-static int
+-pfm_no_open(struct inode *irrelevant, struct file *dontcare)
+-{
+-	DPRINT(("pfm_no_open called\n"));
+-	return -ENXIO;
+-}
+-
+-
+-
+-static const struct file_operations pfm_file_ops = {
+-	.llseek   = no_llseek,
+-	.read     = pfm_read,
+-	.write    = pfm_write,
+-	.poll     = pfm_poll,
+-	.ioctl    = pfm_ioctl,
+-	.open     = pfm_no_open,	/* special open code to disallow open via /proc */
+-	.fasync   = pfm_fasync,
+-	.release  = pfm_close,
+-	.flush	  = pfm_flush
+-};
+-
+-static int
+-pfmfs_delete_dentry(struct dentry *dentry)
+-{
+-	return 1;
+-}
+-
+-static struct dentry_operations pfmfs_dentry_operations = {
+-	.d_delete = pfmfs_delete_dentry,
+-};
+-
+-
+-static int
+-pfm_alloc_fd(struct file **cfile)
+-{
+-	int fd, ret = 0;
+-	struct file *file = NULL;
+-	struct inode * inode;
+-	char name[32];
+-	struct qstr this;
+-
+-	fd = get_unused_fd();
+-	if (fd < 0) return -ENFILE;
+-
+-	ret = -ENFILE;
+-
+-	file = get_empty_filp();
+-	if (!file) goto out;
+-
+-	/*
+-	 * allocate a new inode
+-	 */
+-	inode = new_inode(pfmfs_mnt->mnt_sb);
+-	if (!inode) goto out;
+-
+-	DPRINT(("new inode ino=%ld @%p\n", inode->i_ino, inode));
+-
+-	inode->i_mode = S_IFCHR|S_IRUGO;
+-	inode->i_uid  = current->fsuid;
+-	inode->i_gid  = current->fsgid;
+-
+-	sprintf(name, "[%lu]", inode->i_ino);
+-	this.name = name;
+-	this.len  = strlen(name);
+-	this.hash = inode->i_ino;
+-
+-	ret = -ENOMEM;
+-
+-	/*
+-	 * allocate a new dcache entry
+-	 */
+-	file->f_path.dentry = d_alloc(pfmfs_mnt->mnt_sb->s_root, &this);
+-	if (!file->f_path.dentry) goto out;
+-
+-	file->f_path.dentry->d_op = &pfmfs_dentry_operations;
+-
+-	d_add(file->f_path.dentry, inode);
+-	file->f_path.mnt = mntget(pfmfs_mnt);
+-	file->f_mapping = inode->i_mapping;
+-
+-	file->f_op    = &pfm_file_ops;
+-	file->f_mode  = FMODE_READ;
+-	file->f_flags = O_RDONLY;
+-	file->f_pos   = 0;
+-
+-	/*
+-	 * may have to delay until context is attached?
+-	 */
+-	fd_install(fd, file);
+-
+-	/*
+-	 * the file structure we will use
+-	 */
+-	*cfile = file;
+-
+-	return fd;
+-out:
+-	if (file) put_filp(file);
+-	put_unused_fd(fd);
+-	return ret;
+-}
+-
+-static void
+-pfm_free_fd(int fd, struct file *file)
+-{
+-	struct files_struct *files = current->files;
+-	struct fdtable *fdt;
+-
+-	/* 
+-	 * there ie no fd_uninstall(), so we do it here
+-	 */
+-	spin_lock(&files->file_lock);
+-	fdt = files_fdtable(files);
+-	rcu_assign_pointer(fdt->fd[fd], NULL);
+-	spin_unlock(&files->file_lock);
+-
+-	if (file)
+-		put_filp(file);
+-	put_unused_fd(fd);
+-}
+-
+-static int
+-pfm_remap_buffer(struct vm_area_struct *vma, unsigned long buf, unsigned long addr, unsigned long size)
+-{
+-	DPRINT(("CPU%d buf=0x%lx addr=0x%lx size=%ld\n", smp_processor_id(), buf, addr, size));
+-
+-	while (size > 0) {
+-		unsigned long pfn = ia64_tpa(buf) >> PAGE_SHIFT;
+-
+-
+-		if (remap_pfn_range(vma, addr, pfn, PAGE_SIZE, PAGE_READONLY))
+-			return -ENOMEM;
+-
+-		addr  += PAGE_SIZE;
+-		buf   += PAGE_SIZE;
+-		size  -= PAGE_SIZE;
+-	}
+-	return 0;
+-}
+-
+-/*
+- * allocate a sampling buffer and remaps it into the user address space of the task
+- */
+-static int
+-pfm_smpl_buffer_alloc(struct task_struct *task, struct file *filp, pfm_context_t *ctx, unsigned long rsize, void **user_vaddr)
+-{
+-	struct mm_struct *mm = task->mm;
+-	struct vm_area_struct *vma = NULL;
+-	unsigned long size;
+-	void *smpl_buf;
+-
+-
+-	/*
+-	 * the fixed header + requested size and align to page boundary
+-	 */
+-	size = PAGE_ALIGN(rsize);
+-
+-	DPRINT(("sampling buffer rsize=%lu size=%lu bytes\n", rsize, size));
+-
+-	/*
+-	 * check requested size to avoid Denial-of-service attacks
+-	 * XXX: may have to refine this test
+-	 * Check against address space limit.
+-	 *
+-	 * if ((mm->total_vm << PAGE_SHIFT) + len> task->rlim[RLIMIT_AS].rlim_cur)
+-	 * 	return -ENOMEM;
+-	 */
+-	if (size > task->signal->rlim[RLIMIT_MEMLOCK].rlim_cur)
+-		return -ENOMEM;
+-
+-	/*
+-	 * We do the easy to undo allocations first.
+- 	 *
+-	 * pfm_rvmalloc(), clears the buffer, so there is no leak
+-	 */
+-	smpl_buf = pfm_rvmalloc(size);
+-	if (smpl_buf == NULL) {
+-		DPRINT(("Can't allocate sampling buffer\n"));
+-		return -ENOMEM;
+-	}
+-
+-	DPRINT(("smpl_buf @%p\n", smpl_buf));
+-
+-	/* allocate vma */
+-	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+-	if (!vma) {
+-		DPRINT(("Cannot allocate vma\n"));
+-		goto error_kmem;
+-	}
+-
+-	/*
+-	 * partially initialize the vma for the sampling buffer
+-	 */
+-	vma->vm_mm	     = mm;
+-	vma->vm_file	     = filp;
+-	vma->vm_flags	     = VM_READ| VM_MAYREAD |VM_RESERVED;
+-	vma->vm_page_prot    = PAGE_READONLY; /* XXX may need to change */
+-
+-	/*
+-	 * Now we have everything we need and we can initialize
+-	 * and connect all the data structures
+-	 */
+-
+-	ctx->ctx_smpl_hdr   = smpl_buf;
+-	ctx->ctx_smpl_size  = size; /* aligned size */
+-
+-	/*
+-	 * Let's do the difficult operations next.
+-	 *
+-	 * now we atomically find some area in the address space and
+-	 * remap the buffer in it.
+-	 */
+-	down_write(&task->mm->mmap_sem);
+-
+-	/* find some free area in address space, must have mmap sem held */
+-	vma->vm_start = pfm_get_unmapped_area(NULL, 0, size, 0, MAP_PRIVATE|MAP_ANONYMOUS, 0);
+-	if (vma->vm_start == 0UL) {
+-		DPRINT(("Cannot find unmapped area for size %ld\n", size));
+-		up_write(&task->mm->mmap_sem);
+-		goto error;
+-	}
+-	vma->vm_end = vma->vm_start + size;
+-	vma->vm_pgoff = vma->vm_start >> PAGE_SHIFT;
+-
+-	DPRINT(("aligned size=%ld, hdr=%p mapped @0x%lx\n", size, ctx->ctx_smpl_hdr, vma->vm_start));
+-
+-	/* can only be applied to current task, need to have the mm semaphore held when called */
+-	if (pfm_remap_buffer(vma, (unsigned long)smpl_buf, vma->vm_start, size)) {
+-		DPRINT(("Can't remap buffer\n"));
+-		up_write(&task->mm->mmap_sem);
+-		goto error;
+-	}
+-
+-	get_file(filp);
+-
+-	/*
+-	 * now insert the vma in the vm list for the process, must be
+-	 * done with mmap lock held
+-	 */
+-	insert_vm_struct(mm, vma);
+-
+-	mm->total_vm  += size >> PAGE_SHIFT;
+-	vm_stat_account(vma->vm_mm, vma->vm_flags, vma->vm_file,
+-							vma_pages(vma));
+-	up_write(&task->mm->mmap_sem);
+-
+-	/*
+-	 * keep track of user level virtual address
+-	 */
+-	ctx->ctx_smpl_vaddr = (void *)vma->vm_start;
+-	*(unsigned long *)user_vaddr = vma->vm_start;
+-
+-	return 0;
+-
+-error:
+-	kmem_cache_free(vm_area_cachep, vma);
+-error_kmem:
+-	pfm_rvfree(smpl_buf, size);
+-
+-	return -ENOMEM;
+-}
+-
+-/*
+- * XXX: do something better here
+- */
+-static int
+-pfm_bad_permissions(struct task_struct *task)
+-{
+-	/* inspired by ptrace_attach() */
+-	DPRINT(("cur: uid=%d gid=%d task: euid=%d suid=%d uid=%d egid=%d sgid=%d\n",
+-		current->uid,
+-		current->gid,
+-		task->euid,
+-		task->suid,
+-		task->uid,
+-		task->egid,
+-		task->sgid));
+-
+-	return ((current->uid != task->euid)
+-	    || (current->uid != task->suid)
+-	    || (current->uid != task->uid)
+-	    || (current->gid != task->egid)
+-	    || (current->gid != task->sgid)
+-	    || (current->gid != task->gid)) && !capable(CAP_SYS_PTRACE);
+-}
+-
+-static int
+-pfarg_is_sane(struct task_struct *task, pfarg_context_t *pfx)
+-{
+-	int ctx_flags;
+-
+-	/* valid signal */
+-
+-	ctx_flags = pfx->ctx_flags;
+-
+-	if (ctx_flags & PFM_FL_SYSTEM_WIDE) {
+-
+-		/*
+-		 * cannot block in this mode
+-		 */
+-		if (ctx_flags & PFM_FL_NOTIFY_BLOCK) {
+-			DPRINT(("cannot use blocking mode when in system wide monitoring\n"));
+-			return -EINVAL;
+-		}
+-	} else {
+-	}
+-	/* probably more to add here */
+-
+-	return 0;
+-}
+-
+-static int
+-pfm_setup_buffer_fmt(struct task_struct *task, struct file *filp, pfm_context_t *ctx, unsigned int ctx_flags,
+-		     unsigned int cpu, pfarg_context_t *arg)
+-{
+-	pfm_buffer_fmt_t *fmt = NULL;
+-	unsigned long size = 0UL;
+-	void *uaddr = NULL;
+-	void *fmt_arg = NULL;
+-	int ret = 0;
+-#define PFM_CTXARG_BUF_ARG(a)	(pfm_buffer_fmt_t *)(a+1)
+-
+-	/* invoke and lock buffer format, if found */
+-	fmt = pfm_find_buffer_fmt(arg->ctx_smpl_buf_id);
+-	if (fmt == NULL) {
+-		DPRINT(("[%d] cannot find buffer format\n", task_pid_nr(task)));
+-		return -EINVAL;
+-	}
+-
+-	/*
+-	 * buffer argument MUST be contiguous to pfarg_context_t
+-	 */
+-	if (fmt->fmt_arg_size) fmt_arg = PFM_CTXARG_BUF_ARG(arg);
+-
+-	ret = pfm_buf_fmt_validate(fmt, task, ctx_flags, cpu, fmt_arg);
+-
+-	DPRINT(("[%d] after validate(0x%x,%d,%p)=%d\n", task_pid_nr(task), ctx_flags, cpu, fmt_arg, ret));
+-
+-	if (ret) goto error;
+-
+-	/* link buffer format and context */
+-	ctx->ctx_buf_fmt = fmt;
+-
+-	/*
+-	 * check if buffer format wants to use perfmon buffer allocation/mapping service
+-	 */
+-	ret = pfm_buf_fmt_getsize(fmt, task, ctx_flags, cpu, fmt_arg, &size);
+-	if (ret) goto error;
+-
+-	if (size) {
+-		/*
+-		 * buffer is always remapped into the caller's address space
+-		 */
+-		ret = pfm_smpl_buffer_alloc(current, filp, ctx, size, &uaddr);
+-		if (ret) goto error;
+-
+-		/* keep track of user address of buffer */
+-		arg->ctx_smpl_vaddr = uaddr;
+-	}
+-	ret = pfm_buf_fmt_init(fmt, task, ctx->ctx_smpl_hdr, ctx_flags, cpu, fmt_arg);
+-
+-error:
+-	return ret;
+-}
+-
+-static void
+-pfm_reset_pmu_state(pfm_context_t *ctx)
+-{
+-	int i;
+-
+-	/*
+-	 * install reset values for PMC.
+-	 */
+-	for (i=1; PMC_IS_LAST(i) == 0; i++) {
+-		if (PMC_IS_IMPL(i) == 0) continue;
+-		ctx->ctx_pmcs[i] = PMC_DFL_VAL(i);
+-		DPRINT(("pmc[%d]=0x%lx\n", i, ctx->ctx_pmcs[i]));
+-	}
+-	/*
+-	 * PMD registers are set to 0UL when the context in memset()
+-	 */
+-
+-	/*
+-	 * On context switched restore, we must restore ALL pmc and ALL pmd even
+-	 * when they are not actively used by the task. In UP, the incoming process
+-	 * may otherwise pick up left over PMC, PMD state from the previous process.
+-	 * As opposed to PMD, stale PMC can cause harm to the incoming
+-	 * process because they may change what is being measured.
+-	 * Therefore, we must systematically reinstall the entire
+-	 * PMC state. In SMP, the same thing is possible on the
+-	 * same CPU but also on between 2 CPUs.
+-	 *
+-	 * The problem with PMD is information leaking especially
+-	 * to user level when psr.sp=0
+-	 *
+-	 * There is unfortunately no easy way to avoid this problem
+-	 * on either UP or SMP. This definitively slows down the
+-	 * pfm_load_regs() function.
+-	 */
+-
+-	 /*
+-	  * bitmask of all PMCs accessible to this context
+-	  *
+-	  * PMC0 is treated differently.
+-	  */
+-	ctx->ctx_all_pmcs[0] = pmu_conf->impl_pmcs[0] & ~0x1;
+-
+-	/*
+-	 * bitmask of all PMDs that are accessible to this context
+-	 */
+-	ctx->ctx_all_pmds[0] = pmu_conf->impl_pmds[0];
+-
+-	DPRINT(("<%d> all_pmcs=0x%lx all_pmds=0x%lx\n", ctx->ctx_fd, ctx->ctx_all_pmcs[0],ctx->ctx_all_pmds[0]));
+-
+-	/*
+-	 * useful in case of re-enable after disable
+-	 */
+-	ctx->ctx_used_ibrs[0] = 0UL;
+-	ctx->ctx_used_dbrs[0] = 0UL;
+-}
+-
+-static int
+-pfm_ctx_getsize(void *arg, size_t *sz)
+-{
+-	pfarg_context_t *req = (pfarg_context_t *)arg;
+-	pfm_buffer_fmt_t *fmt;
+-
+-	*sz = 0;
+-
+-	if (!pfm_uuid_cmp(req->ctx_smpl_buf_id, pfm_null_uuid)) return 0;
+-
+-	fmt = pfm_find_buffer_fmt(req->ctx_smpl_buf_id);
+-	if (fmt == NULL) {
+-		DPRINT(("cannot find buffer format\n"));
+-		return -EINVAL;
+-	}
+-	/* get just enough to copy in user parameters */
+-	*sz = fmt->fmt_arg_size;
+-	DPRINT(("arg_size=%lu\n", *sz));
+-
+-	return 0;
+-}
+-
+-
+-
+-/*
+- * cannot attach if :
+- * 	- kernel task
+- * 	- task not owned by caller
+- * 	- task incompatible with context mode
+- */
+-static int
+-pfm_task_incompatible(pfm_context_t *ctx, struct task_struct *task)
+-{
+-	/*
+-	 * no kernel task or task not owner by caller
+-	 */
+-	if (task->mm == NULL) {
+-		DPRINT(("task [%d] has not memory context (kernel thread)\n", task_pid_nr(task)));
+-		return -EPERM;
+-	}
+-	if (pfm_bad_permissions(task)) {
+-		DPRINT(("no permission to attach to  [%d]\n", task_pid_nr(task)));
+-		return -EPERM;
+-	}
+-	/*
+-	 * cannot block in self-monitoring mode
+-	 */
+-	if (CTX_OVFL_NOBLOCK(ctx) == 0 && task == current) {
+-		DPRINT(("cannot load a blocking context on self for [%d]\n", task_pid_nr(task)));
+-		return -EINVAL;
+-	}
+-
+-	if (task->exit_state == EXIT_ZOMBIE) {
+-		DPRINT(("cannot attach to  zombie task [%d]\n", task_pid_nr(task)));
+-		return -EBUSY;
+-	}
+-
+-	/*
+-	 * always ok for self
+-	 */
+-	if (task == current) return 0;
+-
+-	if ((task->state != TASK_STOPPED) && (task->state != TASK_TRACED)) {
+-		DPRINT(("cannot attach to non-stopped task [%d] state=%ld\n", task_pid_nr(task), task->state));
+-		return -EBUSY;
+-	}
+-	/*
+-	 * make sure the task is off any CPU
+-	 */
+-	wait_task_inactive(task);
+-
+-	/* more to come... */
+-
+-	return 0;
+-}
+-
+-static int
+-pfm_get_task(pfm_context_t *ctx, pid_t pid, struct task_struct **task)
+-{
+-	struct task_struct *p = current;
+-	int ret;
+-
+-	/* XXX: need to add more checks here */
+-	if (pid < 2) return -EPERM;
+-
+-	if (pid != current->pid) {
+-
+-		read_lock(&tasklist_lock);
+-
+-		p = find_task_by_pid(pid);
+-
+-		/* make sure task cannot go away while we operate on it */
+-		if (p) get_task_struct(p);
+-
+-		read_unlock(&tasklist_lock);
+-
+-		if (p == NULL) return -ESRCH;
+-	}
+-
+-	ret = pfm_task_incompatible(ctx, p);
+-	if (ret == 0) {
+-		*task = p;
+-	} else if (p != current) {
+-		pfm_put_task(p);
+-	}
+-	return ret;
+-}
+-
+-
+-
+-static int
+-pfm_context_create(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	pfarg_context_t *req = (pfarg_context_t *)arg;
+-	struct file *filp;
+-	int ctx_flags;
+-	int ret;
+-
+-	/* let's check the arguments first */
+-	ret = pfarg_is_sane(current, req);
+-	if (ret < 0) return ret;
+-
+-	ctx_flags = req->ctx_flags;
+-
+-	ret = -ENOMEM;
+-
+-	ctx = pfm_context_alloc();
+-	if (!ctx) goto error;
+-
+-	ret = pfm_alloc_fd(&filp);
+-	if (ret < 0) goto error_file;
+-
+-	req->ctx_fd = ctx->ctx_fd = ret;
+-
+-	/*
+-	 * attach context to file
+-	 */
+-	filp->private_data = ctx;
+-
+-	/*
+-	 * does the user want to sample?
+-	 */
+-	if (pfm_uuid_cmp(req->ctx_smpl_buf_id, pfm_null_uuid)) {
+-		ret = pfm_setup_buffer_fmt(current, filp, ctx, ctx_flags, 0, req);
+-		if (ret) goto buffer_error;
+-	}
+-
+-	/*
+-	 * init context protection lock
+-	 */
+-	spin_lock_init(&ctx->ctx_lock);
+-
+-	/*
+-	 * context is unloaded
+-	 */
+-	ctx->ctx_state = PFM_CTX_UNLOADED;
+-
+-	/*
+-	 * initialization of context's flags
+-	 */
+-	ctx->ctx_fl_block       = (ctx_flags & PFM_FL_NOTIFY_BLOCK) ? 1 : 0;
+-	ctx->ctx_fl_system      = (ctx_flags & PFM_FL_SYSTEM_WIDE) ? 1: 0;
+-	ctx->ctx_fl_is_sampling = ctx->ctx_buf_fmt ? 1 : 0; /* assume record() is defined */
+-	ctx->ctx_fl_no_msg      = (ctx_flags & PFM_FL_OVFL_NO_MSG) ? 1: 0;
+-	/*
+-	 * will move to set properties
+-	 * ctx->ctx_fl_excl_idle   = (ctx_flags & PFM_FL_EXCL_IDLE) ? 1: 0;
+-	 */
+-
+-	/*
+-	 * init restart semaphore to locked
+-	 */
+-	init_completion(&ctx->ctx_restart_done);
+-
+-	/*
+-	 * activation is used in SMP only
+-	 */
+-	ctx->ctx_last_activation = PFM_INVALID_ACTIVATION;
+-	SET_LAST_CPU(ctx, -1);
+-
+-	/*
+-	 * initialize notification message queue
+-	 */
+-	ctx->ctx_msgq_head = ctx->ctx_msgq_tail = 0;
+-	init_waitqueue_head(&ctx->ctx_msgq_wait);
+-	init_waitqueue_head(&ctx->ctx_zombieq);
+-
+-	DPRINT(("ctx=%p flags=0x%x system=%d notify_block=%d excl_idle=%d no_msg=%d ctx_fd=%d \n",
+-		ctx,
+-		ctx_flags,
+-		ctx->ctx_fl_system,
+-		ctx->ctx_fl_block,
+-		ctx->ctx_fl_excl_idle,
+-		ctx->ctx_fl_no_msg,
+-		ctx->ctx_fd));
+-
+-	/*
+-	 * initialize soft PMU state
+-	 */
+-	pfm_reset_pmu_state(ctx);
+-
+-	return 0;
+-
+-buffer_error:
+-	pfm_free_fd(ctx->ctx_fd, filp);
+-
+-	if (ctx->ctx_buf_fmt) {
+-		pfm_buf_fmt_exit(ctx->ctx_buf_fmt, current, NULL, regs);
+-	}
+-error_file:
+-	pfm_context_free(ctx);
+-
+-error:
+-	return ret;
+-}
+-
+-static inline unsigned long
+-pfm_new_counter_value (pfm_counter_t *reg, int is_long_reset)
+-{
+-	unsigned long val = is_long_reset ? reg->long_reset : reg->short_reset;
+-	unsigned long new_seed, old_seed = reg->seed, mask = reg->mask;
+-	extern unsigned long carta_random32 (unsigned long seed);
+-
+-	if (reg->flags & PFM_REGFL_RANDOM) {
+-		new_seed = carta_random32(old_seed);
+-		val -= (old_seed & mask);	/* counter values are negative numbers! */
+-		if ((mask >> 32) != 0)
+-			/* construct a full 64-bit random value: */
+-			new_seed |= carta_random32(old_seed >> 32) << 32;
+-		reg->seed = new_seed;
+-	}
+-	reg->lval = val;
+-	return val;
+-}
+-
+-static void
+-pfm_reset_regs_masked(pfm_context_t *ctx, unsigned long *ovfl_regs, int is_long_reset)
+-{
+-	unsigned long mask = ovfl_regs[0];
+-	unsigned long reset_others = 0UL;
+-	unsigned long val;
+-	int i;
+-
+-	/*
+-	 * now restore reset value on sampling overflowed counters
+-	 */
+-	mask >>= PMU_FIRST_COUNTER;
+-	for(i = PMU_FIRST_COUNTER; mask; i++, mask >>= 1) {
+-
+-		if ((mask & 0x1UL) == 0UL) continue;
+-
+-		ctx->ctx_pmds[i].val = val = pfm_new_counter_value(ctx->ctx_pmds+ i, is_long_reset);
+-		reset_others        |= ctx->ctx_pmds[i].reset_pmds[0];
+-
+-		DPRINT_ovfl((" %s reset ctx_pmds[%d]=%lx\n", is_long_reset ? "long" : "short", i, val));
+-	}
+-
+-	/*
+-	 * Now take care of resetting the other registers
+-	 */
+-	for(i = 0; reset_others; i++, reset_others >>= 1) {
+-
+-		if ((reset_others & 0x1) == 0) continue;
+-
+-		ctx->ctx_pmds[i].val = val = pfm_new_counter_value(ctx->ctx_pmds + i, is_long_reset);
+-
+-		DPRINT_ovfl(("%s reset_others pmd[%d]=%lx\n",
+-			  is_long_reset ? "long" : "short", i, val));
+-	}
+-}
+-
+-static void
+-pfm_reset_regs(pfm_context_t *ctx, unsigned long *ovfl_regs, int is_long_reset)
+-{
+-	unsigned long mask = ovfl_regs[0];
+-	unsigned long reset_others = 0UL;
+-	unsigned long val;
+-	int i;
+-
+-	DPRINT_ovfl(("ovfl_regs=0x%lx is_long_reset=%d\n", ovfl_regs[0], is_long_reset));
+-
+-	if (ctx->ctx_state == PFM_CTX_MASKED) {
+-		pfm_reset_regs_masked(ctx, ovfl_regs, is_long_reset);
+-		return;
+-	}
+-
+-	/*
+-	 * now restore reset value on sampling overflowed counters
+-	 */
+-	mask >>= PMU_FIRST_COUNTER;
+-	for(i = PMU_FIRST_COUNTER; mask; i++, mask >>= 1) {
+-
+-		if ((mask & 0x1UL) == 0UL) continue;
+-
+-		val           = pfm_new_counter_value(ctx->ctx_pmds+ i, is_long_reset);
+-		reset_others |= ctx->ctx_pmds[i].reset_pmds[0];
+-
+-		DPRINT_ovfl((" %s reset ctx_pmds[%d]=%lx\n", is_long_reset ? "long" : "short", i, val));
+-
+-		pfm_write_soft_counter(ctx, i, val);
+-	}
+-
+-	/*
+-	 * Now take care of resetting the other registers
+-	 */
+-	for(i = 0; reset_others; i++, reset_others >>= 1) {
+-
+-		if ((reset_others & 0x1) == 0) continue;
+-
+-		val = pfm_new_counter_value(ctx->ctx_pmds + i, is_long_reset);
+-
+-		if (PMD_IS_COUNTING(i)) {
+-			pfm_write_soft_counter(ctx, i, val);
+-		} else {
+-			ia64_set_pmd(i, val);
+-		}
+-		DPRINT_ovfl(("%s reset_others pmd[%d]=%lx\n",
+-			  is_long_reset ? "long" : "short", i, val));
+-	}
+-	ia64_srlz_d();
+-}
+-
+-static int
+-pfm_write_pmcs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct task_struct *task;
+-	pfarg_reg_t *req = (pfarg_reg_t *)arg;
+-	unsigned long value, pmc_pm;
+-	unsigned long smpl_pmds, reset_pmds, impl_pmds;
+-	unsigned int cnum, reg_flags, flags, pmc_type;
+-	int i, can_access_pmu = 0, is_loaded, is_system, expert_mode;
+-	int is_monitor, is_counting, state;
+-	int ret = -EINVAL;
+-	pfm_reg_check_t	wr_func;
+-#define PFM_CHECK_PMC_PM(x, y, z) ((x)->ctx_fl_system ^ PMC_PM(y, z))
+-
+-	state     = ctx->ctx_state;
+-	is_loaded = state == PFM_CTX_LOADED ? 1 : 0;
+-	is_system = ctx->ctx_fl_system;
+-	task      = ctx->ctx_task;
+-	impl_pmds = pmu_conf->impl_pmds[0];
+-
+-	if (state == PFM_CTX_ZOMBIE) return -EINVAL;
+-
+-	if (is_loaded) {
+-		/*
+-		 * In system wide and when the context is loaded, access can only happen
+-		 * when the caller is running on the CPU being monitored by the session.
+-		 * It does not have to be the owner (ctx_task) of the context per se.
+-		 */
+-		if (is_system && ctx->ctx_cpu != smp_processor_id()) {
+-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
+-			return -EBUSY;
+-		}
+-		can_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;
+-	}
+-	expert_mode = pfm_sysctl.expert_mode; 
+-
+-	for (i = 0; i < count; i++, req++) {
+-
+-		cnum       = req->reg_num;
+-		reg_flags  = req->reg_flags;
+-		value      = req->reg_value;
+-		smpl_pmds  = req->reg_smpl_pmds[0];
+-		reset_pmds = req->reg_reset_pmds[0];
+-		flags      = 0;
+-
+-
+-		if (cnum >= PMU_MAX_PMCS) {
+-			DPRINT(("pmc%u is invalid\n", cnum));
+-			goto error;
+-		}
+-
+-		pmc_type   = pmu_conf->pmc_desc[cnum].type;
+-		pmc_pm     = (value >> pmu_conf->pmc_desc[cnum].pm_pos) & 0x1;
+-		is_counting = (pmc_type & PFM_REG_COUNTING) == PFM_REG_COUNTING ? 1 : 0;
+-		is_monitor  = (pmc_type & PFM_REG_MONITOR) == PFM_REG_MONITOR ? 1 : 0;
+-
+-		/*
+-		 * we reject all non implemented PMC as well
+-		 * as attempts to modify PMC[0-3] which are used
+-		 * as status registers by the PMU
+-		 */
+-		if ((pmc_type & PFM_REG_IMPL) == 0 || (pmc_type & PFM_REG_CONTROL) == PFM_REG_CONTROL) {
+-			DPRINT(("pmc%u is unimplemented or no-access pmc_type=%x\n", cnum, pmc_type));
+-			goto error;
+-		}
+-		wr_func = pmu_conf->pmc_desc[cnum].write_check;
+-		/*
+-		 * If the PMC is a monitor, then if the value is not the default:
+-		 * 	- system-wide session: PMCx.pm=1 (privileged monitor)
+-		 * 	- per-task           : PMCx.pm=0 (user monitor)
+-		 */
+-		if (is_monitor && value != PMC_DFL_VAL(cnum) && is_system ^ pmc_pm) {
+-			DPRINT(("pmc%u pmc_pm=%lu is_system=%d\n",
+-				cnum,
+-				pmc_pm,
+-				is_system));
+-			goto error;
+-		}
+-
+-		if (is_counting) {
+-			/*
+-		 	 * enforce generation of overflow interrupt. Necessary on all
+-		 	 * CPUs.
+-		 	 */
+-			value |= 1 << PMU_PMC_OI;
+-
+-			if (reg_flags & PFM_REGFL_OVFL_NOTIFY) {
+-				flags |= PFM_REGFL_OVFL_NOTIFY;
+-			}
+-
+-			if (reg_flags & PFM_REGFL_RANDOM) flags |= PFM_REGFL_RANDOM;
+-
+-			/* verify validity of smpl_pmds */
+-			if ((smpl_pmds & impl_pmds) != smpl_pmds) {
+-				DPRINT(("invalid smpl_pmds 0x%lx for pmc%u\n", smpl_pmds, cnum));
+-				goto error;
+-			}
+-
+-			/* verify validity of reset_pmds */
+-			if ((reset_pmds & impl_pmds) != reset_pmds) {
+-				DPRINT(("invalid reset_pmds 0x%lx for pmc%u\n", reset_pmds, cnum));
+-				goto error;
+-			}
+-		} else {
+-			if (reg_flags & (PFM_REGFL_OVFL_NOTIFY|PFM_REGFL_RANDOM)) {
+-				DPRINT(("cannot set ovfl_notify or random on pmc%u\n", cnum));
+-				goto error;
+-			}
+-			/* eventid on non-counting monitors are ignored */
+-		}
+-
+-		/*
+-		 * execute write checker, if any
+-		 */
+-		if (likely(expert_mode == 0 && wr_func)) {
+-			ret = (*wr_func)(task, ctx, cnum, &value, regs);
+-			if (ret) goto error;
+-			ret = -EINVAL;
+-		}
+-
+-		/*
+-		 * no error on this register
+-		 */
+-		PFM_REG_RETFLAG_SET(req->reg_flags, 0);
+-
+-		/*
+-		 * Now we commit the changes to the software state
+-		 */
+-
+-		/*
+-		 * update overflow information
+-		 */
+-		if (is_counting) {
+-			/*
+-		 	 * full flag update each time a register is programmed
+-		 	 */
+-			ctx->ctx_pmds[cnum].flags = flags;
+-
+-			ctx->ctx_pmds[cnum].reset_pmds[0] = reset_pmds;
+-			ctx->ctx_pmds[cnum].smpl_pmds[0]  = smpl_pmds;
+-			ctx->ctx_pmds[cnum].eventid       = req->reg_smpl_eventid;
+-
+-			/*
+-			 * Mark all PMDS to be accessed as used.
+-			 *
+-			 * We do not keep track of PMC because we have to
+-			 * systematically restore ALL of them.
+-			 *
+-			 * We do not update the used_monitors mask, because
+-			 * if we have not programmed them, then will be in
+-			 * a quiescent state, therefore we will not need to
+-			 * mask/restore then when context is MASKED.
+-			 */
+-			CTX_USED_PMD(ctx, reset_pmds);
+-			CTX_USED_PMD(ctx, smpl_pmds);
+-			/*
+-		 	 * make sure we do not try to reset on
+-		 	 * restart because we have established new values
+-		 	 */
+-			if (state == PFM_CTX_MASKED) ctx->ctx_ovfl_regs[0] &= ~1UL << cnum;
+-		}
+-		/*
+-		 * Needed in case the user does not initialize the equivalent
+-		 * PMD. Clearing is done indirectly via pfm_reset_pmu_state() so there is no
+-		 * possible leak here.
+-		 */
+-		CTX_USED_PMD(ctx, pmu_conf->pmc_desc[cnum].dep_pmd[0]);
+-
+-		/*
+-		 * keep track of the monitor PMC that we are using.
+-		 * we save the value of the pmc in ctx_pmcs[] and if
+-		 * the monitoring is not stopped for the context we also
+-		 * place it in the saved state area so that it will be
+-		 * picked up later by the context switch code.
+-		 *
+-		 * The value in ctx_pmcs[] can only be changed in pfm_write_pmcs().
+-		 *
+-		 * The value in th_pmcs[] may be modified on overflow, i.e.,  when
+-		 * monitoring needs to be stopped.
+-		 */
+-		if (is_monitor) CTX_USED_MONITOR(ctx, 1UL << cnum);
+-
+-		/*
+-		 * update context state
+-		 */
+-		ctx->ctx_pmcs[cnum] = value;
+-
+-		if (is_loaded) {
+-			/*
+-			 * write thread state
+-			 */
+-			if (is_system == 0) ctx->th_pmcs[cnum] = value;
+-
+-			/*
+-			 * write hardware register if we can
+-			 */
+-			if (can_access_pmu) {
+-				ia64_set_pmc(cnum, value);
+-			}
+-#ifdef CONFIG_SMP
+-			else {
+-				/*
+-				 * per-task SMP only here
+-				 *
+-			 	 * we are guaranteed that the task is not running on the other CPU,
+-			 	 * we indicate that this PMD will need to be reloaded if the task
+-			 	 * is rescheduled on the CPU it ran last on.
+-			 	 */
+-				ctx->ctx_reload_pmcs[0] |= 1UL << cnum;
+-			}
+-#endif
+-		}
+-
+-		DPRINT(("pmc[%u]=0x%lx ld=%d apmu=%d flags=0x%x all_pmcs=0x%lx used_pmds=0x%lx eventid=%ld smpl_pmds=0x%lx reset_pmds=0x%lx reloads_pmcs=0x%lx used_monitors=0x%lx ovfl_regs=0x%lx\n",
+-			  cnum,
+-			  value,
+-			  is_loaded,
+-			  can_access_pmu,
+-			  flags,
+-			  ctx->ctx_all_pmcs[0],
+-			  ctx->ctx_used_pmds[0],
+-			  ctx->ctx_pmds[cnum].eventid,
+-			  smpl_pmds,
+-			  reset_pmds,
+-			  ctx->ctx_reload_pmcs[0],
+-			  ctx->ctx_used_monitors[0],
+-			  ctx->ctx_ovfl_regs[0]));
+-	}
+-
+-	/*
+-	 * make sure the changes are visible
+-	 */
+-	if (can_access_pmu) ia64_srlz_d();
+-
+-	return 0;
+-error:
+-	PFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);
+-	return ret;
+-}
+-
+-static int
+-pfm_write_pmds(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct task_struct *task;
+-	pfarg_reg_t *req = (pfarg_reg_t *)arg;
+-	unsigned long value, hw_value, ovfl_mask;
+-	unsigned int cnum;
+-	int i, can_access_pmu = 0, state;
+-	int is_counting, is_loaded, is_system, expert_mode;
+-	int ret = -EINVAL;
+-	pfm_reg_check_t wr_func;
+-
+-
+-	state     = ctx->ctx_state;
+-	is_loaded = state == PFM_CTX_LOADED ? 1 : 0;
+-	is_system = ctx->ctx_fl_system;
+-	ovfl_mask = pmu_conf->ovfl_val;
+-	task      = ctx->ctx_task;
+-
+-	if (unlikely(state == PFM_CTX_ZOMBIE)) return -EINVAL;
+-
+-	/*
+-	 * on both UP and SMP, we can only write to the PMC when the task is
+-	 * the owner of the local PMU.
+-	 */
+-	if (likely(is_loaded)) {
+-		/*
+-		 * In system wide and when the context is loaded, access can only happen
+-		 * when the caller is running on the CPU being monitored by the session.
+-		 * It does not have to be the owner (ctx_task) of the context per se.
+-		 */
+-		if (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {
+-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
+-			return -EBUSY;
+-		}
+-		can_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;
+-	}
+-	expert_mode = pfm_sysctl.expert_mode; 
+-
+-	for (i = 0; i < count; i++, req++) {
+-
+-		cnum  = req->reg_num;
+-		value = req->reg_value;
+-
+-		if (!PMD_IS_IMPL(cnum)) {
+-			DPRINT(("pmd[%u] is unimplemented or invalid\n", cnum));
+-			goto abort_mission;
+-		}
+-		is_counting = PMD_IS_COUNTING(cnum);
+-		wr_func     = pmu_conf->pmd_desc[cnum].write_check;
+-
+-		/*
+-		 * execute write checker, if any
+-		 */
+-		if (unlikely(expert_mode == 0 && wr_func)) {
+-			unsigned long v = value;
+-
+-			ret = (*wr_func)(task, ctx, cnum, &v, regs);
+-			if (ret) goto abort_mission;
+-
+-			value = v;
+-			ret   = -EINVAL;
+-		}
+-
+-		/*
+-		 * no error on this register
+-		 */
+-		PFM_REG_RETFLAG_SET(req->reg_flags, 0);
+-
+-		/*
+-		 * now commit changes to software state
+-		 */
+-		hw_value = value;
+-
+-		/*
+-		 * update virtualized (64bits) counter
+-		 */
+-		if (is_counting) {
+-			/*
+-			 * write context state
+-			 */
+-			ctx->ctx_pmds[cnum].lval = value;
+-
+-			/*
+-			 * when context is load we use the split value
+-			 */
+-			if (is_loaded) {
+-				hw_value = value &  ovfl_mask;
+-				value    = value & ~ovfl_mask;
+-			}
+-		}
+-		/*
+-		 * update reset values (not just for counters)
+-		 */
+-		ctx->ctx_pmds[cnum].long_reset  = req->reg_long_reset;
+-		ctx->ctx_pmds[cnum].short_reset = req->reg_short_reset;
+-
+-		/*
+-		 * update randomization parameters (not just for counters)
+-		 */
+-		ctx->ctx_pmds[cnum].seed = req->reg_random_seed;
+-		ctx->ctx_pmds[cnum].mask = req->reg_random_mask;
+-
+-		/*
+-		 * update context value
+-		 */
+-		ctx->ctx_pmds[cnum].val  = value;
+-
+-		/*
+-		 * Keep track of what we use
+-		 *
+-		 * We do not keep track of PMC because we have to
+-		 * systematically restore ALL of them.
+-		 */
+-		CTX_USED_PMD(ctx, PMD_PMD_DEP(cnum));
+-
+-		/*
+-		 * mark this PMD register used as well
+-		 */
+-		CTX_USED_PMD(ctx, RDEP(cnum));
+-
+-		/*
+-		 * make sure we do not try to reset on
+-		 * restart because we have established new values
+-		 */
+-		if (is_counting && state == PFM_CTX_MASKED) {
+-			ctx->ctx_ovfl_regs[0] &= ~1UL << cnum;
+-		}
+-
+-		if (is_loaded) {
+-			/*
+-		 	 * write thread state
+-		 	 */
+-			if (is_system == 0) ctx->th_pmds[cnum] = hw_value;
+-
+-			/*
+-			 * write hardware register if we can
+-			 */
+-			if (can_access_pmu) {
+-				ia64_set_pmd(cnum, hw_value);
+-			} else {
+-#ifdef CONFIG_SMP
+-				/*
+-			 	 * we are guaranteed that the task is not running on the other CPU,
+-			 	 * we indicate that this PMD will need to be reloaded if the task
+-			 	 * is rescheduled on the CPU it ran last on.
+-			 	 */
+-				ctx->ctx_reload_pmds[0] |= 1UL << cnum;
+-#endif
+-			}
+-		}
+-
+-		DPRINT(("pmd[%u]=0x%lx ld=%d apmu=%d, hw_value=0x%lx ctx_pmd=0x%lx  short_reset=0x%lx "
+-			  "long_reset=0x%lx notify=%c seed=0x%lx mask=0x%lx used_pmds=0x%lx reset_pmds=0x%lx reload_pmds=0x%lx all_pmds=0x%lx ovfl_regs=0x%lx\n",
+-			cnum,
+-			value,
+-			is_loaded,
+-			can_access_pmu,
+-			hw_value,
+-			ctx->ctx_pmds[cnum].val,
+-			ctx->ctx_pmds[cnum].short_reset,
+-			ctx->ctx_pmds[cnum].long_reset,
+-			PMC_OVFL_NOTIFY(ctx, cnum) ? 'Y':'N',
+-			ctx->ctx_pmds[cnum].seed,
+-			ctx->ctx_pmds[cnum].mask,
+-			ctx->ctx_used_pmds[0],
+-			ctx->ctx_pmds[cnum].reset_pmds[0],
+-			ctx->ctx_reload_pmds[0],
+-			ctx->ctx_all_pmds[0],
+-			ctx->ctx_ovfl_regs[0]));
+-	}
+-
+-	/*
+-	 * make changes visible
+-	 */
+-	if (can_access_pmu) ia64_srlz_d();
+-
+-	return 0;
+-
+-abort_mission:
+-	/*
+-	 * for now, we have only one possibility for error
+-	 */
+-	PFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);
+-	return ret;
+-}
+-
+-/*
+- * By the way of PROTECT_CONTEXT(), interrupts are masked while we are in this function.
+- * Therefore we know, we do not have to worry about the PMU overflow interrupt. If an
+- * interrupt is delivered during the call, it will be kept pending until we leave, making
+- * it appears as if it had been generated at the UNPROTECT_CONTEXT(). At least we are
+- * guaranteed to return consistent data to the user, it may simply be old. It is not
+- * trivial to treat the overflow while inside the call because you may end up in
+- * some module sampling buffer code causing deadlocks.
+- */
+-static int
+-pfm_read_pmds(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct task_struct *task;
+-	unsigned long val = 0UL, lval, ovfl_mask, sval;
+-	pfarg_reg_t *req = (pfarg_reg_t *)arg;
+-	unsigned int cnum, reg_flags = 0;
+-	int i, can_access_pmu = 0, state;
+-	int is_loaded, is_system, is_counting, expert_mode;
+-	int ret = -EINVAL;
+-	pfm_reg_check_t rd_func;
+-
+-	/*
+-	 * access is possible when loaded only for
+-	 * self-monitoring tasks or in UP mode
+-	 */
+-
+-	state     = ctx->ctx_state;
+-	is_loaded = state == PFM_CTX_LOADED ? 1 : 0;
+-	is_system = ctx->ctx_fl_system;
+-	ovfl_mask = pmu_conf->ovfl_val;
+-	task      = ctx->ctx_task;
+-
+-	if (state == PFM_CTX_ZOMBIE) return -EINVAL;
+-
+-	if (likely(is_loaded)) {
+-		/*
+-		 * In system wide and when the context is loaded, access can only happen
+-		 * when the caller is running on the CPU being monitored by the session.
+-		 * It does not have to be the owner (ctx_task) of the context per se.
+-		 */
+-		if (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {
+-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
+-			return -EBUSY;
+-		}
+-		/*
+-		 * this can be true when not self-monitoring only in UP
+-		 */
+-		can_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;
+-
+-		if (can_access_pmu) ia64_srlz_d();
+-	}
+-	expert_mode = pfm_sysctl.expert_mode; 
+-
+-	DPRINT(("ld=%d apmu=%d ctx_state=%d\n",
+-		is_loaded,
+-		can_access_pmu,
+-		state));
+-
+-	/*
+-	 * on both UP and SMP, we can only read the PMD from the hardware register when
+-	 * the task is the owner of the local PMU.
+-	 */
+-
+-	for (i = 0; i < count; i++, req++) {
+-
+-		cnum        = req->reg_num;
+-		reg_flags   = req->reg_flags;
+-
+-		if (unlikely(!PMD_IS_IMPL(cnum))) goto error;
+-		/*
+-		 * we can only read the register that we use. That includes
+-		 * the one we explicitly initialize AND the one we want included
+-		 * in the sampling buffer (smpl_regs).
+-		 *
+-		 * Having this restriction allows optimization in the ctxsw routine
+-		 * without compromising security (leaks)
+-		 */
+-		if (unlikely(!CTX_IS_USED_PMD(ctx, cnum))) goto error;
+-
+-		sval        = ctx->ctx_pmds[cnum].val;
+-		lval        = ctx->ctx_pmds[cnum].lval;
+-		is_counting = PMD_IS_COUNTING(cnum);
+-
+-		/*
+-		 * If the task is not the current one, then we check if the
+-		 * PMU state is still in the local live register due to lazy ctxsw.
+-		 * If true, then we read directly from the registers.
+-		 */
+-		if (can_access_pmu){
+-			val = ia64_get_pmd(cnum);
+-		} else {
+-			/*
+-			 * context has been saved
+-			 * if context is zombie, then task does not exist anymore.
+-			 * In this case, we use the full value saved in the context (pfm_flush_regs()).
+-			 */
+-			val = is_loaded ? ctx->th_pmds[cnum] : 0UL;
+-		}
+-		rd_func = pmu_conf->pmd_desc[cnum].read_check;
+-
+-		if (is_counting) {
+-			/*
+-			 * XXX: need to check for overflow when loaded
+-			 */
+-			val &= ovfl_mask;
+-			val += sval;
+-		}
+-
+-		/*
+-		 * execute read checker, if any
+-		 */
+-		if (unlikely(expert_mode == 0 && rd_func)) {
+-			unsigned long v = val;
+-			ret = (*rd_func)(ctx->ctx_task, ctx, cnum, &v, regs);
+-			if (ret) goto error;
+-			val = v;
+-			ret = -EINVAL;
+-		}
+-
+-		PFM_REG_RETFLAG_SET(reg_flags, 0);
+-
+-		DPRINT(("pmd[%u]=0x%lx\n", cnum, val));
+-
+-		/*
+-		 * update register return value, abort all if problem during copy.
+-		 * we only modify the reg_flags field. no check mode is fine because
+-		 * access has been verified upfront in sys_perfmonctl().
+-		 */
+-		req->reg_value            = val;
+-		req->reg_flags            = reg_flags;
+-		req->reg_last_reset_val   = lval;
+-	}
+-
+-	return 0;
+-
+-error:
+-	PFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);
+-	return ret;
+-}
+-
+-int
+-pfm_mod_write_pmcs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)
+-{
+-	pfm_context_t *ctx;
+-
+-	if (req == NULL) return -EINVAL;
+-
+- 	ctx = GET_PMU_CTX();
+-
+-	if (ctx == NULL) return -EINVAL;
+-
+-	/*
+-	 * for now limit to current task, which is enough when calling
+-	 * from overflow handler
+-	 */
+-	if (task != current && ctx->ctx_fl_system == 0) return -EBUSY;
+-
+-	return pfm_write_pmcs(ctx, req, nreq, regs);
+-}
+-EXPORT_SYMBOL(pfm_mod_write_pmcs);
+-
+-int
+-pfm_mod_read_pmds(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)
+-{
+-	pfm_context_t *ctx;
+-
+-	if (req == NULL) return -EINVAL;
+-
+- 	ctx = GET_PMU_CTX();
+-
+-	if (ctx == NULL) return -EINVAL;
+-
+-	/*
+-	 * for now limit to current task, which is enough when calling
+-	 * from overflow handler
+-	 */
+-	if (task != current && ctx->ctx_fl_system == 0) return -EBUSY;
+-
+-	return pfm_read_pmds(ctx, req, nreq, regs);
+-}
+-EXPORT_SYMBOL(pfm_mod_read_pmds);
+-
+-/*
+- * Only call this function when a process it trying to
+- * write the debug registers (reading is always allowed)
+- */
+-int
+-pfm_use_debug_registers(struct task_struct *task)
+-{
+-	pfm_context_t *ctx = task->thread.pfm_context;
+-	unsigned long flags;
+-	int ret = 0;
+-
+-	if (pmu_conf->use_rr_dbregs == 0) return 0;
+-
+-	DPRINT(("called for [%d]\n", task_pid_nr(task)));
+-
+-	/*
+-	 * do it only once
+-	 */
+-	if (task->thread.flags & IA64_THREAD_DBG_VALID) return 0;
+-
+-	/*
+-	 * Even on SMP, we do not need to use an atomic here because
+-	 * the only way in is via ptrace() and this is possible only when the
+-	 * process is stopped. Even in the case where the ctxsw out is not totally
+-	 * completed by the time we come here, there is no way the 'stopped' process
+-	 * could be in the middle of fiddling with the pfm_write_ibr_dbr() routine.
+-	 * So this is always safe.
+-	 */
+-	if (ctx && ctx->ctx_fl_using_dbreg == 1) return -1;
+-
+-	LOCK_PFS(flags);
+-
+-	/*
+-	 * We cannot allow setting breakpoints when system wide monitoring
+-	 * sessions are using the debug registers.
+-	 */
+-	if (pfm_sessions.pfs_sys_use_dbregs> 0)
+-		ret = -1;
+-	else
+-		pfm_sessions.pfs_ptrace_use_dbregs++;
+-
+-	DPRINT(("ptrace_use_dbregs=%u  sys_use_dbregs=%u by [%d] ret = %d\n",
+-		  pfm_sessions.pfs_ptrace_use_dbregs,
+-		  pfm_sessions.pfs_sys_use_dbregs,
+-		  task_pid_nr(task), ret));
+-
+-	UNLOCK_PFS(flags);
+-
+-	return ret;
+-}
+-
+-/*
+- * This function is called for every task that exits with the
+- * IA64_THREAD_DBG_VALID set. This indicates a task which was
+- * able to use the debug registers for debugging purposes via
+- * ptrace(). Therefore we know it was not using them for
+- * perfmormance monitoring, so we only decrement the number
+- * of "ptraced" debug register users to keep the count up to date
+- */
+-int
+-pfm_release_debug_registers(struct task_struct *task)
+-{
+-	unsigned long flags;
+-	int ret;
+-
+-	if (pmu_conf->use_rr_dbregs == 0) return 0;
+-
+-	LOCK_PFS(flags);
+-	if (pfm_sessions.pfs_ptrace_use_dbregs == 0) {
+-		printk(KERN_ERR "perfmon: invalid release for [%d] ptrace_use_dbregs=0\n", task_pid_nr(task));
+-		ret = -1;
+-	}  else {
+-		pfm_sessions.pfs_ptrace_use_dbregs--;
+-		ret = 0;
+-	}
+-	UNLOCK_PFS(flags);
+-
+-	return ret;
+-}
+-
+-static int
+-pfm_restart(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct task_struct *task;
+-	pfm_buffer_fmt_t *fmt;
+-	pfm_ovfl_ctrl_t rst_ctrl;
+-	int state, is_system;
+-	int ret = 0;
+-
+-	state     = ctx->ctx_state;
+-	fmt       = ctx->ctx_buf_fmt;
+-	is_system = ctx->ctx_fl_system;
+-	task      = PFM_CTX_TASK(ctx);
+-
+-	switch(state) {
+-		case PFM_CTX_MASKED:
+-			break;
+-		case PFM_CTX_LOADED: 
+-			if (CTX_HAS_SMPL(ctx) && fmt->fmt_restart_active) break;
+-			/* fall through */
+-		case PFM_CTX_UNLOADED:
+-		case PFM_CTX_ZOMBIE:
+-			DPRINT(("invalid state=%d\n", state));
+-			return -EBUSY;
+-		default:
+-			DPRINT(("state=%d, cannot operate (no active_restart handler)\n", state));
+-			return -EINVAL;
+-	}
+-
+-	/*
+- 	 * In system wide and when the context is loaded, access can only happen
+- 	 * when the caller is running on the CPU being monitored by the session.
+- 	 * It does not have to be the owner (ctx_task) of the context per se.
+- 	 */
+-	if (is_system && ctx->ctx_cpu != smp_processor_id()) {
+-		DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
+-		return -EBUSY;
+-	}
+-
+-	/* sanity check */
+-	if (unlikely(task == NULL)) {
+-		printk(KERN_ERR "perfmon: [%d] pfm_restart no task\n", task_pid_nr(current));
+-		return -EINVAL;
+-	}
+-
+-	if (task == current || is_system) {
+-
+-		fmt = ctx->ctx_buf_fmt;
+-
+-		DPRINT(("restarting self %d ovfl=0x%lx\n",
+-			task_pid_nr(task),
+-			ctx->ctx_ovfl_regs[0]));
+-
+-		if (CTX_HAS_SMPL(ctx)) {
+-
+-			prefetch(ctx->ctx_smpl_hdr);
+-
+-			rst_ctrl.bits.mask_monitoring = 0;
+-			rst_ctrl.bits.reset_ovfl_pmds = 0;
+-
+-			if (state == PFM_CTX_LOADED)
+-				ret = pfm_buf_fmt_restart_active(fmt, task, &rst_ctrl, ctx->ctx_smpl_hdr, regs);
+-			else
+-				ret = pfm_buf_fmt_restart(fmt, task, &rst_ctrl, ctx->ctx_smpl_hdr, regs);
+-		} else {
+-			rst_ctrl.bits.mask_monitoring = 0;
+-			rst_ctrl.bits.reset_ovfl_pmds = 1;
+-		}
+-
+-		if (ret == 0) {
+-			if (rst_ctrl.bits.reset_ovfl_pmds)
+-				pfm_reset_regs(ctx, ctx->ctx_ovfl_regs, PFM_PMD_LONG_RESET);
+-
+-			if (rst_ctrl.bits.mask_monitoring == 0) {
+-				DPRINT(("resuming monitoring for [%d]\n", task_pid_nr(task)));
+-
+-				if (state == PFM_CTX_MASKED) pfm_restore_monitoring(task);
+-			} else {
+-				DPRINT(("keeping monitoring stopped for [%d]\n", task_pid_nr(task)));
+-
+-				// cannot use pfm_stop_monitoring(task, regs);
+-			}
+-		}
+-		/*
+-		 * clear overflowed PMD mask to remove any stale information
+-		 */
+-		ctx->ctx_ovfl_regs[0] = 0UL;
+-
+-		/*
+-		 * back to LOADED state
+-		 */
+-		ctx->ctx_state = PFM_CTX_LOADED;
+-
+-		/*
+-		 * XXX: not really useful for self monitoring
+-		 */
+-		ctx->ctx_fl_can_restart = 0;
+-
+-		return 0;
+-	}
+-
+-	/* 
+-	 * restart another task
+-	 */
+-
+-	/*
+-	 * When PFM_CTX_MASKED, we cannot issue a restart before the previous 
+-	 * one is seen by the task.
+-	 */
+-	if (state == PFM_CTX_MASKED) {
+-		if (ctx->ctx_fl_can_restart == 0) return -EINVAL;
+-		/*
+-		 * will prevent subsequent restart before this one is
+-		 * seen by other task
+-		 */
+-		ctx->ctx_fl_can_restart = 0;
+-	}
+-
+-	/*
+-	 * if blocking, then post the semaphore is PFM_CTX_MASKED, i.e.
+-	 * the task is blocked or on its way to block. That's the normal
+-	 * restart path. If the monitoring is not masked, then the task
+-	 * can be actively monitoring and we cannot directly intervene.
+-	 * Therefore we use the trap mechanism to catch the task and
+-	 * force it to reset the buffer/reset PMDs.
+-	 *
+-	 * if non-blocking, then we ensure that the task will go into
+-	 * pfm_handle_work() before returning to user mode.
+-	 *
+-	 * We cannot explicitly reset another task, it MUST always
+-	 * be done by the task itself. This works for system wide because
+-	 * the tool that is controlling the session is logically doing 
+-	 * "self-monitoring".
+-	 */
+-	if (CTX_OVFL_NOBLOCK(ctx) == 0 && state == PFM_CTX_MASKED) {
+-		DPRINT(("unblocking [%d] \n", task_pid_nr(task)));
+-		complete(&ctx->ctx_restart_done);
+-	} else {
+-		DPRINT(("[%d] armed exit trap\n", task_pid_nr(task)));
+-
+-		ctx->ctx_fl_trap_reason = PFM_TRAP_REASON_RESET;
+-
+-		PFM_SET_WORK_PENDING(task, 1);
+-
+-		pfm_set_task_notify(task);
+-
+-		/*
+-		 * XXX: send reschedule if task runs on another CPU
+-		 */
+-	}
+-	return 0;
+-}
+-
+-static int
+-pfm_debug(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	unsigned int m = *(unsigned int *)arg;
+-
+-	pfm_sysctl.debug = m == 0 ? 0 : 1;
+-
+-	printk(KERN_INFO "perfmon debugging %s (timing reset)\n", pfm_sysctl.debug ? "on" : "off");
+-
+-	if (m == 0) {
+-		memset(pfm_stats, 0, sizeof(pfm_stats));
+-		for(m=0; m < NR_CPUS; m++) pfm_stats[m].pfm_ovfl_intr_cycles_min = ~0UL;
+-	}
+-	return 0;
+-}
+-
+-/*
+- * arg can be NULL and count can be zero for this function
+- */
+-static int
+-pfm_write_ibr_dbr(int mode, pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct thread_struct *thread = NULL;
+-	struct task_struct *task;
+-	pfarg_dbreg_t *req = (pfarg_dbreg_t *)arg;
+-	unsigned long flags;
+-	dbreg_t dbreg;
+-	unsigned int rnum;
+-	int first_time;
+-	int ret = 0, state;
+-	int i, can_access_pmu = 0;
+-	int is_system, is_loaded;
+-
+-	if (pmu_conf->use_rr_dbregs == 0) return -EINVAL;
+-
+-	state     = ctx->ctx_state;
+-	is_loaded = state == PFM_CTX_LOADED ? 1 : 0;
+-	is_system = ctx->ctx_fl_system;
+-	task      = ctx->ctx_task;
+-
+-	if (state == PFM_CTX_ZOMBIE) return -EINVAL;
+-
+-	/*
+-	 * on both UP and SMP, we can only write to the PMC when the task is
+-	 * the owner of the local PMU.
+-	 */
+-	if (is_loaded) {
+-		thread = &task->thread;
+-		/*
+-		 * In system wide and when the context is loaded, access can only happen
+-		 * when the caller is running on the CPU being monitored by the session.
+-		 * It does not have to be the owner (ctx_task) of the context per se.
+-		 */
+-		if (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {
+-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
+-			return -EBUSY;
+-		}
+-		can_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;
+-	}
+-
+-	/*
+-	 * we do not need to check for ipsr.db because we do clear ibr.x, dbr.r, and dbr.w
+-	 * ensuring that no real breakpoint can be installed via this call.
+-	 *
+-	 * IMPORTANT: regs can be NULL in this function
+-	 */
+-
+-	first_time = ctx->ctx_fl_using_dbreg == 0;
+-
+-	/*
+-	 * don't bother if we are loaded and task is being debugged
+-	 */
+-	if (is_loaded && (thread->flags & IA64_THREAD_DBG_VALID) != 0) {
+-		DPRINT(("debug registers already in use for [%d]\n", task_pid_nr(task)));
+-		return -EBUSY;
+-	}
+-
+-	/*
+-	 * check for debug registers in system wide mode
+-	 *
+-	 * If though a check is done in pfm_context_load(),
+-	 * we must repeat it here, in case the registers are
+-	 * written after the context is loaded
+-	 */
+-	if (is_loaded) {
+-		LOCK_PFS(flags);
+-
+-		if (first_time && is_system) {
+-			if (pfm_sessions.pfs_ptrace_use_dbregs)
+-				ret = -EBUSY;
+-			else
+-				pfm_sessions.pfs_sys_use_dbregs++;
+-		}
+-		UNLOCK_PFS(flags);
+-	}
+-
+-	if (ret != 0) return ret;
+-
+-	/*
+-	 * mark ourself as user of the debug registers for
+-	 * perfmon purposes.
+-	 */
+-	ctx->ctx_fl_using_dbreg = 1;
+-
+-	/*
+- 	 * clear hardware registers to make sure we don't
+- 	 * pick up stale state.
+-	 *
+-	 * for a system wide session, we do not use
+-	 * thread.dbr, thread.ibr because this process
+-	 * never leaves the current CPU and the state
+-	 * is shared by all processes running on it
+- 	 */
+-	if (first_time && can_access_pmu) {
+-		DPRINT(("[%d] clearing ibrs, dbrs\n", task_pid_nr(task)));
+-		for (i=0; i < pmu_conf->num_ibrs; i++) {
+-			ia64_set_ibr(i, 0UL);
+-			ia64_dv_serialize_instruction();
+-		}
+-		ia64_srlz_i();
+-		for (i=0; i < pmu_conf->num_dbrs; i++) {
+-			ia64_set_dbr(i, 0UL);
+-			ia64_dv_serialize_data();
+-		}
+-		ia64_srlz_d();
+-	}
+-
+-	/*
+-	 * Now install the values into the registers
+-	 */
+-	for (i = 0; i < count; i++, req++) {
+-
+-		rnum      = req->dbreg_num;
+-		dbreg.val = req->dbreg_value;
+-
+-		ret = -EINVAL;
+-
+-		if ((mode == PFM_CODE_RR && rnum >= PFM_NUM_IBRS) || ((mode == PFM_DATA_RR) && rnum >= PFM_NUM_DBRS)) {
+-			DPRINT(("invalid register %u val=0x%lx mode=%d i=%d count=%d\n",
+-				  rnum, dbreg.val, mode, i, count));
+-
+-			goto abort_mission;
+-		}
+-
+-		/*
+-		 * make sure we do not install enabled breakpoint
+-		 */
+-		if (rnum & 0x1) {
+-			if (mode == PFM_CODE_RR)
+-				dbreg.ibr.ibr_x = 0;
+-			else
+-				dbreg.dbr.dbr_r = dbreg.dbr.dbr_w = 0;
+-		}
+-
+-		PFM_REG_RETFLAG_SET(req->dbreg_flags, 0);
+-
+-		/*
+-		 * Debug registers, just like PMC, can only be modified
+-		 * by a kernel call. Moreover, perfmon() access to those
+-		 * registers are centralized in this routine. The hardware
+-		 * does not modify the value of these registers, therefore,
+-		 * if we save them as they are written, we can avoid having
+-		 * to save them on context switch out. This is made possible
+-		 * by the fact that when perfmon uses debug registers, ptrace()
+-		 * won't be able to modify them concurrently.
+-		 */
+-		if (mode == PFM_CODE_RR) {
+-			CTX_USED_IBR(ctx, rnum);
+-
+-			if (can_access_pmu) {
+-				ia64_set_ibr(rnum, dbreg.val);
+-				ia64_dv_serialize_instruction();
+-			}
+-
+-			ctx->ctx_ibrs[rnum] = dbreg.val;
+-
+-			DPRINT(("write ibr%u=0x%lx used_ibrs=0x%x ld=%d apmu=%d\n",
+-				rnum, dbreg.val, ctx->ctx_used_ibrs[0], is_loaded, can_access_pmu));
+-		} else {
+-			CTX_USED_DBR(ctx, rnum);
+-
+-			if (can_access_pmu) {
+-				ia64_set_dbr(rnum, dbreg.val);
+-				ia64_dv_serialize_data();
+-			}
+-			ctx->ctx_dbrs[rnum] = dbreg.val;
+-
+-			DPRINT(("write dbr%u=0x%lx used_dbrs=0x%x ld=%d apmu=%d\n",
+-				rnum, dbreg.val, ctx->ctx_used_dbrs[0], is_loaded, can_access_pmu));
+-		}
+-	}
+-
+-	return 0;
+-
+-abort_mission:
+-	/*
+-	 * in case it was our first attempt, we undo the global modifications
+-	 */
+-	if (first_time) {
+-		LOCK_PFS(flags);
+-		if (ctx->ctx_fl_system) {
+-			pfm_sessions.pfs_sys_use_dbregs--;
+-		}
+-		UNLOCK_PFS(flags);
+-		ctx->ctx_fl_using_dbreg = 0;
+-	}
+-	/*
+-	 * install error return flag
+-	 */
+-	PFM_REG_RETFLAG_SET(req->dbreg_flags, PFM_REG_RETFL_EINVAL);
+-
+-	return ret;
+-}
+-
+-static int
+-pfm_write_ibrs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	return pfm_write_ibr_dbr(PFM_CODE_RR, ctx, arg, count, regs);
+-}
+-
+-static int
+-pfm_write_dbrs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	return pfm_write_ibr_dbr(PFM_DATA_RR, ctx, arg, count, regs);
+-}
+-
+-int
+-pfm_mod_write_ibrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)
+-{
+-	pfm_context_t *ctx;
+-
+-	if (req == NULL) return -EINVAL;
+-
+- 	ctx = GET_PMU_CTX();
+-
+-	if (ctx == NULL) return -EINVAL;
+-
+-	/*
+-	 * for now limit to current task, which is enough when calling
+-	 * from overflow handler
+-	 */
+-	if (task != current && ctx->ctx_fl_system == 0) return -EBUSY;
+-
+-	return pfm_write_ibrs(ctx, req, nreq, regs);
+-}
+-EXPORT_SYMBOL(pfm_mod_write_ibrs);
+-
+-int
+-pfm_mod_write_dbrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)
+-{
+-	pfm_context_t *ctx;
+-
+-	if (req == NULL) return -EINVAL;
+-
+- 	ctx = GET_PMU_CTX();
+-
+-	if (ctx == NULL) return -EINVAL;
+-
+-	/*
+-	 * for now limit to current task, which is enough when calling
+-	 * from overflow handler
+-	 */
+-	if (task != current && ctx->ctx_fl_system == 0) return -EBUSY;
+-
+-	return pfm_write_dbrs(ctx, req, nreq, regs);
+-}
+-EXPORT_SYMBOL(pfm_mod_write_dbrs);
+-
+-
+-static int
+-pfm_get_features(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	pfarg_features_t *req = (pfarg_features_t *)arg;
+-
+-	req->ft_version = PFM_VERSION;
+-	return 0;
+-}
+-
+-static int
+-pfm_stop(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct pt_regs *tregs;
+-	struct task_struct *task = PFM_CTX_TASK(ctx);
+-	int state, is_system;
+-
+-	state     = ctx->ctx_state;
+-	is_system = ctx->ctx_fl_system;
+-
+-	/*
+-	 * context must be attached to issue the stop command (includes LOADED,MASKED,ZOMBIE)
+-	 */
+-	if (state == PFM_CTX_UNLOADED) return -EINVAL;
+-
+-	/*
+- 	 * In system wide and when the context is loaded, access can only happen
+- 	 * when the caller is running on the CPU being monitored by the session.
+- 	 * It does not have to be the owner (ctx_task) of the context per se.
+- 	 */
+-	if (is_system && ctx->ctx_cpu != smp_processor_id()) {
+-		DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
+-		return -EBUSY;
+-	}
+-	DPRINT(("task [%d] ctx_state=%d is_system=%d\n",
+-		task_pid_nr(PFM_CTX_TASK(ctx)),
+-		state,
+-		is_system));
+-	/*
+-	 * in system mode, we need to update the PMU directly
+-	 * and the user level state of the caller, which may not
+-	 * necessarily be the creator of the context.
+-	 */
+-	if (is_system) {
+-		/*
+-		 * Update local PMU first
+-		 *
+-		 * disable dcr pp
+-		 */
+-		ia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) & ~IA64_DCR_PP);
+-		ia64_srlz_i();
+-
+-		/*
+-		 * update local cpuinfo
+-		 */
+-		PFM_CPUINFO_CLEAR(PFM_CPUINFO_DCR_PP);
+-
+-		/*
+-		 * stop monitoring, does srlz.i
+-		 */
+-		pfm_clear_psr_pp();
+-
+-		/*
+-		 * stop monitoring in the caller
+-		 */
+-		ia64_psr(regs)->pp = 0;
+-
+-		return 0;
+-	}
+-	/*
+-	 * per-task mode
+-	 */
+-
+-	if (task == current) {
+-		/* stop monitoring  at kernel level */
+-		pfm_clear_psr_up();
+-
+-		/*
+-	 	 * stop monitoring at the user level
+-	 	 */
+-		ia64_psr(regs)->up = 0;
+-	} else {
+-		tregs = task_pt_regs(task);
+-
+-		/*
+-	 	 * stop monitoring at the user level
+-	 	 */
+-		ia64_psr(tregs)->up = 0;
+-
+-		/*
+-		 * monitoring disabled in kernel at next reschedule
+-		 */
+-		ctx->ctx_saved_psr_up = 0;
+-		DPRINT(("task=[%d]\n", task_pid_nr(task)));
+-	}
+-	return 0;
+-}
+-
+-
+-static int
+-pfm_start(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct pt_regs *tregs;
+-	int state, is_system;
+-
+-	state     = ctx->ctx_state;
+-	is_system = ctx->ctx_fl_system;
+-
+-	if (state != PFM_CTX_LOADED) return -EINVAL;
+-
+-	/*
+- 	 * In system wide and when the context is loaded, access can only happen
+- 	 * when the caller is running on the CPU being monitored by the session.
+- 	 * It does not have to be the owner (ctx_task) of the context per se.
+- 	 */
+-	if (is_system && ctx->ctx_cpu != smp_processor_id()) {
+-		DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
+-		return -EBUSY;
+-	}
+-
+-	/*
+-	 * in system mode, we need to update the PMU directly
+-	 * and the user level state of the caller, which may not
+-	 * necessarily be the creator of the context.
+-	 */
+-	if (is_system) {
+-
+-		/*
+-		 * set user level psr.pp for the caller
+-		 */
+-		ia64_psr(regs)->pp = 1;
+-
+-		/*
+-		 * now update the local PMU and cpuinfo
+-		 */
+-		PFM_CPUINFO_SET(PFM_CPUINFO_DCR_PP);
+-
+-		/*
+-		 * start monitoring at kernel level
+-		 */
+-		pfm_set_psr_pp();
+-
+-		/* enable dcr pp */
+-		ia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) | IA64_DCR_PP);
+-		ia64_srlz_i();
+-
+-		return 0;
+-	}
+-
+-	/*
+-	 * per-process mode
+-	 */
+-
+-	if (ctx->ctx_task == current) {
+-
+-		/* start monitoring at kernel level */
+-		pfm_set_psr_up();
+-
+-		/*
+-		 * activate monitoring at user level
+-		 */
+-		ia64_psr(regs)->up = 1;
+-
+-	} else {
+-		tregs = task_pt_regs(ctx->ctx_task);
+-
+-		/*
+-		 * start monitoring at the kernel level the next
+-		 * time the task is scheduled
+-		 */
+-		ctx->ctx_saved_psr_up = IA64_PSR_UP;
+-
+-		/*
+-		 * activate monitoring at user level
+-		 */
+-		ia64_psr(tregs)->up = 1;
+-	}
+-	return 0;
+-}
+-
+-static int
+-pfm_get_pmc_reset(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	pfarg_reg_t *req = (pfarg_reg_t *)arg;
+-	unsigned int cnum;
+-	int i;
+-	int ret = -EINVAL;
+-
+-	for (i = 0; i < count; i++, req++) {
+-
+-		cnum = req->reg_num;
+-
+-		if (!PMC_IS_IMPL(cnum)) goto abort_mission;
+-
+-		req->reg_value = PMC_DFL_VAL(cnum);
+-
+-		PFM_REG_RETFLAG_SET(req->reg_flags, 0);
+-
+-		DPRINT(("pmc_reset_val pmc[%u]=0x%lx\n", cnum, req->reg_value));
+-	}
+-	return 0;
+-
+-abort_mission:
+-	PFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);
+-	return ret;
+-}
+-
+-static int
+-pfm_check_task_exist(pfm_context_t *ctx)
+-{
+-	struct task_struct *g, *t;
+-	int ret = -ESRCH;
+-
+-	read_lock(&tasklist_lock);
+-
+-	do_each_thread (g, t) {
+-		if (t->thread.pfm_context == ctx) {
+-			ret = 0;
+-			break;
+-		}
+-	} while_each_thread (g, t);
+-
+-	read_unlock(&tasklist_lock);
+-
+-	DPRINT(("pfm_check_task_exist: ret=%d ctx=%p\n", ret, ctx));
+-
+-	return ret;
+-}
+-
+-static int
+-pfm_context_load(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct task_struct *task;
+-	struct thread_struct *thread;
+-	struct pfm_context_t *old;
+-	unsigned long flags;
+-#ifndef CONFIG_SMP
+-	struct task_struct *owner_task = NULL;
+-#endif
+-	pfarg_load_t *req = (pfarg_load_t *)arg;
+-	unsigned long *pmcs_source, *pmds_source;
+-	int the_cpu;
+-	int ret = 0;
+-	int state, is_system, set_dbregs = 0;
+-
+-	state     = ctx->ctx_state;
+-	is_system = ctx->ctx_fl_system;
+-	/*
+-	 * can only load from unloaded or terminated state
+-	 */
+-	if (state != PFM_CTX_UNLOADED) {
+-		DPRINT(("cannot load to [%d], invalid ctx_state=%d\n",
+-			req->load_pid,
+-			ctx->ctx_state));
+-		return -EBUSY;
+-	}
+-
+-	DPRINT(("load_pid [%d] using_dbreg=%d\n", req->load_pid, ctx->ctx_fl_using_dbreg));
+-
+-	if (CTX_OVFL_NOBLOCK(ctx) == 0 && req->load_pid == current->pid) {
+-		DPRINT(("cannot use blocking mode on self\n"));
+-		return -EINVAL;
+-	}
+-
+-	ret = pfm_get_task(ctx, req->load_pid, &task);
+-	if (ret) {
+-		DPRINT(("load_pid [%d] get_task=%d\n", req->load_pid, ret));
+-		return ret;
+-	}
+-
+-	ret = -EINVAL;
+-
+-	/*
+-	 * system wide is self monitoring only
+-	 */
+-	if (is_system && task != current) {
+-		DPRINT(("system wide is self monitoring only load_pid=%d\n",
+-			req->load_pid));
+-		goto error;
+-	}
+-
+-	thread = &task->thread;
+-
+-	ret = 0;
+-	/*
+-	 * cannot load a context which is using range restrictions,
+-	 * into a task that is being debugged.
+-	 */
+-	if (ctx->ctx_fl_using_dbreg) {
+-		if (thread->flags & IA64_THREAD_DBG_VALID) {
+-			ret = -EBUSY;
+-			DPRINT(("load_pid [%d] task is debugged, cannot load range restrictions\n", req->load_pid));
+-			goto error;
+-		}
+-		LOCK_PFS(flags);
+-
+-		if (is_system) {
+-			if (pfm_sessions.pfs_ptrace_use_dbregs) {
+-				DPRINT(("cannot load [%d] dbregs in use\n",
+-							task_pid_nr(task)));
+-				ret = -EBUSY;
+-			} else {
+-				pfm_sessions.pfs_sys_use_dbregs++;
+-				DPRINT(("load [%d] increased sys_use_dbreg=%u\n", task_pid_nr(task), pfm_sessions.pfs_sys_use_dbregs));
+-				set_dbregs = 1;
+-			}
+-		}
+-
+-		UNLOCK_PFS(flags);
+-
+-		if (ret) goto error;
+-	}
+-
+-	/*
+-	 * SMP system-wide monitoring implies self-monitoring.
+-	 *
+-	 * The programming model expects the task to
+-	 * be pinned on a CPU throughout the session.
+-	 * Here we take note of the current CPU at the
+-	 * time the context is loaded. No call from
+-	 * another CPU will be allowed.
+-	 *
+-	 * The pinning via shed_setaffinity()
+-	 * must be done by the calling task prior
+-	 * to this call.
+-	 *
+-	 * systemwide: keep track of CPU this session is supposed to run on
+-	 */
+-	the_cpu = ctx->ctx_cpu = smp_processor_id();
+-
+-	ret = -EBUSY;
+-	/*
+-	 * now reserve the session
+-	 */
+-	ret = pfm_reserve_session(current, is_system, the_cpu);
+-	if (ret) goto error;
+-
+-	/*
+-	 * task is necessarily stopped at this point.
+-	 *
+-	 * If the previous context was zombie, then it got removed in
+-	 * pfm_save_regs(). Therefore we should not see it here.
+-	 * If we see a context, then this is an active context
+-	 *
+-	 * XXX: needs to be atomic
+-	 */
+-	DPRINT(("before cmpxchg() old_ctx=%p new_ctx=%p\n",
+-		thread->pfm_context, ctx));
+-
+-	ret = -EBUSY;
+-	old = ia64_cmpxchg(acq, &thread->pfm_context, NULL, ctx, sizeof(pfm_context_t *));
+-	if (old != NULL) {
+-		DPRINT(("load_pid [%d] already has a context\n", req->load_pid));
+-		goto error_unres;
+-	}
+-
+-	pfm_reset_msgq(ctx);
+-
+-	ctx->ctx_state = PFM_CTX_LOADED;
+-
+-	/*
+-	 * link context to task
+-	 */
+-	ctx->ctx_task = task;
+-
+-	if (is_system) {
+-		/*
+-		 * we load as stopped
+-		 */
+-		PFM_CPUINFO_SET(PFM_CPUINFO_SYST_WIDE);
+-		PFM_CPUINFO_CLEAR(PFM_CPUINFO_DCR_PP);
+-
+-		if (ctx->ctx_fl_excl_idle) PFM_CPUINFO_SET(PFM_CPUINFO_EXCL_IDLE);
+-	} else {
+-		thread->flags |= IA64_THREAD_PM_VALID;
+-	}
+-
+-	/*
+-	 * propagate into thread-state
+-	 */
+-	pfm_copy_pmds(task, ctx);
+-	pfm_copy_pmcs(task, ctx);
+-
+-	pmcs_source = ctx->th_pmcs;
+-	pmds_source = ctx->th_pmds;
+-
+-	/*
+-	 * always the case for system-wide
+-	 */
+-	if (task == current) {
+-
+-		if (is_system == 0) {
+-
+-			/* allow user level control */
+-			ia64_psr(regs)->sp = 0;
+-			DPRINT(("clearing psr.sp for [%d]\n", task_pid_nr(task)));
+-
+-			SET_LAST_CPU(ctx, smp_processor_id());
+-			INC_ACTIVATION();
+-			SET_ACTIVATION(ctx);
+-#ifndef CONFIG_SMP
+-			/*
+-			 * push the other task out, if any
+-			 */
+-			owner_task = GET_PMU_OWNER();
+-			if (owner_task) pfm_lazy_save_regs(owner_task);
+-#endif
+-		}
+-		/*
+-		 * load all PMD from ctx to PMU (as opposed to thread state)
+-		 * restore all PMC from ctx to PMU
+-		 */
+-		pfm_restore_pmds(pmds_source, ctx->ctx_all_pmds[0]);
+-		pfm_restore_pmcs(pmcs_source, ctx->ctx_all_pmcs[0]);
+-
+-		ctx->ctx_reload_pmcs[0] = 0UL;
+-		ctx->ctx_reload_pmds[0] = 0UL;
+-
+-		/*
+-		 * guaranteed safe by earlier check against DBG_VALID
+-		 */
+-		if (ctx->ctx_fl_using_dbreg) {
+-			pfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);
+-			pfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);
+-		}
+-		/*
+-		 * set new ownership
+-		 */
+-		SET_PMU_OWNER(task, ctx);
+-
+-		DPRINT(("context loaded on PMU for [%d]\n", task_pid_nr(task)));
+-	} else {
+-		/*
+-		 * when not current, task MUST be stopped, so this is safe
+-		 */
+-		regs = task_pt_regs(task);
+-
+-		/* force a full reload */
+-		ctx->ctx_last_activation = PFM_INVALID_ACTIVATION;
+-		SET_LAST_CPU(ctx, -1);
+-
+-		/* initial saved psr (stopped) */
+-		ctx->ctx_saved_psr_up = 0UL;
+-		ia64_psr(regs)->up = ia64_psr(regs)->pp = 0;
+-	}
+-
+-	ret = 0;
+-
+-error_unres:
+-	if (ret) pfm_unreserve_session(ctx, ctx->ctx_fl_system, the_cpu);
+-error:
+-	/*
+-	 * we must undo the dbregs setting (for system-wide)
+-	 */
+-	if (ret && set_dbregs) {
+-		LOCK_PFS(flags);
+-		pfm_sessions.pfs_sys_use_dbregs--;
+-		UNLOCK_PFS(flags);
+-	}
+-	/*
+-	 * release task, there is now a link with the context
+-	 */
+-	if (is_system == 0 && task != current) {
+-		pfm_put_task(task);
+-
+-		if (ret == 0) {
+-			ret = pfm_check_task_exist(ctx);
+-			if (ret) {
+-				ctx->ctx_state = PFM_CTX_UNLOADED;
+-				ctx->ctx_task  = NULL;
+-			}
+-		}
+-	}
+-	return ret;
+-}
+-
+-/*
+- * in this function, we do not need to increase the use count
+- * for the task via get_task_struct(), because we hold the
+- * context lock. If the task were to disappear while having
+- * a context attached, it would go through pfm_exit_thread()
+- * which also grabs the context lock  and would therefore be blocked
+- * until we are here.
+- */
+-static void pfm_flush_pmds(struct task_struct *, pfm_context_t *ctx);
+-
+-static int
+-pfm_context_unload(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
+-{
+-	struct task_struct *task = PFM_CTX_TASK(ctx);
+-	struct pt_regs *tregs;
+-	int prev_state, is_system;
+-	int ret;
+-
+-	DPRINT(("ctx_state=%d task [%d]\n", ctx->ctx_state, task ? task_pid_nr(task) : -1));
+-
+-	prev_state = ctx->ctx_state;
+-	is_system  = ctx->ctx_fl_system;
+-
+-	/*
+-	 * unload only when necessary
+-	 */
+-	if (prev_state == PFM_CTX_UNLOADED) {
+-		DPRINT(("ctx_state=%d, nothing to do\n", prev_state));
+-		return 0;
+-	}
+-
+-	/*
+-	 * clear psr and dcr bits
+-	 */
+-	ret = pfm_stop(ctx, NULL, 0, regs);
+-	if (ret) return ret;
+-
+-	ctx->ctx_state = PFM_CTX_UNLOADED;
+-
+-	/*
+-	 * in system mode, we need to update the PMU directly
+-	 * and the user level state of the caller, which may not
+-	 * necessarily be the creator of the context.
+-	 */
+-	if (is_system) {
+-
+-		/*
+-		 * Update cpuinfo
+-		 *
+-		 * local PMU is taken care of in pfm_stop()
+-		 */
+-		PFM_CPUINFO_CLEAR(PFM_CPUINFO_SYST_WIDE);
+-		PFM_CPUINFO_CLEAR(PFM_CPUINFO_EXCL_IDLE);
+-
+-		/*
+-		 * save PMDs in context
+-		 * release ownership
+-		 */
+-		pfm_flush_pmds(current, ctx);
+-
+-		/*
+-		 * at this point we are done with the PMU
+-		 * so we can unreserve the resource.
+-		 */
+-		if (prev_state != PFM_CTX_ZOMBIE) 
+-			pfm_unreserve_session(ctx, 1 , ctx->ctx_cpu);
+-
+-		/*
+-		 * disconnect context from task
+-		 */
+-		task->thread.pfm_context = NULL;
+-		/*
+-		 * disconnect task from context
+-		 */
+-		ctx->ctx_task = NULL;
+-
+-		/*
+-		 * There is nothing more to cleanup here.
+-		 */
+-		return 0;
+-	}
+-
+-	/*
+-	 * per-task mode
+-	 */
+-	tregs = task == current ? regs : task_pt_regs(task);
+-
+-	if (task == current) {
+-		/*
+-		 * cancel user level control
+-		 */
+-		ia64_psr(regs)->sp = 1;
+-
+-		DPRINT(("setting psr.sp for [%d]\n", task_pid_nr(task)));
+-	}
+-	/*
+-	 * save PMDs to context
+-	 * release ownership
+-	 */
+-	pfm_flush_pmds(task, ctx);
+-
+-	/*
+-	 * at this point we are done with the PMU
+-	 * so we can unreserve the resource.
+-	 *
+-	 * when state was ZOMBIE, we have already unreserved.
+-	 */
+-	if (prev_state != PFM_CTX_ZOMBIE) 
+-		pfm_unreserve_session(ctx, 0 , ctx->ctx_cpu);
+-
+-	/*
+-	 * reset activation counter and psr
+-	 */
+-	ctx->ctx_last_activation = PFM_INVALID_ACTIVATION;
+-	SET_LAST_CPU(ctx, -1);
+-
+-	/*
+-	 * PMU state will not be restored
+-	 */
+-	task->thread.flags &= ~IA64_THREAD_PM_VALID;
+-
+-	/*
+-	 * break links between context and task
+-	 */
+-	task->thread.pfm_context  = NULL;
+-	ctx->ctx_task             = NULL;
+-
+-	PFM_SET_WORK_PENDING(task, 0);
+-
+-	ctx->ctx_fl_trap_reason  = PFM_TRAP_REASON_NONE;
+-	ctx->ctx_fl_can_restart  = 0;
+-	ctx->ctx_fl_going_zombie = 0;
+-
+-	DPRINT(("disconnected [%d] from context\n", task_pid_nr(task)));
+-
+-	return 0;
+-}
+-
+-
+-/*
+- * called only from exit_thread(): task == current
+- * we come here only if current has a context attached (loaded or masked)
+- */
+-void
+-pfm_exit_thread(struct task_struct *task)
+-{
+-	pfm_context_t *ctx;
+-	unsigned long flags;
+-	struct pt_regs *regs = task_pt_regs(task);
+-	int ret, state;
+-	int free_ok = 0;
+-
+-	ctx = PFM_GET_CTX(task);
+-
+-	PROTECT_CTX(ctx, flags);
+-
+-	DPRINT(("state=%d task [%d]\n", ctx->ctx_state, task_pid_nr(task)));
+-
+-	state = ctx->ctx_state;
+-	switch(state) {
+-		case PFM_CTX_UNLOADED:
+-			/*
+-	 		 * only comes to this function if pfm_context is not NULL, i.e., cannot
+-			 * be in unloaded state
+-	 		 */
+-			printk(KERN_ERR "perfmon: pfm_exit_thread [%d] ctx unloaded\n", task_pid_nr(task));
+-			break;
+-		case PFM_CTX_LOADED:
+-		case PFM_CTX_MASKED:
+-			ret = pfm_context_unload(ctx, NULL, 0, regs);
+-			if (ret) {
+-				printk(KERN_ERR "perfmon: pfm_exit_thread [%d] state=%d unload failed %d\n", task_pid_nr(task), state, ret);
+-			}
+-			DPRINT(("ctx unloaded for current state was %d\n", state));
+-
+-			pfm_end_notify_user(ctx);
+-			break;
+-		case PFM_CTX_ZOMBIE:
+-			ret = pfm_context_unload(ctx, NULL, 0, regs);
+-			if (ret) {
+-				printk(KERN_ERR "perfmon: pfm_exit_thread [%d] state=%d unload failed %d\n", task_pid_nr(task), state, ret);
+-			}
+-			free_ok = 1;
+-			break;
+-		default:
+-			printk(KERN_ERR "perfmon: pfm_exit_thread [%d] unexpected state=%d\n", task_pid_nr(task), state);
+-			break;
+-	}
+-	UNPROTECT_CTX(ctx, flags);
+-
+-	{ u64 psr = pfm_get_psr();
+-	  BUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));
+-	  BUG_ON(GET_PMU_OWNER());
+-	  BUG_ON(ia64_psr(regs)->up);
+-	  BUG_ON(ia64_psr(regs)->pp);
+-	}
+-
+-	/*
+-	 * All memory free operations (especially for vmalloc'ed memory)
+-	 * MUST be done with interrupts ENABLED.
+-	 */
+-	if (free_ok) pfm_context_free(ctx);
+-}
+-
+-/*
+- * functions MUST be listed in the increasing order of their index (see permfon.h)
+- */
+-#define PFM_CMD(name, flags, arg_count, arg_type, getsz) { name, #name, flags, arg_count, sizeof(arg_type), getsz }
+-#define PFM_CMD_S(name, flags) { name, #name, flags, 0, 0, NULL }
+-#define PFM_CMD_PCLRWS	(PFM_CMD_FD|PFM_CMD_ARG_RW|PFM_CMD_STOP)
+-#define PFM_CMD_PCLRW	(PFM_CMD_FD|PFM_CMD_ARG_RW)
+-#define PFM_CMD_NONE	{ NULL, "no-cmd", 0, 0, 0, NULL}
+-
+-static pfm_cmd_desc_t pfm_cmd_tab[]={
+-/* 0  */PFM_CMD_NONE,
+-/* 1  */PFM_CMD(pfm_write_pmcs, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_reg_t, NULL),
+-/* 2  */PFM_CMD(pfm_write_pmds, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_reg_t, NULL),
+-/* 3  */PFM_CMD(pfm_read_pmds, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_reg_t, NULL),
+-/* 4  */PFM_CMD_S(pfm_stop, PFM_CMD_PCLRWS),
+-/* 5  */PFM_CMD_S(pfm_start, PFM_CMD_PCLRWS),
+-/* 6  */PFM_CMD_NONE,
+-/* 7  */PFM_CMD_NONE,
+-/* 8  */PFM_CMD(pfm_context_create, PFM_CMD_ARG_RW, 1, pfarg_context_t, pfm_ctx_getsize),
+-/* 9  */PFM_CMD_NONE,
+-/* 10 */PFM_CMD_S(pfm_restart, PFM_CMD_PCLRW),
+-/* 11 */PFM_CMD_NONE,
+-/* 12 */PFM_CMD(pfm_get_features, PFM_CMD_ARG_RW, 1, pfarg_features_t, NULL),
+-/* 13 */PFM_CMD(pfm_debug, 0, 1, unsigned int, NULL),
+-/* 14 */PFM_CMD_NONE,
+-/* 15 */PFM_CMD(pfm_get_pmc_reset, PFM_CMD_ARG_RW, PFM_CMD_ARG_MANY, pfarg_reg_t, NULL),
+-/* 16 */PFM_CMD(pfm_context_load, PFM_CMD_PCLRWS, 1, pfarg_load_t, NULL),
+-/* 17 */PFM_CMD_S(pfm_context_unload, PFM_CMD_PCLRWS),
+-/* 18 */PFM_CMD_NONE,
+-/* 19 */PFM_CMD_NONE,
+-/* 20 */PFM_CMD_NONE,
+-/* 21 */PFM_CMD_NONE,
+-/* 22 */PFM_CMD_NONE,
+-/* 23 */PFM_CMD_NONE,
+-/* 24 */PFM_CMD_NONE,
+-/* 25 */PFM_CMD_NONE,
+-/* 26 */PFM_CMD_NONE,
+-/* 27 */PFM_CMD_NONE,
+-/* 28 */PFM_CMD_NONE,
+-/* 29 */PFM_CMD_NONE,
+-/* 30 */PFM_CMD_NONE,
+-/* 31 */PFM_CMD_NONE,
+-/* 32 */PFM_CMD(pfm_write_ibrs, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_dbreg_t, NULL),
+-/* 33 */PFM_CMD(pfm_write_dbrs, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_dbreg_t, NULL)
+-};
+-#define PFM_CMD_COUNT	(sizeof(pfm_cmd_tab)/sizeof(pfm_cmd_desc_t))
+-
+-static int
+-pfm_check_task_state(pfm_context_t *ctx, int cmd, unsigned long flags)
+-{
+-	struct task_struct *task;
+-	int state, old_state;
+-
+-recheck:
+-	state = ctx->ctx_state;
+-	task  = ctx->ctx_task;
+-
+-	if (task == NULL) {
+-		DPRINT(("context %d no task, state=%d\n", ctx->ctx_fd, state));
+-		return 0;
+-	}
+-
+-	DPRINT(("context %d state=%d [%d] task_state=%ld must_stop=%d\n",
+-		ctx->ctx_fd,
+-		state,
+-		task_pid_nr(task),
+-		task->state, PFM_CMD_STOPPED(cmd)));
+-
+-	/*
+-	 * self-monitoring always ok.
+-	 *
+-	 * for system-wide the caller can either be the creator of the
+-	 * context (to one to which the context is attached to) OR
+-	 * a task running on the same CPU as the session.
+-	 */
+-	if (task == current || ctx->ctx_fl_system) return 0;
+-
+-	/*
+-	 * we are monitoring another thread
+-	 */
+-	switch(state) {
+-		case PFM_CTX_UNLOADED:
+-			/*
+-			 * if context is UNLOADED we are safe to go
+-			 */
+-			return 0;
+-		case PFM_CTX_ZOMBIE:
+-			/*
+-			 * no command can operate on a zombie context
+-			 */
+-			DPRINT(("cmd %d state zombie cannot operate on context\n", cmd));
+-			return -EINVAL;
+-		case PFM_CTX_MASKED:
+-			/*
+-			 * PMU state has been saved to software even though
+-			 * the thread may still be running.
+-			 */
+-			if (cmd != PFM_UNLOAD_CONTEXT) return 0;
+-	}
+-
+-	/*
+-	 * context is LOADED or MASKED. Some commands may need to have 
+-	 * the task stopped.
+-	 *
+-	 * We could lift this restriction for UP but it would mean that
+-	 * the user has no guarantee the task would not run between
+-	 * two successive calls to perfmonctl(). That's probably OK.
+-	 * If this user wants to ensure the task does not run, then
+-	 * the task must be stopped.
+-	 */
+-	if (PFM_CMD_STOPPED(cmd)) {
+-		if ((task->state != TASK_STOPPED) && (task->state != TASK_TRACED)) {
+-			DPRINT(("[%d] task not in stopped state\n", task_pid_nr(task)));
+-			return -EBUSY;
+-		}
+-		/*
+-		 * task is now stopped, wait for ctxsw out
+-		 *
+-		 * This is an interesting point in the code.
+-		 * We need to unprotect the context because
+-		 * the pfm_save_regs() routines needs to grab
+-		 * the same lock. There are danger in doing
+-		 * this because it leaves a window open for
+-		 * another task to get access to the context
+-		 * and possibly change its state. The one thing
+-		 * that is not possible is for the context to disappear
+-		 * because we are protected by the VFS layer, i.e.,
+-		 * get_fd()/put_fd().
+-		 */
+-		old_state = state;
+-
+-		UNPROTECT_CTX(ctx, flags);
+-
+-		wait_task_inactive(task);
+-
+-		PROTECT_CTX(ctx, flags);
+-
+-		/*
+-		 * we must recheck to verify if state has changed
+-		 */
+-		if (ctx->ctx_state != old_state) {
+-			DPRINT(("old_state=%d new_state=%d\n", old_state, ctx->ctx_state));
+-			goto recheck;
+-		}
+-	}
+-	return 0;
+-}
+-
+-/*
+- * system-call entry point (must return long)
+- */
+-asmlinkage long
+-sys_perfmonctl (int fd, int cmd, void __user *arg, int count)
+-{
+-	struct file *file = NULL;
+-	pfm_context_t *ctx = NULL;
+-	unsigned long flags = 0UL;
+-	void *args_k = NULL;
+-	long ret; /* will expand int return types */
+-	size_t base_sz, sz, xtra_sz = 0;
+-	int narg, completed_args = 0, call_made = 0, cmd_flags;
+-	int (*func)(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);
+-	int (*getsize)(void *arg, size_t *sz);
+-#define PFM_MAX_ARGSIZE	4096
+-
+-	/*
+-	 * reject any call if perfmon was disabled at initialization
+-	 */
+-	if (unlikely(pmu_conf == NULL)) return -ENOSYS;
+-
+-	if (unlikely(cmd < 0 || cmd >= PFM_CMD_COUNT)) {
+-		DPRINT(("invalid cmd=%d\n", cmd));
+-		return -EINVAL;
+-	}
+-
+-	func      = pfm_cmd_tab[cmd].cmd_func;
+-	narg      = pfm_cmd_tab[cmd].cmd_narg;
+-	base_sz   = pfm_cmd_tab[cmd].cmd_argsize;
+-	getsize   = pfm_cmd_tab[cmd].cmd_getsize;
+-	cmd_flags = pfm_cmd_tab[cmd].cmd_flags;
+-
+-	if (unlikely(func == NULL)) {
+-		DPRINT(("invalid cmd=%d\n", cmd));
+-		return -EINVAL;
+-	}
+-
+-	DPRINT(("cmd=%s idx=%d narg=0x%x argsz=%lu count=%d\n",
+-		PFM_CMD_NAME(cmd),
+-		cmd,
+-		narg,
+-		base_sz,
+-		count));
+-
+-	/*
+-	 * check if number of arguments matches what the command expects
+-	 */
+-	if (unlikely((narg == PFM_CMD_ARG_MANY && count <= 0) || (narg > 0 && narg != count)))
+-		return -EINVAL;
+-
+-restart_args:
+-	sz = xtra_sz + base_sz*count;
+-	/*
+-	 * limit abuse to min page size
+-	 */
+-	if (unlikely(sz > PFM_MAX_ARGSIZE)) {
+-		printk(KERN_ERR "perfmon: [%d] argument too big %lu\n", task_pid_nr(current), sz);
+-		return -E2BIG;
+-	}
+-
+-	/*
+-	 * allocate default-sized argument buffer
+-	 */
+-	if (likely(count && args_k == NULL)) {
+-		args_k = kmalloc(PFM_MAX_ARGSIZE, GFP_KERNEL);
+-		if (args_k == NULL) return -ENOMEM;
+-	}
+-
+-	ret = -EFAULT;
+-
+-	/*
+-	 * copy arguments
+-	 *
+-	 * assume sz = 0 for command without parameters
+-	 */
+-	if (sz && copy_from_user(args_k, arg, sz)) {
+-		DPRINT(("cannot copy_from_user %lu bytes @%p\n", sz, arg));
+-		goto error_args;
+-	}
+-
+-	/*
+-	 * check if command supports extra parameters
+-	 */
+-	if (completed_args == 0 && getsize) {
+-		/*
+-		 * get extra parameters size (based on main argument)
+-		 */
+-		ret = (*getsize)(args_k, &xtra_sz);
+-		if (ret) goto error_args;
+-
+-		completed_args = 1;
+-
+-		DPRINT(("restart_args sz=%lu xtra_sz=%lu\n", sz, xtra_sz));
+-
+-		/* retry if necessary */
+-		if (likely(xtra_sz)) goto restart_args;
+-	}
+-
+-	if (unlikely((cmd_flags & PFM_CMD_FD) == 0)) goto skip_fd;
+-
+-	ret = -EBADF;
+-
+-	file = fget(fd);
+-	if (unlikely(file == NULL)) {
+-		DPRINT(("invalid fd %d\n", fd));
+-		goto error_args;
+-	}
+-	if (unlikely(PFM_IS_FILE(file) == 0)) {
+-		DPRINT(("fd %d not related to perfmon\n", fd));
+-		goto error_args;
+-	}
+-
+-	ctx = (pfm_context_t *)file->private_data;
+-	if (unlikely(ctx == NULL)) {
+-		DPRINT(("no context for fd %d\n", fd));
+-		goto error_args;
+-	}
+-	prefetch(&ctx->ctx_state);
+-
+-	PROTECT_CTX(ctx, flags);
+-
+-	/*
+-	 * check task is stopped
+-	 */
+-	ret = pfm_check_task_state(ctx, cmd, flags);
+-	if (unlikely(ret)) goto abort_locked;
+-
+-skip_fd:
+-	ret = (*func)(ctx, args_k, count, task_pt_regs(current));
+-
+-	call_made = 1;
+-
+-abort_locked:
+-	if (likely(ctx)) {
+-		DPRINT(("context unlocked\n"));
+-		UNPROTECT_CTX(ctx, flags);
+-	}
+-
+-	/* copy argument back to user, if needed */
+-	if (call_made && PFM_CMD_RW_ARG(cmd) && copy_to_user(arg, args_k, base_sz*count)) ret = -EFAULT;
+-
+-error_args:
+-	if (file)
+-		fput(file);
+-
+-	kfree(args_k);
+-
+-	DPRINT(("cmd=%s ret=%ld\n", PFM_CMD_NAME(cmd), ret));
+-
+-	return ret;
+-}
+-
+-static void
+-pfm_resume_after_ovfl(pfm_context_t *ctx, unsigned long ovfl_regs, struct pt_regs *regs)
+-{
+-	pfm_buffer_fmt_t *fmt = ctx->ctx_buf_fmt;
+-	pfm_ovfl_ctrl_t rst_ctrl;
+-	int state;
+-	int ret = 0;
+-
+-	state = ctx->ctx_state;
+-	/*
+-	 * Unlock sampling buffer and reset index atomically
+-	 * XXX: not really needed when blocking
+-	 */
+-	if (CTX_HAS_SMPL(ctx)) {
+-
+-		rst_ctrl.bits.mask_monitoring = 0;
+-		rst_ctrl.bits.reset_ovfl_pmds = 0;
+-
+-		if (state == PFM_CTX_LOADED)
+-			ret = pfm_buf_fmt_restart_active(fmt, current, &rst_ctrl, ctx->ctx_smpl_hdr, regs);
+-		else
+-			ret = pfm_buf_fmt_restart(fmt, current, &rst_ctrl, ctx->ctx_smpl_hdr, regs);
+-	} else {
+-		rst_ctrl.bits.mask_monitoring = 0;
+-		rst_ctrl.bits.reset_ovfl_pmds = 1;
+-	}
+-
+-	if (ret == 0) {
+-		if (rst_ctrl.bits.reset_ovfl_pmds) {
+-			pfm_reset_regs(ctx, &ovfl_regs, PFM_PMD_LONG_RESET);
+-		}
+-		if (rst_ctrl.bits.mask_monitoring == 0) {
+-			DPRINT(("resuming monitoring\n"));
+-			if (ctx->ctx_state == PFM_CTX_MASKED) pfm_restore_monitoring(current);
+-		} else {
+-			DPRINT(("stopping monitoring\n"));
+-			//pfm_stop_monitoring(current, regs);
+-		}
+-		ctx->ctx_state = PFM_CTX_LOADED;
+-	}
+-}
+-
+-/*
+- * context MUST BE LOCKED when calling
+- * can only be called for current
+- */
+-static void
+-pfm_context_force_terminate(pfm_context_t *ctx, struct pt_regs *regs)
+-{
+-	int ret;
+-
+-	DPRINT(("entering for [%d]\n", task_pid_nr(current)));
+-
+-	ret = pfm_context_unload(ctx, NULL, 0, regs);
+-	if (ret) {
+-		printk(KERN_ERR "pfm_context_force_terminate: [%d] unloaded failed with %d\n", task_pid_nr(current), ret);
+-	}
+-
+-	/*
+-	 * and wakeup controlling task, indicating we are now disconnected
+-	 */
+-	wake_up_interruptible(&ctx->ctx_zombieq);
+-
+-	/*
+-	 * given that context is still locked, the controlling
+-	 * task will only get access when we return from
+-	 * pfm_handle_work().
+-	 */
+-}
+-
+-static int pfm_ovfl_notify_user(pfm_context_t *ctx, unsigned long ovfl_pmds);
+- /*
+-  * pfm_handle_work() can be called with interrupts enabled
+-  * (TIF_NEED_RESCHED) or disabled. The down_interruptible
+-  * call may sleep, therefore we must re-enable interrupts
+-  * to avoid deadlocks. It is safe to do so because this function
+-  * is called ONLY when returning to user level (PUStk=1), in which case
+-  * there is no risk of kernel stack overflow due to deep
+-  * interrupt nesting.
+-  */
+-void
+-pfm_handle_work(void)
+-{
+-	pfm_context_t *ctx;
+-	struct pt_regs *regs;
+-	unsigned long flags, dummy_flags;
+-	unsigned long ovfl_regs;
+-	unsigned int reason;
+-	int ret;
+-
+-	ctx = PFM_GET_CTX(current);
+-	if (ctx == NULL) {
+-		printk(KERN_ERR "perfmon: [%d] has no PFM context\n", task_pid_nr(current));
+-		return;
+-	}
+-
+-	PROTECT_CTX(ctx, flags);
+-
+-	PFM_SET_WORK_PENDING(current, 0);
+-
+-	pfm_clear_task_notify();
+-
+-	regs = task_pt_regs(current);
+-
+-	/*
+-	 * extract reason for being here and clear
+-	 */
+-	reason = ctx->ctx_fl_trap_reason;
+-	ctx->ctx_fl_trap_reason = PFM_TRAP_REASON_NONE;
+-	ovfl_regs = ctx->ctx_ovfl_regs[0];
+-
+-	DPRINT(("reason=%d state=%d\n", reason, ctx->ctx_state));
+-
+-	/*
+-	 * must be done before we check for simple-reset mode
+-	 */
+-	if (ctx->ctx_fl_going_zombie || ctx->ctx_state == PFM_CTX_ZOMBIE) goto do_zombie;
+-
+-
+-	//if (CTX_OVFL_NOBLOCK(ctx)) goto skip_blocking;
+-	if (reason == PFM_TRAP_REASON_RESET) goto skip_blocking;
+-
+-	/*
+-	 * restore interrupt mask to what it was on entry.
+-	 * Could be enabled/diasbled.
+-	 */
+-	UNPROTECT_CTX(ctx, flags);
+-
+-	/*
+-	 * force interrupt enable because of down_interruptible()
+-	 */
+-	local_irq_enable();
+-
+-	DPRINT(("before block sleeping\n"));
+-
+-	/*
+-	 * may go through without blocking on SMP systems
+-	 * if restart has been received already by the time we call down()
+-	 */
+-	ret = wait_for_completion_interruptible(&ctx->ctx_restart_done);
+-
+-	DPRINT(("after block sleeping ret=%d\n", ret));
+-
+-	/*
+-	 * lock context and mask interrupts again
+-	 * We save flags into a dummy because we may have
+-	 * altered interrupts mask compared to entry in this
+-	 * function.
+-	 */
+-	PROTECT_CTX(ctx, dummy_flags);
+-
+-	/*
+-	 * we need to read the ovfl_regs only after wake-up
+-	 * because we may have had pfm_write_pmds() in between
+-	 * and that can changed PMD values and therefore 
+-	 * ovfl_regs is reset for these new PMD values.
+-	 */
+-	ovfl_regs = ctx->ctx_ovfl_regs[0];
+-
+-	if (ctx->ctx_fl_going_zombie) {
+-do_zombie:
+-		DPRINT(("context is zombie, bailing out\n"));
+-		pfm_context_force_terminate(ctx, regs);
+-		goto nothing_to_do;
+-	}
+-	/*
+-	 * in case of interruption of down() we don't restart anything
+-	 */
+-	if (ret < 0) goto nothing_to_do;
+-
+-skip_blocking:
+-	pfm_resume_after_ovfl(ctx, ovfl_regs, regs);
+-	ctx->ctx_ovfl_regs[0] = 0UL;
+-
+-nothing_to_do:
+-	/*
+-	 * restore flags as they were upon entry
+-	 */
+-	UNPROTECT_CTX(ctx, flags);
+-}
+-
+-static int
+-pfm_notify_user(pfm_context_t *ctx, pfm_msg_t *msg)
+-{
+-	if (ctx->ctx_state == PFM_CTX_ZOMBIE) {
+-		DPRINT(("ignoring overflow notification, owner is zombie\n"));
+-		return 0;
+-	}
+-
+-	DPRINT(("waking up somebody\n"));
+-
+-	if (msg) wake_up_interruptible(&ctx->ctx_msgq_wait);
+-
+-	/*
+-	 * safe, we are not in intr handler, nor in ctxsw when
+-	 * we come here
+-	 */
+-	kill_fasync (&ctx->ctx_async_queue, SIGIO, POLL_IN);
+-
+-	return 0;
+-}
+-
+-static int
+-pfm_ovfl_notify_user(pfm_context_t *ctx, unsigned long ovfl_pmds)
+-{
+-	pfm_msg_t *msg = NULL;
+-
+-	if (ctx->ctx_fl_no_msg == 0) {
+-		msg = pfm_get_new_msg(ctx);
+-		if (msg == NULL) {
+-			printk(KERN_ERR "perfmon: pfm_ovfl_notify_user no more notification msgs\n");
+-			return -1;
+-		}
+-
+-		msg->pfm_ovfl_msg.msg_type         = PFM_MSG_OVFL;
+-		msg->pfm_ovfl_msg.msg_ctx_fd       = ctx->ctx_fd;
+-		msg->pfm_ovfl_msg.msg_active_set   = 0;
+-		msg->pfm_ovfl_msg.msg_ovfl_pmds[0] = ovfl_pmds;
+-		msg->pfm_ovfl_msg.msg_ovfl_pmds[1] = 0UL;
+-		msg->pfm_ovfl_msg.msg_ovfl_pmds[2] = 0UL;
+-		msg->pfm_ovfl_msg.msg_ovfl_pmds[3] = 0UL;
+-		msg->pfm_ovfl_msg.msg_tstamp       = 0UL;
+-	}
+-
+-	DPRINT(("ovfl msg: msg=%p no_msg=%d fd=%d ovfl_pmds=0x%lx\n",
+-		msg,
+-		ctx->ctx_fl_no_msg,
+-		ctx->ctx_fd,
+-		ovfl_pmds));
+-
+-	return pfm_notify_user(ctx, msg);
+-}
+-
+-static int
+-pfm_end_notify_user(pfm_context_t *ctx)
+-{
+-	pfm_msg_t *msg;
+-
+-	msg = pfm_get_new_msg(ctx);
+-	if (msg == NULL) {
+-		printk(KERN_ERR "perfmon: pfm_end_notify_user no more notification msgs\n");
+-		return -1;
+-	}
+-	/* no leak */
+-	memset(msg, 0, sizeof(*msg));
+-
+-	msg->pfm_end_msg.msg_type    = PFM_MSG_END;
+-	msg->pfm_end_msg.msg_ctx_fd  = ctx->ctx_fd;
+-	msg->pfm_ovfl_msg.msg_tstamp = 0UL;
+-
+-	DPRINT(("end msg: msg=%p no_msg=%d ctx_fd=%d\n",
+-		msg,
+-		ctx->ctx_fl_no_msg,
+-		ctx->ctx_fd));
+-
+-	return pfm_notify_user(ctx, msg);
+-}
+-
+-/*
+- * main overflow processing routine.
+- * it can be called from the interrupt path or explicitly during the context switch code
+- */
+-static void
+-pfm_overflow_handler(struct task_struct *task, pfm_context_t *ctx, u64 pmc0, struct pt_regs *regs)
+-{
+-	pfm_ovfl_arg_t *ovfl_arg;
+-	unsigned long mask;
+-	unsigned long old_val, ovfl_val, new_val;
+-	unsigned long ovfl_notify = 0UL, ovfl_pmds = 0UL, smpl_pmds = 0UL, reset_pmds;
+-	unsigned long tstamp;
+-	pfm_ovfl_ctrl_t	ovfl_ctrl;
+-	unsigned int i, has_smpl;
+-	int must_notify = 0;
+-
+-	if (unlikely(ctx->ctx_state == PFM_CTX_ZOMBIE)) goto stop_monitoring;
+-
+-	/*
+-	 * sanity test. Should never happen
+-	 */
+-	if (unlikely((pmc0 & 0x1) == 0)) goto sanity_check;
+-
+-	tstamp   = ia64_get_itc();
+-	mask     = pmc0 >> PMU_FIRST_COUNTER;
+-	ovfl_val = pmu_conf->ovfl_val;
+-	has_smpl = CTX_HAS_SMPL(ctx);
+-
+-	DPRINT_ovfl(("pmc0=0x%lx pid=%d iip=0x%lx, %s "
+-		     "used_pmds=0x%lx\n",
+-			pmc0,
+-			task ? task_pid_nr(task): -1,
+-			(regs ? regs->cr_iip : 0),
+-			CTX_OVFL_NOBLOCK(ctx) ? "nonblocking" : "blocking",
+-			ctx->ctx_used_pmds[0]));
+-
+-
+-	/*
+-	 * first we update the virtual counters
+-	 * assume there was a prior ia64_srlz_d() issued
+-	 */
+-	for (i = PMU_FIRST_COUNTER; mask ; i++, mask >>= 1) {
+-
+-		/* skip pmd which did not overflow */
+-		if ((mask & 0x1) == 0) continue;
+-
+-		/*
+-		 * Note that the pmd is not necessarily 0 at this point as qualified events
+-		 * may have happened before the PMU was frozen. The residual count is not
+-		 * taken into consideration here but will be with any read of the pmd via
+-		 * pfm_read_pmds().
+-		 */
+-		old_val              = new_val = ctx->ctx_pmds[i].val;
+-		new_val             += 1 + ovfl_val;
+-		ctx->ctx_pmds[i].val = new_val;
+-
+-		/*
+-		 * check for overflow condition
+-		 */
+-		if (likely(old_val > new_val)) {
+-			ovfl_pmds |= 1UL << i;
+-			if (PMC_OVFL_NOTIFY(ctx, i)) ovfl_notify |= 1UL << i;
+-		}
+-
+-		DPRINT_ovfl(("ctx_pmd[%d].val=0x%lx old_val=0x%lx pmd=0x%lx ovfl_pmds=0x%lx ovfl_notify=0x%lx\n",
+-			i,
+-			new_val,
+-			old_val,
+-			ia64_get_pmd(i) & ovfl_val,
+-			ovfl_pmds,
+-			ovfl_notify));
+-	}
+-
+-	/*
+-	 * there was no 64-bit overflow, nothing else to do
+-	 */
+-	if (ovfl_pmds == 0UL) return;
+-
+-	/* 
+-	 * reset all control bits
+-	 */
+-	ovfl_ctrl.val = 0;
+-	reset_pmds    = 0UL;
+-
+-	/*
+-	 * if a sampling format module exists, then we "cache" the overflow by 
+-	 * calling the module's handler() routine.
+-	 */
+-	if (has_smpl) {
+-		unsigned long start_cycles, end_cycles;
+-		unsigned long pmd_mask;
+-		int j, k, ret = 0;
+-		int this_cpu = smp_processor_id();
+-
+-		pmd_mask = ovfl_pmds >> PMU_FIRST_COUNTER;
+-		ovfl_arg = &ctx->ctx_ovfl_arg;
+-
+-		prefetch(ctx->ctx_smpl_hdr);
+-
+-		for(i=PMU_FIRST_COUNTER; pmd_mask && ret == 0; i++, pmd_mask >>=1) {
+-
+-			mask = 1UL << i;
+-
+-			if ((pmd_mask & 0x1) == 0) continue;
+-
+-			ovfl_arg->ovfl_pmd      = (unsigned char )i;
+-			ovfl_arg->ovfl_notify   = ovfl_notify & mask ? 1 : 0;
+-			ovfl_arg->active_set    = 0;
+-			ovfl_arg->ovfl_ctrl.val = 0; /* module must fill in all fields */
+-			ovfl_arg->smpl_pmds[0]  = smpl_pmds = ctx->ctx_pmds[i].smpl_pmds[0];
+-
+-			ovfl_arg->pmd_value      = ctx->ctx_pmds[i].val;
+-			ovfl_arg->pmd_last_reset = ctx->ctx_pmds[i].lval;
+-			ovfl_arg->pmd_eventid    = ctx->ctx_pmds[i].eventid;
+-
+-			/*
+-		 	 * copy values of pmds of interest. Sampling format may copy them
+-		 	 * into sampling buffer.
+-		 	 */
+-			if (smpl_pmds) {
+-				for(j=0, k=0; smpl_pmds; j++, smpl_pmds >>=1) {
+-					if ((smpl_pmds & 0x1) == 0) continue;
+-					ovfl_arg->smpl_pmds_values[k++] = PMD_IS_COUNTING(j) ?  pfm_read_soft_counter(ctx, j) : ia64_get_pmd(j);
+-					DPRINT_ovfl(("smpl_pmd[%d]=pmd%u=0x%lx\n", k-1, j, ovfl_arg->smpl_pmds_values[k-1]));
+-				}
+-			}
+-
+-			pfm_stats[this_cpu].pfm_smpl_handler_calls++;
+-
+-			start_cycles = ia64_get_itc();
+-
+-			/*
+-		 	 * call custom buffer format record (handler) routine
+-		 	 */
+-			ret = (*ctx->ctx_buf_fmt->fmt_handler)(task, ctx->ctx_smpl_hdr, ovfl_arg, regs, tstamp);
+-
+-			end_cycles = ia64_get_itc();
+-
+-			/*
+-			 * For those controls, we take the union because they have
+-			 * an all or nothing behavior.
+-			 */
+-			ovfl_ctrl.bits.notify_user     |= ovfl_arg->ovfl_ctrl.bits.notify_user;
+-			ovfl_ctrl.bits.block_task      |= ovfl_arg->ovfl_ctrl.bits.block_task;
+-			ovfl_ctrl.bits.mask_monitoring |= ovfl_arg->ovfl_ctrl.bits.mask_monitoring;
+-			/*
+-			 * build the bitmask of pmds to reset now
+-			 */
+-			if (ovfl_arg->ovfl_ctrl.bits.reset_ovfl_pmds) reset_pmds |= mask;
+-
+-			pfm_stats[this_cpu].pfm_smpl_handler_cycles += end_cycles - start_cycles;
+-		}
+-		/*
+-		 * when the module cannot handle the rest of the overflows, we abort right here
+-		 */
+-		if (ret && pmd_mask) {
+-			DPRINT(("handler aborts leftover ovfl_pmds=0x%lx\n",
+-				pmd_mask<<PMU_FIRST_COUNTER));
+-		}
+-		/*
+-		 * remove the pmds we reset now from the set of pmds to reset in pfm_restart()
+-		 */
+-		ovfl_pmds &= ~reset_pmds;
+-	} else {
+-		/*
+-		 * when no sampling module is used, then the default
+-		 * is to notify on overflow if requested by user
+-		 */
+-		ovfl_ctrl.bits.notify_user     = ovfl_notify ? 1 : 0;
+-		ovfl_ctrl.bits.block_task      = ovfl_notify ? 1 : 0;
+-		ovfl_ctrl.bits.mask_monitoring = ovfl_notify ? 1 : 0; /* XXX: change for saturation */
+-		ovfl_ctrl.bits.reset_ovfl_pmds = ovfl_notify ? 0 : 1;
+-		/*
+-		 * if needed, we reset all overflowed pmds
+-		 */
+-		if (ovfl_notify == 0) reset_pmds = ovfl_pmds;
+-	}
+-
+-	DPRINT_ovfl(("ovfl_pmds=0x%lx reset_pmds=0x%lx\n", ovfl_pmds, reset_pmds));
+-
+-	/*
+-	 * reset the requested PMD registers using the short reset values
+-	 */
+-	if (reset_pmds) {
+-		unsigned long bm = reset_pmds;
+-		pfm_reset_regs(ctx, &bm, PFM_PMD_SHORT_RESET);
+-	}
+-
+-	if (ovfl_notify && ovfl_ctrl.bits.notify_user) {
+-		/*
+-		 * keep track of what to reset when unblocking
+-		 */
+-		ctx->ctx_ovfl_regs[0] = ovfl_pmds;
+-
+-		/*
+-		 * check for blocking context 
+-		 */
+-		if (CTX_OVFL_NOBLOCK(ctx) == 0 && ovfl_ctrl.bits.block_task) {
+-
+-			ctx->ctx_fl_trap_reason = PFM_TRAP_REASON_BLOCK;
+-
+-			/*
+-			 * set the perfmon specific checking pending work for the task
+-			 */
+-			PFM_SET_WORK_PENDING(task, 1);
+-
+-			/*
+-			 * when coming from ctxsw, current still points to the
+-			 * previous task, therefore we must work with task and not current.
+-			 */
+-			pfm_set_task_notify(task);
+-		}
+-		/*
+-		 * defer until state is changed (shorten spin window). the context is locked
+-		 * anyway, so the signal receiver would come spin for nothing.
+-		 */
+-		must_notify = 1;
+-	}
+-
+-	DPRINT_ovfl(("owner [%d] pending=%ld reason=%u ovfl_pmds=0x%lx ovfl_notify=0x%lx masked=%d\n",
+-			GET_PMU_OWNER() ? task_pid_nr(GET_PMU_OWNER()) : -1,
+-			PFM_GET_WORK_PENDING(task),
+-			ctx->ctx_fl_trap_reason,
+-			ovfl_pmds,
+-			ovfl_notify,
+-			ovfl_ctrl.bits.mask_monitoring ? 1 : 0));
+-	/*
+-	 * in case monitoring must be stopped, we toggle the psr bits
+-	 */
+-	if (ovfl_ctrl.bits.mask_monitoring) {
+-		pfm_mask_monitoring(task);
+-		ctx->ctx_state = PFM_CTX_MASKED;
+-		ctx->ctx_fl_can_restart = 1;
+-	}
+-
+-	/*
+-	 * send notification now
+-	 */
+-	if (must_notify) pfm_ovfl_notify_user(ctx, ovfl_notify);
+-
+-	return;
+-
+-sanity_check:
+-	printk(KERN_ERR "perfmon: CPU%d overflow handler [%d] pmc0=0x%lx\n",
+-			smp_processor_id(),
+-			task ? task_pid_nr(task) : -1,
+-			pmc0);
+-	return;
+-
+-stop_monitoring:
+-	/*
+-	 * in SMP, zombie context is never restored but reclaimed in pfm_load_regs().
+-	 * Moreover, zombies are also reclaimed in pfm_save_regs(). Therefore we can
+-	 * come here as zombie only if the task is the current task. In which case, we
+-	 * can access the PMU  hardware directly.
+-	 *
+-	 * Note that zombies do have PM_VALID set. So here we do the minimal.
+-	 *
+-	 * In case the context was zombified it could not be reclaimed at the time
+-	 * the monitoring program exited. At this point, the PMU reservation has been
+-	 * returned, the sampiing buffer has been freed. We must convert this call
+-	 * into a spurious interrupt. However, we must also avoid infinite overflows
+-	 * by stopping monitoring for this task. We can only come here for a per-task
+-	 * context. All we need to do is to stop monitoring using the psr bits which
+-	 * are always task private. By re-enabling secure montioring, we ensure that
+-	 * the monitored task will not be able to re-activate monitoring.
+-	 * The task will eventually be context switched out, at which point the context
+-	 * will be reclaimed (that includes releasing ownership of the PMU).
+-	 *
+-	 * So there might be a window of time where the number of per-task session is zero
+-	 * yet one PMU might have a owner and get at most one overflow interrupt for a zombie
+-	 * context. This is safe because if a per-task session comes in, it will push this one
+-	 * out and by the virtue on pfm_save_regs(), this one will disappear. If a system wide
+-	 * session is force on that CPU, given that we use task pinning, pfm_save_regs() will
+-	 * also push our zombie context out.
+-	 *
+-	 * Overall pretty hairy stuff....
+-	 */
+-	DPRINT(("ctx is zombie for [%d], converted to spurious\n", task ? task_pid_nr(task): -1));
+-	pfm_clear_psr_up();
+-	ia64_psr(regs)->up = 0;
+-	ia64_psr(regs)->sp = 1;
+-	return;
+-}
+-
+-static int
+-pfm_do_interrupt_handler(int irq, void *arg, struct pt_regs *regs)
+-{
+-	struct task_struct *task;
+-	pfm_context_t *ctx;
+-	unsigned long flags;
+-	u64 pmc0;
+-	int this_cpu = smp_processor_id();
+-	int retval = 0;
+-
+-	pfm_stats[this_cpu].pfm_ovfl_intr_count++;
+-
+-	/*
+-	 * srlz.d done before arriving here
+-	 */
+-	pmc0 = ia64_get_pmc(0);
+-
+-	task = GET_PMU_OWNER();
+-	ctx  = GET_PMU_CTX();
+-
+-	/*
+-	 * if we have some pending bits set
+-	 * assumes : if any PMC0.bit[63-1] is set, then PMC0.fr = 1
+-	 */
+-	if (PMC0_HAS_OVFL(pmc0) && task) {
+-		/*
+-		 * we assume that pmc0.fr is always set here
+-		 */
+-
+-		/* sanity check */
+-		if (!ctx) goto report_spurious1;
+-
+-		if (ctx->ctx_fl_system == 0 && (task->thread.flags & IA64_THREAD_PM_VALID) == 0) 
+-			goto report_spurious2;
+-
+-		PROTECT_CTX_NOPRINT(ctx, flags);
+-
+-		pfm_overflow_handler(task, ctx, pmc0, regs);
+-
+-		UNPROTECT_CTX_NOPRINT(ctx, flags);
+-
+-	} else {
+-		pfm_stats[this_cpu].pfm_spurious_ovfl_intr_count++;
+-		retval = -1;
+-	}
+-	/*
+-	 * keep it unfrozen at all times
+-	 */
+-	pfm_unfreeze_pmu();
+-
+-	return retval;
+-
+-report_spurious1:
+-	printk(KERN_INFO "perfmon: spurious overflow interrupt on CPU%d: process %d has no PFM context\n",
+-		this_cpu, task_pid_nr(task));
+-	pfm_unfreeze_pmu();
+-	return -1;
+-report_spurious2:
+-	printk(KERN_INFO "perfmon: spurious overflow interrupt on CPU%d: process %d, invalid flag\n", 
+-		this_cpu, 
+-		task_pid_nr(task));
+-	pfm_unfreeze_pmu();
+-	return -1;
+-}
+-
+-static irqreturn_t
+-pfm_interrupt_handler(int irq, void *arg)
+-{
+-	unsigned long start_cycles, total_cycles;
+-	unsigned long min, max;
+-	int this_cpu;
+-	int ret;
+-	struct pt_regs *regs = get_irq_regs();
+-
+-	this_cpu = get_cpu();
+-	if (likely(!pfm_alt_intr_handler)) {
+-		min = pfm_stats[this_cpu].pfm_ovfl_intr_cycles_min;
+-		max = pfm_stats[this_cpu].pfm_ovfl_intr_cycles_max;
+-
+-		start_cycles = ia64_get_itc();
+-
+-		ret = pfm_do_interrupt_handler(irq, arg, regs);
+-
+-		total_cycles = ia64_get_itc();
+-
+-		/*
+-		 * don't measure spurious interrupts
+-		 */
+-		if (likely(ret == 0)) {
+-			total_cycles -= start_cycles;
+-
+-			if (total_cycles < min) pfm_stats[this_cpu].pfm_ovfl_intr_cycles_min = total_cycles;
+-			if (total_cycles > max) pfm_stats[this_cpu].pfm_ovfl_intr_cycles_max = total_cycles;
+-
+-			pfm_stats[this_cpu].pfm_ovfl_intr_cycles += total_cycles;
+-		}
+-	}
+-	else {
+-		(*pfm_alt_intr_handler->handler)(irq, arg, regs);
+-	}
+-
+-	put_cpu_no_resched();
+-	return IRQ_HANDLED;
+-}
+-
+-/*
+- * /proc/perfmon interface, for debug only
+- */
+-
+-#define PFM_PROC_SHOW_HEADER	((void *)NR_CPUS+1)
+-
+-static void *
+-pfm_proc_start(struct seq_file *m, loff_t *pos)
+-{
+-	if (*pos == 0) {
+-		return PFM_PROC_SHOW_HEADER;
+-	}
+-
+-	while (*pos <= NR_CPUS) {
+-		if (cpu_online(*pos - 1)) {
+-			return (void *)*pos;
+-		}
+-		++*pos;
+-	}
+-	return NULL;
+-}
+-
+-static void *
+-pfm_proc_next(struct seq_file *m, void *v, loff_t *pos)
+-{
+-	++*pos;
+-	return pfm_proc_start(m, pos);
+-}
+-
+-static void
+-pfm_proc_stop(struct seq_file *m, void *v)
+-{
+-}
+-
+-static void
+-pfm_proc_show_header(struct seq_file *m)
+-{
+-	struct list_head * pos;
+-	pfm_buffer_fmt_t * entry;
+-	unsigned long flags;
+-
+- 	seq_printf(m,
+-		"perfmon version           : %u.%u\n"
+-		"model                     : %s\n"
+-		"fastctxsw                 : %s\n"
+-		"expert mode               : %s\n"
+-		"ovfl_mask                 : 0x%lx\n"
+-		"PMU flags                 : 0x%x\n",
+-		PFM_VERSION_MAJ, PFM_VERSION_MIN,
+-		pmu_conf->pmu_name,
+-		pfm_sysctl.fastctxsw > 0 ? "Yes": "No",
+-		pfm_sysctl.expert_mode > 0 ? "Yes": "No",
+-		pmu_conf->ovfl_val,
+-		pmu_conf->flags);
+-
+-  	LOCK_PFS(flags);
+-
+- 	seq_printf(m,
+- 		"proc_sessions             : %u\n"
+- 		"sys_sessions              : %u\n"
+- 		"sys_use_dbregs            : %u\n"
+- 		"ptrace_use_dbregs         : %u\n",
+- 		pfm_sessions.pfs_task_sessions,
+- 		pfm_sessions.pfs_sys_sessions,
+- 		pfm_sessions.pfs_sys_use_dbregs,
+- 		pfm_sessions.pfs_ptrace_use_dbregs);
+-
+-  	UNLOCK_PFS(flags);
+-
+-	spin_lock(&pfm_buffer_fmt_lock);
+-
+-	list_for_each(pos, &pfm_buffer_fmt_list) {
+-		entry = list_entry(pos, pfm_buffer_fmt_t, fmt_list);
+-		seq_printf(m, "format                    : %02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x %s\n",
+-			entry->fmt_uuid[0],
+-			entry->fmt_uuid[1],
+-			entry->fmt_uuid[2],
+-			entry->fmt_uuid[3],
+-			entry->fmt_uuid[4],
+-			entry->fmt_uuid[5],
+-			entry->fmt_uuid[6],
+-			entry->fmt_uuid[7],
+-			entry->fmt_uuid[8],
+-			entry->fmt_uuid[9],
+-			entry->fmt_uuid[10],
+-			entry->fmt_uuid[11],
+-			entry->fmt_uuid[12],
+-			entry->fmt_uuid[13],
+-			entry->fmt_uuid[14],
+-			entry->fmt_uuid[15],
+-			entry->fmt_name);
+-	}
+-	spin_unlock(&pfm_buffer_fmt_lock);
+-
+-}
+-
+-static int
+-pfm_proc_show(struct seq_file *m, void *v)
+-{
+-	unsigned long psr;
+-	unsigned int i;
+-	int cpu;
+-
+-	if (v == PFM_PROC_SHOW_HEADER) {
+-		pfm_proc_show_header(m);
+-		return 0;
+-	}
+-
+-	/* show info for CPU (v - 1) */
+-
+-	cpu = (long)v - 1;
+-	seq_printf(m,
+-		"CPU%-2d overflow intrs      : %lu\n"
+-		"CPU%-2d overflow cycles     : %lu\n"
+-		"CPU%-2d overflow min        : %lu\n"
+-		"CPU%-2d overflow max        : %lu\n"
+-		"CPU%-2d smpl handler calls  : %lu\n"
+-		"CPU%-2d smpl handler cycles : %lu\n"
+-		"CPU%-2d spurious intrs      : %lu\n"
+-		"CPU%-2d replay   intrs      : %lu\n"
+-		"CPU%-2d syst_wide           : %d\n"
+-		"CPU%-2d dcr_pp              : %d\n"
+-		"CPU%-2d exclude idle        : %d\n"
+-		"CPU%-2d owner               : %d\n"
+-		"CPU%-2d context             : %p\n"
+-		"CPU%-2d activations         : %lu\n",
+-		cpu, pfm_stats[cpu].pfm_ovfl_intr_count,
+-		cpu, pfm_stats[cpu].pfm_ovfl_intr_cycles,
+-		cpu, pfm_stats[cpu].pfm_ovfl_intr_cycles_min,
+-		cpu, pfm_stats[cpu].pfm_ovfl_intr_cycles_max,
+-		cpu, pfm_stats[cpu].pfm_smpl_handler_calls,
+-		cpu, pfm_stats[cpu].pfm_smpl_handler_cycles,
+-		cpu, pfm_stats[cpu].pfm_spurious_ovfl_intr_count,
+-		cpu, pfm_stats[cpu].pfm_replay_ovfl_intr_count,
+-		cpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_SYST_WIDE ? 1 : 0,
+-		cpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_DCR_PP ? 1 : 0,
+-		cpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_EXCL_IDLE ? 1 : 0,
+-		cpu, pfm_get_cpu_data(pmu_owner, cpu) ? pfm_get_cpu_data(pmu_owner, cpu)->pid: -1,
+-		cpu, pfm_get_cpu_data(pmu_ctx, cpu),
+-		cpu, pfm_get_cpu_data(pmu_activation_number, cpu));
+-
+-	if (num_online_cpus() == 1 && pfm_sysctl.debug > 0) {
+-
+-		psr = pfm_get_psr();
+-
+-		ia64_srlz_d();
+-
+-		seq_printf(m, 
+-			"CPU%-2d psr                 : 0x%lx\n"
+-			"CPU%-2d pmc0                : 0x%lx\n", 
+-			cpu, psr,
+-			cpu, ia64_get_pmc(0));
+-
+-		for (i=0; PMC_IS_LAST(i) == 0;  i++) {
+-			if (PMC_IS_COUNTING(i) == 0) continue;
+-   			seq_printf(m, 
+-				"CPU%-2d pmc%u                : 0x%lx\n"
+-   				"CPU%-2d pmd%u                : 0x%lx\n", 
+-				cpu, i, ia64_get_pmc(i),
+-				cpu, i, ia64_get_pmd(i));
+-  		}
+-	}
+-	return 0;
+-}
+-
+-struct seq_operations pfm_seq_ops = {
+-	.start =	pfm_proc_start,
+- 	.next =		pfm_proc_next,
+- 	.stop =		pfm_proc_stop,
+- 	.show =		pfm_proc_show
+-};
+-
+-static int
+-pfm_proc_open(struct inode *inode, struct file *file)
+-{
+-	return seq_open(file, &pfm_seq_ops);
+-}
+-
+-
+-/*
+- * we come here as soon as local_cpu_data->pfm_syst_wide is set. this happens
+- * during pfm_enable() hence before pfm_start(). We cannot assume monitoring
+- * is active or inactive based on mode. We must rely on the value in
+- * local_cpu_data->pfm_syst_info
+- */
+-void
+-pfm_syst_wide_update_task(struct task_struct *task, unsigned long info, int is_ctxswin)
+-{
+-	struct pt_regs *regs;
+-	unsigned long dcr;
+-	unsigned long dcr_pp;
+-
+-	dcr_pp = info & PFM_CPUINFO_DCR_PP ? 1 : 0;
+-
+-	/*
+-	 * pid 0 is guaranteed to be the idle task. There is one such task with pid 0
+-	 * on every CPU, so we can rely on the pid to identify the idle task.
+-	 */
+-	if ((info & PFM_CPUINFO_EXCL_IDLE) == 0 || task->pid) {
+-		regs = task_pt_regs(task);
+-		ia64_psr(regs)->pp = is_ctxswin ? dcr_pp : 0;
+-		return;
+-	}
+-	/*
+-	 * if monitoring has started
+-	 */
+-	if (dcr_pp) {
+-		dcr = ia64_getreg(_IA64_REG_CR_DCR);
+-		/*
+-		 * context switching in?
+-		 */
+-		if (is_ctxswin) {
+-			/* mask monitoring for the idle task */
+-			ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
+-			pfm_clear_psr_pp();
+-			ia64_srlz_i();
+-			return;
+-		}
+-		/*
+-		 * context switching out
+-		 * restore monitoring for next task
+-		 *
+-		 * Due to inlining this odd if-then-else construction generates
+-		 * better code.
+-		 */
+-		ia64_setreg(_IA64_REG_CR_DCR, dcr |IA64_DCR_PP);
+-		pfm_set_psr_pp();
+-		ia64_srlz_i();
+-	}
+-}
+-
+-#ifdef CONFIG_SMP
+-
+-static void
+-pfm_force_cleanup(pfm_context_t *ctx, struct pt_regs *regs)
+-{
+-	struct task_struct *task = ctx->ctx_task;
+-
+-	ia64_psr(regs)->up = 0;
+-	ia64_psr(regs)->sp = 1;
+-
+-	if (GET_PMU_OWNER() == task) {
+-		DPRINT(("cleared ownership for [%d]\n",
+-					task_pid_nr(ctx->ctx_task)));
+-		SET_PMU_OWNER(NULL, NULL);
+-	}
+-
+-	/*
+-	 * disconnect the task from the context and vice-versa
+-	 */
+-	PFM_SET_WORK_PENDING(task, 0);
+-
+-	task->thread.pfm_context  = NULL;
+-	task->thread.flags       &= ~IA64_THREAD_PM_VALID;
+-
+-	DPRINT(("force cleanup for [%d]\n",  task_pid_nr(task)));
+-}
+-
+-
+-/*
+- * in 2.6, interrupts are masked when we come here and the runqueue lock is held
+- */
+-void
+-pfm_save_regs(struct task_struct *task)
+-{
+-	pfm_context_t *ctx;
+-	unsigned long flags;
+-	u64 psr;
+-
+-
+-	ctx = PFM_GET_CTX(task);
+-	if (ctx == NULL) return;
+-
+-	/*
+- 	 * we always come here with interrupts ALREADY disabled by
+- 	 * the scheduler. So we simply need to protect against concurrent
+-	 * access, not CPU concurrency.
+-	 */
+-	flags = pfm_protect_ctx_ctxsw(ctx);
+-
+-	if (ctx->ctx_state == PFM_CTX_ZOMBIE) {
+-		struct pt_regs *regs = task_pt_regs(task);
+-
+-		pfm_clear_psr_up();
+-
+-		pfm_force_cleanup(ctx, regs);
+-
+-		BUG_ON(ctx->ctx_smpl_hdr);
+-
+-		pfm_unprotect_ctx_ctxsw(ctx, flags);
+-
+-		pfm_context_free(ctx);
+-		return;
+-	}
+-
+-	/*
+-	 * save current PSR: needed because we modify it
+-	 */
+-	ia64_srlz_d();
+-	psr = pfm_get_psr();
+-
+-	BUG_ON(psr & (IA64_PSR_I));
+-
+-	/*
+-	 * stop monitoring:
+-	 * This is the last instruction which may generate an overflow
+-	 *
+-	 * We do not need to set psr.sp because, it is irrelevant in kernel.
+-	 * It will be restored from ipsr when going back to user level
+-	 */
+-	pfm_clear_psr_up();
+-
+-	/*
+-	 * keep a copy of psr.up (for reload)
+-	 */
+-	ctx->ctx_saved_psr_up = psr & IA64_PSR_UP;
+-
+-	/*
+-	 * release ownership of this PMU.
+-	 * PM interrupts are masked, so nothing
+-	 * can happen.
+-	 */
+-	SET_PMU_OWNER(NULL, NULL);
+-
+-	/*
+-	 * we systematically save the PMD as we have no
+-	 * guarantee we will be schedule at that same
+-	 * CPU again.
+-	 */
+-	pfm_save_pmds(ctx->th_pmds, ctx->ctx_used_pmds[0]);
+-
+-	/*
+-	 * save pmc0 ia64_srlz_d() done in pfm_save_pmds()
+-	 * we will need it on the restore path to check
+-	 * for pending overflow.
+-	 */
+-	ctx->th_pmcs[0] = ia64_get_pmc(0);
+-
+-	/*
+-	 * unfreeze PMU if had pending overflows
+-	 */
+-	if (ctx->th_pmcs[0] & ~0x1UL) pfm_unfreeze_pmu();
+-
+-	/*
+-	 * finally, allow context access.
+-	 * interrupts will still be masked after this call.
+-	 */
+-	pfm_unprotect_ctx_ctxsw(ctx, flags);
+-}
+-
+-#else /* !CONFIG_SMP */
+-void
+-pfm_save_regs(struct task_struct *task)
+-{
+-	pfm_context_t *ctx;
+-	u64 psr;
+-
+-	ctx = PFM_GET_CTX(task);
+-	if (ctx == NULL) return;
+-
+-	/*
+-	 * save current PSR: needed because we modify it
+-	 */
+-	psr = pfm_get_psr();
+-
+-	BUG_ON(psr & (IA64_PSR_I));
+-
+-	/*
+-	 * stop monitoring:
+-	 * This is the last instruction which may generate an overflow
+-	 *
+-	 * We do not need to set psr.sp because, it is irrelevant in kernel.
+-	 * It will be restored from ipsr when going back to user level
+-	 */
+-	pfm_clear_psr_up();
+-
+-	/*
+-	 * keep a copy of psr.up (for reload)
+-	 */
+-	ctx->ctx_saved_psr_up = psr & IA64_PSR_UP;
+-}
+-
+-static void
+-pfm_lazy_save_regs (struct task_struct *task)
+-{
+-	pfm_context_t *ctx;
+-	unsigned long flags;
+-
+-	{ u64 psr  = pfm_get_psr();
+-	  BUG_ON(psr & IA64_PSR_UP);
+-	}
+-
+-	ctx = PFM_GET_CTX(task);
+-
+-	/*
+-	 * we need to mask PMU overflow here to
+-	 * make sure that we maintain pmc0 until
+-	 * we save it. overflow interrupts are
+-	 * treated as spurious if there is no
+-	 * owner.
+-	 *
+-	 * XXX: I don't think this is necessary
+-	 */
+-	PROTECT_CTX(ctx,flags);
+-
+-	/*
+-	 * release ownership of this PMU.
+-	 * must be done before we save the registers.
+-	 *
+-	 * after this call any PMU interrupt is treated
+-	 * as spurious.
+-	 */
+-	SET_PMU_OWNER(NULL, NULL);
+-
+-	/*
+-	 * save all the pmds we use
+-	 */
+-	pfm_save_pmds(ctx->th_pmds, ctx->ctx_used_pmds[0]);
+-
+-	/*
+-	 * save pmc0 ia64_srlz_d() done in pfm_save_pmds()
+-	 * it is needed to check for pended overflow
+-	 * on the restore path
+-	 */
+-	ctx->th_pmcs[0] = ia64_get_pmc(0);
+-
+-	/*
+-	 * unfreeze PMU if had pending overflows
+-	 */
+-	if (ctx->th_pmcs[0] & ~0x1UL) pfm_unfreeze_pmu();
+-
+-	/*
+-	 * now get can unmask PMU interrupts, they will
+-	 * be treated as purely spurious and we will not
+-	 * lose any information
+-	 */
+-	UNPROTECT_CTX(ctx,flags);
+-}
+-#endif /* CONFIG_SMP */
+-
+-#ifdef CONFIG_SMP
+-/*
+- * in 2.6, interrupts are masked when we come here and the runqueue lock is held
+- */
+-void
+-pfm_load_regs (struct task_struct *task)
+-{
+-	pfm_context_t *ctx;
+-	unsigned long pmc_mask = 0UL, pmd_mask = 0UL;
+-	unsigned long flags;
+-	u64 psr, psr_up;
+-	int need_irq_resend;
+-
+-	ctx = PFM_GET_CTX(task);
+-	if (unlikely(ctx == NULL)) return;
+-
+-	BUG_ON(GET_PMU_OWNER());
+-
+-	/*
+-	 * possible on unload
+-	 */
+-	if (unlikely((task->thread.flags & IA64_THREAD_PM_VALID) == 0)) return;
+-
+-	/*
+- 	 * we always come here with interrupts ALREADY disabled by
+- 	 * the scheduler. So we simply need to protect against concurrent
+-	 * access, not CPU concurrency.
+-	 */
+-	flags = pfm_protect_ctx_ctxsw(ctx);
+-	psr   = pfm_get_psr();
+-
+-	need_irq_resend = pmu_conf->flags & PFM_PMU_IRQ_RESEND;
+-
+-	BUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));
+-	BUG_ON(psr & IA64_PSR_I);
+-
+-	if (unlikely(ctx->ctx_state == PFM_CTX_ZOMBIE)) {
+-		struct pt_regs *regs = task_pt_regs(task);
+-
+-		BUG_ON(ctx->ctx_smpl_hdr);
+-
+-		pfm_force_cleanup(ctx, regs);
+-
+-		pfm_unprotect_ctx_ctxsw(ctx, flags);
+-
+-		/*
+-		 * this one (kmalloc'ed) is fine with interrupts disabled
+-		 */
+-		pfm_context_free(ctx);
+-
+-		return;
+-	}
+-
+-	/*
+-	 * we restore ALL the debug registers to avoid picking up
+-	 * stale state.
+-	 */
+-	if (ctx->ctx_fl_using_dbreg) {
+-		pfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);
+-		pfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);
+-	}
+-	/*
+-	 * retrieve saved psr.up
+-	 */
+-	psr_up = ctx->ctx_saved_psr_up;
+-
+-	/*
+-	 * if we were the last user of the PMU on that CPU,
+-	 * then nothing to do except restore psr
+-	 */
+-	if (GET_LAST_CPU(ctx) == smp_processor_id() && ctx->ctx_last_activation == GET_ACTIVATION()) {
+-
+-		/*
+-		 * retrieve partial reload masks (due to user modifications)
+-		 */
+-		pmc_mask = ctx->ctx_reload_pmcs[0];
+-		pmd_mask = ctx->ctx_reload_pmds[0];
+-
+-	} else {
+-		/*
+-	 	 * To avoid leaking information to the user level when psr.sp=0,
+-	 	 * we must reload ALL implemented pmds (even the ones we don't use).
+-	 	 * In the kernel we only allow PFM_READ_PMDS on registers which
+-	 	 * we initialized or requested (sampling) so there is no risk there.
+-	 	 */
+-		pmd_mask = pfm_sysctl.fastctxsw ?  ctx->ctx_used_pmds[0] : ctx->ctx_all_pmds[0];
+-
+-		/*
+-	 	 * ALL accessible PMCs are systematically reloaded, unused registers
+-	 	 * get their default (from pfm_reset_pmu_state()) values to avoid picking
+-	 	 * up stale configuration.
+-	 	 *
+-	 	 * PMC0 is never in the mask. It is always restored separately.
+-	 	 */
+-		pmc_mask = ctx->ctx_all_pmcs[0];
+-	}
+-	/*
+-	 * when context is MASKED, we will restore PMC with plm=0
+-	 * and PMD with stale information, but that's ok, nothing
+-	 * will be captured.
+-	 *
+-	 * XXX: optimize here
+-	 */
+-	if (pmd_mask) pfm_restore_pmds(ctx->th_pmds, pmd_mask);
+-	if (pmc_mask) pfm_restore_pmcs(ctx->th_pmcs, pmc_mask);
+-
+-	/*
+-	 * check for pending overflow at the time the state
+-	 * was saved.
+-	 */
+-	if (unlikely(PMC0_HAS_OVFL(ctx->th_pmcs[0]))) {
+-		/*
+-		 * reload pmc0 with the overflow information
+-		 * On McKinley PMU, this will trigger a PMU interrupt
+-		 */
+-		ia64_set_pmc(0, ctx->th_pmcs[0]);
+-		ia64_srlz_d();
+-		ctx->th_pmcs[0] = 0UL;
+-
+-		/*
+-		 * will replay the PMU interrupt
+-		 */
+-		if (need_irq_resend) ia64_resend_irq(IA64_PERFMON_VECTOR);
+-
+-		pfm_stats[smp_processor_id()].pfm_replay_ovfl_intr_count++;
+-	}
+-
+-	/*
+-	 * we just did a reload, so we reset the partial reload fields
+-	 */
+-	ctx->ctx_reload_pmcs[0] = 0UL;
+-	ctx->ctx_reload_pmds[0] = 0UL;
+-
+-	SET_LAST_CPU(ctx, smp_processor_id());
+-
+-	/*
+-	 * dump activation value for this PMU
+-	 */
+-	INC_ACTIVATION();
+-	/*
+-	 * record current activation for this context
+-	 */
+-	SET_ACTIVATION(ctx);
+-
+-	/*
+-	 * establish new ownership. 
+-	 */
+-	SET_PMU_OWNER(task, ctx);
+-
+-	/*
+-	 * restore the psr.up bit. measurement
+-	 * is active again.
+-	 * no PMU interrupt can happen at this point
+-	 * because we still have interrupts disabled.
+-	 */
+-	if (likely(psr_up)) pfm_set_psr_up();
+-
+-	/*
+-	 * allow concurrent access to context
+-	 */
+-	pfm_unprotect_ctx_ctxsw(ctx, flags);
+-}
+-#else /*  !CONFIG_SMP */
+-/*
+- * reload PMU state for UP kernels
+- * in 2.5 we come here with interrupts disabled
+- */
+-void
+-pfm_load_regs (struct task_struct *task)
+-{
+-	pfm_context_t *ctx;
+-	struct task_struct *owner;
+-	unsigned long pmd_mask, pmc_mask;
+-	u64 psr, psr_up;
+-	int need_irq_resend;
+-
+-	owner = GET_PMU_OWNER();
+-	ctx   = PFM_GET_CTX(task);
+-	psr   = pfm_get_psr();
+-
+-	BUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));
+-	BUG_ON(psr & IA64_PSR_I);
+-
+-	/*
+-	 * we restore ALL the debug registers to avoid picking up
+-	 * stale state.
+-	 *
+-	 * This must be done even when the task is still the owner
+-	 * as the registers may have been modified via ptrace()
+-	 * (not perfmon) by the previous task.
+-	 */
+-	if (ctx->ctx_fl_using_dbreg) {
+-		pfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);
+-		pfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);
+-	}
+-
+-	/*
+-	 * retrieved saved psr.up
+-	 */
+-	psr_up = ctx->ctx_saved_psr_up;
+-	need_irq_resend = pmu_conf->flags & PFM_PMU_IRQ_RESEND;
+-
+-	/*
+-	 * short path, our state is still there, just
+-	 * need to restore psr and we go
+-	 *
+-	 * we do not touch either PMC nor PMD. the psr is not touched
+-	 * by the overflow_handler. So we are safe w.r.t. to interrupt
+-	 * concurrency even without interrupt masking.
+-	 */
+-	if (likely(owner == task)) {
+-		if (likely(psr_up)) pfm_set_psr_up();
+-		return;
+-	}
+-
+-	/*
+-	 * someone else is still using the PMU, first push it out and
+-	 * then we'll be able to install our stuff !
+-	 *
+-	 * Upon return, there will be no owner for the current PMU
+-	 */
+-	if (owner) pfm_lazy_save_regs(owner);
+-
+-	/*
+-	 * To avoid leaking information to the user level when psr.sp=0,
+-	 * we must reload ALL implemented pmds (even the ones we don't use).
+-	 * In the kernel we only allow PFM_READ_PMDS on registers which
+-	 * we initialized or requested (sampling) so there is no risk there.
+-	 */
+-	pmd_mask = pfm_sysctl.fastctxsw ?  ctx->ctx_used_pmds[0] : ctx->ctx_all_pmds[0];
+-
+-	/*
+-	 * ALL accessible PMCs are systematically reloaded, unused registers
+-	 * get their default (from pfm_reset_pmu_state()) values to avoid picking
+-	 * up stale configuration.
+-	 *
+-	 * PMC0 is never in the mask. It is always restored separately
+-	 */
+-	pmc_mask = ctx->ctx_all_pmcs[0];
+-
+-	pfm_restore_pmds(ctx->th_pmds, pmd_mask);
+-	pfm_restore_pmcs(ctx->th_pmcs, pmc_mask);
+-
+-	/*
+-	 * check for pending overflow at the time the state
+-	 * was saved.
+-	 */
+-	if (unlikely(PMC0_HAS_OVFL(ctx->th_pmcs[0]))) {
+-		/*
+-		 * reload pmc0 with the overflow information
+-		 * On McKinley PMU, this will trigger a PMU interrupt
+-		 */
+-		ia64_set_pmc(0, ctx->th_pmcs[0]);
+-		ia64_srlz_d();
+-
+-		ctx->th_pmcs[0] = 0UL;
+-
+-		/*
+-		 * will replay the PMU interrupt
+-		 */
+-		if (need_irq_resend) ia64_resend_irq(IA64_PERFMON_VECTOR);
+-
+-		pfm_stats[smp_processor_id()].pfm_replay_ovfl_intr_count++;
+-	}
+-
+-	/*
+-	 * establish new ownership. 
+-	 */
+-	SET_PMU_OWNER(task, ctx);
+-
+-	/*
+-	 * restore the psr.up bit. measurement
+-	 * is active again.
+-	 * no PMU interrupt can happen at this point
+-	 * because we still have interrupts disabled.
+-	 */
+-	if (likely(psr_up)) pfm_set_psr_up();
+-}
+-#endif /* CONFIG_SMP */
+-
+-/*
+- * this function assumes monitoring is stopped
+- */
+-static void
+-pfm_flush_pmds(struct task_struct *task, pfm_context_t *ctx)
+-{
+-	u64 pmc0;
+-	unsigned long mask2, val, pmd_val, ovfl_val;
+-	int i, can_access_pmu = 0;
+-	int is_self;
+-
+-	/*
+-	 * is the caller the task being monitored (or which initiated the
+-	 * session for system wide measurements)
+-	 */
+-	is_self = ctx->ctx_task == task ? 1 : 0;
+-
+-	/*
+-	 * can access PMU is task is the owner of the PMU state on the current CPU
+-	 * or if we are running on the CPU bound to the context in system-wide mode
+-	 * (that is not necessarily the task the context is attached to in this mode).
+-	 * In system-wide we always have can_access_pmu true because a task running on an
+-	 * invalid processor is flagged earlier in the call stack (see pfm_stop).
+-	 */
+-	can_access_pmu = (GET_PMU_OWNER() == task) || (ctx->ctx_fl_system && ctx->ctx_cpu == smp_processor_id());
+-	if (can_access_pmu) {
+-		/*
+-		 * Mark the PMU as not owned
+-		 * This will cause the interrupt handler to do nothing in case an overflow
+-		 * interrupt was in-flight
+-		 * This also guarantees that pmc0 will contain the final state
+-		 * It virtually gives us full control on overflow processing from that point
+-		 * on.
+-		 */
+-		SET_PMU_OWNER(NULL, NULL);
+-		DPRINT(("releasing ownership\n"));
+-
+-		/*
+-		 * read current overflow status:
+-		 *
+-		 * we are guaranteed to read the final stable state
+-		 */
+-		ia64_srlz_d();
+-		pmc0 = ia64_get_pmc(0); /* slow */
+-
+-		/*
+-		 * reset freeze bit, overflow status information destroyed
+-		 */
+-		pfm_unfreeze_pmu();
+-	} else {
+-		pmc0 = ctx->th_pmcs[0];
+-		/*
+-		 * clear whatever overflow status bits there were
+-		 */
+-		ctx->th_pmcs[0] = 0;
+-	}
+-	ovfl_val = pmu_conf->ovfl_val;
+-	/*
+-	 * we save all the used pmds
+-	 * we take care of overflows for counting PMDs
+-	 *
+-	 * XXX: sampling situation is not taken into account here
+-	 */
+-	mask2 = ctx->ctx_used_pmds[0];
+-
+-	DPRINT(("is_self=%d ovfl_val=0x%lx mask2=0x%lx\n", is_self, ovfl_val, mask2));
+-
+-	for (i = 0; mask2; i++, mask2>>=1) {
+-
+-		/* skip non used pmds */
+-		if ((mask2 & 0x1) == 0) continue;
+-
+-		/*
+-		 * can access PMU always true in system wide mode
+-		 */
+-		val = pmd_val = can_access_pmu ? ia64_get_pmd(i) : ctx->th_pmds[i];
+-
+-		if (PMD_IS_COUNTING(i)) {
+-			DPRINT(("[%d] pmd[%d] ctx_pmd=0x%lx hw_pmd=0x%lx\n",
+-				task_pid_nr(task),
+-				i,
+-				ctx->ctx_pmds[i].val,
+-				val & ovfl_val));
+-
+-			/*
+-			 * we rebuild the full 64 bit value of the counter
+-			 */
+-			val = ctx->ctx_pmds[i].val + (val & ovfl_val);
+-
+-			/*
+-			 * now everything is in ctx_pmds[] and we need
+-			 * to clear the saved context from save_regs() such that
+-			 * pfm_read_pmds() gets the correct value
+-			 */
+-			pmd_val = 0UL;
+-
+-			/*
+-			 * take care of overflow inline
+-			 */
+-			if (pmc0 & (1UL << i)) {
+-				val += 1 + ovfl_val;
+-				DPRINT(("[%d] pmd[%d] overflowed\n", task_pid_nr(task), i));
+-			}
+-		}
+-
+-		DPRINT(("[%d] ctx_pmd[%d]=0x%lx  pmd_val=0x%lx\n", task_pid_nr(task), i, val, pmd_val));
+-
+-		if (is_self) ctx->th_pmds[i] = pmd_val;
+-
+-		ctx->ctx_pmds[i].val = val;
+-	}
+-}
+-
+-static struct irqaction perfmon_irqaction = {
+-	.handler = pfm_interrupt_handler,
+-	.flags   = IRQF_DISABLED,
+-	.name    = "perfmon"
+-};
+-
+-static void
+-pfm_alt_save_pmu_state(void *data)
+-{
+-	struct pt_regs *regs;
+-
+-	regs = task_pt_regs(current);
+-
+-	DPRINT(("called\n"));
+-
+-	/*
+-	 * should not be necessary but
+-	 * let's take not risk
+-	 */
+-	pfm_clear_psr_up();
+-	pfm_clear_psr_pp();
+-	ia64_psr(regs)->pp = 0;
+-
+-	/*
+-	 * This call is required
+-	 * May cause a spurious interrupt on some processors
+-	 */
+-	pfm_freeze_pmu();
+-
+-	ia64_srlz_d();
+-}
+-
+-void
+-pfm_alt_restore_pmu_state(void *data)
+-{
+-	struct pt_regs *regs;
+-
+-	regs = task_pt_regs(current);
+-
+-	DPRINT(("called\n"));
+-
+-	/*
+-	 * put PMU back in state expected
+-	 * by perfmon
+-	 */
+-	pfm_clear_psr_up();
+-	pfm_clear_psr_pp();
+-	ia64_psr(regs)->pp = 0;
+-
+-	/*
+-	 * perfmon runs with PMU unfrozen at all times
+-	 */
+-	pfm_unfreeze_pmu();
+-
+-	ia64_srlz_d();
+-}
+-
+-int
+-pfm_install_alt_pmu_interrupt(pfm_intr_handler_desc_t *hdl)
+-{
+-	int ret, i;
+-	int reserve_cpu;
+-
+-	/* some sanity checks */
+-	if (hdl == NULL || hdl->handler == NULL) return -EINVAL;
+-
+-	/* do the easy test first */
+-	if (pfm_alt_intr_handler) return -EBUSY;
+-
+-	/* one at a time in the install or remove, just fail the others */
+-	if (!spin_trylock(&pfm_alt_install_check)) {
+-		return -EBUSY;
+-	}
+-
+-	/* reserve our session */
+-	for_each_online_cpu(reserve_cpu) {
+-		ret = pfm_reserve_session(NULL, 1, reserve_cpu);
+-		if (ret) goto cleanup_reserve;
+-	}
+-
+-	/* save the current system wide pmu states */
+-	ret = on_each_cpu(pfm_alt_save_pmu_state, NULL, 0, 1);
+-	if (ret) {
+-		DPRINT(("on_each_cpu() failed: %d\n", ret));
+-		goto cleanup_reserve;
+-	}
+-
+-	/* officially change to the alternate interrupt handler */
+-	pfm_alt_intr_handler = hdl;
+-
+-	spin_unlock(&pfm_alt_install_check);
+-
+-	return 0;
+-
+-cleanup_reserve:
+-	for_each_online_cpu(i) {
+-		/* don't unreserve more than we reserved */
+-		if (i >= reserve_cpu) break;
+-
+-		pfm_unreserve_session(NULL, 1, i);
+-	}
+-
+-	spin_unlock(&pfm_alt_install_check);
+-
+-	return ret;
+-}
+-EXPORT_SYMBOL_GPL(pfm_install_alt_pmu_interrupt);
+-
+-int
+-pfm_remove_alt_pmu_interrupt(pfm_intr_handler_desc_t *hdl)
+-{
+-	int i;
+-	int ret;
+-
+-	if (hdl == NULL) return -EINVAL;
+-
+-	/* cannot remove someone else's handler! */
+-	if (pfm_alt_intr_handler != hdl) return -EINVAL;
+-
+-	/* one at a time in the install or remove, just fail the others */
+-	if (!spin_trylock(&pfm_alt_install_check)) {
+-		return -EBUSY;
+-	}
+-
+-	pfm_alt_intr_handler = NULL;
+-
+-	ret = on_each_cpu(pfm_alt_restore_pmu_state, NULL, 0, 1);
+-	if (ret) {
+-		DPRINT(("on_each_cpu() failed: %d\n", ret));
+-	}
+-
+-	for_each_online_cpu(i) {
+-		pfm_unreserve_session(NULL, 1, i);
+-	}
+-
+-	spin_unlock(&pfm_alt_install_check);
+-
+-	return 0;
+-}
+-EXPORT_SYMBOL_GPL(pfm_remove_alt_pmu_interrupt);
+-
+-/*
+- * perfmon initialization routine, called from the initcall() table
+- */
+-static int init_pfm_fs(void);
+-
+-static int __init
+-pfm_probe_pmu(void)
+-{
+-	pmu_config_t **p;
+-	int family;
+-
+-	family = local_cpu_data->family;
+-	p      = pmu_confs;
+-
+-	while(*p) {
+-		if ((*p)->probe) {
+-			if ((*p)->probe() == 0) goto found;
+-		} else if ((*p)->pmu_family == family || (*p)->pmu_family == 0xff) {
+-			goto found;
+-		}
+-		p++;
+-	}
+-	return -1;
+-found:
+-	pmu_conf = *p;
+-	return 0;
+-}
+-
+-static const struct file_operations pfm_proc_fops = {
+-	.open		= pfm_proc_open,
+-	.read		= seq_read,
+-	.llseek		= seq_lseek,
+-	.release	= seq_release,
+-};
+-
+-int __init
+-pfm_init(void)
+-{
+-	unsigned int n, n_counters, i;
+-
+-	printk("perfmon: version %u.%u IRQ %u\n",
+-		PFM_VERSION_MAJ,
+-		PFM_VERSION_MIN,
+-		IA64_PERFMON_VECTOR);
+-
+-	if (pfm_probe_pmu()) {
+-		printk(KERN_INFO "perfmon: disabled, there is no support for processor family %d\n", 
+-				local_cpu_data->family);
+-		return -ENODEV;
+-	}
+-
+-	/*
+-	 * compute the number of implemented PMD/PMC from the
+-	 * description tables
+-	 */
+-	n = 0;
+-	for (i=0; PMC_IS_LAST(i) == 0;  i++) {
+-		if (PMC_IS_IMPL(i) == 0) continue;
+-		pmu_conf->impl_pmcs[i>>6] |= 1UL << (i&63);
+-		n++;
+-	}
+-	pmu_conf->num_pmcs = n;
+-
+-	n = 0; n_counters = 0;
+-	for (i=0; PMD_IS_LAST(i) == 0;  i++) {
+-		if (PMD_IS_IMPL(i) == 0) continue;
+-		pmu_conf->impl_pmds[i>>6] |= 1UL << (i&63);
+-		n++;
+-		if (PMD_IS_COUNTING(i)) n_counters++;
+-	}
+-	pmu_conf->num_pmds      = n;
+-	pmu_conf->num_counters  = n_counters;
+-
+-	/*
+-	 * sanity checks on the number of debug registers
+-	 */
+-	if (pmu_conf->use_rr_dbregs) {
+-		if (pmu_conf->num_ibrs > IA64_NUM_DBG_REGS) {
+-			printk(KERN_INFO "perfmon: unsupported number of code debug registers (%u)\n", pmu_conf->num_ibrs);
+-			pmu_conf = NULL;
+-			return -1;
+-		}
+-		if (pmu_conf->num_dbrs > IA64_NUM_DBG_REGS) {
+-			printk(KERN_INFO "perfmon: unsupported number of data debug registers (%u)\n", pmu_conf->num_ibrs);
+-			pmu_conf = NULL;
+-			return -1;
+-		}
+-	}
+-
+-	printk("perfmon: %s PMU detected, %u PMCs, %u PMDs, %u counters (%lu bits)\n",
+-	       pmu_conf->pmu_name,
+-	       pmu_conf->num_pmcs,
+-	       pmu_conf->num_pmds,
+-	       pmu_conf->num_counters,
+-	       ffz(pmu_conf->ovfl_val));
+-
+-	/* sanity check */
+-	if (pmu_conf->num_pmds >= PFM_NUM_PMD_REGS || pmu_conf->num_pmcs >= PFM_NUM_PMC_REGS) {
+-		printk(KERN_ERR "perfmon: not enough pmc/pmd, perfmon disabled\n");
+-		pmu_conf = NULL;
+-		return -1;
+-	}
+-
+-	/*
+-	 * create /proc/perfmon (mostly for debugging purposes)
+-	 */
+- 	perfmon_dir = create_proc_entry("perfmon", S_IRUGO, NULL);
+-	if (perfmon_dir == NULL) {
+-		printk(KERN_ERR "perfmon: cannot create /proc entry, perfmon disabled\n");
+-		pmu_conf = NULL;
+-		return -1;
+-	}
+-  	/*
+- 	 * install customized file operations for /proc/perfmon entry
+- 	 */
+- 	perfmon_dir->proc_fops = &pfm_proc_fops;
+-
+-	/*
+-	 * create /proc/sys/kernel/perfmon (for debugging purposes)
+-	 */
+-	pfm_sysctl_header = register_sysctl_table(pfm_sysctl_root);
+-
+-	/*
+-	 * initialize all our spinlocks
+-	 */
+-	spin_lock_init(&pfm_sessions.pfs_lock);
+-	spin_lock_init(&pfm_buffer_fmt_lock);
+-
+-	init_pfm_fs();
+-
+-	for(i=0; i < NR_CPUS; i++) pfm_stats[i].pfm_ovfl_intr_cycles_min = ~0UL;
+-
+-	return 0;
+-}
+-
+-__initcall(pfm_init);
+-
+-/*
+- * this function is called before pfm_init()
+- */
+-void
+-pfm_init_percpu (void)
+-{
+-	static int first_time=1;
+-	/*
+-	 * make sure no measurement is active
+-	 * (may inherit programmed PMCs from EFI).
+-	 */
+-	pfm_clear_psr_pp();
+-	pfm_clear_psr_up();
+-
+-	/*
+-	 * we run with the PMU not frozen at all times
+-	 */
+-	pfm_unfreeze_pmu();
+-
+-	if (first_time) {
+-		register_percpu_irq(IA64_PERFMON_VECTOR, &perfmon_irqaction);
+-		first_time=0;
+-	}
+-
+-	ia64_setreg(_IA64_REG_CR_PMV, IA64_PERFMON_VECTOR);
+-	ia64_srlz_d();
+-}
+-
+-/*
+- * used for debug purposes only
+- */
+-void
+-dump_pmu_state(const char *from)
+-{
+-	struct task_struct *task;
+-	struct pt_regs *regs;
+-	pfm_context_t *ctx;
+-	unsigned long psr, dcr, info, flags;
+-	int i, this_cpu;
+-
+-	local_irq_save(flags);
+-
+-	this_cpu = smp_processor_id();
+-	regs     = task_pt_regs(current);
+-	info     = PFM_CPUINFO_GET();
+-	dcr      = ia64_getreg(_IA64_REG_CR_DCR);
+-
+-	if (info == 0 && ia64_psr(regs)->pp == 0 && (dcr & IA64_DCR_PP) == 0) {
+-		local_irq_restore(flags);
+-		return;
+-	}
+-
+-	printk("CPU%d from %s() current [%d] iip=0x%lx %s\n", 
+-		this_cpu, 
+-		from, 
+-		task_pid_nr(current),
+-		regs->cr_iip,
+-		current->comm);
+-
+-	task = GET_PMU_OWNER();
+-	ctx  = GET_PMU_CTX();
+-
+-	printk("->CPU%d owner [%d] ctx=%p\n", this_cpu, task ? task_pid_nr(task) : -1, ctx);
+-
+-	psr = pfm_get_psr();
+-
+-	printk("->CPU%d pmc0=0x%lx psr.pp=%d psr.up=%d dcr.pp=%d syst_info=0x%lx user_psr.up=%d user_psr.pp=%d\n", 
+-		this_cpu,
+-		ia64_get_pmc(0),
+-		psr & IA64_PSR_PP ? 1 : 0,
+-		psr & IA64_PSR_UP ? 1 : 0,
+-		dcr & IA64_DCR_PP ? 1 : 0,
+-		info,
+-		ia64_psr(regs)->up,
+-		ia64_psr(regs)->pp);
+-
+-	ia64_psr(regs)->up = 0;
+-	ia64_psr(regs)->pp = 0;
+-
+-	for (i=1; PMC_IS_LAST(i) == 0; i++) {
+-		if (PMC_IS_IMPL(i) == 0) continue;
+-		printk("->CPU%d pmc[%d]=0x%lx thread_pmc[%d]=0x%lx\n", this_cpu, i, ia64_get_pmc(i), i, ctx->th_pmcs[i]);
+-	}
+-
+-	for (i=1; PMD_IS_LAST(i) == 0; i++) {
+-		if (PMD_IS_IMPL(i) == 0) continue;
+-		printk("->CPU%d pmd[%d]=0x%lx thread_pmd[%d]=0x%lx\n", this_cpu, i, ia64_get_pmd(i), i, ctx->th_pmds[i]);
+-	}
+-
+-	if (ctx) {
+-		printk("->CPU%d ctx_state=%d vaddr=%p addr=%p fd=%d ctx_task=[%d] saved_psr_up=0x%lx\n",
+-				this_cpu,
+-				ctx->ctx_state,
+-				ctx->ctx_smpl_vaddr,
+-				ctx->ctx_smpl_hdr,
+-				ctx->ctx_msgq_head,
+-				ctx->ctx_msgq_tail,
+-				ctx->ctx_saved_psr_up);
+-	}
+-	local_irq_restore(flags);
+-}
+-
+-/*
+- * called from process.c:copy_thread(). task is new child.
+- */
+-void
+-pfm_inherit(struct task_struct *task, struct pt_regs *regs)
+-{
+-	struct thread_struct *thread;
+-
+-	DPRINT(("perfmon: pfm_inherit clearing state for [%d]\n", task_pid_nr(task)));
+-
+-	thread = &task->thread;
+-
+-	/*
+-	 * cut links inherited from parent (current)
+-	 */
+-	thread->pfm_context = NULL;
+-
+-	PFM_SET_WORK_PENDING(task, 0);
+-
+-	/*
+-	 * the psr bits are already set properly in copy_threads()
+-	 */
+-}
+-#else  /* !CONFIG_PERFMON */
+-asmlinkage long
+-sys_perfmonctl (int fd, int cmd, void *arg, int count)
+-{
+-	return -ENOSYS;
+-}
+-#endif /* CONFIG_PERFMON */
+--- a/arch/ia64/kernel/perfmon_default_smpl.c
++++ /dev/null
+@@ -1,296 +0,0 @@
+-/*
+- * Copyright (C) 2002-2003 Hewlett-Packard Co
+- *               Stephane Eranian <eranian@hpl.hp.com>
+- *
+- * This file implements the default sampling buffer format
+- * for the Linux/ia64 perfmon-2 subsystem.
+- */
+-#include <linux/kernel.h>
+-#include <linux/types.h>
+-#include <linux/module.h>
+-#include <linux/init.h>
+-#include <asm/delay.h>
+-#include <linux/smp.h>
+-
+-#include <asm/perfmon.h>
+-#include <asm/perfmon_default_smpl.h>
+-
+-MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+-MODULE_DESCRIPTION("perfmon default sampling format");
+-MODULE_LICENSE("GPL");
+-
+-#define DEFAULT_DEBUG 1
+-
+-#ifdef DEFAULT_DEBUG
+-#define DPRINT(a) \
+-	do { \
+-		if (unlikely(pfm_sysctl.debug >0)) { printk("%s.%d: CPU%d ", __FUNCTION__, __LINE__, smp_processor_id()); printk a; } \
+-	} while (0)
+-
+-#define DPRINT_ovfl(a) \
+-	do { \
+-		if (unlikely(pfm_sysctl.debug > 0 && pfm_sysctl.debug_ovfl >0)) { printk("%s.%d: CPU%d ", __FUNCTION__, __LINE__, smp_processor_id()); printk a; } \
+-	} while (0)
+-
+-#else
+-#define DPRINT(a)
+-#define DPRINT_ovfl(a)
+-#endif
+-
+-static int
+-default_validate(struct task_struct *task, unsigned int flags, int cpu, void *data)
+-{
+-	pfm_default_smpl_arg_t *arg = (pfm_default_smpl_arg_t*)data;
+-	int ret = 0;
+-
+-	if (data == NULL) {
+-		DPRINT(("[%d] no argument passed\n", task_pid_nr(task)));
+-		return -EINVAL;
+-	}
+-
+-	DPRINT(("[%d] validate flags=0x%x CPU%d\n", task_pid_nr(task), flags, cpu));
+-
+-	/*
+-	 * must hold at least the buffer header + one minimally sized entry
+-	 */
+-	if (arg->buf_size < PFM_DEFAULT_SMPL_MIN_BUF_SIZE) return -EINVAL;
+-
+-	DPRINT(("buf_size=%lu\n", arg->buf_size));
+-
+-	return ret;
+-}
+-
+-static int
+-default_get_size(struct task_struct *task, unsigned int flags, int cpu, void *data, unsigned long *size)
+-{
+-	pfm_default_smpl_arg_t *arg = (pfm_default_smpl_arg_t *)data;
+-
+-	/*
+-	 * size has been validated in default_validate
+-	 */
+-	*size = arg->buf_size;
+-
+-	return 0;
+-}
+-
+-static int
+-default_init(struct task_struct *task, void *buf, unsigned int flags, int cpu, void *data)
+-{
+-	pfm_default_smpl_hdr_t *hdr;
+-	pfm_default_smpl_arg_t *arg = (pfm_default_smpl_arg_t *)data;
+-
+-	hdr = (pfm_default_smpl_hdr_t *)buf;
+-
+-	hdr->hdr_version      = PFM_DEFAULT_SMPL_VERSION;
+-	hdr->hdr_buf_size     = arg->buf_size;
+-	hdr->hdr_cur_offs     = sizeof(*hdr);
+-	hdr->hdr_overflows    = 0UL;
+-	hdr->hdr_count        = 0UL;
+-
+-	DPRINT(("[%d] buffer=%p buf_size=%lu hdr_size=%lu hdr_version=%u cur_offs=%lu\n",
+-		task_pid_nr(task),
+-		buf,
+-		hdr->hdr_buf_size,
+-		sizeof(*hdr),
+-		hdr->hdr_version,
+-		hdr->hdr_cur_offs));
+-
+-	return 0;
+-}
+-
+-static int
+-default_handler(struct task_struct *task, void *buf, pfm_ovfl_arg_t *arg, struct pt_regs *regs, unsigned long stamp)
+-{
+-	pfm_default_smpl_hdr_t *hdr;
+-	pfm_default_smpl_entry_t *ent;
+-	void *cur, *last;
+-	unsigned long *e, entry_size;
+-	unsigned int npmds, i;
+-	unsigned char ovfl_pmd;
+-	unsigned char ovfl_notify;
+-
+-	if (unlikely(buf == NULL || arg == NULL|| regs == NULL || task == NULL)) {
+-		DPRINT(("[%d] invalid arguments buf=%p arg=%p\n", task->pid, buf, arg));
+-		return -EINVAL;
+-	}
+-
+-	hdr         = (pfm_default_smpl_hdr_t *)buf;
+-	cur         = buf+hdr->hdr_cur_offs;
+-	last        = buf+hdr->hdr_buf_size;
+-	ovfl_pmd    = arg->ovfl_pmd;
+-	ovfl_notify = arg->ovfl_notify;
+-
+-	/*
+-	 * precheck for sanity
+-	 */
+-	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE) goto full;
+-
+-	npmds = hweight64(arg->smpl_pmds[0]);
+-
+-	ent = (pfm_default_smpl_entry_t *)cur;
+-
+-	prefetch(arg->smpl_pmds_values);
+-
+-	entry_size = sizeof(*ent) + (npmds << 3);
+-
+-	/* position for first pmd */
+-	e = (unsigned long *)(ent+1);
+-
+-	hdr->hdr_count++;
+-
+-	DPRINT_ovfl(("[%d] count=%lu cur=%p last=%p free_bytes=%lu ovfl_pmd=%d ovfl_notify=%d npmds=%u\n",
+-			task->pid,
+-			hdr->hdr_count,
+-			cur, last,
+-			last-cur,
+-			ovfl_pmd,
+-			ovfl_notify, npmds));
+-
+-	/*
+-	 * current = task running at the time of the overflow.
+-	 *
+-	 * per-task mode:
+-	 * 	- this is ususally the task being monitored.
+-	 * 	  Under certain conditions, it might be a different task
+-	 *
+-	 * system-wide:
+-	 * 	- this is not necessarily the task controlling the session
+-	 */
+-	ent->pid            = current->pid;
+-	ent->ovfl_pmd  	    = ovfl_pmd;
+-	ent->last_reset_val = arg->pmd_last_reset; //pmd[0].reg_last_reset_val;
+-
+-	/*
+-	 * where did the fault happen (includes slot number)
+-	 */
+-	ent->ip = regs->cr_iip | ((regs->cr_ipsr >> 41) & 0x3);
+-
+-	ent->tstamp    = stamp;
+-	ent->cpu       = smp_processor_id();
+-	ent->set       = arg->active_set;
+-	ent->tgid      = current->tgid;
+-
+-	/*
+-	 * selectively store PMDs in increasing index number
+-	 */
+-	if (npmds) {
+-		unsigned long *val = arg->smpl_pmds_values;
+-		for(i=0; i < npmds; i++) {
+-			*e++ = *val++;
+-		}
+-	}
+-
+-	/*
+-	 * update position for next entry
+-	 */
+-	hdr->hdr_cur_offs += entry_size;
+-	cur               += entry_size;
+-
+-	/*
+-	 * post check to avoid losing the last sample
+-	 */
+-	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE) goto full;
+-
+-	/*
+-	 * keep same ovfl_pmds, ovfl_notify
+-	 */
+-	arg->ovfl_ctrl.bits.notify_user     = 0;
+-	arg->ovfl_ctrl.bits.block_task      = 0;
+-	arg->ovfl_ctrl.bits.mask_monitoring = 0;
+-	arg->ovfl_ctrl.bits.reset_ovfl_pmds = 1; /* reset before returning from interrupt handler */
+-
+-	return 0;
+-full:
+-	DPRINT_ovfl(("sampling buffer full free=%lu, count=%lu, ovfl_notify=%d\n", last-cur, hdr->hdr_count, ovfl_notify));
+-
+-	/*
+-	 * increment number of buffer overflow.
+-	 * important to detect duplicate set of samples.
+-	 */
+-	hdr->hdr_overflows++;
+-
+-	/*
+-	 * if no notification requested, then we saturate the buffer
+-	 */
+-	if (ovfl_notify == 0) {
+-		arg->ovfl_ctrl.bits.notify_user     = 0;
+-		arg->ovfl_ctrl.bits.block_task      = 0;
+-		arg->ovfl_ctrl.bits.mask_monitoring = 1;
+-		arg->ovfl_ctrl.bits.reset_ovfl_pmds = 0;
+-	} else {
+-		arg->ovfl_ctrl.bits.notify_user     = 1;
+-		arg->ovfl_ctrl.bits.block_task      = 1; /* ignored for non-blocking context */
+-		arg->ovfl_ctrl.bits.mask_monitoring = 1;
+-		arg->ovfl_ctrl.bits.reset_ovfl_pmds = 0; /* no reset now */
+-	}
+-	return -1; /* we are full, sorry */
+-}
+-
+-static int
+-default_restart(struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs)
+-{
+-	pfm_default_smpl_hdr_t *hdr;
+-
+-	hdr = (pfm_default_smpl_hdr_t *)buf;
+-
+-	hdr->hdr_count    = 0UL;
+-	hdr->hdr_cur_offs = sizeof(*hdr);
+-
+-	ctrl->bits.mask_monitoring = 0;
+-	ctrl->bits.reset_ovfl_pmds = 1; /* uses long-reset values */
+-
+-	return 0;
+-}
+-
+-static int
+-default_exit(struct task_struct *task, void *buf, struct pt_regs *regs)
+-{
+-	DPRINT(("[%d] exit(%p)\n", task_pid_nr(task), buf));
+-	return 0;
+-}
+-
+-static pfm_buffer_fmt_t default_fmt={
+- 	.fmt_name 	    = "default_format",
+- 	.fmt_uuid	    = PFM_DEFAULT_SMPL_UUID,
+- 	.fmt_arg_size	    = sizeof(pfm_default_smpl_arg_t),
+- 	.fmt_validate	    = default_validate,
+- 	.fmt_getsize	    = default_get_size,
+- 	.fmt_init	    = default_init,
+- 	.fmt_handler	    = default_handler,
+- 	.fmt_restart	    = default_restart,
+- 	.fmt_restart_active = default_restart,
+- 	.fmt_exit	    = default_exit,
+-};
+-
+-static int __init
+-pfm_default_smpl_init_module(void)
+-{
+-	int ret;
+-
+-	ret = pfm_register_buffer_fmt(&default_fmt);
+-	if (ret == 0) {
+-		printk("perfmon_default_smpl: %s v%u.%u registered\n",
+-			default_fmt.fmt_name,
+-			PFM_DEFAULT_SMPL_VERSION_MAJ,
+-			PFM_DEFAULT_SMPL_VERSION_MIN);
+-	} else {
+-		printk("perfmon_default_smpl: %s cannot register ret=%d\n",
+-			default_fmt.fmt_name,
+-			ret);
+-	}
+-
+-	return ret;
+-}
+-
+-static void __exit
+-pfm_default_smpl_cleanup_module(void)
+-{
+-	int ret;
+-	ret = pfm_unregister_buffer_fmt(default_fmt.fmt_uuid);
+-
+-	printk("perfmon_default_smpl: unregister %s=%d\n", default_fmt.fmt_name, ret);
+-}
+-
+-module_init(pfm_default_smpl_init_module);
+-module_exit(pfm_default_smpl_cleanup_module);
+-
+--- a/arch/ia64/kernel/perfmon_generic.h
++++ /dev/null
+@@ -1,45 +0,0 @@
+-/*
+- * This file contains the generic PMU register description tables
+- * and pmc checker used by perfmon.c.
+- *
+- * Copyright (C) 2002-2003  Hewlett Packard Co
+- *               Stephane Eranian <eranian@hpl.hp.com>
+- */
+-
+-static pfm_reg_desc_t pfm_gen_pmc_desc[PMU_MAX_PMCS]={
+-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
+-};
+-
+-static pfm_reg_desc_t pfm_gen_pmd_desc[PMU_MAX_PMDS]={
+-/* pmd0  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
+-/* pmd1  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
+-/* pmd2  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
+-/* pmd3  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
+-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
+-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
+-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
+-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
+-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
+-};
+-
+-/*
+- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+- */
+-static pmu_config_t pmu_conf_gen={
+-	.pmu_name   = "Generic",
+-	.pmu_family = 0xff, /* any */
+-	.ovfl_val   = (1UL << 32) - 1,
+-	.num_ibrs   = 0, /* does not use */
+-	.num_dbrs   = 0, /* does not use */
+-	.pmd_desc   = pfm_gen_pmd_desc,
+-	.pmc_desc   = pfm_gen_pmc_desc
+-};
+-
+--- a/arch/ia64/kernel/perfmon_itanium.h
++++ /dev/null
+@@ -1,115 +0,0 @@
+-/*
+- * This file contains the Itanium PMU register description tables
+- * and pmc checker used by perfmon.c.
+- *
+- * Copyright (C) 2002-2003  Hewlett Packard Co
+- *               Stephane Eranian <eranian@hpl.hp.com>
+- */
+-static int pfm_ita_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
+-
+-static pfm_reg_desc_t pfm_ita_pmc_desc[PMU_MAX_PMCS]={
+-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc8  */ { PFM_REG_CONFIG  , 0, 0xf00000003ffffff8UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc9  */ { PFM_REG_CONFIG  , 0, 0xf00000003ffffff8UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc10 */ { PFM_REG_MONITOR , 6, 0x0UL, -1UL, NULL, NULL, {RDEP(0)|RDEP(1),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc11 */ { PFM_REG_MONITOR , 6, 0x0000000010000000UL, -1UL, NULL, pfm_ita_pmc_check, {RDEP(2)|RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc12 */ { PFM_REG_MONITOR , 6, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc13 */ { PFM_REG_CONFIG  , 0, 0x0003ffff00000001UL, -1UL, NULL, pfm_ita_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
+-};
+-
+-static pfm_reg_desc_t pfm_ita_pmd_desc[PMU_MAX_PMDS]={
+-/* pmd0  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(1),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
+-/* pmd1  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(0),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
+-/* pmd2  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
+-/* pmd3  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
+-/* pmd4  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
+-/* pmd5  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
+-/* pmd6  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
+-/* pmd7  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
+-/* pmd8  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd9  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd10 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd11 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd12 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd13 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd14 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd15 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd16 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd17 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(3),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
+-	    { PFM_REG_END     , 0, 0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
+-};
+-
+-static int
+-pfm_ita_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
+-{
+-	int ret;
+-	int is_loaded;
+-
+-	/* sanitfy check */
+-	if (ctx == NULL) return -EINVAL;
+-
+-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
+-
+-	/*
+-	 * we must clear the (instruction) debug registers if pmc13.ta bit is cleared
+-	 * before they are written (fl_using_dbreg==0) to avoid picking up stale information.
+-	 */
+-	if (cnum == 13 && is_loaded && ((*val & 0x1) == 0UL) && ctx->ctx_fl_using_dbreg == 0) {
+-
+-		DPRINT(("pmc[%d]=0x%lx has active pmc13.ta cleared, clearing ibr\n", cnum, *val));
+-
+-		/* don't mix debug with perfmon */
+-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
+-
+-		/*
+-		 * a count of 0 will mark the debug registers as in use and also
+-		 * ensure that they are properly cleared.
+-		 */
+-		ret = pfm_write_ibr_dbr(1, ctx, NULL, 0, regs);
+-		if (ret) return ret;
+-	}
+-
+-	/*
+-	 * we must clear the (data) debug registers if pmc11.pt bit is cleared
+-	 * before they are written (fl_using_dbreg==0) to avoid picking up stale information.
+-	 */
+-	if (cnum == 11 && is_loaded && ((*val >> 28)& 0x1) == 0 && ctx->ctx_fl_using_dbreg == 0) {
+-
+-		DPRINT(("pmc[%d]=0x%lx has active pmc11.pt cleared, clearing dbr\n", cnum, *val));
+-
+-		/* don't mix debug with perfmon */
+-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
+-
+-		/*
+-		 * a count of 0 will mark the debug registers as in use and also
+-		 * ensure that they are properly cleared.
+-		 */
+-		ret = pfm_write_ibr_dbr(0, ctx, NULL, 0, regs);
+-		if (ret) return ret;
+-	}
+-	return 0;
+-}
+-
+-/*
+- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+- */
+-static pmu_config_t pmu_conf_ita={
+-	.pmu_name      = "Itanium",
+-	.pmu_family    = 0x7,
+-	.ovfl_val      = (1UL << 32) - 1,
+-	.pmd_desc      = pfm_ita_pmd_desc,
+-	.pmc_desc      = pfm_ita_pmc_desc,
+-	.num_ibrs      = 8,
+-	.num_dbrs      = 8,
+-	.use_rr_dbregs = 1, /* debug register are use for range retrictions */
+-};
+-
+-
+--- a/arch/ia64/kernel/perfmon_mckinley.h
++++ /dev/null
+@@ -1,187 +0,0 @@
+-/*
+- * This file contains the McKinley PMU register description tables
+- * and pmc checker used by perfmon.c.
+- *
+- * Copyright (C) 2002-2003  Hewlett Packard Co
+- *               Stephane Eranian <eranian@hpl.hp.com>
+- */
+-static int pfm_mck_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
+-
+-static pfm_reg_desc_t pfm_mck_pmc_desc[PMU_MAX_PMCS]={
+-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x0000000000800000UL, 0xfffff7fUL, NULL, pfm_mck_pmc_check, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc8  */ { PFM_REG_CONFIG  , 0, 0xffffffff3fffffffUL, 0xffffffff3ffffffbUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc9  */ { PFM_REG_CONFIG  , 0, 0xffffffff3ffffffcUL, 0xffffffff3ffffffbUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc10 */ { PFM_REG_MONITOR , 4, 0x0UL, 0xffffUL, NULL, pfm_mck_pmc_check, {RDEP(0)|RDEP(1),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc11 */ { PFM_REG_MONITOR , 6, 0x0UL, 0x30f01cf, NULL,  pfm_mck_pmc_check, {RDEP(2)|RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc12 */ { PFM_REG_MONITOR , 6, 0x0UL, 0xffffUL, NULL,  pfm_mck_pmc_check, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc13 */ { PFM_REG_CONFIG  , 0, 0x00002078fefefefeUL, 0x1e00018181818UL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc14 */ { PFM_REG_CONFIG  , 0, 0x0db60db60db60db6UL, 0x2492UL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-/* pmc15 */ { PFM_REG_CONFIG  , 0, 0x00000000fffffff0UL, 0xfUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
+-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
+-};
+-
+-static pfm_reg_desc_t pfm_mck_pmd_desc[PMU_MAX_PMDS]={
+-/* pmd0  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(1),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
+-/* pmd1  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(0),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
+-/* pmd2  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
+-/* pmd3  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
+-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
+-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
+-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
+-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
+-/* pmd8  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd9  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd10 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd11 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd12 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd13 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd14 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd15 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd16 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
+-/* pmd17 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(3),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
+-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
+-};
+-
+-/*
+- * PMC reserved fields must have their power-up values preserved
+- */
+-static int
+-pfm_mck_reserved(unsigned int cnum, unsigned long *val, struct pt_regs *regs)
+-{
+-	unsigned long tmp1, tmp2, ival = *val;
+-
+-	/* remove reserved areas from user value */
+-	tmp1 = ival & PMC_RSVD_MASK(cnum);
+-
+-	/* get reserved fields values */
+-	tmp2 = PMC_DFL_VAL(cnum) & ~PMC_RSVD_MASK(cnum);
+-
+-	*val = tmp1 | tmp2;
+-
+-	DPRINT(("pmc[%d]=0x%lx, mask=0x%lx, reset=0x%lx, val=0x%lx\n",
+-		  cnum, ival, PMC_RSVD_MASK(cnum), PMC_DFL_VAL(cnum), *val));
+-	return 0;
+-}
+-
+-/*
+- * task can be NULL if the context is unloaded
+- */
+-static int
+-pfm_mck_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
+-{
+-	int ret = 0, check_case1 = 0;
+-	unsigned long val8 = 0, val14 = 0, val13 = 0;
+-	int is_loaded;
+-
+-	/* first preserve the reserved fields */
+-	pfm_mck_reserved(cnum, val, regs);
+-
+-	/* sanitfy check */
+-	if (ctx == NULL) return -EINVAL;
+-
+-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
+-
+-	/*
+-	 * we must clear the debug registers if pmc13 has a value which enable
+-	 * memory pipeline event constraints. In this case we need to clear the
+-	 * the debug registers if they have not yet been accessed. This is required
+-	 * to avoid picking stale state.
+-	 * PMC13 is "active" if:
+-	 * 	one of the pmc13.cfg_dbrpXX field is different from 0x3
+-	 * AND
+-	 * 	at the corresponding pmc13.ena_dbrpXX is set.
+-	 */
+-	DPRINT(("cnum=%u val=0x%lx, using_dbreg=%d loaded=%d\n", cnum, *val, ctx->ctx_fl_using_dbreg, is_loaded));
+-
+-	if (cnum == 13 && is_loaded
+-	    && (*val & 0x1e00000000000UL) && (*val & 0x18181818UL) != 0x18181818UL && ctx->ctx_fl_using_dbreg == 0) {
+-
+-		DPRINT(("pmc[%d]=0x%lx has active pmc13 settings, clearing dbr\n", cnum, *val));
+-
+-		/* don't mix debug with perfmon */
+-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
+-
+-		/*
+-		 * a count of 0 will mark the debug registers as in use and also
+-		 * ensure that they are properly cleared.
+-		 */
+-		ret = pfm_write_ibr_dbr(PFM_DATA_RR, ctx, NULL, 0, regs);
+-		if (ret) return ret;
+-	}
+-	/*
+-	 * we must clear the (instruction) debug registers if any pmc14.ibrpX bit is enabled
+-	 * before they are (fl_using_dbreg==0) to avoid picking up stale information.
+-	 */
+-	if (cnum == 14 && is_loaded && ((*val & 0x2222UL) != 0x2222UL) && ctx->ctx_fl_using_dbreg == 0) {
+-
+-		DPRINT(("pmc[%d]=0x%lx has active pmc14 settings, clearing ibr\n", cnum, *val));
+-
+-		/* don't mix debug with perfmon */
+-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
+-
+-		/*
+-		 * a count of 0 will mark the debug registers as in use and also
+-		 * ensure that they are properly cleared.
+-		 */
+-		ret = pfm_write_ibr_dbr(PFM_CODE_RR, ctx, NULL, 0, regs);
+-		if (ret) return ret;
+-
+-	}
+-
+-	switch(cnum) {
+-		case  4: *val |= 1UL << 23; /* force power enable bit */
+-			 break;
+-		case  8: val8 = *val;
+-			 val13 = ctx->ctx_pmcs[13];
+-			 val14 = ctx->ctx_pmcs[14];
+-			 check_case1 = 1;
+-			 break;
+-		case 13: val8  = ctx->ctx_pmcs[8];
+-			 val13 = *val;
+-			 val14 = ctx->ctx_pmcs[14];
+-			 check_case1 = 1;
+-			 break;
+-		case 14: val8  = ctx->ctx_pmcs[8];
+-			 val13 = ctx->ctx_pmcs[13];
+-			 val14 = *val;
+-			 check_case1 = 1;
+-			 break;
+-	}
+-	/* check illegal configuration which can produce inconsistencies in tagging
+-	 * i-side events in L1D and L2 caches
+-	 */
+-	if (check_case1) {
+-		ret =   ((val13 >> 45) & 0xf) == 0
+-		   && ((val8 & 0x1) == 0)
+-		   && ((((val14>>1) & 0x3) == 0x2 || ((val14>>1) & 0x3) == 0x0)
+-		       ||(((val14>>4) & 0x3) == 0x2 || ((val14>>4) & 0x3) == 0x0));
+-
+-		if (ret) DPRINT((KERN_DEBUG "perfmon: failure check_case1\n"));
+-	}
+-
+-	return ret ? -EINVAL : 0;
+-}
+-
+-/*
+- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+- */
+-static pmu_config_t pmu_conf_mck={
+-	.pmu_name      = "Itanium 2",
+-	.pmu_family    = 0x1f,
+-	.flags	       = PFM_PMU_IRQ_RESEND,
+-	.ovfl_val      = (1UL << 47) - 1,
+-	.pmd_desc      = pfm_mck_pmd_desc,
+-	.pmc_desc      = pfm_mck_pmc_desc,
+-	.num_ibrs       = 8,
+-	.num_dbrs       = 8,
+-	.use_rr_dbregs = 1 /* debug register are use for range restrictions */
+-};
+-
+-
+--- a/arch/ia64/kernel/perfmon_montecito.h
++++ /dev/null
+@@ -1,269 +0,0 @@
+-/*
+- * This file contains the Montecito PMU register description tables
+- * and pmc checker used by perfmon.c.
+- *
+- * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+- *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
+- */
+-static int pfm_mont_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
+-
+-#define RDEP_MONT_ETB	(RDEP(38)|RDEP(39)|RDEP(48)|RDEP(49)|RDEP(50)|RDEP(51)|RDEP(52)|RDEP(53)|RDEP(54)|\
+-			 RDEP(55)|RDEP(56)|RDEP(57)|RDEP(58)|RDEP(59)|RDEP(60)|RDEP(61)|RDEP(62)|RDEP(63))
+-#define RDEP_MONT_DEAR  (RDEP(32)|RDEP(33)|RDEP(36))
+-#define RDEP_MONT_IEAR  (RDEP(34)|RDEP(35))
+-
+-static pfm_reg_desc_t pfm_mont_pmc_desc[PMU_MAX_PMCS]={
+-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(4),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(5),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(6),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(7),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc8  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(8),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc9  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(9),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc10 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(10),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc11 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(11),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc12 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(12),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc13 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(13),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc14 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(14),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc15 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(15),0, 0, 0}, {0,0, 0, 0}},
+-/* pmc16 */ { PFM_REG_NOTIMPL, },
+-/* pmc17 */ { PFM_REG_NOTIMPL, },
+-/* pmc18 */ { PFM_REG_NOTIMPL, },
+-/* pmc19 */ { PFM_REG_NOTIMPL, },
+-/* pmc20 */ { PFM_REG_NOTIMPL, },
+-/* pmc21 */ { PFM_REG_NOTIMPL, },
+-/* pmc22 */ { PFM_REG_NOTIMPL, },
+-/* pmc23 */ { PFM_REG_NOTIMPL, },
+-/* pmc24 */ { PFM_REG_NOTIMPL, },
+-/* pmc25 */ { PFM_REG_NOTIMPL, },
+-/* pmc26 */ { PFM_REG_NOTIMPL, },
+-/* pmc27 */ { PFM_REG_NOTIMPL, },
+-/* pmc28 */ { PFM_REG_NOTIMPL, },
+-/* pmc29 */ { PFM_REG_NOTIMPL, },
+-/* pmc30 */ { PFM_REG_NOTIMPL, },
+-/* pmc31 */ { PFM_REG_NOTIMPL, },
+-/* pmc32 */ { PFM_REG_CONFIG,  0, 0x30f01ffffffffffUL, 0x30f01ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc33 */ { PFM_REG_CONFIG,  0, 0x0,  0x1ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc34 */ { PFM_REG_CONFIG,  0, 0xf01ffffffffffUL, 0xf01ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc35 */ { PFM_REG_CONFIG,  0, 0x0,  0x1ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc36 */ { PFM_REG_CONFIG,  0, 0xfffffff0, 0xf, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc37 */ { PFM_REG_MONITOR, 4, 0x0, 0x3fff, NULL, pfm_mont_pmc_check, {RDEP_MONT_IEAR, 0, 0, 0}, {0, 0, 0, 0}},
+-/* pmc38 */ { PFM_REG_CONFIG,  0, 0xdb6, 0x2492, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc39 */ { PFM_REG_MONITOR, 6, 0x0, 0xffcf, NULL, pfm_mont_pmc_check, {RDEP_MONT_ETB,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc40 */ { PFM_REG_MONITOR, 6, 0x2000000, 0xf01cf, NULL, pfm_mont_pmc_check, {RDEP_MONT_DEAR,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc41 */ { PFM_REG_CONFIG,  0, 0x00002078fefefefeUL, 0x1e00018181818UL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
+-/* pmc42 */ { PFM_REG_MONITOR, 6, 0x0, 0x7ff4f, NULL, pfm_mont_pmc_check, {RDEP_MONT_ETB,0, 0, 0}, {0,0, 0, 0}},
+-	    { PFM_REG_END    , 0, 0x0, -1, NULL, NULL, {0,}, {0,}}, /* end marker */
+-};
+-
+-static pfm_reg_desc_t pfm_mont_pmd_desc[PMU_MAX_PMDS]={
+-/* pmd0  */ { PFM_REG_NOTIMPL, }, 
+-/* pmd1  */ { PFM_REG_NOTIMPL, },
+-/* pmd2  */ { PFM_REG_NOTIMPL, },
+-/* pmd3  */ { PFM_REG_NOTIMPL, },
+-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(4),0, 0, 0}},
+-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(5),0, 0, 0}},
+-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(6),0, 0, 0}},
+-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(7),0, 0, 0}},
+-/* pmd8  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(8),0, 0, 0}}, 
+-/* pmd9  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(9),0, 0, 0}},
+-/* pmd10 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(10),0, 0, 0}},
+-/* pmd11 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(11),0, 0, 0}},
+-/* pmd12 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(12),0, 0, 0}},
+-/* pmd13 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(13),0, 0, 0}},
+-/* pmd14 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(14),0, 0, 0}},
+-/* pmd15 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(15),0, 0, 0}},
+-/* pmd16 */ { PFM_REG_NOTIMPL, },
+-/* pmd17 */ { PFM_REG_NOTIMPL, },
+-/* pmd18 */ { PFM_REG_NOTIMPL, },
+-/* pmd19 */ { PFM_REG_NOTIMPL, },
+-/* pmd20 */ { PFM_REG_NOTIMPL, },
+-/* pmd21 */ { PFM_REG_NOTIMPL, },
+-/* pmd22 */ { PFM_REG_NOTIMPL, },
+-/* pmd23 */ { PFM_REG_NOTIMPL, },
+-/* pmd24 */ { PFM_REG_NOTIMPL, },
+-/* pmd25 */ { PFM_REG_NOTIMPL, },
+-/* pmd26 */ { PFM_REG_NOTIMPL, },
+-/* pmd27 */ { PFM_REG_NOTIMPL, },
+-/* pmd28 */ { PFM_REG_NOTIMPL, },
+-/* pmd29 */ { PFM_REG_NOTIMPL, },
+-/* pmd30 */ { PFM_REG_NOTIMPL, },
+-/* pmd31 */ { PFM_REG_NOTIMPL, },
+-/* pmd32 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(33)|RDEP(36),0, 0, 0}, {RDEP(40),0, 0, 0}},
+-/* pmd33 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(32)|RDEP(36),0, 0, 0}, {RDEP(40),0, 0, 0}},
+-/* pmd34 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(35),0, 0, 0}, {RDEP(37),0, 0, 0}},
+-/* pmd35 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(34),0, 0, 0}, {RDEP(37),0, 0, 0}},
+-/* pmd36 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(32)|RDEP(33),0, 0, 0}, {RDEP(40),0, 0, 0}},
+-/* pmd37 */ { PFM_REG_NOTIMPL, },
+-/* pmd38 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd39 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd40 */ { PFM_REG_NOTIMPL, },
+-/* pmd41 */ { PFM_REG_NOTIMPL, },
+-/* pmd42 */ { PFM_REG_NOTIMPL, },
+-/* pmd43 */ { PFM_REG_NOTIMPL, },
+-/* pmd44 */ { PFM_REG_NOTIMPL, },
+-/* pmd45 */ { PFM_REG_NOTIMPL, },
+-/* pmd46 */ { PFM_REG_NOTIMPL, },
+-/* pmd47 */ { PFM_REG_NOTIMPL, },
+-/* pmd48 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd49 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd50 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd51 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd52 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd53 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd54 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd55 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd56 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd57 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd58 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd59 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd60 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd61 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd62 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-/* pmd63 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
+-	    { PFM_REG_END   , 0, 0x0, -1, NULL, NULL, {0,}, {0,}}, /* end marker */
+-};
+-
+-/*
+- * PMC reserved fields must have their power-up values preserved
+- */
+-static int
+-pfm_mont_reserved(unsigned int cnum, unsigned long *val, struct pt_regs *regs)
+-{
+-	unsigned long tmp1, tmp2, ival = *val;
+-
+-	/* remove reserved areas from user value */
+-	tmp1 = ival & PMC_RSVD_MASK(cnum);
+-
+-	/* get reserved fields values */
+-	tmp2 = PMC_DFL_VAL(cnum) & ~PMC_RSVD_MASK(cnum);
+-
+-	*val = tmp1 | tmp2;
+-
+-	DPRINT(("pmc[%d]=0x%lx, mask=0x%lx, reset=0x%lx, val=0x%lx\n",
+-		  cnum, ival, PMC_RSVD_MASK(cnum), PMC_DFL_VAL(cnum), *val));
+-	return 0;
+-}
+-
+-/*
+- * task can be NULL if the context is unloaded
+- */
+-static int
+-pfm_mont_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
+-{
+-	int ret = 0;
+-	unsigned long val32 = 0, val38 = 0, val41 = 0;
+-	unsigned long tmpval;
+-	int check_case1 = 0;
+-	int is_loaded;
+-
+-	/* first preserve the reserved fields */
+-	pfm_mont_reserved(cnum, val, regs);
+-
+-	tmpval = *val;
+-
+-	/* sanity check */
+-	if (ctx == NULL) return -EINVAL;
+-
+-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
+-
+-	/*
+-	 * we must clear the debug registers if pmc41 has a value which enable
+-	 * memory pipeline event constraints. In this case we need to clear the
+-	 * the debug registers if they have not yet been accessed. This is required
+-	 * to avoid picking stale state.
+-	 * PMC41 is "active" if:
+-	 * 	one of the pmc41.cfg_dtagXX field is different from 0x3
+-	 * AND
+-	 * 	at the corresponding pmc41.en_dbrpXX is set.
+-	 * AND
+-	 *	ctx_fl_using_dbreg == 0  (i.e., dbr not yet used)
+-	 */
+-	DPRINT(("cnum=%u val=0x%lx, using_dbreg=%d loaded=%d\n", cnum, tmpval, ctx->ctx_fl_using_dbreg, is_loaded));
+-
+-	if (cnum == 41 && is_loaded 
+-	    && (tmpval & 0x1e00000000000UL) && (tmpval & 0x18181818UL) != 0x18181818UL && ctx->ctx_fl_using_dbreg == 0) {
+-
+-		DPRINT(("pmc[%d]=0x%lx has active pmc41 settings, clearing dbr\n", cnum, tmpval));
+-
+-		/* don't mix debug with perfmon */
+-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
+-
+-		/*
+-		 * a count of 0 will mark the debug registers if:
+-		 * AND
+-		 */
+-		ret = pfm_write_ibr_dbr(PFM_DATA_RR, ctx, NULL, 0, regs);
+-		if (ret) return ret;
+-	}
+-	/*
+-	 * we must clear the (instruction) debug registers if:
+-	 * 	pmc38.ig_ibrpX is 0 (enabled)
+-	 * AND
+-	 *	ctx_fl_using_dbreg == 0  (i.e., dbr not yet used)
+-	 */
+-	if (cnum == 38 && is_loaded && ((tmpval & 0x492UL) != 0x492UL) && ctx->ctx_fl_using_dbreg == 0) {
+-
+-		DPRINT(("pmc38=0x%lx has active pmc38 settings, clearing ibr\n", tmpval));
+-
+-		/* don't mix debug with perfmon */
+-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
+-
+-		/*
+-		 * a count of 0 will mark the debug registers as in use and also
+-		 * ensure that they are properly cleared.
+-		 */
+-		ret = pfm_write_ibr_dbr(PFM_CODE_RR, ctx, NULL, 0, regs);
+-		if (ret) return ret;
+-
+-	}
+-	switch(cnum) {
+-		case  32: val32 = *val;
+-			  val38 = ctx->ctx_pmcs[38];
+-			  val41 = ctx->ctx_pmcs[41];
+-			  check_case1 = 1;
+-			  break;
+-		case  38: val38 = *val;
+-			  val32 = ctx->ctx_pmcs[32];
+-			  val41 = ctx->ctx_pmcs[41];
+-			  check_case1 = 1;
+-			  break;
+-		case  41: val41 = *val;
+-			  val32 = ctx->ctx_pmcs[32];
+-			  val38 = ctx->ctx_pmcs[38];
+-			  check_case1 = 1;
+-			  break;
+-	}
+-	/* check illegal configuration which can produce inconsistencies in tagging
+-	 * i-side events in L1D and L2 caches
+-	 */
+-	if (check_case1) {
+-		ret =   (((val41 >> 45) & 0xf) == 0 && ((val32>>57) & 0x1) == 0)
+-		     && ((((val38>>1) & 0x3) == 0x2 || ((val38>>1) & 0x3) == 0)
+-		     ||  (((val38>>4) & 0x3) == 0x2 || ((val38>>4) & 0x3) == 0));
+-		if (ret) {
+-			DPRINT(("invalid config pmc38=0x%lx pmc41=0x%lx pmc32=0x%lx\n", val38, val41, val32));
+-			return -EINVAL;
+-		}
+-	}
+-	*val = tmpval;
+-	return 0;
+-}
+-
+-/*
+- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+- */
+-static pmu_config_t pmu_conf_mont={
+-	.pmu_name        = "Montecito",
+-	.pmu_family      = 0x20,
+-	.flags           = PFM_PMU_IRQ_RESEND,
+-	.ovfl_val        = (1UL << 47) - 1,
+-	.pmd_desc        = pfm_mont_pmd_desc,
+-	.pmc_desc        = pfm_mont_pmc_desc,
+-	.num_ibrs        = 8,
+-	.num_dbrs        = 8,
+-	.use_rr_dbregs   = 1 /* debug register are use for range retrictions */
+-};
+--- a/arch/ia64/kernel/process.c
++++ b/arch/ia64/kernel/process.c
+@@ -42,13 +42,10 @@
+ #include <asm/uaccess.h>
+ #include <asm/unwind.h>
+ #include <asm/user.h>
++#include <linux/perfmon.h>
+ 
+ #include "entry.h"
+ 
+-#ifdef CONFIG_PERFMON
+-# include <asm/perfmon.h>
+-#endif
+-
+ #include "sigframe.h"
+ 
+ void (*ia64_mark_idle)(int);
+@@ -167,32 +164,24 @@ do_notify_resume_user (sigset_t *unused,
+ 		return;
+ 	}
+ 
+-#ifdef CONFIG_PERFMON
+-	if (current->thread.pfm_needs_checking)
+-		pfm_handle_work();
+-#endif
++	/* process perfmon asynchronous work (e.g. block thread or reset) */
++	if (test_thread_flag(TIF_PERFMON_WORK))
++		pfm_handle_work(task_pt_regs(current));
+ 
+ 	/* deal with pending signal delivery */
+ 	if (test_thread_flag(TIF_SIGPENDING)||test_thread_flag(TIF_RESTORE_SIGMASK))
+ 		ia64_do_signal(scr, in_syscall);
+ }
+ 
+-static int pal_halt        = 1;
+ static int can_do_pal_halt = 1;
+ 
+ static int __init nohalt_setup(char * str)
+ {
+-	pal_halt = can_do_pal_halt = 0;
++	can_do_pal_halt = 0;
+ 	return 1;
+ }
+ __setup("nohalt", nohalt_setup);
+ 
+-void
+-update_pal_halt_status(int status)
+-{
+-	can_do_pal_halt = pal_halt && status;
+-}
+-
+ /*
+  * We use this if we don't have any better idle routine..
+  */
+@@ -201,6 +190,22 @@ default_idle (void)
+ {
+ 	local_irq_enable();
+ 	while (!need_resched()) {
++#ifdef CONFIG_PERFMON
++		u64 psr = 0;
++		/*
++		 * If requested, we stop the PMU to avoid
++		 * measuring across the core idle loop.
++		 *
++		 * dcr.pp is not modified on purpose
++		 * it is used when coming out of
++		 * safe_halt() via interrupt
++		 */
++		if ((__get_cpu_var(pfm_syst_info) & PFM_ITA_CPUINFO_IDLE_EXCL)) {
++			psr = ia64_getreg(_IA64_REG_PSR);
++			if (psr & IA64_PSR_PP)
++				ia64_rsm(IA64_PSR_PP);
++		}
++#endif
+ 		if (can_do_pal_halt) {
+ 			local_irq_disable();
+ 			if (!need_resched()) {
+@@ -209,6 +214,12 @@ default_idle (void)
+ 			local_irq_enable();
+ 		} else
+ 			cpu_relax();
++#ifdef CONFIG_PERFMON
++		if ((__get_cpu_var(pfm_syst_info) & PFM_ITA_CPUINFO_IDLE_EXCL)) {
++			if (psr & IA64_PSR_PP)
++				ia64_ssm(IA64_PSR_PP);
++		}
++#endif
+ 	}
+ }
+ 
+@@ -322,22 +333,9 @@ cpu_idle (void)
+ void
+ ia64_save_extra (struct task_struct *task)
+ {
+-#ifdef CONFIG_PERFMON
+-	unsigned long info;
+-#endif
+-
+ 	if ((task->thread.flags & IA64_THREAD_DBG_VALID) != 0)
+ 		ia64_save_debug_regs(&task->thread.dbr[0]);
+ 
+-#ifdef CONFIG_PERFMON
+-	if ((task->thread.flags & IA64_THREAD_PM_VALID) != 0)
+-		pfm_save_regs(task);
+-
+-	info = __get_cpu_var(pfm_syst_info);
+-	if (info & PFM_CPUINFO_SYST_WIDE)
+-		pfm_syst_wide_update_task(task, info, 0);
+-#endif
+-
+ #ifdef CONFIG_IA32_SUPPORT
+ 	if (IS_IA32_PROCESS(task_pt_regs(task)))
+ 		ia32_save_state(task);
+@@ -347,22 +345,9 @@ ia64_save_extra (struct task_struct *tas
+ void
+ ia64_load_extra (struct task_struct *task)
+ {
+-#ifdef CONFIG_PERFMON
+-	unsigned long info;
+-#endif
+-
+ 	if ((task->thread.flags & IA64_THREAD_DBG_VALID) != 0)
+ 		ia64_load_debug_regs(&task->thread.dbr[0]);
+ 
+-#ifdef CONFIG_PERFMON
+-	if ((task->thread.flags & IA64_THREAD_PM_VALID) != 0)
+-		pfm_load_regs(task);
+-
+-	info = __get_cpu_var(pfm_syst_info);
+-	if (info & PFM_CPUINFO_SYST_WIDE) 
+-		pfm_syst_wide_update_task(task, info, 1);
+-#endif
+-
+ #ifdef CONFIG_IA32_SUPPORT
+ 	if (IS_IA32_PROCESS(task_pt_regs(task)))
+ 		ia32_load_state(task);
+@@ -488,8 +473,7 @@ copy_thread (int nr, unsigned long clone
+ 	 * call behavior where scratch registers are preserved across
+ 	 * system calls (unless used by the system call itself).
+ 	 */
+-#	define THREAD_FLAGS_TO_CLEAR	(IA64_THREAD_FPH_VALID | IA64_THREAD_DBG_VALID \
+-					 | IA64_THREAD_PM_VALID)
++#	define THREAD_FLAGS_TO_CLEAR	(IA64_THREAD_FPH_VALID | IA64_THREAD_DBG_VALID)
+ #	define THREAD_FLAGS_TO_SET	0
+ 	p->thread.flags = ((current->thread.flags & ~THREAD_FLAGS_TO_CLEAR)
+ 			   | THREAD_FLAGS_TO_SET);
+@@ -511,10 +495,8 @@ copy_thread (int nr, unsigned long clone
+ 	}
+ #endif
+ 
+-#ifdef CONFIG_PERFMON
+-	if (current->thread.pfm_context)
+-		pfm_inherit(p, child_ptregs);
+-#endif
++	pfm_copy_thread(p);
++
+ 	return retval;
+ }
+ 
+@@ -753,15 +735,13 @@ exit_thread (void)
+ {
+ 
+ 	ia64_drop_fpu(current);
+-#ifdef CONFIG_PERFMON
+-       /* if needed, stop monitoring and flush state to perfmon context */
+-	if (current->thread.pfm_context)
+-		pfm_exit_thread(current);
++
++        /* if needed, stop monitoring and flush state to perfmon context */
++	pfm_exit_thread(current);
+ 
+ 	/* free debug register resources */
+-	if (current->thread.flags & IA64_THREAD_DBG_VALID)
+-		pfm_release_debug_registers(current);
+-#endif
++	pfm_release_dbregs(current);
++
+ 	if (IS_IA32_PROCESS(task_pt_regs(current)))
+ 		ia32_drop_ia64_partial_page_list(current);
+ }
+--- a/arch/ia64/kernel/ptrace.c
++++ b/arch/ia64/kernel/ptrace.c
+@@ -17,6 +17,7 @@
+ #include <linux/security.h>
+ #include <linux/audit.h>
+ #include <linux/signal.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/pgtable.h>
+ #include <asm/processor.h>
+@@ -25,9 +26,6 @@
+ #include <asm/system.h>
+ #include <asm/uaccess.h>
+ #include <asm/unwind.h>
+-#ifdef CONFIG_PERFMON
+-#include <asm/perfmon.h>
+-#endif
+ 
+ #include "entry.h"
+ 
+@@ -1064,7 +1062,6 @@ access_uarea (struct task_struct *child,
+ 				"address 0x%lx\n", addr);
+ 			return -1;
+ 		}
+-#ifdef CONFIG_PERFMON
+ 		/*
+ 		 * Check if debug registers are used by perfmon. This
+ 		 * test must be done once we know that we can do the
+@@ -1082,8 +1079,8 @@ access_uarea (struct task_struct *child,
+ 		 * IA64_THREAD_DBG_VALID. The registers are restored
+ 		 * by the PMU context switch code.
+ 		 */
+-		if (pfm_use_debug_registers(child)) return -1;
+-#endif
++		if (pfm_use_dbregs(child))
++			return -1;
+ 
+ 		if (!(child->thread.flags & IA64_THREAD_DBG_VALID)) {
+ 			child->thread.flags |= IA64_THREAD_DBG_VALID;
+--- a/arch/ia64/kernel/setup.c
++++ b/arch/ia64/kernel/setup.c
+@@ -45,6 +45,7 @@
+ #include <linux/cpufreq.h>
+ #include <linux/kexec.h>
+ #include <linux/crash_dump.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/ia32.h>
+ #include <asm/machvec.h>
+@@ -974,6 +975,8 @@ cpu_init (void)
+ 	}
+ 	platform_cpu_init();
+ 	pm_idle = default_idle;
++
++	pfm_init_percpu();
+ }
+ 
+ void __init
+--- a/arch/ia64/kernel/smpboot.c
++++ b/arch/ia64/kernel/smpboot.c
+@@ -39,6 +39,7 @@
+ #include <linux/efi.h>
+ #include <linux/percpu.h>
+ #include <linux/bitops.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/atomic.h>
+ #include <asm/cache.h>
+@@ -380,10 +381,6 @@ smp_callin (void)
+ 	extern void ia64_init_itm(void);
+ 	extern volatile int time_keeper_id;
+ 
+-#ifdef CONFIG_PERFMON
+-	extern void pfm_init_percpu(void);
+-#endif
+-
+ 	cpuid = smp_processor_id();
+ 	phys_id = hard_smp_processor_id();
+ 	itc_master = time_keeper_id;
+@@ -409,10 +406,6 @@ smp_callin (void)
+ 
+ 	ia64_mca_cmc_vector_setup();	/* Setup vector on AP */
+ 
+-#ifdef CONFIG_PERFMON
+-	pfm_init_percpu();
+-#endif
+-
+ 	local_irq_enable();
+ 
+ 	if (!(sal_platform_features & IA64_SAL_PLATFORM_FEATURE_ITC_DRIFT)) {
+@@ -749,6 +742,7 @@ int __cpu_disable(void)
+ 	fixup_irqs();
+ 	local_flush_tlb_all();
+ 	cpu_clear(cpu, cpu_callin_map);
++	pfm_cpu_disable();
+ 	return 0;
+ }
+ 
+--- a/arch/ia64/kernel/sys_ia64.c
++++ b/arch/ia64/kernel/sys_ia64.c
+@@ -284,3 +284,10 @@ sys_pciconfig_write (unsigned long bus, 
+ }
+ 
+ #endif /* CONFIG_PCI */
++
++#ifndef CONFIG_PERFMON
++asmlinkage long sys_perfmonctl (int fd, int cmd, void __user *arg, int count)
++{
++	return -ENOSYS;
++}
++#endif
+--- a/arch/ia64/lib/Makefile
++++ b/arch/ia64/lib/Makefile
+@@ -13,7 +13,6 @@ lib-y := __divsi3.o __udivsi3.o __modsi3
+ 
+ obj-$(CONFIG_ITANIUM)	+= copy_page.o copy_user.o memcpy.o
+ obj-$(CONFIG_MCKINLEY)	+= copy_page_mck.o memcpy_mck.o
+-lib-$(CONFIG_PERFMON)	+= carta_random.o
+ 
+ AFLAGS___divdi3.o	=
+ AFLAGS___udivdi3.o	= -DUNSIGNED
+--- a/arch/ia64/oprofile/init.c
++++ b/arch/ia64/oprofile/init.c
+@@ -12,8 +12,8 @@
+ #include <linux/init.h>
+ #include <linux/errno.h>
+  
+-extern int perfmon_init(struct oprofile_operations * ops);
+-extern void perfmon_exit(void);
++extern int op_perfmon_init(struct oprofile_operations * ops);
++extern void op_perfmon_exit(void);
+ extern void ia64_backtrace(struct pt_regs * const regs, unsigned int depth);
+ 
+ int __init oprofile_arch_init(struct oprofile_operations * ops)
+@@ -22,7 +22,7 @@ int __init oprofile_arch_init(struct opr
+ 
+ #ifdef CONFIG_PERFMON
+ 	/* perfmon_init() can fail, but we have no way to report it */
+-	ret = perfmon_init(ops);
++	ret = op_perfmon_init(ops);
+ #endif
+ 	ops->backtrace = ia64_backtrace;
+ 
+@@ -33,6 +33,6 @@ int __init oprofile_arch_init(struct opr
+ void oprofile_arch_exit(void)
+ {
+ #ifdef CONFIG_PERFMON
+-	perfmon_exit();
++	op_perfmon_exit();
+ #endif
+ }
+--- a/arch/ia64/oprofile/perfmon.c
++++ b/arch/ia64/oprofile/perfmon.c
+@@ -10,19 +10,21 @@
+ #include <linux/kernel.h>
+ #include <linux/oprofile.h>
+ #include <linux/sched.h>
+-#include <asm/perfmon.h>
++#include <linux/module.h>
++#include <linux/perfmon.h>
+ #include <asm/ptrace.h>
+ #include <asm/errno.h>
+ 
+ static int allow_ints;
+ 
+ static int
+-perfmon_handler(struct task_struct *task, void *buf, pfm_ovfl_arg_t *arg,
+-                struct pt_regs *regs, unsigned long stamp)
++perfmon_handler(void *buf, struct pfm_ovfl_arg *arg,
++                unsigned long ip, u64 stamp, void *data)
+ {
++	struct pt_regs *regs = data;
+ 	int event = arg->pmd_eventid;
+  
+-	arg->ovfl_ctrl.bits.reset_ovfl_pmds = 1;
++	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+ 
+ 	/* the owner of the oprofile event buffer may have exited
+ 	 * without perfmon being shutdown (e.g. SIGSEGV)
+@@ -45,17 +47,13 @@ static void perfmon_stop(void)
+ 	allow_ints = 0;
+ }
+ 
+-
+-#define OPROFILE_FMT_UUID { \
+-	0x77, 0x7a, 0x6e, 0x61, 0x20, 0x65, 0x73, 0x69, 0x74, 0x6e, 0x72, 0x20, 0x61, 0x65, 0x0a, 0x6c }
+-
+-static pfm_buffer_fmt_t oprofile_fmt = {
+- 	.fmt_name 	    = "oprofile_format",
+- 	.fmt_uuid	    = OPROFILE_FMT_UUID,
+- 	.fmt_handler	    = perfmon_handler,
++static struct pfm_smpl_fmt oprofile_fmt = {
++	.fmt_name = "OProfile",
++	.fmt_handler = perfmon_handler,
++	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
++	.owner = THIS_MODULE
+ };
+ 
+-
+ static char * get_cpu_type(void)
+ {
+ 	__u8 family = local_cpu_data->family;
+@@ -75,9 +73,9 @@ static char * get_cpu_type(void)
+ 
+ static int using_perfmon;
+ 
+-int perfmon_init(struct oprofile_operations * ops)
++int __init op_perfmon_init(struct oprofile_operations * ops)
+ {
+-	int ret = pfm_register_buffer_fmt(&oprofile_fmt);
++	int ret = pfm_fmt_register(&oprofile_fmt);
+ 	if (ret)
+ 		return -ENODEV;
+ 
+@@ -90,10 +88,10 @@ int perfmon_init(struct oprofile_operati
+ }
+ 
+ 
+-void perfmon_exit(void)
++void __exit op_perfmon_exit(void)
+ {
+ 	if (!using_perfmon)
+ 		return;
+ 
+-	pfm_unregister_buffer_fmt(oprofile_fmt.fmt_uuid);
++	pfm_fmt_unregister(&oprofile_fmt);
+ }
+--- /dev/null
++++ b/arch/ia64/perfmon/Kconfig
+@@ -0,0 +1,58 @@
++menu "Hardware Performance Monitoring support"
++config PERFMON
++	bool "Perfmon2 performance monitoring interface"
++	default n
++	help
++	Enables the perfmon2 interface to access the hardware
++	performance counters. See <http://perfmon2.sf.net/> for
++	more details.
++
++config PERFMON_DEBUG
++	bool "Perfmon debugging"
++	default n
++	depends on PERFMON
++	help
++	Enables perfmon debugging support
++
++config IA64_PERFMON_COMPAT
++	bool "Enable old perfmon-2 compatbility mode"
++	default n
++	depends on PERFMON
++	help
++	Enable this option to allow performance tools which used the old
++	perfmon-2 interface to continue to work. Old tools are those using
++	the obsolete commands and arguments. Check your programs and look
++	in include/asm-ia64/perfmon_compat.h for more information.
++
++config IA64_PERFMON_GENERIC
++	tristate "Generic IA-64 PMU support"
++	depends on PERFMON
++	default n
++	help
++	Enables generic IA-64 PMU support.
++	The generic PMU is defined by the IA-64 architecture document.
++	This option should only be necessary when running with a PMU that
++	is not yet explicitely supported. Even then, there is no guarantee
++	that this support will work.
++
++config IA64_PERFMON_ITANIUM
++	tristate "Itanium (Merced) Performance Monitoring support"
++	depends on PERFMON
++	default n
++	help
++	Enables Itanium (Merced) PMU support.
++
++config IA64_PERFMON_MCKINLEY
++	tristate "Itanium 2 (McKinley) Performance Monitoring  support"
++	depends on PERFMON
++	default n
++	help
++	Enables Itanium 2 (McKinley, Madison, Deerfield) PMU support.
++
++config IA64_PERFMON_MONTECITO
++	tristate "Itanium 2 9000 (Montecito) Performance Monitoring  support"
++	depends on PERFMON
++	default n
++	help
++	Enables support for Itanium 2 9000 (Montecito) PMU.
++endmenu
+--- /dev/null
++++ b/arch/ia64/perfmon/Makefile
+@@ -0,0 +1,11 @@
++#
++# Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++# Contributed by Stephane Eranian <eranian@hpl.hp.com>
++#
++obj-$(CONFIG_PERFMON)			+= perfmon.o
++obj-$(CONFIG_IA64_PERFMON_COMPAT)	+= perfmon_default_smpl.o \
++					   perfmon_compat.o
++obj-$(CONFIG_IA64_PERFMON_GENERIC)	+= perfmon_generic.o
++obj-$(CONFIG_IA64_PERFMON_ITANIUM)	+= perfmon_itanium.o
++obj-$(CONFIG_IA64_PERFMON_MCKINLEY)	+= perfmon_mckinley.o
++obj-$(CONFIG_IA64_PERFMON_MONTECITO)	+= perfmon_montecito.o
+--- /dev/null
++++ b/arch/ia64/perfmon/perfmon.c
+@@ -0,0 +1,949 @@
++/*
++ * This file implements the IA-64 specific
++ * support for the perfmon2 interface
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++struct pfm_arch_session {
++	u32	pfs_sys_use_dbr;    /* syswide session uses dbr */
++	u32	pfs_ptrace_use_dbr; /* a thread uses dbr via ptrace()*/
++};
++
++DEFINE_PER_CPU(u32, pfm_syst_info);
++
++static struct pfm_arch_session pfm_arch_sessions;
++static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_arch_sessions_lock);
++
++static inline void pfm_clear_psr_pp(void)
++{
++	ia64_rsm(IA64_PSR_PP);
++}
++
++static inline void pfm_set_psr_pp(void)
++{
++	ia64_ssm(IA64_PSR_PP);
++}
++
++static inline void pfm_clear_psr_up(void)
++{
++	ia64_rsm(IA64_PSR_UP);
++}
++
++static inline void pfm_set_psr_up(void)
++{
++	ia64_ssm(IA64_PSR_UP);
++}
++
++static inline void pfm_set_psr_l(u64 val)
++{
++	ia64_setreg(_IA64_REG_PSR_L, val);
++}
++
++static inline void pfm_restore_ibrs(u64 *ibrs, unsigned int nibrs)
++{
++	unsigned int i;
++
++	for (i = 0; i < nibrs; i++) {
++		ia64_set_ibr(i, ibrs[i]);
++		ia64_dv_serialize_instruction();
++	}
++	ia64_srlz_i();
++}
++
++static inline void pfm_restore_dbrs(u64 *dbrs, unsigned int ndbrs)
++{
++	unsigned int i;
++
++	for (i = 0; i < ndbrs; i++) {
++		ia64_set_dbr(i, dbrs[i]);
++		ia64_dv_serialize_data();
++	}
++	ia64_srlz_d();
++}
++
++irqreturn_t pmu_interrupt_handler(int irq, void *arg)
++{
++	struct pt_regs *regs;
++	regs = get_irq_regs();
++	irq_enter();
++	pfm_interrupt_handler(instruction_pointer(regs), regs);
++	irq_exit();
++	return IRQ_HANDLED;
++}
++static struct irqaction perfmon_irqaction = {
++	.handler = pmu_interrupt_handler,
++	.flags = IRQF_DISABLED, /* means keep interrupts masked */
++	.name = "perfmon"
++};
++
++void pfm_arch_quiesce_pmu_percpu(void)
++{
++	u64 dcr;
++	/*
++	 * make sure no measurement is active
++	 * (may inherit programmed PMCs from EFI).
++	 */
++	pfm_clear_psr_pp();
++	pfm_clear_psr_up();
++
++	/*
++	 * ensure dcr.pp is cleared
++	 */
++	dcr = ia64_getreg(_IA64_REG_CR_DCR);
++	ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
++
++	/*
++	 * we run with the PMU not frozen at all times
++	 */
++	ia64_set_pmc(0, 0);
++	ia64_srlz_d();
++}
++
++void pfm_arch_init_percpu(void)
++{
++	pfm_arch_quiesce_pmu_percpu();
++	/*
++	 * program PMU interrupt vector
++	 */
++	ia64_setreg(_IA64_REG_CR_PMV, IA64_PERFMON_VECTOR);
++	ia64_srlz_d();
++}
++
++int pfm_arch_context_create(struct pfm_context *ctx, u32 ctx_flags)
++{
++	struct pfm_arch_context *ctx_arch;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	ctx_arch->flags.use_dbr = 0;
++	ctx_arch->flags.insecure = (ctx_flags & PFM_ITA_FL_INSECURE) ? 1: 0;
++
++	PFM_DBG("insecure=%d", ctx_arch->flags.insecure);
++
++	return 0;
++}
++
++/*
++ * Called from pfm_ctxsw(). Task is guaranteed to be current.
++ * Context is locked. Interrupts are masked. Monitoring may be active.
++ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
++ *
++ * Return:
++ * 	non-zero : did not save PMDs (as part of stopping the PMU)
++ * 	       0 : saved PMDs (no need to save them in caller)
++ */
++int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
++		       struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++	u64 psr, tmp;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * save current PSR: needed because we modify it
++	 */
++	ia64_srlz_d();
++	psr = ia64_getreg(_IA64_REG_PSR);
++
++	/*
++	 * stop monitoring:
++	 * This is the last instruction which may generate an overflow
++	 *
++	 * we do not clear ipsr.up
++	 */
++	pfm_clear_psr_up();
++	ia64_srlz_d();
++
++	/*
++	 * extract overflow status bits
++	 */
++	tmp =  ia64_get_pmc(0) & ~0xf;
++
++	/*
++	 * keep a copy of psr.up (for reload)
++	 */
++	ctx_arch->ctx_saved_psr_up = psr & IA64_PSR_UP;
++
++	/*
++	 * save overflow status bits
++	 */
++	set->povfl_pmds[0] = tmp;
++
++	/*
++	 * record how many pending overflows
++	 * XXX: assume identity mapping for counters
++	 */
++	set->npend_ovfls = ia64_popcnt(tmp);
++
++	/*
++	 * make sure the PMU is unfrozen for the next task
++	 */
++	if (set->npend_ovfls) {
++		ia64_set_pmc(0, 0);
++		ia64_srlz_d();
++	}
++	return 1;
++}
++
++/*
++ * Called from pfm_ctxsw(). Task is guaranteed to be current.
++ * set cannot be NULL. Context is locked. Interrupts are masked.
++ * Caller has already restored all PMD and PMC registers.
++ *
++ * must reactivate monitoring
++ */
++void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx,
++		      struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * when monitoring is not explicitly started
++	 * then psr_up = 0, in which case we do not
++	 * need to restore
++	 */
++	if (likely(ctx_arch->ctx_saved_psr_up)) {
++		pfm_set_psr_up();
++		ia64_srlz_d();
++	}
++}
++
++int pfm_arch_reserve_session(struct pfm_context *ctx, u32 cpu)
++{
++	struct pfm_arch_context *ctx_arch;
++	int is_system;
++	int ret = 0;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	is_system = ctx->flags.system;
++
++	spin_lock(&pfm_arch_sessions_lock);
++
++	if (is_system && ctx_arch->flags.use_dbr) {
++		PFM_DBG("syswide context uses dbregs");
++
++		if (pfm_arch_sessions.pfs_ptrace_use_dbr) {
++			PFM_DBG("cannot reserve syswide context: "
++				  "dbregs in use by ptrace");
++			ret = -EBUSY;
++		} else {
++			pfm_arch_sessions.pfs_sys_use_dbr++;
++		}
++	}
++	spin_unlock(&pfm_arch_sessions_lock);
++
++	return ret;
++}
++
++void pfm_arch_release_session(struct pfm_context *ctx, u32 cpu)
++{
++	struct pfm_arch_context *ctx_arch;
++	int is_system;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	is_system = ctx->flags.system;
++
++	spin_lock(&pfm_arch_sessions_lock);
++
++	if (is_system && ctx_arch->flags.use_dbr) {
++		pfm_arch_sessions.pfs_sys_use_dbr--;
++	}
++	spin_unlock(&pfm_arch_sessions_lock);
++}
++
++/*
++ * function called from pfm_load_context_*(). Task is not guaranteed to be
++ * current task. If not then other task is guaranteed stopped and off any CPU.
++ * context is locked and interrupts are masked.
++ *
++ * On PFM_LOAD_CONTEXT, the interface guarantees monitoring is stopped.
++ *
++ * For system-wide task is NULL
++ */
++int pfm_arch_load_context(struct pfm_context *ctx, struct pfm_event_set *set,
++			  struct task_struct *task)
++{
++	struct pfm_arch_context *ctx_arch;
++	struct pt_regs *regs;
++	int ret = 0;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * cannot load a context which is using range restrictions,
++	 * into a thread that is being debugged.
++	 *
++	 * if one set out of several is using the debug registers, then
++	 * we assume the context as whole is using them.
++	 */
++	if (ctx_arch->flags.use_dbr) {
++		if (ctx->flags.system) {
++			spin_lock(&pfm_arch_sessions_lock);
++
++			if (pfm_arch_sessions.pfs_ptrace_use_dbr) {
++				PFM_DBG("cannot reserve syswide context: "
++					"dbregs in use by ptrace");
++				ret = -EBUSY;
++			} else {
++				pfm_arch_sessions.pfs_sys_use_dbr++;
++				PFM_DBG("pfs_sys_use_dbr=%u", pfm_arch_sessions.pfs_sys_use_dbr);
++			}
++			spin_unlock(&pfm_arch_sessions_lock);
++
++		} else if (task->thread.flags & IA64_THREAD_DBG_VALID) {
++			PFM_DBG("load_pid [%d] thread is debugged, cannot "
++				  "use range restrictions", task->pid);
++			ret = -EBUSY;
++		}
++		if (ret)
++			return ret;
++	}
++
++	/*
++	 * We need to intervene on context switch to toggle the
++	 * psr.pp bit in system-wide. As such, we set the TIF
++	 * flag so that pfm_arch_ctxswout_sys() and the
++	 * pfm_arch_ctxswin_sys() functions get called
++	 * from pfm_ctxsw_sys();
++	 */
++	if (ctx->flags.system) {
++		set_thread_flag(TIF_PERFMON_CTXSW);
++		PFM_DBG("[%d] set TIF", current->pid);
++		return 0;
++	}
++
++	regs = task_pt_regs(task);
++
++	/*
++	 * self-monitoring systematically allows user level control
++	 */
++	if (task != current) {
++		/*
++		 * when not current, task is stopped, so this is safe
++		 */
++		ctx_arch->ctx_saved_psr_up = 0;
++		ia64_psr(regs)->up = ia64_psr(regs)->pp = 0;
++	} else
++		ctx_arch->flags.insecure = 1;
++
++	/*
++	 * allow user level control (start/stop/read pmd) if:
++	 * 	- self-monitoring
++	 * 	- requested at context creation (PFM_IA64_FL_INSECURE)
++	 *
++	 * There is not security hole with PFM_IA64_FL_INSECURE because
++	 * when not self-monitored, the caller must have permissions to
++	 * attached to the task.
++	 */
++	if (ctx_arch->flags.insecure) {
++		ia64_psr(regs)->sp = 0;
++		PFM_DBG("clearing psr.sp for [%d]", task->pid);
++	}
++	return 0;
++}
++
++int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
++{
++#define PFM_SETFL_BOTH_SWITCH	(PFM_SETFL_OVFL_SWITCH|PFM_SETFL_TIME_SWITCH)
++#define PFM_ITA_SETFL_BOTH_INTR	(PFM_ITA_SETFL_INTR_ONLY|\
++				 PFM_ITA_SETFL_EXCL_INTR)
++
++/* exclude return value field */
++#define PFM_SETFL_ALL_MASK	( PFM_ITA_SETFL_BOTH_INTR \
++				| PFM_SETFL_BOTH_SWITCH \
++				| PFM_ITA_SETFL_IDLE_EXCL)
++
++	if ((flags & ~PFM_SETFL_ALL_MASK)) {
++		PFM_DBG("invalid flags=0x%x", flags);
++		return -EINVAL;
++	}
++
++	if ((flags & PFM_ITA_SETFL_BOTH_INTR) == PFM_ITA_SETFL_BOTH_INTR) {
++		PFM_DBG("both excl intr and ontr only are set");
++		return -EINVAL;
++	}
++
++	if ((flags & PFM_ITA_SETFL_IDLE_EXCL) && !ctx->flags.system) {
++		PFM_DBG("idle exclude flag only for system-wide context");
++		return -EINVAL;
++	}
++	return 0;
++}
++
++/*
++ * function called from pfm_unload_context_*(). Context is locked.
++ * interrupts are masked. task is not guaranteed to be current task.
++ * Access to PMU is not guaranteed.
++ *
++ * function must do whatever arch-specific action is required on unload
++ * of a context.
++ *
++ * called for both system-wide and per-thread. task is NULL for ssytem-wide
++ */
++int pfm_arch_unload_context(struct pfm_context *ctx, struct task_struct *task)
++{
++	struct pfm_arch_context *ctx_arch;
++	struct pt_regs *regs;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	if (ctx->flags.system) {
++		/*
++		 * disable context switch hook
++		 */
++		clear_thread_flag(TIF_PERFMON_CTXSW);
++
++		if (ctx_arch->flags.use_dbr) {
++			spin_lock(&pfm_arch_sessions_lock);
++			pfm_arch_sessions.pfs_sys_use_dbr--;
++			PFM_DBG("sys_use_dbr=%u", pfm_arch_sessions.pfs_sys_use_dbr);
++			spin_unlock(&pfm_arch_sessions_lock);
++		}
++		return 0;
++	}
++
++	regs = task_pt_regs(task);
++
++	/*
++	 * cancel user level control for per-task context
++	 */
++	ia64_psr(regs)->sp = 1;
++	PFM_DBG("setting psr.sp for [%d]", task->pid);
++	return 0;
++}
++
++/*
++ * mask monitoring by setting the privilege level to 0
++ * we cannot use psr.pp/psr.up for this, it is controlled by
++ * the user
++ */
++void pfm_arch_mask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	unsigned long mask;
++	unsigned int i;
++
++	/*
++	 * as an optimization we look at the first 64 PMC
++	 * registers only starting at PMC4.
++	 */
++	mask = arch_info->mask_pmcs[0] >> PFM_ITA_FCNTR;
++	for(i = PFM_ITA_FCNTR; mask; i++, mask >>=1) {
++		if (likely(mask & 0x1))
++			ia64_set_pmc(i, set->pmcs[i] & ~0xfUL);
++	}
++	/*
++	 * make changes visisble
++	 */
++	ia64_srlz_d();
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
++ * context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMD registers from set.
++ */
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++	unsigned long *mask;
++	u16 i, num;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	if (ctx_arch->flags.insecure) {
++		num = pfm_pmu_conf->regs.num_rw_pmd;
++		mask = pfm_pmu_conf->regs.rw_pmds;
++	} else {
++		num = set->nused_pmds;
++		mask = set->used_pmds;
++	}
++	/*
++	 * must restore all implemented read-write PMDS to avoid leaking
++	 * information especially when PFM_IA64_FL_INSECURE is set.
++	 *
++	 * XXX: should check PFM_IA64_FL_INSECURE==0 and use used_pmd instead
++	 */
++	for (i = 0; num; i++) {
++		if (likely(test_bit(i, mask))) {
++			pfm_arch_write_pmd(ctx, i, set->pmds[i].value);
++			num--;
++		}
++	}
++	ia64_srlz_d();
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
++ * context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMC registers from set if needed
++ */
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	u64 mask2 = 0, val, plm;
++	unsigned long impl_mask, mask_pmcs;
++	unsigned int i;
++
++	/*
++	 * as an optimization we only look at the first 64
++	 * PMC registers. In fact, we should never scan the
++	 * entire impl_pmcs because ibr/dbr are implemented
++	 * separately.
++	 *
++	 * always skip PMC0-PMC3. PMC0 taken care of when saving
++	 * state. PMC1-PMC3 not used until we get counters in
++	 * the 60 and above index range.
++	 */
++	impl_mask = pfm_pmu_conf->regs.pmcs[0] >> PFM_ITA_FCNTR;
++	mask_pmcs = arch_info->mask_pmcs[0] >> PFM_ITA_FCNTR;
++	plm = ctx->state == PFM_CTX_MASKED ? ~0xf : ~0x0;
++
++	for (i = PFM_ITA_FCNTR;
++	     impl_mask;
++	     i++, impl_mask >>=1, mask_pmcs >>=1) {
++		if (likely(impl_mask & 0x1)) {
++			mask2 = mask_pmcs & 0x1 ? plm : ~0;
++			val = set->pmcs[i] & mask2;
++			ia64_set_pmc(i, val);
++			PFM_DBG_ovfl("pmc%u=0x%lx", i, val);
++		}
++	}
++	/*
++	 * restore DBR/IBR
++	 */
++	if (set->priv_flags & PFM_ITA_SETFL_USE_DBR) {
++		pfm_restore_ibrs(set->pmcs+256, 8);
++		pfm_restore_dbrs(set->pmcs+264, 8);
++	}
++	ia64_srlz_d();
++}
++
++void pfm_arch_unmask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	u64 psr;
++	int is_system;
++
++	is_system = ctx->flags.system;
++
++	psr = ia64_getreg(_IA64_REG_PSR);
++
++	/*
++	 * monitoring is masked via the PMC.plm
++	 *
++	 * As we restore their value, we do not want each counter to
++	 * restart right away. We stop monitoring using the PSR,
++	 * restore the PMC (and PMD) and then re-establish the psr
++	 * as it was. Note that there can be no pending overflow at
++	 * this point, because monitoring is still MASKED.
++	 *
++	 * Because interrupts are masked we can avoid changing
++	 * DCR.pp.
++	 */
++	if (is_system)
++		pfm_clear_psr_pp();
++	else
++		pfm_clear_psr_up();
++
++	ia64_srlz_d();
++
++	pfm_arch_restore_pmcs(ctx, set);
++
++	/*
++	 * restore psr
++	 *
++	 * monitoring may start right now but interrupts
++	 * are still masked
++	 */
++	pfm_set_psr_l(psr);
++	ia64_srlz_d();
++}
++
++/*
++ * Called from pfm_stop()
++ *
++ * For per-thread:
++ *   task is not necessarily current. If not current task, then
++ *   task is guaranteed stopped and off any cpu. Access to PMU
++ *   is not guaranteed. Interrupts are masked. Context is locked.
++ *   Set is the active set.
++ *
++ * must disable active monitoring. ctx cannot be NULL
++ */
++void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
++		   struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++	struct pt_regs *regs;
++	u64 dcr, psr;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	regs = task_pt_regs(task);
++
++	if (!ctx->flags.system) {
++		/*
++		 * in ZOMBIE state we always have task == current due to
++		 * pfm_exit_thread()
++		 */
++		ia64_psr(regs)->up = 0;
++		ctx_arch->ctx_saved_psr_up = 0;
++
++		/*
++		 * in case of ZOMBIE state, there is no unload to clear
++		 * insecure monitoring, so we do it in stop instead.
++		 */
++		if (ctx->state == PFM_CTX_ZOMBIE)
++			ia64_psr(regs)->sp = 1;
++
++		if (task == current) {
++			pfm_clear_psr_up();
++			ia64_srlz_d();
++		}
++	} else if (ctx->flags.started) { /* do not stop twice */
++		dcr = ia64_getreg(_IA64_REG_CR_DCR);
++		psr = ia64_getreg(_IA64_REG_PSR);
++
++		ia64_psr(regs)->pp = 0;
++		ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
++		pfm_clear_psr_pp();
++		ia64_srlz_d();
++
++		if (set->flags & PFM_ITA_SETFL_IDLE_EXCL) {
++			PFM_DBG("disabling idle exclude");
++			__get_cpu_var(pfm_syst_info) &= ~PFM_ITA_CPUINFO_IDLE_EXCL;
++		}
++	}
++}
++
++/*
++ * called from pfm_start()
++ *
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-thread:
++ * 	Task is not necessarily current. If not current task, then task
++ * 	is guaranteed stopped and off any cpu. No access to PMU is task
++ *	is not current.
++ *
++ * For system-wide:
++ * 	task is always current
++ *
++ * must enable active monitoring.
++ */
++void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
++		    struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++	struct pt_regs *regs;
++	u64 dcr, dcr_pp, psr_pp;
++	u32 flags;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	regs = task_pt_regs(task);
++	flags = set->flags;
++
++	/*
++	 * per-thread mode
++	 */
++	if (!ctx->flags.system) {
++
++		ia64_psr(regs)->up = 1;
++
++		if (task == current) {
++			pfm_set_psr_up();
++			ia64_srlz_d();
++		} else {
++			/*
++			 * activate monitoring at next ctxswin
++			 */
++			ctx_arch->ctx_saved_psr_up = IA64_PSR_UP;
++		}
++		return;
++	}
++
++	/*
++	 * system-wide mode
++	 */
++	dcr = ia64_getreg(_IA64_REG_CR_DCR);
++	if (flags & PFM_ITA_SETFL_INTR_ONLY) {
++		dcr_pp = 1;
++		psr_pp = 0;
++	} else if (flags & PFM_ITA_SETFL_EXCL_INTR) {
++		dcr_pp = 0;
++		psr_pp = 1;
++	} else {
++		dcr_pp = psr_pp = 1;
++	}
++	PFM_DBG("dcr_pp=%lu psr_pp=%lu", dcr_pp, psr_pp);
++
++	/*
++	 * update dcr_pp and psr_pp
++	 */
++	if (dcr_pp)
++		ia64_setreg(_IA64_REG_CR_DCR, dcr | IA64_DCR_PP);
++	else
++		ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
++
++	if (psr_pp) {
++		pfm_set_psr_pp();
++		ia64_psr(regs)->pp = 1;
++	} else {
++		pfm_clear_psr_pp();
++		ia64_psr(regs)->pp = 0;
++	}
++	ia64_srlz_d();
++
++	if (set->flags & PFM_ITA_SETFL_IDLE_EXCL) {
++		PFM_DBG("enable idle exclude");
++		__get_cpu_var(pfm_syst_info) |= PFM_ITA_CPUINFO_IDLE_EXCL;
++	}
++}
++
++/*
++ * Only call this function when a process is trying to
++ * write the debug registers (reading is always allowed)
++ * called from arch/ia64/kernel/ptrace.c:access_uarea()
++ */
++int __pfm_use_dbregs(struct task_struct *task)
++{
++	struct pfm_arch_context *ctx_arch;
++	struct pfm_context *ctx;
++	unsigned long flags;
++	int ret = 0;
++
++	PFM_DBG("called for [%d]", task->pid);
++
++	ctx = task->pfm_context;
++
++	/*
++	 * do it only once
++	 */
++	if (task->thread.flags & IA64_THREAD_DBG_VALID) {
++		PFM_DBG("IA64_THREAD_DBG_VALID already set");
++		return 0;
++	}
++	if (ctx) {
++		spin_lock_irqsave(&ctx->lock, flags);
++		ctx_arch = pfm_ctx_arch(ctx);
++
++		if (ctx_arch->flags.use_dbr == 1) {
++			PFM_DBG("PMU using dbregs already, no ptrace access");
++			ret = -1;
++		}
++		spin_unlock_irqrestore(&ctx->lock, flags);
++		if (ret)
++			return ret;
++	}
++
++	spin_lock(&pfm_arch_sessions_lock);
++
++	/*
++	 * We cannot allow setting breakpoints when system wide monitoring
++	 * sessions are using the debug registers.
++	 */
++	if (!pfm_arch_sessions.pfs_sys_use_dbr)
++		pfm_arch_sessions.pfs_ptrace_use_dbr++;
++	else
++		ret = -1;
++
++	PFM_DBG("ptrace_use_dbr=%u  sys_use_dbr=%u by [%d] ret = %d",
++		  pfm_arch_sessions.pfs_ptrace_use_dbr,
++		  pfm_arch_sessions.pfs_sys_use_dbr,
++		  task->pid, ret);
++
++	spin_unlock(&pfm_arch_sessions_lock);
++	if (ret)
++		return ret;
++#ifndef CONFIG_SMP
++	/*
++	 * in UP, we need to check whether the current
++	 * owner of the PMU is not using the debug registers
++	 * for monitoring. Because we are using a lazy
++	 * save on ctxswout, we must force a save in this
++	 * case because the debug registers are being
++	 * modified by another task. We save the current
++	 * PMD registers, and clear ownership. In ctxswin,
++	 * full state will be reloaded.
++	 *
++	 * Note: we overwrite task.
++	 */
++	task = __get_cpu_var(pmu_owner);
++	ctx = __get_cpu_var(pmu_ctx);
++
++	if (task == NULL)
++		return 0;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	if (ctx_arch->flags.use_dbr)
++		pfm_save_pmds_release(ctx);
++#endif
++	return 0;
++}
++
++/*
++ * This function is called for every task that exits with the
++ * IA64_THREAD_DBG_VALID set. This indicates a task which was
++ * able to use the debug registers for debugging purposes via
++ * ptrace(). Therefore we know it was not using them for
++ * perfmormance monitoring, so we only decrement the number
++ * of "ptraced" debug register users to keep the count up to date
++ */
++int __pfm_release_dbregs(struct task_struct *task)
++{
++	int ret;
++
++	spin_lock(&pfm_arch_sessions_lock);
++
++	if (pfm_arch_sessions.pfs_ptrace_use_dbr == 0) {
++		PFM_ERR("invalid release for [%d] ptrace_use_dbr=0", task->pid);
++		ret = -1;
++	}  else {
++		pfm_arch_sessions.pfs_ptrace_use_dbr--;
++		ret = 0;
++	}
++	spin_unlock(&pfm_arch_sessions_lock);
++
++	return ret;
++}
++
++int pfm_ia64_mark_dbregs_used(struct pfm_context *ctx,
++			      struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++	struct task_struct *task;
++	struct thread_struct *thread;
++	int ret = 0, state;
++	int i, can_access_pmu = 0;
++	int is_loaded, is_system;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	state = ctx->state;
++	task = ctx->task;
++	is_loaded = state == PFM_CTX_LOADED || state == PFM_CTX_MASKED;
++	is_system = ctx->flags.system;
++	can_access_pmu = __get_cpu_var(pmu_owner) == task || is_system;
++
++	if (is_loaded == 0)
++		goto done;
++
++	if (is_system == 0) {
++		thread = &(task->thread);
++
++		/*
++		 * cannot use debug registers for montioring if they are
++		 * already used for debugging
++		 */
++		if (thread->flags & IA64_THREAD_DBG_VALID) {
++			PFM_DBG("debug registers already in use for [%d]",
++				  task->pid);
++			return -EBUSY;
++		}
++	}
++
++	/*
++	 * check for debug registers in system wide mode
++	 */
++	spin_lock(&pfm_arch_sessions_lock);
++
++	if (is_system) {
++		if (pfm_arch_sessions.pfs_ptrace_use_dbr)
++			ret = -EBUSY;
++		else
++			pfm_arch_sessions.pfs_sys_use_dbr++;
++	}
++
++	spin_unlock(&pfm_arch_sessions_lock);
++
++	if (ret != 0)
++		return ret;
++
++	/*
++	 * clear hardware registers to make sure we don't
++	 * pick up stale state.
++	 */
++	if (can_access_pmu) {
++		PFM_DBG("clearing ibrs, dbrs");
++		for (i = 0; i < 8; i++) {
++			ia64_set_ibr(i, 0);
++			ia64_dv_serialize_instruction();
++		}
++		ia64_srlz_i();
++		for (i = 0; i < 8; i++) {
++			ia64_set_dbr(i, 0);
++			ia64_dv_serialize_data();
++		}
++		ia64_srlz_d();
++	}
++done:
++	/*
++	 * debug registers are now in use
++	 */
++	ctx_arch->flags.use_dbr = 1;
++	set->priv_flags |= PFM_ITA_SETFL_USE_DBR;
++	PFM_DBG("set%u use_dbr=1", set->id);
++	return 0;
++}
++EXPORT_SYMBOL(pfm_ia64_mark_dbregs_used);
++
++char *pfm_arch_get_pmu_module_name(void)
++{
++	switch(local_cpu_data->family) {
++	case 0x07:
++		return "perfmon_itanium";
++	case 0x1f:
++		return "perfmon_mckinley";
++	case 0x20:
++		return "perfmon_montecito";
++	default:
++		return "perfmon_generic";
++	}
++	return NULL;
++}
++
++/*
++ * global arch-specific intialization, called only once
++ */
++int __init pfm_arch_init(void)
++{
++	int ret;
++
++	spin_lock_init(&pfm_arch_sessions_lock);
++
++#ifdef CONFIG_IA64_PERFMON_COMPAT
++	ret = pfm_ia64_compat_init();
++	if (ret)
++		return ret;
++#endif
++	register_percpu_irq(IA64_PERFMON_VECTOR, &perfmon_irqaction);
++
++
++	return 0;
++}
+--- /dev/null
++++ b/arch/ia64/perfmon/perfmon_compat.c
+@@ -0,0 +1,1247 @@
++/*
++ * This file implements the IA-64 specific
++ * support for the perfmon2 interface
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/interrupt.h>
++#include <linux/module.h>
++#include <linux/file.h>
++#include <linux/vmalloc.h>
++#include <linux/proc_fs.h>
++#include <linux/perfmon.h>
++#include <asm/perfmon_compat.h>
++#include <asm/uaccess.h>
++
++asmlinkage long sys_pfm_stop(int fd);
++asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *st);
++asmlinkage long sys_pfm_unload_context(int fd);
++asmlinkage long sys_pfm_restart(int fd);
++asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ld);
++
++extern ssize_t __pfm_read(struct pfm_context *ctx,
++			  union pfarg_msg *msg_buf,
++			  int non_block);
++/*
++ * function providing some help for backward compatiblity with old IA-64
++ * applications. In the old model, certain attributes of a counter were
++ * passed via the PMC, now they are passed via the PMD.
++ */
++static int pfm_compat_update_pmd(struct pfm_context *ctx, u16 set_id, u16 cnum,
++		          u32 rflags,
++			  unsigned long *smpl_pmds,
++		          unsigned long *reset_pmds,
++			  u64 eventid)
++{
++	struct pfm_event_set *set;
++	int is_counting;
++	unsigned long *impl_pmds;
++	u32 flags = 0;
++	u16 max_pmd;
++
++	impl_pmds = pfm_pmu_conf->regs.pmds;
++	max_pmd	= pfm_pmu_conf->regs.max_pmd;
++
++	/*
++	 * given that we do not maintain PMC ->PMD dependencies
++	 * we cannot figure out what to do in case PMCxx != PMDxx
++	 */
++	if (cnum > max_pmd)
++		return 0;
++
++	/*
++	 * assumes PMCxx controls PMDxx which is always true for counters
++	 * on Itanium PMUs.
++	 */
++	is_counting = pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_C64;
++	set = pfm_find_set(ctx, set_id, 0);
++
++	/*
++	 * for v2.0, we only allowed counting PMD to generate
++	 * user-level notifications. Same thing with randomization.
++	 */
++	if (is_counting) {
++		if (rflags & PFM_REGFL_OVFL_NOTIFY)
++			flags |= PFM_REGFL_OVFL_NOTIFY;
++		if (rflags & PFM_REGFL_RANDOM)
++			flags |= PFM_REGFL_RANDOM;
++		/*
++		 * verify validity of smpl_pmds
++		 */
++		if (unlikely(bitmap_subset(smpl_pmds,
++					   impl_pmds, max_pmd) == 0)) {
++			PFM_DBG("invalid smpl_pmds=0x%llx for pmd%u",
++				  (unsigned long long)smpl_pmds[0], cnum);
++			return -EINVAL;
++		}
++		/*
++		 * verify validity of reset_pmds
++		 */
++		if (unlikely(bitmap_subset(reset_pmds,
++					   impl_pmds, max_pmd) == 0)) {
++			PFM_DBG("invalid reset_pmds=0x%lx for pmd%u",
++				  reset_pmds[0], cnum);
++			return -EINVAL;
++		}
++		/*
++		 * ensures that a PFM_READ_PMDS succeeds with a
++		 * corresponding PFM_WRITE_PMDS
++		 */
++		__set_bit(cnum, set->used_pmds);
++
++	} else if (rflags & (PFM_REGFL_OVFL_NOTIFY|PFM_REGFL_RANDOM)) {
++		PFM_DBG("cannot set ovfl_notify or random on pmd%u", cnum);
++		return -EINVAL;
++	}
++
++	set->pmds[cnum].flags = flags;
++
++	if (is_counting) {
++		bitmap_copy(set->pmds[cnum].reset_pmds,
++			    reset_pmds,
++			    max_pmd);
++
++		bitmap_copy(set->pmds[cnum].smpl_pmds,
++			    smpl_pmds,
++			    max_pmd);
++
++		set->pmds[cnum].eventid = eventid;
++
++		/*
++		 * update ovfl_notify
++		 */
++		if (rflags & PFM_REGFL_OVFL_NOTIFY)
++			__set_bit(cnum, set->ovfl_notify);
++		else
++			__clear_bit(cnum, set->ovfl_notify);
++
++	}
++	PFM_DBG("pmd%u flags=0x%x eventid=0x%lx r_pmds=0x%lx s_pmds=0x%lx",
++		  cnum, flags,
++		  eventid,
++		  reset_pmds[0],
++		  smpl_pmds[0]);
++
++	return 0;
++}
++
++
++int __pfm_write_ibrs_old(struct pfm_context *ctx, void *arg, int count)
++{
++	struct pfarg_dbreg *req = arg;
++	struct pfarg_pmc pmc;
++	int i, ret = 0;
++
++	memset(&pmc, 0, sizeof(pmc));
++
++	for (i = 0; i < count; i++, req++) {
++		pmc.reg_num   = 256+req->dbreg_num;
++		pmc.reg_value = req->dbreg_value;
++		pmc.reg_flags = 0;
++		pmc.reg_set   = req->dbreg_set;
++
++		ret = __pfm_write_pmcs(ctx, &pmc, 1);
++
++		req->dbreg_flags &= ~PFM_REG_RETFL_MASK;
++		req->dbreg_flags |= pmc.reg_flags;
++
++		if (ret)
++			return ret;
++	}
++	return 0;
++}
++
++static long pfm_write_ibrs_old(int fd, void __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_dbreg *req = NULL;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
++		return -EINVAL;
++
++	sz = count*sizeof(*req);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (ret == 0)
++		ret = __pfm_write_ibrs_old(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	if (copy_to_user(ureq, req, sz))
++		ret = -EFAULT;
++
++	kfree(fptr);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++int __pfm_write_dbrs_old(struct pfm_context *ctx, void *arg, int count)
++{
++	struct pfarg_dbreg *req = arg;
++	struct pfarg_pmc pmc;
++	int i, ret = 0;
++
++	memset(&pmc, 0, sizeof(pmc));
++
++	for (i = 0; i < count; i++, req++) {
++		pmc.reg_num   = 264+req->dbreg_num;
++		pmc.reg_value = req->dbreg_value;
++		pmc.reg_flags = 0;
++		pmc.reg_set   = req->dbreg_set;
++
++		ret = __pfm_write_pmcs(ctx, &pmc, 1);
++
++		req->dbreg_flags &= ~PFM_REG_RETFL_MASK;
++		req->dbreg_flags |= pmc.reg_flags;
++		if (ret)
++			return ret;
++	}
++	return 0;
++}
++
++static long pfm_write_dbrs_old(int fd, void __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_dbreg *req = NULL;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
++		return -EINVAL;
++
++	sz = count*sizeof(*req);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (ret == 0)
++		ret = __pfm_write_dbrs_old(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	if (copy_to_user(ureq, req, sz))
++		ret = -EFAULT;
++
++	kfree(fptr);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++int __pfm_write_pmcs_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
++			 int count)
++{
++	struct pfarg_pmc req;
++	unsigned int i;
++	int ret, error_code;
++
++	memset(&req, 0, sizeof(req));
++
++	for (i = 0; i < count; i++, req_old++) {
++		req.reg_num   = req_old->reg_num;
++		req.reg_set   = req_old->reg_set;
++		req.reg_flags = 0;
++		req.reg_value = req_old->reg_value;
++
++		ret = __pfm_write_pmcs(ctx, (void *)&req, 1);
++		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
++		req_old->reg_flags |= req.reg_flags;
++
++		if (ret)
++			return ret;
++
++		ret = pfm_compat_update_pmd(ctx, req_old->reg_set,
++				      req_old->reg_num,
++				      (u32)req_old->reg_flags,
++				      req_old->reg_smpl_pmds,
++				      req_old->reg_reset_pmds,
++				      req_old->reg_smpl_eventid);
++
++		error_code = ret ? PFM_REG_RETFL_EINVAL : 0;
++		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
++		req_old->reg_flags |= error_code;
++
++		if (ret)
++			return ret;
++	}
++	return 0;
++}
++
++static long pfm_write_pmcs_old(int fd, void __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_reg *req = NULL;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
++		return -EINVAL;
++
++	sz = count*sizeof(*req);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (ret == 0)
++		ret = __pfm_write_pmcs_old(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	if (copy_to_user(ureq, req, sz))
++		ret = -EFAULT;
++
++	kfree(fptr);
++
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++int __pfm_write_pmds_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
++			 int count)
++{
++	struct pfarg_pmd req;
++	int i, ret;
++
++	memset(&req, 0, sizeof(req));
++
++	for (i = 0; i < count; i++, req_old++) {
++		req.reg_num   = req_old->reg_num;
++		req.reg_set   = req_old->reg_set;
++		req.reg_value = req_old->reg_value;
++		req.reg_flags = req_old->reg_flags;
++
++		req.reg_long_reset  = req_old->reg_long_reset;
++		req.reg_short_reset = req_old->reg_short_reset;
++		req.reg_random_mask = req_old->reg_random_mask;
++		/*
++		 * reg_random_seed is ignored since v2.3
++		 */
++
++		/*
++		 * skip last_reset_val not used for writing
++		 * skip smpl_pmds, reset_pmds, eventid, ovfl_swtch_cnt
++		 * as set in pfm_write_pmcs_old.
++		 */
++		req.reg_ovfl_switch_cnt = req_old->reg_ovfl_switch_cnt;
++
++		ret = __pfm_write_pmds(ctx, (void *)&req, 1, 1);
++
++		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
++		req_old->reg_flags |= req.reg_flags;
++
++		if (ret)
++			return ret;
++	}
++	return 0;
++}
++
++static long pfm_write_pmds_old(int fd, void __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_reg *req = NULL;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
++		return -EINVAL;
++
++	sz = count*sizeof(*req);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (ret == 0)
++		ret = __pfm_write_pmds_old(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	if (copy_to_user(ureq, req, sz))
++		ret = -EFAULT;
++
++	kfree(fptr);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++int __pfm_read_pmds_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
++			int count)
++{
++	struct pfarg_pmd req;
++	int i, ret;
++
++	memset(&req, 0, sizeof(req));
++
++	for (i = 0; i < count; i++, req_old++) {
++		req.reg_num   = req_old->reg_num;
++		req.reg_set   = req_old->reg_set;
++
++		/* skip value not used for reading */
++		req.reg_flags = req_old->reg_flags;
++
++		/* skip short/long_reset not used for reading */
++		/* skip last_reset_val not used for reading */
++		/* skip ovfl_switch_cnt not used for reading */
++
++		ret = __pfm_read_pmds(ctx, (void *)&req, 1);
++
++		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
++		req_old->reg_flags |= req.reg_flags;
++		if (ret)
++			return ret;
++
++		/* update fields */
++		req_old->reg_value = req.reg_value;
++
++		req_old->reg_last_reset_val  = req.reg_last_reset_val;
++		req_old->reg_ovfl_switch_cnt = req.reg_ovfl_switch_cnt;
++	}
++	return 0;
++}
++
++static long pfm_read_pmds_old(int fd, void __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_reg *req = NULL;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
++		return -EINVAL;
++
++	sz = count*sizeof(*req);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (ret == 0)
++		ret = __pfm_read_pmds_old(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	if (copy_to_user(ureq, req, sz))
++		ret = -EFAULT;
++
++	kfree(fptr);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++/*
++ * OBSOLETE: use /proc/perfmon_map instead
++ */
++static long pfm_get_default_pmcs_old(int fd, void __user *ureq, int count)
++{
++	struct pfarg_reg *req = NULL;
++	void *fptr;
++	size_t sz;
++	int ret, i;
++	unsigned int cnum;
++
++	if (count < 1)
++		return -EINVAL;
++
++	/*
++	 * ensure the pfm_pmu_conf does not disappear while
++	 * we use it
++	 */
++	ret = pfm_pmu_conf_get(1);
++	if (ret)
++		return ret;
++
++	sz = count*sizeof(*ureq);
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++
++	for (i = 0; i < count; i++, req++) {
++		cnum   = req->reg_num;
++
++		if (i >= PFM_MAX_PMCS ||
++		    (pfm_pmu_conf->pmc_desc[cnum].type & PFM_REG_I) == 0) {
++			req->reg_flags = PFM_REG_RETFL_EINVAL;
++			break;
++		}
++		req->reg_value = pfm_pmu_conf->pmc_desc[cnum].dfl_val;
++		req->reg_flags = 0;
++
++		PFM_DBG("pmc[%u]=0x%lx", cnum, req->reg_value);
++	}
++
++	if (copy_to_user(ureq, req, sz))
++		ret = -EFAULT;
++
++	kfree(fptr);
++error:
++	pfm_pmu_conf_put();
++
++	return ret;
++}
++
++/*
++ * allocate a sampling buffer and remaps it into the user address space of
++ * the task. This is only in compatibility mode
++ *
++ * function called ONLY on current task
++ */
++int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx, size_t rsize,
++			      struct file *filp)
++{
++	struct mm_struct *mm = current->mm;
++	struct vm_area_struct *vma = NULL;
++	struct pfm_arch_context *ctx_arch;
++	size_t size;
++	int ret;
++	extern struct vm_operations_struct pfm_buf_map_vm_ops;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * allocate buffer + map desc
++	 */
++	ret = pfm_smpl_buffer_alloc(ctx, rsize);
++	if (ret)
++		return ret;
++
++	size = ctx->smpl_size;
++
++
++	/* allocate vma */
++	vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
++	if (!vma) {
++		PFM_DBG("Cannot allocate vma");
++		goto error_kmem;
++	}
++	memset(vma, 0, sizeof(*vma));
++
++	/*
++	 * partially initialize the vma for the sampling buffer
++	 */
++	vma->vm_mm	     = mm;
++	vma->vm_flags	     = VM_READ| VM_MAYREAD |VM_RESERVED;
++	vma->vm_page_prot    = PAGE_READONLY;
++	vma->vm_ops	     = &pfm_buf_map_vm_ops;
++	vma->vm_file	     = filp;
++	vma->vm_private_data = ctx;
++	vma->vm_pgoff        = 0;
++
++	/*
++	 * simulate effect of mmap()
++	 */
++	get_file(filp);
++
++	/*
++	 * Let's do the difficult operations next.
++	 *
++	 * now we atomically find some area in the address space and
++	 * remap the buffer into it.
++	 */
++	down_write(&current->mm->mmap_sem);
++
++	/* find some free area in address space, must have mmap sem held */
++	vma->vm_start = get_unmapped_area(NULL, 0, size, 0,
++					  MAP_PRIVATE|MAP_ANONYMOUS);
++	if (vma->vm_start == 0) {
++		PFM_DBG("cannot find unmapped area of size %zu", size);
++		up_write(&current->mm->mmap_sem);
++		goto error;
++	}
++	vma->vm_end = vma->vm_start + size;
++
++	PFM_DBG("aligned_size=%zu mapped @0x%lx", size, vma->vm_start);
++	/*
++	 * now insert the vma in the vm list for the process, must be
++	 * done with mmap lock held
++	 */
++	insert_vm_struct(mm, vma);
++
++	mm->total_vm  += size >> PAGE_SHIFT;
++
++	up_write(&current->mm->mmap_sem);
++
++	/*
++	 * IMPORTANT: we do not issue the fput()
++	 * because we want to increase the ref count
++	 * on the descriptor to simulate what mmap()
++	 * would do
++	 */
++
++	/*
++	 * used to propagate vaddr to syscall stub
++	 */
++	ctx_arch->ctx_smpl_vaddr = (void *)vma->vm_start;
++
++	return 0;
++error:
++	kmem_cache_free(vm_area_cachep, vma);
++error_kmem:
++	pfm_release_buf_space(ctx, ctx->smpl_size);
++	vfree(ctx->smpl_addr);
++	return -ENOMEM;
++}
++
++#define PFM_DEFAULT_SMPL_UUID { \
++		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82,\
++		0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
++
++static pfm_uuid_t old_default_uuid = PFM_DEFAULT_SMPL_UUID;
++static pfm_uuid_t null_uuid;
++
++/*
++ * function invoked in case, pfm_context_create fails
++ * at the last operation, copy_to_user. It needs to
++ * undo memory allocations and free the file descriptor
++ */
++static void pfm_undo_create_context_fd(int fd, struct pfm_context *ctx)
++{
++	struct files_struct *files = current->files;
++	struct file *file;
++	int fput_needed;
++
++	file = fget_light(fd, &fput_needed);
++	/*
++	 * there is no fd_uninstall(), so we do it
++	 * here. put_unused_fd() does not remove the
++	 * effect of fd_install().
++	 */
++
++	spin_lock(&files->file_lock);
++	files->fd_array[fd] = NULL;
++	spin_unlock(&files->file_lock);
++
++	fput_light(file, fput_needed);
++
++	/*
++	 * decrement ref count and kill file
++	 */
++	put_filp(file);
++
++	put_unused_fd(fd);
++
++	pfm_context_free(ctx);
++}
++
++static int pfm_get_smpl_arg_old(pfm_uuid_t uuid, void __user *fmt_uarg,
++				size_t usize, void **arg,
++				struct pfm_smpl_fmt **fmt)
++{
++	struct pfm_smpl_fmt *f;
++	void *addr = NULL;
++	size_t sz;
++	int ret;
++
++	if (!memcmp(uuid, null_uuid, sizeof(pfm_uuid_t)))
++		return 0;
++
++	if (memcmp(uuid, old_default_uuid, sizeof(pfm_uuid_t))) {
++		PFM_DBG("compatibility mode supports only default sampling format");
++		return -EINVAL;
++	}
++	/*
++	 * find fmt and increase refcount
++	 */
++	f = pfm_smpl_fmt_get("default-old");
++	if (f == NULL) {
++		PFM_DBG("default-old buffer format not found");
++		return -EINVAL;
++	}
++
++	/*
++	 * expected format argument size
++	 */
++	sz = f->fmt_arg_size;
++
++	/*
++	 * check user size matches expected size
++	 * usize = -1 is for IA-64 backward compatibility
++	 */
++	ret = -EINVAL;
++	if (sz != usize && usize != -1) {
++		PFM_DBG("invalid arg size %zu, format expects %zu",
++			usize, sz);
++		goto error;
++	}
++
++	ret = -ENOMEM;
++	addr = kmalloc(sz, GFP_KERNEL);
++	if (addr == NULL)
++		goto error;
++
++	ret = -EFAULT;
++	if (copy_from_user(addr, fmt_uarg, sz))
++		goto error;
++
++	*arg = addr;
++	*fmt = f;
++	return 0;
++
++error:
++	kfree(addr);
++	pfm_smpl_fmt_put(f);
++	return ret;
++}
++
++static long pfm_create_context_old(int fd, void __user *ureq, int count)
++{
++	struct pfm_context *new_ctx;
++	struct pfm_arch_context *ctx_arch;
++	struct pfm_smpl_fmt *fmt = NULL;
++	struct pfarg_context req_old;
++	void __user *usmpl_arg;
++	void *smpl_arg = NULL;
++	struct pfarg_ctx req;
++	int ret;
++
++	if (count != 1)
++		return -EINVAL;
++
++	if (copy_from_user(&req_old, ureq, sizeof(req_old)))
++		return -EFAULT;
++
++	memset(&req, 0, sizeof(req));
++
++	/*
++	 * sampling format args are following pfarg_context
++	 */
++	usmpl_arg = ureq+sizeof(req_old);
++
++	ret = pfm_get_smpl_arg_old(req_old.ctx_smpl_buf_id, usmpl_arg, -1,
++			           &smpl_arg, &fmt);
++	if (ret)
++		return ret;
++
++	req.ctx_flags = req_old.ctx_flags;
++
++	/*
++	 * returns file descriptor if >=0, or error code */
++	ret = __pfm_create_context(&req, fmt, smpl_arg, PFM_COMPAT, &new_ctx);
++	if (ret >= 0) {
++		ctx_arch = pfm_ctx_arch(new_ctx);
++		req_old.ctx_fd = ret;
++		req_old.ctx_smpl_vaddr = ctx_arch->ctx_smpl_vaddr;
++	}
++
++	if (copy_to_user(ureq, &req_old, sizeof(req_old))) {
++		pfm_undo_create_context_fd(req_old.ctx_fd, new_ctx);
++		ret = -EFAULT;
++	}
++
++	kfree(smpl_arg);
++
++	return ret;
++}
++
++/*
++ * obsolete call: use /proc/perfmon
++ */
++static long pfm_get_features_old(int fd, void __user *arg, int count)
++{
++	struct pfarg_features req;
++	int ret = 0;
++
++	if (count != 1)
++		return -EINVAL;
++
++	memset(&req, 0, sizeof(req));
++
++	req.ft_version = PFM_VERSION;
++
++	if (copy_to_user(arg, &req, sizeof(req)))
++		ret = -EFAULT;
++
++	return ret;
++}
++
++static long pfm_debug_old(int fd, void __user *arg, int count)
++{
++	int m;
++
++	if (count != 1)
++		return -EINVAL;
++
++	if (get_user(m, (int __user*)arg))
++		return -EFAULT;
++
++
++	pfm_controls.debug = m == 0 ? 0 : 1;
++
++	PFM_INFO("debugging %s (timing reset)",
++		 pfm_controls.debug ? "on" : "off");
++
++	if (m == 0)
++		for_each_online_cpu(m) {
++			memset(&per_cpu(pfm_stats,m), 0,
++			       sizeof(struct pfm_stats));
++		}
++	return 0;
++}
++
++static long pfm_unload_context_old(int fd, void __user *arg, int count)
++{
++	if (count)
++		return -EINVAL;
++
++	return sys_pfm_unload_context(fd);
++}
++
++static long pfm_restart_old(int fd, void __user *arg, int count)
++{
++	if (count)
++		return -EINVAL;
++
++	return sys_pfm_restart(fd);
++}
++
++static long pfm_stop_old(int fd, void __user *arg, int count)
++{
++	if (count)
++		return -EINVAL;
++
++	return sys_pfm_stop(fd);
++}
++
++static long pfm_start_old(int fd, void __user *arg, int count)
++{
++	if (count > 1)
++		return -EINVAL;
++
++	return sys_pfm_start(fd, arg);
++}
++
++static long pfm_load_context_old(int fd, void __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct task_struct *task;
++	struct file *filp;
++	unsigned long flags;
++	struct pfarg_load req;
++	int ret, fput_needed;
++
++	if (count != 1)
++		return -EINVAL;
++
++	if (copy_from_user(&req, ureq, sizeof(req)))
++		return -EFAULT;
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	task = NULL;
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	/*
++	 * in per-thread mode (not self-monitoring), get a reference
++	 * on task to monitor. This must be done with interrupts enabled
++	 * Upon succesful return, refcount on task is increased.
++	 *
++	 * fget_light() is protecting the context.
++	 */
++	if (!ctx->flags.system) {
++		if (req.load_pid != current->pid) {
++			ret = pfm_get_task(ctx, req.load_pid, &task);
++			if (ret)
++				goto error;
++		} else
++			task = current;
++	}
++	/*
++	 * irqsave is required to avoid race in case context is already
++	 * loaded or with switch timeout in the case of self-monitoring
++	 */
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	/*
++	 * the new interface requires the desired CPU to be explicitely set
++	 * in this field. the kernel then checks you are on the right CPU
++	 */
++	if (ctx->flags.system)
++		req.load_pid = smp_processor_id();
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags);
++	if (!ret)
++		ret = __pfm_load_context(ctx, &req, task);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	/*
++	 * in per-thread mode (not self-monitoring), we need
++	 * to decrease refcount on task to monitor:
++	 *   - load successful: we have a reference to the task in ctx->task
++	 *   - load failed    : undo the effect of pfm_get_task()
++	 */
++	if (task && task != current)
++		put_task_struct(task);
++
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++/*
++ * perfmon command descriptions
++ */
++struct pfm_cmd_desc {
++	long (*cmd_func)(int fd, void __user *arg, int count);
++};
++
++/*
++ * functions MUST be listed in the increasing order of
++ * their index (see permfon.h)
++ */
++#define PFM_CMD(name)  \
++	{ .cmd_func = name,  \
++	}
++#define PFM_CMD_NONE		\
++	{ .cmd_func = NULL   \
++	}
++
++static struct pfm_cmd_desc pfm_cmd_tab[]={
++/* 0  */PFM_CMD_NONE,
++/* 1  */PFM_CMD(pfm_write_pmcs_old),
++/* 2  */PFM_CMD(pfm_write_pmds_old),
++/* 3  */PFM_CMD(pfm_read_pmds_old),
++/* 4  */PFM_CMD(pfm_stop_old),
++/* 5  */PFM_CMD(pfm_start_old),
++/* 6  */PFM_CMD_NONE,
++/* 7  */PFM_CMD_NONE,
++/* 8  */PFM_CMD(pfm_create_context_old),
++/* 9  */PFM_CMD_NONE,
++/* 10 */PFM_CMD(pfm_restart_old),
++/* 11 */PFM_CMD_NONE,
++/* 12 */PFM_CMD(pfm_get_features_old),
++/* 13 */PFM_CMD(pfm_debug_old),
++/* 14 */PFM_CMD_NONE,
++/* 15 */PFM_CMD(pfm_get_default_pmcs_old),
++/* 16 */PFM_CMD(pfm_load_context_old),
++/* 17 */PFM_CMD(pfm_unload_context_old),
++/* 18 */PFM_CMD_NONE,
++/* 19 */PFM_CMD_NONE,
++/* 20 */PFM_CMD_NONE,
++/* 21 */PFM_CMD_NONE,
++/* 22 */PFM_CMD_NONE,
++/* 23 */PFM_CMD_NONE,
++/* 24 */PFM_CMD_NONE,
++/* 25 */PFM_CMD_NONE,
++/* 26 */PFM_CMD_NONE,
++/* 27 */PFM_CMD_NONE,
++/* 28 */PFM_CMD_NONE,
++/* 29 */PFM_CMD_NONE,
++/* 30 */PFM_CMD_NONE,
++/* 31 */PFM_CMD_NONE,
++/* 32 */PFM_CMD(pfm_write_ibrs_old),
++/* 33 */PFM_CMD(pfm_write_dbrs_old),
++};
++#define PFM_CMD_COUNT ARRAY_SIZE(pfm_cmd_tab)
++
++/*
++ * system-call entry point (must return long)
++ */
++asmlinkage long sys_perfmonctl (int fd, int cmd, void __user *arg, int count)
++{
++	if (perfmon_disabled)
++		return -ENOSYS;
++
++	if (unlikely(cmd < 0 || cmd >= PFM_CMD_COUNT
++		     || pfm_cmd_tab[cmd].cmd_func == NULL)) {
++		PFM_DBG("invalid cmd=%d", cmd);
++		return -EINVAL;
++	}
++	return (long)pfm_cmd_tab[cmd].cmd_func(fd, arg, count);
++}
++
++/*
++ * Called from pfm_read() for a perfmon v2.0 context.
++ *
++ * compatibility mode pfm_read() routine. We need a separate
++ * routine because the definition of the message has changed.
++ * The pfm_msg and pfarg_msg structures are different.
++ *
++ * return: sizeof(pfm_msg_t) on success, -errno otherwise
++ */
++ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
++			     char __user *buf,
++			     int non_block,
++			     size_t size)
++{
++	union pfarg_msg msg_buf;
++	pfm_msg_t old_msg_buf;
++	pfm_ovfl_msg_t *o_msg;
++	struct pfarg_ovfl_msg *n_msg;
++	int ret;
++
++	PFM_DBG("msg=%p size=%zu", buf, size);
++
++	/*
++	 * cannot extract partial messages.
++	 * check even when there is no message
++	 *
++	 * cannot extract more than one message per call. Bytes
++	 * above sizeof(msg) are ignored.
++	 */
++	if (size < sizeof(old_msg_buf)) {
++		PFM_DBG("message is too small size=%zu must be >=%zu)",
++			size,
++			sizeof(old_msg_buf));
++		return -EINVAL;
++	}
++
++	ret =  __pfm_read(ctx, &msg_buf, non_block);
++	if (ret < 1)
++		return ret;
++
++	/*
++	 * force return value to old message size
++	 */
++	ret = sizeof(old_msg_buf);
++
++	o_msg = &old_msg_buf.pfm_ovfl_msg;
++	n_msg = &msg_buf.pfm_ovfl_msg;
++
++	switch(msg_buf.type) {
++		case PFM_MSG_OVFL:
++			o_msg->msg_type   = PFM_MSG_OVFL;
++			o_msg->msg_ctx_fd = 0;
++			o_msg->msg_active_set = n_msg->msg_active_set;
++			o_msg->msg_tstamp = 0;
++
++			o_msg->msg_ovfl_pmds[0] = n_msg->msg_ovfl_pmds[0];
++			o_msg->msg_ovfl_pmds[1] = n_msg->msg_ovfl_pmds[1];
++			o_msg->msg_ovfl_pmds[2] = n_msg->msg_ovfl_pmds[2];
++			o_msg->msg_ovfl_pmds[3] = n_msg->msg_ovfl_pmds[3];
++			break;
++		case PFM_MSG_END:
++			o_msg->msg_type = PFM_MSG_END;
++			o_msg->msg_ctx_fd = 0;
++			o_msg->msg_tstamp = 0;
++			break;
++		default:
++			PFM_DBG("unknown msg type=%d", msg_buf.type);
++	}
++	if(copy_to_user(buf, &old_msg_buf, sizeof(old_msg_buf)))
++		ret = -EFAULT;
++	PFM_DBG_ovfl("ret=%d", ret);
++	return ret;
++}
++
++/*
++ * legacy /proc/perfmon simplified interface (we only maintain the
++ * global information (no more per-cpu stats, use
++ * /sys/devices/system/cpu/cpuXX/perfmon
++ */
++static struct proc_dir_entry 	*perfmon_proc;
++
++static void *pfm_proc_start(struct seq_file *m, loff_t *pos)
++{
++	if (*pos == 0)
++		return (void *)1;
++
++	return NULL;
++}
++
++static void *pfm_proc_next(struct seq_file *m, void *v, loff_t *pos)
++{
++	++*pos;
++	return pfm_proc_start(m, pos);
++}
++
++static void pfm_proc_stop(struct seq_file *m, void *v)
++{
++}
++
++/*
++ * this is a simplified version of the legacy /proc/perfmon.
++ * We have retained ONLY the key information that tools are actually
++ * using
++ */
++static void pfm_proc_show_header(struct seq_file *m)
++{
++	char buf[128];
++
++	pfm_sysfs_session_show(buf, sizeof(buf), 3);
++
++	seq_printf(m, "perfmon version            : %u.%u\n",
++		PFM_VERSION_MAJ, PFM_VERSION_MIN);
++
++	seq_printf(m, "model                      : %s", buf);
++}
++
++static int pfm_proc_show(struct seq_file *m, void *v)
++{
++	pfm_proc_show_header(m);
++	return 0;
++}
++
++struct seq_operations pfm_proc_seq_ops = {
++	.start = pfm_proc_start,
++	.next =	pfm_proc_next,
++	.stop =	pfm_proc_stop,
++	.show =	pfm_proc_show
++};
++
++static int pfm_proc_open(struct inode *inode, struct file *file)
++{
++	return seq_open(file, &pfm_proc_seq_ops);
++}
++
++
++static struct file_operations pfm_proc_fops = {
++	.open = pfm_proc_open,
++	.read = seq_read,
++	.llseek	= seq_lseek,
++	.release = seq_release,
++};
++
++/*
++ * called from pfm_arch_init(), global initialization, called once
++ */
++int __init pfm_ia64_compat_init(void)
++{
++	/*
++	 * create /proc/perfmon
++	 */
++	perfmon_proc = create_proc_entry("perfmon", S_IRUGO, NULL);
++	if (perfmon_proc == NULL) {
++		PFM_ERR("cannot create /proc entry, perfmon disabled");
++		return -1;
++	}
++	perfmon_proc->proc_fops = &pfm_proc_fops;
++	return 0;
++}
+--- /dev/null
++++ b/arch/ia64/perfmon/perfmon_default_smpl.c
+@@ -0,0 +1,268 @@
++/*
++ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file implements the old default sampling buffer format
++ * for the Linux/ia64 perfmon-2 subsystem. This is for backward
++ * compatibility only. use the new default format in perfmon/
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/kernel.h>
++#include <linux/types.h>
++#include <linux/module.h>
++#include <linux/init.h>
++#include <asm/delay.h>
++#include <linux/smp.h>
++#include <linux/sysctl.h>
++
++#ifdef MODULE
++#define FMT_FLAGS	0
++#else
++#define FMT_FLAGS	PFM_FMTFL_IS_BUILTIN
++#endif
++
++#include <linux/perfmon.h>
++#include <asm-ia64/perfmon_default_smpl.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("perfmon old default sampling format");
++MODULE_LICENSE("GPL");
++
++static int pfm_default_fmt_validate(u32 flags, u16 npmds, void *data)
++{
++	struct pfm_default_smpl_arg *arg = data;
++	size_t min_buf_size;
++
++	if (data == NULL) {
++		PFM_DBG("no argument passed");
++		return -EINVAL;
++	}
++
++	/*
++	 * compute min buf size. All PMD are manipulated as 64bit entities
++	 */
++	min_buf_size = sizeof(struct pfm_default_smpl_hdr)
++	             + (sizeof(struct pfm_default_smpl_entry)
++		     + (npmds*sizeof(u64)));
++
++	PFM_DBG("validate flags=0x%x npmds=%u min_buf_size=%lu "
++		  "buf_size=%lu CPU%d", flags, npmds, min_buf_size,
++		  arg->buf_size, smp_processor_id());
++
++	/*
++	 * must hold at least the buffer header + one minimally sized entry
++	 */
++	if (arg->buf_size < min_buf_size) return -EINVAL;
++
++	return 0;
++}
++
++static int pfm_default_fmt_get_size(unsigned int flags, void *data,
++				    size_t *size)
++{
++	struct pfm_default_smpl_arg *arg = data;
++
++	/*
++	 * size has been validated in default_validate
++	 */
++	*size = arg->buf_size;
++
++	return 0;
++}
++
++static int pfm_default_fmt_init(struct pfm_context *ctx, void *buf,
++				u32 flags, u16 npmds, void *data)
++{
++	struct pfm_default_smpl_hdr *hdr;
++	struct pfm_default_smpl_arg *arg = data;
++
++	hdr = buf;
++
++	hdr->hdr_version      = PFM_DEFAULT_SMPL_VERSION;
++	hdr->hdr_buf_size     = arg->buf_size;
++	hdr->hdr_cur_offs     = sizeof(*hdr);
++	hdr->hdr_overflows    = 0;
++	hdr->hdr_count        = 0;
++
++	PFM_DBG("buffer=%p buf_size=%lu hdr_size=%lu "
++		  "hdr_version=%u cur_offs=%lu",
++		  buf,
++		  hdr->hdr_buf_size,
++		  sizeof(*hdr),
++		  hdr->hdr_version,
++		  hdr->hdr_cur_offs);
++
++	return 0;
++}
++
++static int pfm_default_fmt_handler(void *buf, struct pfm_ovfl_arg *arg,
++				   unsigned long ip, u64 tstamp, void *data)
++{
++	struct pfm_default_smpl_hdr *hdr;
++	struct pfm_default_smpl_entry *ent;
++	void *cur, *last;
++	u64 *e;
++	size_t entry_size;
++	u16 npmds, i, ovfl_pmd;
++
++	hdr         = buf;
++	cur         = buf+hdr->hdr_cur_offs;
++	last        = buf+hdr->hdr_buf_size;
++	ovfl_pmd    = arg->ovfl_pmd;
++
++	/*
++	 * precheck for sanity
++	 */
++	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE) goto full;
++
++	npmds = arg->num_smpl_pmds;
++
++	ent = cur;
++
++	prefetch(arg->smpl_pmds_values);
++
++	entry_size = sizeof(*ent) + (npmds << 3);
++
++	/* position for first pmd */
++	e = (unsigned long *)(ent+1);
++
++	hdr->hdr_count++;
++
++	PFM_DBG_ovfl("count=%lu cur=%p last=%p free_bytes=%lu "
++		       "ovfl_pmd=%d npmds=%u",
++		       hdr->hdr_count,
++		       cur, last,
++		       last-cur,
++		       ovfl_pmd,
++		       npmds);
++
++	/*
++	 * current = task running at the time of the overflow.
++	 *
++	 * per-task mode:
++	 * 	- this is ususally the task being monitored.
++	 * 	  Under certain conditions, it might be a different task
++	 *
++	 * system-wide:
++	 * 	- this is not necessarily the task controlling the session
++	 */
++	ent->pid            = current->pid;
++	ent->ovfl_pmd  	    = ovfl_pmd;
++	ent->last_reset_val = arg->pmd_last_reset;
++
++	/*
++	 * where did the fault happen (includes slot number)
++	 */
++	ent->ip = ip;
++
++	ent->tstamp    = tstamp;
++	ent->cpu       = smp_processor_id();
++	ent->set       = arg->active_set;
++	ent->tgid      = current->tgid;
++
++	/*
++	 * selectively store PMDs in increasing index number
++	 */
++	if (npmds) {
++		u64 *val = arg->smpl_pmds_values;
++		for(i=0; i < npmds; i++) {
++			*e++ = *val++;
++		}
++	}
++
++	/*
++	 * update position for next entry
++	 */
++	hdr->hdr_cur_offs += entry_size;
++	cur               += entry_size;
++
++	/*
++	 * post check to avoid losing the last sample
++	 */
++	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE) goto full;
++
++	/*
++	 * reset before returning from interrupt handler
++	 */
++	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
++	return 0;
++full:
++	PFM_DBG_ovfl("smpl buffer full free=%lu, count=%lu",
++		       last-cur, hdr->hdr_count);
++
++	/*
++	 * increment number of buffer overflow.
++	 * important to detect duplicate set of samples.
++	 */
++	hdr->hdr_overflows++;
++
++	/*
++	 * request notification and masking of monitoring.
++	 * Notification is still subject to the overflowed
++	 */
++	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY| PFM_OVFL_CTRL_MASK;
++
++	return -ENOBUFS; /* we are full, sorry */
++}
++
++static int pfm_default_fmt_restart(int is_active, u32 *ovfl_ctrl, void *buf)
++{
++	struct pfm_default_smpl_hdr *hdr;
++
++	hdr = buf;
++
++	hdr->hdr_count    = 0;
++	hdr->hdr_cur_offs = sizeof(*hdr);
++
++	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
++
++	return 0;
++}
++
++static int pfm_default_fmt_exit(void *buf)
++{
++	return 0;
++}
++
++static struct pfm_smpl_fmt default_fmt={
++	.fmt_name = "default-old",
++	.fmt_version = 0x10000,
++	.fmt_arg_size = sizeof(struct pfm_default_smpl_arg),
++	.fmt_validate = pfm_default_fmt_validate,
++	.fmt_getsize = pfm_default_fmt_get_size,
++	.fmt_init = pfm_default_fmt_init,
++	.fmt_handler = pfm_default_fmt_handler,
++	.fmt_restart = pfm_default_fmt_restart,
++	.fmt_exit = pfm_default_fmt_exit,
++	.fmt_flags = FMT_FLAGS,
++	.owner= THIS_MODULE
++};
++
++static int pfm_default_fmt_init_module(void)
++{
++	int ret;
++
++	return pfm_fmt_register(&default_fmt);
++	return ret;
++}
++
++static void pfm_default_fmt_cleanup_module(void)
++{
++	pfm_fmt_unregister(&default_fmt);
++}
++
++module_init(pfm_default_fmt_init_module);
++module_exit(pfm_default_fmt_cleanup_module);
+--- /dev/null
++++ b/arch/ia64/perfmon/perfmon_generic.c
+@@ -0,0 +1,148 @@
++/*
++ * This file contains the generic PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
++ * contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <asm/pal.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("Generic IA-64 PMU description tables");
++MODULE_LICENSE("GPL");
++
++#define RDEP(x)	(1UL << (x))
++
++#define PFM_IA64GEN_MASK_PMCS	(RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7))
++#define PFM_IA64GEN_RSVD	(0xffffffffffff0080UL)
++#define PFM_IA64GEN_NO64	(1UL<<5)
++
++/* forward declaration */
++static struct pfm_pmu_config pfm_ia64gen_pmu_conf;
++
++static struct pfm_arch_pmu_info pfm_ia64gen_pmu_info={
++	.mask_pmcs = {PFM_IA64GEN_MASK_PMCS,},
++};
++
++static struct pfm_regmap_desc pfm_ia64gen_pmc_desc[]={
++/* pmc0  */ PMX_NA,
++/* pmc1  */ PMX_NA,
++/* pmc2  */ PMX_NA,
++/* pmc3  */ PMX_NA,
++/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 4),
++/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 5),
++/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 6),
++/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 7)
++};
++#define PFM_IA64GEN_NUM_PMCS ARRAY_SIZE(pfm_ia64gen_pmc_desc)
++
++static struct pfm_regmap_desc pfm_ia64gen_pmd_desc[]={
++/* pmd0  */ PMX_NA,
++/* pmd1  */ PMX_NA,
++/* pmd2  */ PMX_NA,
++/* pmd3  */ PMX_NA,
++/* pmd4  */ PMD_D(PFM_REG_C, "PMD4", 4),
++/* pmd5  */ PMD_D(PFM_REG_C, "PMD5", 5),
++/* pmd6  */ PMD_D(PFM_REG_C, "PMD6", 6),
++/* pmd7  */ PMD_D(PFM_REG_C, "PMD7", 7)
++};
++#define PFM_IA64GEN_NUM_PMDS ARRAY_SIZE(pfm_ia64gen_pmd_desc)
++
++static int pfm_ia64gen_pmc_check(struct pfm_context *ctx,
++				 struct pfm_event_set *set,
++			         struct pfarg_pmc *req)
++{
++#define PFM_IA64GEN_PMC_PM_POS6	(1UL<< 6)
++	u64 tmpval;
++	int is_system;
++
++	is_system = ctx->flags.system;
++	tmpval = req->reg_value;
++
++	switch(req->reg_num) {
++		case  4:
++		case  5:
++		case  6:
++		case  7:
++			/* set pmc.oi for 64-bit emulation */
++			tmpval |= 1UL << 5;
++
++			if (is_system)
++				tmpval |= PFM_IA64GEN_PMC_PM_POS6;
++			else
++				tmpval &= ~PFM_IA64GEN_PMC_PM_POS6;
++			break;
++
++	}
++	req->reg_value = tmpval;
++
++	return 0;
++}
++
++/*
++ * matches anything
++ */
++static int pfm_ia64gen_probe_pmu(void)
++{
++	u64 pm_buffer[16];
++	pal_perf_mon_info_u_t pm_info;
++
++	/*
++	 * call PAL_PERFMON_INFO to retrieve counter width which
++	 * is implementation specific
++	 */
++	if (ia64_pal_perf_mon_info(pm_buffer, &pm_info))
++		return -1;
++
++	pfm_ia64gen_pmu_conf.counter_width = pm_info.pal_perf_mon_info_s.width;
++
++	return 0;
++}
++
++/*
++ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
++ */
++static struct pfm_pmu_config pfm_ia64gen_pmu_conf={
++	.pmu_name = "Generic IA-64",
++	.counter_width = 0, /* computed from PAL_PERFMON_INFO */
++	.pmd_desc = pfm_ia64gen_pmd_desc,
++	.pmc_desc = pfm_ia64gen_pmc_desc,
++	.probe_pmu = pfm_ia64gen_probe_pmu,
++	.num_pmc_entries = PFM_IA64GEN_NUM_PMCS,
++	.num_pmd_entries = PFM_IA64GEN_NUM_PMDS,
++	.pmc_write_check = pfm_ia64gen_pmc_check,
++	.version = "1.0",
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.arch_info = & pfm_ia64gen_pmu_info
++	/* no read/write checkers */
++};
++
++static int __init pfm_gen_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_ia64gen_pmu_conf);
++}
++
++static void __exit pfm_gen_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_ia64gen_pmu_conf);
++}
++
++module_init(pfm_gen_pmu_init_module);
++module_exit(pfm_gen_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/ia64/perfmon/perfmon_itanium.c
+@@ -0,0 +1,229 @@
++/*
++ * This file contains the Itanium PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("Itanium (Merced) PMU description tables");
++MODULE_LICENSE("GPL");
++
++#define RDEP(x)	(1ULL << (x))
++
++#define PFM_ITA_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|RDEP(10)|RDEP(11)|\
++			   RDEP(12))
++
++#define PFM_ITA_NO64	(1ULL<<5)
++
++static struct pfm_arch_pmu_info pfm_ita_pmu_info={
++	.mask_pmcs = {PFM_ITA_MASK_PMCS,},
++};
++/* reserved bits are 1 in the mask */
++#define PFM_ITA_RSVD 0xfffffffffc8000a0UL
++/*
++ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
++ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
++ * but this is fine because they are handled separately in the IA-64 specific
++ * code.
++ */
++static struct pfm_regmap_desc pfm_ita_pmc_desc[]={
++/* pmc0  */ PMX_NA,
++/* pmc1  */ PMX_NA,
++/* pmc2  */ PMX_NA,
++/* pmc3  */ PMX_NA,
++/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 4),
++/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 5),
++/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 6),
++/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 7),
++/* pmc8  */ PMC_D(PFM_REG_W  , "PMC8" , 0xfffffffe3ffffff8UL, 0xfff00000001c0000UL, 0, 8),
++/* pmc9  */ PMC_D(PFM_REG_W  , "PMC9" , 0xfffffffe3ffffff8UL, 0xfff00000001c0000UL, 0, 9),
++/* pmc10 */ PMC_D(PFM_REG_W  , "PMC10", 0x0, 0xfffffffff3f0ff30UL, 0, 10),
++/* pmc11 */ PMC_D(PFM_REG_W  , "PMC11", 0x10000000UL, 0xffffffffecf0ff30UL, 0, 11),
++/* pmc12 */ PMC_D(PFM_REG_W  , "PMC12", 0x0, 0xffffffffffff0030UL, 0, 12),
++/* pmc13 */ PMC_D(PFM_REG_W  , "PMC13", 0x3ffff00000001UL, 0xfffffffffffffffeUL, 0, 13),
++/* pmc14 */ PMX_NA,
++/* pmc15 */ PMX_NA,
++/* pmc16 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc24 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc32 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc40 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc48 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc56 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc64 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc72 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc80 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc88 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc96 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc104 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc112 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc120 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc128 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc136 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc144 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc152 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc160 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc168 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc176 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc184 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc192 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc200 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc208 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc216 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc224 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc232 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc240 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc248 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc256 */ PMC_D(PFM_REG_W  , "IBR0", 0x0, 0, 0, 0),
++/* pmc257 */ PMC_D(PFM_REG_W  , "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
++/* pmc258 */ PMC_D(PFM_REG_W  , "IBR2", 0x0, 0, 0, 2),
++/* pmc259 */ PMC_D(PFM_REG_W  , "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
++/* pmc260 */ PMC_D(PFM_REG_W  , "IBR4", 0x0, 0, 0, 4),
++/* pmc261 */ PMC_D(PFM_REG_W  , "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
++/* pmc262 */ PMC_D(PFM_REG_W  , "IBR6", 0x0, 0, 0, 6),
++/* pmc263 */ PMC_D(PFM_REG_W  , "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
++/* pmc264 */ PMC_D(PFM_REG_W  , "DBR0", 0x0, 0, 0, 0),
++/* pmc265 */ PMC_D(PFM_REG_W  , "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
++/* pmc266 */ PMC_D(PFM_REG_W  , "DBR2", 0x0, 0, 0, 2),
++/* pmc267 */ PMC_D(PFM_REG_W  , "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
++/* pmc268 */ PMC_D(PFM_REG_W  , "DBR4", 0x0, 0, 0, 4),
++/* pmc269 */ PMC_D(PFM_REG_W  , "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
++/* pmc270 */ PMC_D(PFM_REG_W  , "DBR6", 0x0, 0, 0, 6),
++/* pmc271 */ PMC_D(PFM_REG_W  , "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
++};
++#define PFM_ITA_NUM_PMCS ARRAY_SIZE(pfm_ita_pmc_desc)
++
++static struct pfm_regmap_desc pfm_ita_pmd_desc[]={
++/* pmd0  */ PMD_D(PFM_REG_I , "PMD0", 0),
++/* pmd1  */ PMD_D(PFM_REG_I , "PMD1", 1),
++/* pmd2  */ PMD_D(PFM_REG_I , "PMD2", 2),
++/* pmd3  */ PMD_D(PFM_REG_I , "PMD3", 3),
++/* pmd4  */ PMD_D(PFM_REG_C , "PMD4", 4),
++/* pmd5  */ PMD_D(PFM_REG_C , "PMD5", 5),
++/* pmd6  */ PMD_D(PFM_REG_C , "PMD6", 6),
++/* pmd7  */ PMD_D(PFM_REG_C , "PMD7", 7),
++/* pmd8  */ PMD_D(PFM_REG_I , "PMD8", 8),
++/* pmd9  */ PMD_D(PFM_REG_I , "PMD9", 9),
++/* pmd10 */ PMD_D(PFM_REG_I , "PMD10", 10),
++/* pmd11 */ PMD_D(PFM_REG_I , "PMD11", 11),
++/* pmd12 */ PMD_D(PFM_REG_I , "PMD12", 12),
++/* pmd13 */ PMD_D(PFM_REG_I , "PMD13", 13),
++/* pmd14 */ PMD_D(PFM_REG_I , "PMD14", 14),
++/* pmd15 */ PMD_D(PFM_REG_I , "PMD15", 15),
++/* pmd16 */ PMD_D(PFM_REG_I , "PMD16", 16),
++/* pmd17 */ PMD_D(PFM_REG_I , "PMD17", 17)
++};
++#define PFM_ITA_NUM_PMDS ARRAY_SIZE(pfm_ita_pmd_desc)
++
++static int pfm_ita_pmc_check(struct pfm_context *ctx,
++			     struct pfm_event_set *set,
++			     struct pfarg_pmc *req)
++{
++#define PFM_ITA_PMC_PM_POS6	(1UL<< 6)
++	struct pfm_arch_context *ctx_arch;
++	u64 tmpval;
++	u16 cnum;
++	int ret = 0, is_system;
++
++	tmpval = req->reg_value;
++	cnum   = req->reg_num;
++	ctx_arch = pfm_ctx_arch(ctx);
++	is_system = ctx->flags.system;
++
++	switch(cnum) {
++		case  4:
++		case  5:
++		case  6:
++		case  7:
++		case 10:
++		case 11:
++		case 12: if (is_system)
++				 tmpval |= PFM_ITA_PMC_PM_POS6;
++			 else
++				 tmpval &= ~PFM_ITA_PMC_PM_POS6;
++			 break;
++	}
++
++	/*
++	 * we must clear the (instruction) debug registers if pmc13.ta bit is
++	 * cleared before they are written (fl_using_dbreg==0) to avoid
++	 * picking up stale information.
++	 */
++	if (cnum == 13 && ((tmpval & 0x1) == 0)
++		&& ctx_arch->flags.use_dbr == 0) {
++		PFM_DBG("pmc13 has pmc13.ta cleared, clearing ibr");
++		ret = pfm_ia64_mark_dbregs_used(ctx, set);
++		if (ret) return ret;
++	}
++
++	/*
++	 * we must clear the (data) debug registers if pmc11.pt bit is cleared
++	 * before they are written (fl_using_dbreg==0) to avoid picking up
++	 * stale information.
++	 */
++	if (cnum == 11 && ((tmpval >> 28)& 0x1) == 0
++		&& ctx_arch->flags.use_dbr == 0) {
++		PFM_DBG("pmc11 has pmc11.pt cleared, clearing dbr");
++		ret = pfm_ia64_mark_dbregs_used(ctx, set);
++		if (ret) return ret;
++	}
++
++	req->reg_value = tmpval;
++
++	return 0;
++}
++
++static int pfm_ita_probe_pmu(void)
++{
++	return local_cpu_data->family == 0x7 && !ia64_platform_is("hpsim")
++		? 0 : -1;
++}
++
++/*
++ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
++ */
++static struct pfm_pmu_config pfm_ita_pmu_conf={
++	.pmu_name = "Itanium",
++	.counter_width = 32,
++	.pmd_desc = pfm_ita_pmd_desc,
++	.pmc_desc = pfm_ita_pmc_desc,
++	.pmc_write_check = pfm_ita_pmc_check,
++	.num_pmc_entries = PFM_ITA_NUM_PMCS,
++	.num_pmd_entries = PFM_ITA_NUM_PMDS,
++	.probe_pmu = pfm_ita_probe_pmu,
++	.version = "1.0",
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.arch_info = &pfm_ita_pmu_info
++};
++
++static int __init pfm_ita_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_ita_pmu_conf);
++}
++
++static void __exit pfm_ita_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_ita_pmu_conf);
++}
++
++module_init(pfm_ita_pmu_init_module);
++module_exit(pfm_ita_pmu_cleanup_module);
++
+--- /dev/null
++++ b/arch/ia64/perfmon/perfmon_mckinley.c
+@@ -0,0 +1,285 @@
++/*
++ * This file contains the McKinley PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("Itanium 2 (McKinley) PMU description tables");
++MODULE_LICENSE("GPL");
++
++#define RDEP(x)	(1UL << (x))
++
++#define PFM_MCK_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|RDEP(10)|RDEP(11)|\
++			   RDEP(12))
++
++#define PFM_MCK_NO64	(1UL<<5)
++
++static struct pfm_arch_pmu_info pfm_mck_pmu_info={
++	.mask_pmcs = {PFM_MCK_MASK_PMCS,},
++};
++
++/* reserved bits are 1 in the mask */
++#define PFM_ITA2_RSVD 0xfffffffffc8000a0UL
++
++/*
++ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
++ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
++ * but this is fine because they are handled separately in the IA-64 specific
++ * code.
++ */
++static struct pfm_regmap_desc pfm_mck_pmc_desc[]={
++/* pmc0  */ PMX_NA,
++/* pmc1  */ PMX_NA,
++/* pmc2  */ PMX_NA,
++/* pmc3  */ PMX_NA,
++/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x800020UL, 0xfffffffffc8000a0, PFM_MCK_NO64, 4),
++/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 5),
++/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 6),
++/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 7),
++/* pmc8  */ PMC_D(PFM_REG_W  , "PMC8" , 0xffffffff3fffffffUL, 0xc0000004UL, 0, 8),
++/* pmc9  */ PMC_D(PFM_REG_W  , "PMC9" , 0xffffffff3ffffffcUL, 0xc0000004UL, 0, 9),
++/* pmc10 */ PMC_D(PFM_REG_W  , "PMC10", 0x0, 0xffffffffffff0000UL, 0, 10),
++/* pmc11 */ PMC_D(PFM_REG_W  , "PMC11", 0x0, 0xfffffffffcf0fe30UL, 0, 11),
++/* pmc12 */ PMC_D(PFM_REG_W  , "PMC12", 0x0, 0xffffffffffff0000UL, 0, 12),
++/* pmc13 */ PMC_D(PFM_REG_W  , "PMC13", 0x2078fefefefeUL, 0xfffe1fffe7e7e7e7UL, 0, 13),
++/* pmc14 */ PMC_D(PFM_REG_W  , "PMC14", 0x0db60db60db60db6UL, 0xffffffffffffdb6dUL, 0, 14),
++/* pmc15 */ PMC_D(PFM_REG_W  , "PMC15", 0xfffffff0UL, 0xfffffffffffffff0UL, 0, 15),
++/* pmc16 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc24 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc32 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc40 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc48 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc56 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc64 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc72 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc80 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc88 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc96 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc104 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc112 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc120 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc128 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc136 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc144 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc152 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc160 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc168 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc176 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc184 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc192 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc200 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc208 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc216 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc224 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc232 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc240 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc248 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc256 */ PMC_D(PFM_REG_W  , "IBR0", 0x0, 0, 0, 0),
++/* pmc257 */ PMC_D(PFM_REG_W  , "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
++/* pmc258 */ PMC_D(PFM_REG_W  , "IBR2", 0x0, 0, 0, 2),
++/* pmc259 */ PMC_D(PFM_REG_W  , "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
++/* pmc260 */ PMC_D(PFM_REG_W  , "IBR4", 0x0, 0, 0, 4),
++/* pmc261 */ PMC_D(PFM_REG_W  , "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
++/* pmc262 */ PMC_D(PFM_REG_W  , "IBR6", 0x0, 0, 0, 6),
++/* pmc263 */ PMC_D(PFM_REG_W  , "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
++/* pmc264 */ PMC_D(PFM_REG_W  , "DBR0", 0x0, 0, 0, 0),
++/* pmc265 */ PMC_D(PFM_REG_W  , "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
++/* pmc266 */ PMC_D(PFM_REG_W  , "DBR2", 0x0, 0, 0, 2),
++/* pmc267 */ PMC_D(PFM_REG_W  , "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
++/* pmc268 */ PMC_D(PFM_REG_W  , "DBR4", 0x0, 0, 0, 4),
++/* pmc269 */ PMC_D(PFM_REG_W  , "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
++/* pmc270 */ PMC_D(PFM_REG_W  , "DBR6", 0x0, 0, 0, 6),
++/* pmc271 */ PMC_D(PFM_REG_W  , "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
++};
++#define PFM_MCK_NUM_PMCS ARRAY_SIZE(pfm_mck_pmc_desc)
++
++static struct pfm_regmap_desc pfm_mck_pmd_desc[]={
++/* pmd0  */ PMD_D(PFM_REG_I, "PMD0", 0),
++/* pmd1  */ PMD_D(PFM_REG_I, "PMD1", 1),
++/* pmd2  */ PMD_D(PFM_REG_I, "PMD2", 2),
++/* pmd3  */ PMD_D(PFM_REG_I, "PMD3", 3),
++/* pmd4  */ PMD_D(PFM_REG_C, "PMD4", 4),
++/* pmd5  */ PMD_D(PFM_REG_C, "PMD5", 5),
++/* pmd6  */ PMD_D(PFM_REG_C, "PMD6", 6),
++/* pmd7  */ PMD_D(PFM_REG_C, "PMD7", 7),
++/* pmd8  */ PMD_D(PFM_REG_I, "PMD8", 8),
++/* pmd9  */ PMD_D(PFM_REG_I, "PMD9", 9),
++/* pmd10 */ PMD_D(PFM_REG_I, "PMD10", 10),
++/* pmd11 */ PMD_D(PFM_REG_I, "PMD11", 11),
++/* pmd12 */ PMD_D(PFM_REG_I, "PMD12", 12),
++/* pmd13 */ PMD_D(PFM_REG_I, "PMD13", 13),
++/* pmd14 */ PMD_D(PFM_REG_I, "PMD14", 14),
++/* pmd15 */ PMD_D(PFM_REG_I, "PMD15", 15),
++/* pmd16 */ PMD_D(PFM_REG_I, "PMD16", 16),
++/* pmd17 */ PMD_D(PFM_REG_I, "PMD17", 17)
++};
++#define PFM_MCK_NUM_PMDS ARRAY_SIZE(pfm_mck_pmd_desc)
++
++static int pfm_mck_pmc_check(struct pfm_context *ctx,
++			     struct pfm_event_set *set,
++			     struct pfarg_pmc *req)
++{
++	struct pfm_arch_context *ctx_arch;
++	u64 val8 = 0, val14 = 0, val13 = 0;
++	u64 tmpval;
++	u16 cnum;
++	int ret = 0, check_case1 = 0;
++	int is_system;
++
++	tmpval = req->reg_value;
++	cnum = req->reg_num;
++	ctx_arch = pfm_ctx_arch(ctx);
++	is_system = ctx->flags.system;
++
++#define PFM_MCK_PMC_PM_POS6	(1UL<< 6)
++#define PFM_MCK_PMC_PM_POS4	(1UL<< 4)
++
++	switch(cnum) {
++		case  4:
++		case  5:
++		case  6:
++		case  7:
++		case 11:
++		case 12: if (is_system)
++				 tmpval |= PFM_MCK_PMC_PM_POS6;
++			 else
++				 tmpval &= ~PFM_MCK_PMC_PM_POS6;
++			 break;
++
++		case  8: val8 = tmpval;
++			 val13 = set->pmcs[13];
++			 val14 = set->pmcs[14];
++			 check_case1 = 1;
++			 break;
++
++		case 10: if (is_system)
++				 tmpval |= PFM_MCK_PMC_PM_POS4;
++			 else
++				 tmpval &= ~PFM_MCK_PMC_PM_POS4;
++			 break;
++
++		case 13:
++			 val8 = set->pmcs[8];
++			 val13 = tmpval;
++			 val14 = set->pmcs[14];
++			 check_case1 = 1;
++			 break;
++
++		case 14:
++			 val8 = set->pmcs[8];
++			 val13 = set->pmcs[13];
++			 val14 = tmpval;
++			 check_case1 = 1;
++			 break;
++	}
++
++	/*
++	 * check illegal configuration which can produce inconsistencies
++	 * in tagging i-side events in L1D and L2 caches
++	 */
++	if (check_case1) {
++		ret = (((val13 >> 45) & 0xf) == 0 && ((val8 & 0x1) == 0))
++		    && ((((val14>>1) & 0x3) == 0x2 || ((val14>>1) & 0x3) == 0x0)
++		    ||(((val14>>4) & 0x3) == 0x2 || ((val14>>4) & 0x3) == 0x0));
++
++		if (ret) {
++			PFM_DBG("perfmon: invalid config pmc8=0x%lx "
++				  "pmc13=0x%lx pmc14=0x%lx",
++				  val8, val13, val14);
++			return -EINVAL;
++		}
++	}
++
++	/*
++	 * check if configuration implicitely activates the use of
++	 * the debug registers. If true, then we ensure that this is
++	 * possible and that we do not pick up stale value in the HW
++	 * registers.
++	 *
++	 * We postpone the checks of pmc13 and pmc14 to avoid side effects
++	 * in case of errors
++	 */
++
++	/*
++	 * pmc13 is "active" if:
++	 * 	one of the pmc13.cfg_dbrpXX field is different from 0x3
++	 * AND
++	 * 	at the corresponding pmc13.ena_dbrpXX is set.
++	 */
++	if (cnum == 13 && (tmpval & 0x1e00000000000UL)
++	    && (tmpval & 0x18181818UL) != 0x18181818UL
++		&& ctx_arch->flags.use_dbr == 0) {
++		PFM_DBG("pmc13=0x%lx active", tmpval);
++		ret = pfm_ia64_mark_dbregs_used(ctx, set);
++		if (ret) return ret;
++	}
++
++	/*
++	 *  if any pmc14.ibrpX bit is enabled we must clear the ibrs
++	 */
++	if (cnum == 14 && ((tmpval & 0x2222UL) != 0x2222UL)
++		&& ctx_arch->flags.use_dbr == 0) {
++		PFM_DBG("pmc14=0x%lx active", tmpval);
++		ret = pfm_ia64_mark_dbregs_used(ctx, set);
++		if (ret) return ret;
++	}
++
++	req->reg_value = tmpval;
++
++	return 0;
++}
++
++static int pfm_mck_probe_pmu(void)
++{
++	return local_cpu_data->family == 0x1f ? 0 : -1;
++}
++
++/*
++ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
++ */
++static struct pfm_pmu_config pfm_mck_pmu_conf={
++	.pmu_name = "Itanium 2",
++	.counter_width = 47,
++	.pmd_desc = pfm_mck_pmd_desc,
++	.pmc_desc = pfm_mck_pmc_desc,
++	.pmc_write_check = pfm_mck_pmc_check,
++	.num_pmc_entries = PFM_MCK_NUM_PMCS,
++	.num_pmd_entries = PFM_MCK_NUM_PMDS,
++	.probe_pmu = pfm_mck_probe_pmu,
++	.version = "1.0",
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.arch_info = &pfm_mck_pmu_info,
++};
++
++static int __init pfm_mck_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_mck_pmu_conf);
++}
++
++static void __exit pfm_mck_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_mck_pmu_conf);
++}
++
++module_init(pfm_mck_pmu_init_module);
++module_exit(pfm_mck_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/ia64/perfmon/perfmon_montecito.c
+@@ -0,0 +1,404 @@
++/*
++ * This file contains the McKinley PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <linux/smp.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("Dual-Core Itanium 2 (Montecito) PMU description table");
++MODULE_LICENSE("GPL");
++
++#define RDEP(x)	(1UL << (x))
++
++#define PFM_MONT_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|\
++			    RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|\
++			    RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|\
++			    RDEP(37)|RDEP(39)|RDEP(40)|RDEP(42))
++
++#define PFM_MONT_NO64	(1UL<<5)
++
++static struct pfm_arch_pmu_info pfm_mont_pmu_info={
++	.mask_pmcs = {PFM_MONT_MASK_PMCS,},
++};
++
++#define PFM_MONT_RSVD 0xffffffff838000a0UL
++/*
++ *
++ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
++ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
++ * but this is fine because they are handled separately in the IA-64 specific
++ * code.
++ *
++ * For PMC4-PMC15, PMC40: we force pmc.ism=2 (IA-64 mode only)
++ */
++static struct pfm_regmap_desc pfm_mont_pmc_desc[]={
++/* pmc0  */ PMX_NA,
++/* pmc1  */ PMX_NA,
++/* pmc2  */ PMX_NA,
++/* pmc3  */ PMX_NA,
++/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 4),
++/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 5),
++/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 6),
++/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 7),
++/* pmc8  */ PMC_D(PFM_REG_W64, "PMC8" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 8),
++/* pmc9  */ PMC_D(PFM_REG_W64, "PMC9" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 9),
++/* pmc10 */ PMC_D(PFM_REG_W64, "PMC10", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 10),
++/* pmc11 */ PMC_D(PFM_REG_W64, "PMC11", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 11),
++/* pmc12 */ PMC_D(PFM_REG_W64, "PMC12", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 12),
++/* pmc13 */ PMC_D(PFM_REG_W64, "PMC13", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 13),
++/* pmc14 */ PMC_D(PFM_REG_W64, "PMC14", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 14),
++/* pmc15 */ PMC_D(PFM_REG_W64, "PMC15", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 15),
++/* pmc16 */ PMX_NA,
++/* pmc17 */ PMX_NA,
++/* pmc18 */ PMX_NA,
++/* pmc19 */ PMX_NA,
++/* pmc20 */ PMX_NA,
++/* pmc21 */ PMX_NA,
++/* pmc22 */ PMX_NA,
++/* pmc23 */ PMX_NA,
++/* pmc24 */ PMX_NA,
++/* pmc25 */ PMX_NA,
++/* pmc26 */ PMX_NA,
++/* pmc27 */ PMX_NA,
++/* pmc28 */ PMX_NA,
++/* pmc29 */ PMX_NA,
++/* pmc30 */ PMX_NA,
++/* pmc31 */ PMX_NA,
++/* pmc32 */ PMC_D(PFM_REG_W , "PMC32", 0x30f01ffffffffffUL, 0xfcf0fe0000000000UL, 0, 32),
++/* pmc33 */ PMC_D(PFM_REG_W , "PMC33", 0x0, 0xfffffe0000000000UL, 0, 33),
++/* pmc34 */ PMC_D(PFM_REG_W , "PMC34", 0xf01ffffffffffUL, 0xfff0fe0000000000UL, 0, 34),
++/* pmc35 */ PMC_D(PFM_REG_W , "PMC35", 0x0,  0x1ffffffffffUL, 0, 35),
++/* pmc36 */ PMC_D(PFM_REG_W , "PMC36", 0xfffffff0UL, 0xfffffffffffffff0UL, 0, 36),
++/* pmc37 */ PMC_D(PFM_REG_W , "PMC37", 0x0, 0xffffffffffffc000UL, 0, 37),
++/* pmc38 */ PMC_D(PFM_REG_W , "PMC38", 0xdb6UL, 0xffffffffffffdb6dUL, 0, 38),
++/* pmc39 */ PMC_D(PFM_REG_W , "PMC39", 0x0, 0xffffffffffff0030UL, 0, 39),
++/* pmc40 */ PMC_D(PFM_REG_W , "PMC40", 0x2000000UL, 0xfffffffffff0fe30UL, 0, 40),
++/* pmc41 */ PMC_D(PFM_REG_W , "PMC41", 0x00002078fefefefeUL, 0xfffe1fffe7e7e7e7UL, 0, 41),
++/* pmc42 */ PMC_D(PFM_REG_W , "PMC42", 0x0, 0xfff800b0UL, 0, 42),
++/* pmc43 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc48 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc56 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc64 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc72 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc80 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc88 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc96 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc104 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc112 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc120 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc128 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc136 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc144 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc152 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc160 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc168 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc176 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc184 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc192 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc200 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc208 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc216 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc224 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc232 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc240 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc248 */ PMX_NA, PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,PMX_NA,
++/* pmc256 */ PMC_D(PFM_REG_W, "IBR0", 0x0, 0, 0, 0),
++/* pmc257 */ PMC_D(PFM_REG_W, "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
++/* pmc258 */ PMC_D(PFM_REG_W, "IBR2", 0x0, 0, 0, 2),
++/* pmc259 */ PMC_D(PFM_REG_W, "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
++/* pmc260 */ PMC_D(PFM_REG_W, "IBR4", 0x0, 0, 0, 4),
++/* pmc261 */ PMC_D(PFM_REG_W, "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
++/* pmc262 */ PMC_D(PFM_REG_W, "IBR6", 0x0, 0, 0, 6),
++/* pmc263 */ PMC_D(PFM_REG_W, "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
++/* pmc264 */ PMC_D(PFM_REG_W, "DBR0", 0x0, 0, 0, 0),
++/* pmc265 */ PMC_D(PFM_REG_W, "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
++/* pmc266 */ PMC_D(PFM_REG_W, "DBR2", 0x0, 0, 0, 2),
++/* pmc267 */ PMC_D(PFM_REG_W, "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
++/* pmc268 */ PMC_D(PFM_REG_W, "DBR4", 0x0, 0, 0, 4),
++/* pmc269 */ PMC_D(PFM_REG_W, "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
++/* pmc270 */ PMC_D(PFM_REG_W, "DBR6", 0x0, 0, 0, 6),
++/* pmc271 */ PMC_D(PFM_REG_W, "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
++};
++#define PFM_MONT_NUM_PMCS ARRAY_SIZE(pfm_mont_pmc_desc)
++
++static struct pfm_regmap_desc pfm_mont_pmd_desc[]={
++/* pmd0  */ PMX_NA,
++/* pmd1  */ PMX_NA,
++/* pmd2  */ PMX_NA,
++/* pmd3  */ PMX_NA,
++/* pmd4  */ PMD_D(PFM_REG_C, "PMD4", 4),
++/* pmd5  */ PMD_D(PFM_REG_C, "PMD5", 5),
++/* pmd6  */ PMD_D(PFM_REG_C, "PMD6", 6),
++/* pmd7  */ PMD_D(PFM_REG_C, "PMD7", 7),
++/* pmd8  */ PMD_D(PFM_REG_C, "PMD8", 8),
++/* pmd9  */ PMD_D(PFM_REG_C, "PMD9", 9),
++/* pmd10 */ PMD_D(PFM_REG_C, "PMD10", 10),
++/* pmd11 */ PMD_D(PFM_REG_C, "PMD11", 11),
++/* pmd12 */ PMD_D(PFM_REG_C, "PMD12", 12),
++/* pmd13 */ PMD_D(PFM_REG_C, "PMD13", 13),
++/* pmd14 */ PMD_D(PFM_REG_C, "PMD14", 14),
++/* pmd15 */ PMD_D(PFM_REG_C, "PMD15", 15),
++/* pmd16 */ PMX_NA,
++/* pmd17 */ PMX_NA,
++/* pmd18 */ PMX_NA,
++/* pmd19 */ PMX_NA,
++/* pmd20 */ PMX_NA,
++/* pmd21 */ PMX_NA,
++/* pmd22 */ PMX_NA,
++/* pmd23 */ PMX_NA,
++/* pmd24 */ PMX_NA,
++/* pmd25 */ PMX_NA,
++/* pmd26 */ PMX_NA,
++/* pmd27 */ PMX_NA,
++/* pmd28 */ PMX_NA,
++/* pmd29 */ PMX_NA,
++/* pmd30 */ PMX_NA,
++/* pmd31 */ PMX_NA,
++/* pmd32 */ PMD_D(PFM_REG_I, "PMD32", 32),
++/* pmd33 */ PMD_D(PFM_REG_I, "PMD33", 33),
++/* pmd34 */ PMD_D(PFM_REG_I, "PMD34", 34),
++/* pmd35 */ PMD_D(PFM_REG_I, "PMD35", 35),
++/* pmd36 */ PMD_D(PFM_REG_I, "PMD36", 36),
++/* pmd37 */ PMX_NA,
++/* pmd38 */ PMD_D(PFM_REG_I, "PMD38", 38),
++/* pmd39 */ PMD_D(PFM_REG_I, "PMD39", 39),
++/* pmd40 */ PMX_NA,
++/* pmd41 */ PMX_NA,
++/* pmd42 */ PMX_NA,
++/* pmd43 */ PMX_NA,
++/* pmd44 */ PMX_NA,
++/* pmd45 */ PMX_NA,
++/* pmd46 */ PMX_NA,
++/* pmd47 */ PMX_NA,
++/* pmd48 */ PMD_D(PFM_REG_I, "PMD48", 48),
++/* pmd49 */ PMD_D(PFM_REG_I, "PMD49", 49),
++/* pmd50 */ PMD_D(PFM_REG_I, "PMD50", 50),
++/* pmd51 */ PMD_D(PFM_REG_I, "PMD51", 51),
++/* pmd52 */ PMD_D(PFM_REG_I, "PMD52", 52),
++/* pmd53 */ PMD_D(PFM_REG_I, "PMD53", 53),
++/* pmd54 */ PMD_D(PFM_REG_I, "PMD54", 54),
++/* pmd55 */ PMD_D(PFM_REG_I, "PMD55", 55),
++/* pmd56 */ PMD_D(PFM_REG_I, "PMD56", 56),
++/* pmd57 */ PMD_D(PFM_REG_I, "PMD57", 57),
++/* pmd58 */ PMD_D(PFM_REG_I, "PMD58", 58),
++/* pmd59 */ PMD_D(PFM_REG_I, "PMD59", 59),
++/* pmd60 */ PMD_D(PFM_REG_I, "PMD60", 60),
++/* pmd61 */ PMD_D(PFM_REG_I, "PMD61", 61),
++/* pmd62 */ PMD_D(PFM_REG_I, "PMD62", 62),
++/* pmd63 */ PMD_D(PFM_REG_I, "PMD63", 63)
++};
++#define PFM_MONT_NUM_PMDS ARRAY_SIZE(pfm_mont_pmd_desc)
++
++static int pfm_mont_has_ht;
++
++static int pfm_mont_pmc_check(struct pfm_context *ctx,
++			      struct pfm_event_set *set,
++			      struct pfarg_pmc *req)
++{
++	struct pfm_arch_context *ctx_arch;
++	u64 val32 = 0, val38 = 0, val41 = 0;
++	u64 tmpval;
++	u16 cnum;
++	int ret = 0, check_case1 = 0;
++	int is_system;
++
++	tmpval = req->reg_value;
++	cnum = req->reg_num;
++	ctx_arch = pfm_ctx_arch(ctx);
++	is_system = ctx->flags.system;
++
++#define PFM_MONT_PMC_PM_POS6	(1UL<<6)
++#define PFM_MONT_PMC_PM_POS4	(1UL<<4)
++
++	switch(cnum) {
++		case  4:
++		case  5:
++		case  6:
++		case  7:
++		case  8:
++		case  9: if (is_system)
++				 tmpval |= PFM_MONT_PMC_PM_POS6;
++			 else
++				 tmpval &= ~PFM_MONT_PMC_PM_POS6;
++			 break;
++		case 10:
++		case 11:
++		case 12:
++		case 13:
++		case 14:
++		case 15: if ((req->reg_flags & PFM_REGFL_NO_EMUL64) == 0) {
++				 if (pfm_mont_has_ht) {
++					PFM_INFO("perfmon: Errata 121 PMD10/PMD15 cannot be used to overflow"
++						 "when threads on on");
++					return -EINVAL;
++				 }
++			 }
++			 if (is_system)
++				 tmpval |= PFM_MONT_PMC_PM_POS6;
++			 else
++				 tmpval &= ~PFM_MONT_PMC_PM_POS6;
++			 break;
++		case 39:
++		case 40:
++		case 42: if (pfm_mont_has_ht && ((req->reg_value >> 8) & 0x7) == 4) {
++				 PFM_INFO("perfmon: Errata 120: IP-EAR not available when threads are on");
++				 return -EINVAL;
++			 }
++			 if (is_system)
++				 tmpval |= PFM_MONT_PMC_PM_POS6;
++			 else
++				 tmpval &= ~PFM_MONT_PMC_PM_POS6;
++			 break;
++
++		case  32: val32 = tmpval;
++			  val38 = set->pmcs[38];
++			  val41 = set->pmcs[41];
++			  check_case1 = 1;
++			  break;
++
++		case  37:
++			  if (is_system)
++				 tmpval |= PFM_MONT_PMC_PM_POS4;
++			 else
++				 tmpval &= ~PFM_MONT_PMC_PM_POS4;
++			 break;
++
++		case  38: val38 = tmpval;
++			  val32 = set->pmcs[32];
++			  val41 = set->pmcs[41];
++			  check_case1 = 1;
++			  break;
++		case  41: val41 = tmpval;
++			  val32 = set->pmcs[32];
++			  val38 = set->pmcs[38];
++			  check_case1 = 1;
++			  break;
++	}
++
++	if (check_case1) {
++		ret = (((val41 >> 45) & 0xf) == 0 && ((val32>>57) & 0x1) == 0)
++		     && ((((val38>>1) & 0x3) == 0x2 || ((val38>>1) & 0x3) == 0)
++		     || (((val38>>4) & 0x3) == 0x2 || ((val38>>4) & 0x3) == 0));
++		if (ret) {
++			PFM_DBG("perfmon: invalid config pmc38=0x%lx "
++				"pmc41=0x%lx pmc32=0x%lx",
++				val38, val41, val32);
++			return -EINVAL;
++		}
++	}
++
++	/*
++	 * check if configuration implicitely activates the use of the
++	 * debug registers. If true, then we ensure that this is possible
++	 * and that we do not pick up stale value in the HW registers.
++	 */
++
++	/*
++	 *
++	 * pmc41 is "active" if:
++	 * 	one of the pmc41.cfgdtagXX field is different from 0x3
++	 * AND
++	 * 	the corsesponding pmc41.en_dbrpXX is set.
++	 * AND
++	 *	ctx_fl_use_dbr (dbr not yet used)
++	 */
++	if (cnum == 41
++	    && (tmpval & 0x1e00000000000)
++		&& (tmpval & 0x18181818) != 0x18181818
++		&& ctx_arch->flags.use_dbr == 0) {
++		PFM_DBG("pmc41=0x%lx active, clearing dbr", tmpval);
++		ret = pfm_ia64_mark_dbregs_used(ctx, set);
++		if (ret) return ret;
++	}
++	/*
++	 * we must clear the (instruction) debug registers if:
++	 * 	pmc38.ig_ibrpX is 0 (enabled)
++	 * and
++	 * 	fl_use_dbr == 0 (dbr not yet used)
++	 */
++	if (cnum == 38 && ((tmpval & 0x492) != 0x492)
++		&& ctx_arch->flags.use_dbr == 0) {
++		PFM_DBG("pmc38=0x%lx active pmc38, clearing ibr", tmpval);
++		ret = pfm_ia64_mark_dbregs_used(ctx, set);
++		if (ret) return ret;
++
++	}
++	req->reg_value = tmpval;
++	return 0;
++}
++
++static void pfm_handle_errata(void)
++{
++	pfm_mont_has_ht = 1;
++
++	PFM_INFO("activating workaround for errata 120 "
++		 "(Disable IP-EAR when threads are on)");
++
++	PFM_INFO("activating workaround for Errata 121 "
++		 "(PMC10-PMC15 cannot be used to overflow"
++		 " when threads are on");
++}
++static int pfm_mont_probe_pmu(void)
++{
++	if (local_cpu_data->family != 0x20)
++		return -1;
++
++	/*
++	 * the 2 errata must be activated when
++	 * threads are/can be enabled
++	 */
++	if (is_multithreading_enabled())
++		pfm_handle_errata();
++
++	return 0;
++}
++
++/*
++ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
++ */
++static struct pfm_pmu_config pfm_mont_pmu_conf={
++	.pmu_name = "Montecito",
++	.counter_width = 47,
++	.pmd_desc = pfm_mont_pmd_desc,
++	.pmc_desc = pfm_mont_pmc_desc,
++	.num_pmc_entries = PFM_MONT_NUM_PMCS,
++	.num_pmd_entries = PFM_MONT_NUM_PMDS,
++	.pmc_write_check = pfm_mont_pmc_check,
++	.probe_pmu = pfm_mont_probe_pmu,
++	.version = "1.0",
++	.arch_info = &pfm_mont_pmu_info,
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE
++};
++
++static int __init pfm_mont_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_mont_pmu_conf);
++}
++
++static void __exit pfm_mont_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_mont_pmu_conf);
++}
++
++module_init(pfm_mont_pmu_init_module);
++module_exit(pfm_mont_pmu_cleanup_module);
+--- a/arch/mips/Kconfig
++++ b/arch/mips/Kconfig
+@@ -1885,6 +1885,8 @@ config SECCOMP
+ 
+ 	  If unsure, say Y. Only embedded should say N here.
+ 
++source "arch/mips/perfmon/Kconfig"
++
+ endmenu
+ 
+ config RWSEM_GENERIC_SPINLOCK
+--- a/arch/mips/Makefile
++++ b/arch/mips/Makefile
+@@ -148,6 +148,12 @@ endif
+ endif
+ 
+ #
++# Perfmon support
++#
++
++core-$(CONFIG_PERFMON)		+= arch/mips/perfmon/
++
++#
+ # Firmware support
+ #
+ libs-$(CONFIG_ARC)		+= arch/mips/fw/arc/
+--- a/arch/mips/kernel/process.c
++++ b/arch/mips/kernel/process.c
+@@ -27,6 +27,7 @@
+ #include <linux/completion.h>
+ #include <linux/kallsyms.h>
+ #include <linux/random.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/asm.h>
+ #include <asm/bootinfo.h>
+@@ -94,6 +95,7 @@ void start_thread(struct pt_regs * regs,
+ 
+ void exit_thread(void)
+ {
++  pfm_exit_thread(current);
+ }
+ 
+ void flush_thread(void)
+@@ -168,6 +170,8 @@ int copy_thread(int nr, unsigned long cl
+ 	if (clone_flags & CLONE_SETTLS)
+ 		ti->tp_value = regs->regs[7];
+ 
++	pfm_copy_thread(p);
++
+ 	return 0;
+ }
+ 
+--- a/arch/mips/kernel/scall32-o32.S
++++ b/arch/mips/kernel/scall32-o32.S
+@@ -663,6 +663,19 @@ einval:	li	v0, -EINVAL
+ 	sys	sys_timerfd		4
+ 	sys	sys_eventfd		1
+ 	sys	sys_fallocate		6	/* 4320 */
++	sys	sys_pfm_create_context	4
++	sys	sys_pfm_write_pmcs	3
++	sys	sys_pfm_write_pmds	4
++	sys	sys_pfm_read_pmds	3
++	sys	sys_pfm_load_context	2	/* 4325 */
++	sys	sys_pfm_start		2
++	sys	sys_pfm_stop		1
++	sys	sys_pfm_restart		1
++	sys	sys_pfm_create_evtsets	3
++	sys	sys_pfm_getinfo_evtsets 3	/* 4330 */
++	sys	sys_pfm_delete_evtsets	3
++	sys	sys_pfm_unload_context	1
++
+ 	.endm
+ 
+ 	/* We pre-compute the number of _instruction_ bytes needed to
+--- a/arch/mips/kernel/scall64-64.S
++++ b/arch/mips/kernel/scall64-64.S
+@@ -478,4 +478,17 @@ sys_call_table:
+ 	PTR	sys_timerfd
+ 	PTR	sys_eventfd
+ 	PTR	sys_fallocate
++	PTR	sys_pfm_create_context		/* 5280 */
++	PTR	sys_pfm_write_pmcs
++	PTR	sys_pfm_write_pmds
++	PTR	sys_pfm_read_pmds
++	PTR	sys_pfm_load_context
++	PTR	sys_pfm_start			/* 5285 */
++	PTR	sys_pfm_stop
++	PTR	sys_pfm_restart
++	PTR	sys_pfm_create_evtsets
++	PTR	sys_pfm_getinfo_evtsets
++	PTR	sys_pfm_delete_evtsets		/* 5290 */
++	PTR	sys_pfm_unload_context
++
+ 	.size	sys_call_table,.-sys_call_table
+--- a/arch/mips/kernel/scall64-n32.S
++++ b/arch/mips/kernel/scall64-n32.S
+@@ -400,8 +400,21 @@ EXPORT(sysn32_call_table)
+ 	PTR	sys_ioprio_set
+ 	PTR	sys_ioprio_get
+ 	PTR	compat_sys_utimensat
+-	PTR	compat_sys_signalfd		/* 5280 */
++	PTR	compat_sys_signalfd		/* 6280 */
+ 	PTR	compat_sys_timerfd
+ 	PTR	sys_eventfd
+ 	PTR	sys_fallocate
++	PTR	sys_pfm_create_context
++	PTR	sys_pfm_write_pmcs		/* 6285 */
++	PTR	sys_pfm_write_pmds
++	PTR	sys_pfm_read_pmds
++	PTR	sys_pfm_load_context
++	PTR	sys_pfm_start
++	PTR	sys_pfm_stop			/* 6290 */
++	PTR	sys_pfm_restart
++	PTR	sys_pfm_create_evtsets
++	PTR	sys_pfm_getinfo_evtsets
++	PTR	sys_pfm_delete_evtsets
++	PTR	sys_pfm_unload_context
++
+ 	.size	sysn32_call_table,.-sysn32_call_table
+--- a/arch/mips/kernel/scall64-o32.S
++++ b/arch/mips/kernel/scall64-o32.S
+@@ -526,4 +526,16 @@ sys_call_table:
+ 	PTR	compat_sys_timerfd
+ 	PTR	sys_eventfd
+ 	PTR	sys32_fallocate			/* 4320 */
++	PTR	sys_pfm_create_context
++	PTR	sys_pfm_write_pmcs
++	PTR	sys_pfm_write_pmds
++	PTR	sys_pfm_read_pmds
++	PTR	sys_pfm_load_context		/* 4325 */
++	PTR	sys_pfm_start
++	PTR	sys_pfm_stop
++	PTR	sys_pfm_restart
++	PTR	sys_pfm_create_evtsets
++	PTR	sys_pfm_getinfo_evtsets		/* 4330 */
++	PTR	sys_pfm_delete_evtsets
++	PTR	sys_pfm_unload_context
+ 	.size	sys_call_table,.-sys_call_table
+--- a/arch/mips/kernel/signal.c
++++ b/arch/mips/kernel/signal.c
+@@ -20,6 +20,7 @@
+ #include <linux/unistd.h>
+ #include <linux/compiler.h>
+ #include <linux/uaccess.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/abi.h>
+ #include <asm/asm.h>
+@@ -696,6 +697,9 @@ static void do_signal(struct pt_regs *re
+ asmlinkage void do_notify_resume(struct pt_regs *regs, void *unused,
+ 	__u32 thread_info_flags)
+ {
++        if (thread_info_flags & _TIF_PERFMON_WORK)
++		pfm_handle_work(regs);
++
+ 	/* deal with pending signal delivery */
+ 	if (thread_info_flags & (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK))
+ 		do_signal(regs);
+--- a/arch/mips/mips-boards/generic/time.c
++++ b/arch/mips/mips-boards/generic/time.c
+@@ -27,6 +27,7 @@
+ #include <linux/time.h>
+ #include <linux/timex.h>
+ #include <linux/mc146818rtc.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/mipsregs.h>
+ #include <asm/mipsmtregs.h>
+--- /dev/null
++++ b/arch/mips/perfmon/Kconfig
+@@ -0,0 +1,45 @@
++menu "Hardware Performance Monitoring support"
++config PERFMON
++	bool "Perfmon2 performance monitoring interface"
++	default n
++	help
++	Enables the perfmon2 interface to access the hardware
++	performance counters. See <http://perfmon2.sf.net/> for
++	more details.
++
++config PERFMON_FLUSH
++	bool "Flush sampling buffer when modified"
++	depends on PERFMON
++	default n
++	help
++	On some MIPS models, cache aliasing may cause invalid
++	data to be read from the perfmon sampling buffer. Use this option
++	to flush the buffer when it is modified to ensure valid data is
++	visible at the user level.
++
++config PERFMON_ALIGN
++	bool "Align sampling buffer to avoid cache aliasing"
++	depends on PERFMON
++	default n
++	help
++	On some MIPS models, cache aliasing may cause invalid
++	data to be read from the perfmon sampling buffer. By forcing a bigger
++	page alignment (4-page), one can guarantee the buffer virtual address
++	will conflict in the cache with the user level mapping of the buffer
++	thereby ensuring a consistent view by user programs.
++
++config PERFMON_DEBUG
++	bool "Perfmon debugging"
++	depends on PERFMON
++	default n
++	depends on PERFMON
++	help
++	Enables perfmon debugging support
++
++config PERFMON_MIPS64
++	tristate "Support for MIPS64 hardware performance counters"
++	depends on PERFMON
++	default n
++	help
++	Enables support for the MIPS64 hardware performance counters"
++endmenu
+--- /dev/null
++++ b/arch/mips/perfmon/Makefile
+@@ -0,0 +1,2 @@
++obj-$(CONFIG_PERFMON)		+= perfmon.o
++obj-$(CONFIG_PERFMON_MIPS64)	+= perfmon_mips64.o
+--- /dev/null
++++ b/arch/mips/perfmon/perfmon.c
+@@ -0,0 +1,304 @@
++/*
++ * This file implements the MIPS64 specific
++ * support for the perfmon2 interface
++ *
++ * Copyright (c) 2005 Philip J. Mucci
++ *
++ * based on versions for other architectures:
++ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@htrpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/interrupt.h>
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++/*
++ * collect pending overflowed PMDs. Called from pfm_ctxsw()
++ * and from PMU interrupt handler. Must fill in set->povfl_pmds[]
++ * and set->npend_ovfls. Interrupts are masked
++ */
++static void __pfm_get_ovfl_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	u64 new_val, wmask;
++	u64 *used_mask, *intr_pmds;
++	u64 mask[PFM_PMD_BV];
++	unsigned int i, max;
++
++	max = pfm_pmu_conf->regs.max_intr_pmd;
++	intr_pmds = pfm_pmu_conf->regs.intr_pmds;
++	used_mask = set->used_pmds;
++
++	wmask = 1ULL << pfm_pmu_conf->counter_width;
++
++	bitmap_and(cast_ulp(mask),
++		   cast_ulp(intr_pmds),
++		   cast_ulp(used_mask),
++		   max);
++
++	/*
++	 * check all PMD that can generate interrupts
++	 * (that includes counters)
++	 */
++	for (i = 0; i < max; i++) {
++		if (test_bit(i, mask)) {
++			new_val = pfm_arch_read_pmd(ctx, i);
++
++			PFM_DBG_ovfl("pmd%u new_val=0x%llx bit=%d\n",
++				     i, (unsigned long long)new_val,
++				     (new_val&wmask) ? 1 : 0);
++
++ 			if (new_val & wmask) {
++				__set_bit(i, set->povfl_pmds);
++				set->npend_ovfls++;
++			}
++		}
++	}
++}
++
++static void pfm_stop_active(struct task_struct *task, struct pfm_context *ctx,
++			       struct pfm_event_set *set)
++{
++	unsigned int i, max;
++
++	max = pfm_pmu_conf->regs.max_pmc;
++
++	/*
++	 * clear enable bits, assume all pmcs are enable pmcs
++	 */
++	for (i = 0; i < max; i++) {
++		if (test_bit(i, set->used_pmcs))
++			pfm_arch_write_pmc(ctx, i,0);
++	}
++
++	if (set->npend_ovfls)
++		return;
++
++	__pfm_get_ovfl_pmds(ctx, set);
++}
++
++/*
++ * Called from pfm_ctxsw(). Task is guaranteed to be current.
++ * Context is locked. Interrupts are masked. Monitoring is active.
++ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
++ *
++ * for per-thread:
++ * 	must stop monitoring for the task
++ *
++ * Return:
++ * 	non-zero : did not save PMDs (as part of stopping the PMU)
++ * 	       0 : saved PMDs (no need to save them in caller)
++ */
++int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
++			      struct pfm_event_set *set)
++{
++	/*
++	 * disable lazy restore of PMC registers.
++	 */
++	set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
++
++	pfm_stop_active(task, ctx, set);
++
++	return 1;
++}
++
++/*
++ * Called from pfm_stop() and pfm_ctxsw()
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-thread:
++ *   task is not necessarily current. If not current task, then
++ *   task is guaranteed stopped and off any cpu. Access to PMU
++ *   is not guaranteed. Interrupts are masked. Context is locked.
++ *   Set is the active set.
++ *
++ * For system-wide:
++ * 	task is current
++ *
++ * must disable active monitoring. ctx cannot be NULL
++ */
++void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
++		   struct pfm_event_set *set)
++{
++	/*
++	 * no need to go through stop_save()
++	 * if we are already stopped
++	 */
++	if (!ctx->flags.started || ctx->state == PFM_CTX_MASKED)
++		return;
++
++	/*
++	 * stop live registers and collect pending overflow
++	 */
++	if (task == current)
++		pfm_stop_active(task, ctx, set);
++}
++
++/*
++ * called from pfm_start() or pfm_ctxsw() when idle task and
++ * EXCL_IDLE is on.
++ *
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-trhead:
++ * 	Task is not necessarily current. If not current task, then task
++ * 	is guaranteed stopped and off any cpu. Access to PMU is not guaranteed.
++ *
++ * For system-wide:
++ * 	task is always current
++ *
++ * must enable active monitoring.
++ */
++void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
++	            struct pfm_event_set *set)
++{
++	unsigned int i, max_pmc;
++
++	if (task != current)
++		return;
++
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++
++	for (i = 0; i < max_pmc; i++) {
++		if (test_bit(i, set->used_pmcs))
++		    pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
++	}
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
++ * context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMD registers from set.
++ */
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	u64 ovfl_mask, val;
++	u64 *impl_pmds;
++	unsigned int i;
++	unsigned int max_pmd;
++
++	max_pmd = pfm_pmu_conf->regs.max_pmd;
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	impl_pmds = pfm_pmu_conf->regs.pmds;
++
++	/*
++	 * must restore all pmds to avoid leaking
++	 * information to user.
++	 */
++	for (i = 0; i < max_pmd; i++) {
++
++		if (test_bit(i, impl_pmds) == 0)
++			continue;
++
++		val = set->pmds[i].value;
++
++		/*
++		 * set upper bits for counter to ensure
++		 * overflow will trigger
++		 */
++		val &= ovfl_mask;
++
++		pfm_arch_write_pmd(ctx, i, val);
++	}
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw().
++ * Context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMC registers from set, if needed.
++ */
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	u64 *impl_pmcs;
++	unsigned int i, max_pmc;
++
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++	impl_pmcs = pfm_pmu_conf->regs.pmcs;
++
++	/*
++	 * - by default no PMCS measures anything
++	 * - on ctxswout, all used PMCs are disabled (cccr enable bit cleared)
++	 * hence when masked we do not need to restore anything
++	 */
++	if (ctx->state == PFM_CTX_MASKED || ctx->flags.started == 0)
++		return;
++
++	/*
++	 * restore all pmcs
++	 */
++	for (i = 0; i < max_pmc; i++)
++		if (test_bit(i, impl_pmcs))
++			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
++}
++
++char *pfm_arch_get_pmu_module_name(void)
++{
++	switch(cpu_data->cputype) {
++#ifndef CONFIG_SMP
++	case CPU_34K:
++#if defined(CPU_74K)
++	case CPU_74K:
++#endif
++#endif
++	case CPU_SB1:
++	case CPU_SB1A:
++	case CPU_R12000:
++	case CPU_25KF:
++	case CPU_24K:
++	case CPU_20KC:
++	case CPU_5KC:
++	  return "perfmon_mips64";
++	default:
++	  return NULL;
++	}
++	return NULL;
++}
++
++int perfmon_perf_irq(void)
++{
++  /* BLATANTLY STOLEN FROM OPROFILE, then modified */
++  struct pt_regs *regs;
++  unsigned int counters = pfm_pmu_conf->regs.max_pmc;
++  unsigned int control;
++  unsigned int counter;
++
++  regs = get_irq_regs();
++  switch (counters) {
++#define HANDLE_COUNTER(n)						\
++	case n + 1:							\
++		control = read_c0_perfctrl ## n();			\
++		counter = read_c0_perfcntr ## n();			\
++		if ((control & MIPS64_PMC_INT_ENABLE_MASK) &&		\
++		    (counter & MIPS64_PMD_INTERRUPT)) {			\
++			pfm_interrupt_handler(instruction_pointer(regs),\
++					      regs);                    \
++			return(1);					\
++		}
++      HANDLE_COUNTER(3)
++      HANDLE_COUNTER(2)
++      HANDLE_COUNTER(1)
++      HANDLE_COUNTER(0)
++      }
++
++  return 0;
++}
++EXPORT_SYMBOL(perfmon_perf_irq);
+--- /dev/null
++++ b/arch/mips/perfmon/perfmon_mips64.c
+@@ -0,0 +1,224 @@
++/*
++ * This file contains the MIPS64 and decendent PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Copyright (c) 2005 Philip Mucci
++ *
++ * Based on perfmon_p6.c:
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++MODULE_AUTHOR("Philip Mucci <mucci@cs.utk.edu>");
++MODULE_DESCRIPTION("MIPS64 PMU description tables");
++MODULE_LICENSE("GPL");
++
++/*
++ * reserved:
++ * 	- bit 63-9
++ * RSVD: reserved bits must be 1
++ */
++#define PFM_MIPS64_PMC_RSVD 0xfffffffffffff810ULL
++#define PFM_MIPS64_PMC_VAL  (1ULL<<4)
++
++extern int null_perf_irq(struct pt_regs *regs);
++extern int (*perf_irq)(struct pt_regs *regs);
++extern int perfmon_perf_irq(struct pt_regs *regs);
++
++static struct pfm_arch_pmu_info pfm_mips64_pmu_info;
++
++static struct pfm_regmap_desc pfm_mips64_pmc_desc[]={
++/* pmc0 */  PMC_D(PFM_REG_I64, "CP0_25_0", PFM_MIPS64_PMC_VAL, PFM_MIPS64_PMC_RSVD, 0, 0),
++/* pmc1 */  PMC_D(PFM_REG_I64, "CP0_25_1", PFM_MIPS64_PMC_VAL, PFM_MIPS64_PMC_RSVD, 0, 1),
++/* pmc2 */  PMC_D(PFM_REG_I64, "CP0_25_2", PFM_MIPS64_PMC_VAL, PFM_MIPS64_PMC_RSVD, 0, 2),
++/* pmc3 */  PMC_D(PFM_REG_I64, "CP0_25_3", PFM_MIPS64_PMC_VAL, PFM_MIPS64_PMC_RSVD, 0, 3)
++};
++#define PFM_MIPS64_NUM_PMCS ARRAY_SIZE(pfm_mips64_pmc_desc)
++
++static struct pfm_regmap_desc pfm_mips64_pmd_desc[]={
++/* pmd0 */ PMD_D(PFM_REG_C, "CP0_25_0", 0),
++/* pmd1 */ PMD_D(PFM_REG_C, "CP0_25_1", 1),
++/* pmd2 */ PMD_D(PFM_REG_C, "CP0_25_2", 2),
++/* pmd3 */ PMD_D(PFM_REG_C, "CP0_25_3", 3)
++};
++#define PFM_MIPS64_NUM_PMDS ARRAY_SIZE(pfm_mips64_pmd_desc)
++
++static int pfm_mips64_probe_pmu(void)
++{
++	struct cpuinfo_mips *c = &current_cpu_data;
++
++	switch (c->cputype) {
++#ifndef CONFIG_SMP
++	case CPU_34K:
++#if defined(CPU_74K)
++	case CPU_74K:
++#endif
++#endif
++	case CPU_SB1:
++	case CPU_SB1A:
++	case CPU_R12000:
++	case CPU_25KF:
++	case CPU_24K:
++	case CPU_20KC:
++	case CPU_5KC:
++		return 0;
++		break;
++	default:
++		PFM_INFO("Unknown cputype 0x%x",c->cputype);
++	}
++	return -1;
++}
++
++/*
++ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
++ */
++static struct pfm_pmu_config pfm_mips64_pmu_conf = {
++	.pmu_name = "MIPS", /* placeholder */
++	.counter_width = 31,
++	.pmd_desc = pfm_mips64_pmd_desc,
++	.pmc_desc = pfm_mips64_pmc_desc,
++	.num_pmc_entries = PFM_MIPS64_NUM_PMCS,
++	.num_pmd_entries = PFM_MIPS64_NUM_PMDS,
++	.probe_pmu = pfm_mips64_probe_pmu,
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.arch_info = &pfm_mips64_pmu_info
++};
++
++static inline int n_counters(void)
++{
++	if (!(read_c0_config1() & MIPS64_CONFIG_PMC_MASK))
++		return 0;
++	if (!(read_c0_perfctrl0() & MIPS64_PMC_CTR_MASK))
++		return 1;
++	if (!(read_c0_perfctrl1() & MIPS64_PMC_CTR_MASK))
++		return 2;
++	if (!(read_c0_perfctrl2() & MIPS64_PMC_CTR_MASK))
++		return 3;
++	return 4;
++}
++
++static int __init pfm_mips64_pmu_init_module(void)
++{
++	struct cpuinfo_mips *c = &current_cpu_data;
++	int i, ret, num;
++        u64 temp_mask;
++
++	switch (c->cputype) {
++	case CPU_5KC:
++		pfm_mips64_pmu_conf.pmu_name = "MIPS5KC";
++		break;
++	case CPU_R12000:
++	        pfm_mips64_pmu_conf.pmu_name = "MIPSR12000";
++	        break;
++	case CPU_20KC:
++		pfm_mips64_pmu_conf.pmu_name = "MIPS20KC";
++		break;
++	case CPU_24K:
++		pfm_mips64_pmu_conf.pmu_name = "MIPS24K";
++		break;
++	case CPU_25KF:
++		pfm_mips64_pmu_conf.pmu_name = "MIPS25KF";
++		break;
++	case CPU_SB1:
++		pfm_mips64_pmu_conf.pmu_name = "SB1";
++		break;
++	case CPU_SB1A:
++	pfm_mips64_pmu_conf.pmu_name = "SB1A";
++		break;
++#ifndef CONFIG_SMP
++	case CPU_34K:
++		pfm_mips64_pmu_conf.pmu_name = "MIPS34K";
++		break;
++#if defined(CPU_74K)
++	case CPU_74K:
++		pfm_mips64_pmu_conf.pmu_name = "MIPS74K";
++		break;
++#endif
++#endif
++	default:
++		PFM_INFO("Unknown cputype 0x%x",c->cputype);
++		return -1;
++	}
++
++           /* The R14k and older performance counters have to          */
++	   /* be hard-coded, as there is no support for auto-detection */
++	if ((c->cputype==CPU_R12000) || (c->cputype==CPU_R14000)) {
++	   num=4;
++	}
++        else if (c->cputype==CPU_R10000) {
++	   num=2;
++	}
++        else {
++	   num = n_counters();
++	}
++
++	if (num == 0) {
++		PFM_INFO("cputype 0x%x has no counters",c->cputype);
++		return -1;
++	}
++	/* mark remaining counters unavailable */
++	for(i=num; i < PFM_MIPS64_NUM_PMCS; i++) {
++		pfm_mips64_pmc_desc[i].type = PFM_REG_NA;
++	}
++
++	for(i=num; i < PFM_MIPS64_NUM_PMDS; i++) {
++		pfm_mips64_pmd_desc[i].type = PFM_REG_NA;
++	}
++
++	/* set the PMC_RSVD mask */
++	switch (c->cputype) {
++	case CPU_5KC:
++	case CPU_R10000:
++	case CPU_20KC:
++	   /* 4-bits for event */
++	   temp_mask=0xfffffffffffffe10ULL;
++	   break;
++	case CPU_R12000:
++	case CPU_R14000:
++	   /* 5-bits for event */
++	   temp_mask=0xfffffffffffffc10ULL;
++	   break;
++	default:
++	   /* 6-bits for event */
++	   temp_mask=0xfffffffffffff810ULL;
++	}
++        for(i=0; i< PFM_MIPS64_NUM_PMCS;i++) {
++           pfm_mips64_pmc_desc[i].rsvd_msk=temp_mask;
++	}
++
++	pfm_mips64_pmu_conf.num_pmc_entries = num;
++	pfm_mips64_pmu_conf.num_pmd_entries = num;
++
++	pfm_mips64_pmu_info.pmu_style = c->cputype;
++
++	ret = pfm_pmu_register(&pfm_mips64_pmu_conf);
++	if (ret == 0)
++		perf_irq = perfmon_perf_irq;
++	return ret;
++}
++
++static void __exit pfm_mips64_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_mips64_pmu_conf);
++	perf_irq = null_perf_irq;
++}
++
++module_init(pfm_mips64_pmu_init_module);
++module_exit(pfm_mips64_pmu_cleanup_module);
+--- a/arch/powerpc/Kconfig
++++ b/arch/powerpc/Kconfig
+@@ -167,6 +167,8 @@ source "init/Kconfig"
+ 
+ source "arch/powerpc/platforms/Kconfig"
+ 
++source "arch/powerpc/perfmon/Kconfig"
++
+ menu "Kernel options"
+ 
+ config HIGHMEM
+--- a/arch/powerpc/Makefile
++++ b/arch/powerpc/Makefile
+@@ -147,6 +147,7 @@ core-y				+= arch/powerpc/kernel/ \
+ 				   arch/powerpc/platforms/
+ core-$(CONFIG_MATH_EMULATION)	+= arch/powerpc/math-emu/
+ core-$(CONFIG_XMON)		+= arch/powerpc/xmon/
++core-$(CONFIG_PERFMON)		+= arch/powerpc/perfmon/
+ 
+ drivers-$(CONFIG_OPROFILE)	+= arch/powerpc/oprofile/
+ 
+--- a/arch/powerpc/kernel/entry_32.S
++++ b/arch/powerpc/kernel/entry_32.S
+@@ -38,7 +38,7 @@
+  * MSR_KERNEL is > 0x10000 on 4xx/Book-E since it include MSR_CE.
+  */
+ #if MSR_KERNEL >= 0x10000
+-#define LOAD_MSR_KERNEL(r, x)	lis r,(x)@h; ori r,r,(x)@l
++#define LOAD_MSR_KERNEL(r, x)	lis r,(x)@ha; ori r,r,(x)@l
+ #else
+ #define LOAD_MSR_KERNEL(r, x)	li r,(x)
+ #endif
+--- a/arch/powerpc/kernel/entry_64.S
++++ b/arch/powerpc/kernel/entry_64.S
+@@ -608,6 +608,10 @@ user_work:
+ 	b	.ret_from_except_lite
+ 
+ 1:	bl	.save_nvgprs
++#ifdef CONFIG_PERFMON
++	addi	r3,r1,STACK_FRAME_OVERHEAD
++	bl	.pfm_handle_work
++#endif /* CONFIG_PERFMON */
+ 	li	r3,0
+ 	addi	r4,r1,STACK_FRAME_OVERHEAD
+ 	bl	.do_signal
+--- a/arch/powerpc/kernel/process.c
++++ b/arch/powerpc/kernel/process.c
+@@ -33,6 +33,7 @@
+ #include <linux/mqueue.h>
+ #include <linux/hardirq.h>
+ #include <linux/utsname.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/pgtable.h>
+ #include <asm/uaccess.h>
+@@ -346,6 +347,9 @@ struct task_struct *__switch_to(struct t
+ 		new_thread->start_tb = current_tb;
+ 	}
+ #endif
++	if (test_tsk_thread_flag(new, TIF_PERFMON_CTXSW)
++	    || test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW))
++		pfm_ctxsw(prev, new);
+ 
+ 	local_irq_save(flags);
+ 
+@@ -491,6 +495,7 @@ void show_regs(struct pt_regs * regs)
+ void exit_thread(void)
+ {
+ 	discard_lazy_cpu_state();
++	pfm_exit_thread(current);
+ }
+ 
+ void flush_thread(void)
+@@ -609,6 +614,7 @@ int copy_thread(int nr, unsigned long cl
+ #else
+ 	kregs->nip = (unsigned long)ret_from_fork;
+ #endif
++	pfm_copy_thread(p);
+ 
+ 	return 0;
+ }
+--- /dev/null
++++ b/arch/powerpc/perfmon/Kconfig
+@@ -0,0 +1,57 @@
++menu "Hardware Performance Monitoring support"
++config PERFMON
++	bool "Perfmon2 performance monitoring interface"
++	default n
++	help
++	Enables the perfmon2 interface to access the hardware
++	performance counters. See <http://perfmon2.sf.net/> for
++	more details.
++
++config PERFMON_DEBUG
++	bool "Perfmon debugging"
++	default n
++	depends on PERFMON
++	help
++	Enables perfmon debugging support
++
++config PERFMON_POWER4
++	tristate "Support for Power4 hardware performance counters"
++	depends on PERFMON && PPC64
++	default n
++	help
++	Enables support for the Power 4 hardware performance counters
++	If unsure, say M.
++
++config PERFMON_POWER5
++	tristate "Support for Power5 hardware performance counters"
++	depends on PERFMON && PPC64
++	default n
++	help
++	Enables support for the Power 5 hardware performance counters
++	If unsure, say M.
++
++config PERFMON_POWER6
++	tristate "Support for Power6 hardware performance counters"
++	depends on PERFMON && PPC64
++	default n
++	help
++	Enables support for the Power 6 hardware performance counters
++	If unsure, say M.
++
++config PERFMON_PPC32
++	tristate "Support for PPC32 hardware performance counters"
++	depends on PERFMON && PPC32
++	default n
++	help
++	Enables support for the PPC32 hardware performance counters
++	If unsure, say M.
++
++config PERFMON_CELL
++	tristate "Support for Cell hardware performance counters"
++	depends on PERFMON && PPC_CELL
++	default n
++	help
++	Enables support for the Cell hardware performance counters.
++	If unsure, say M.
++
++endmenu
+--- /dev/null
++++ b/arch/powerpc/perfmon/Makefile
+@@ -0,0 +1,6 @@
++obj-$(CONFIG_PERFMON)		+= perfmon.o
++obj-$(CONFIG_PERFMON_POWER4)	+= perfmon_power4.o
++obj-$(CONFIG_PERFMON_POWER5)	+= perfmon_power5.o
++obj-$(CONFIG_PERFMON_POWER6)	+= perfmon_power6.o
++obj-$(CONFIG_PERFMON_PPC32)	+= perfmon_ppc32.o
++obj-$(CONFIG_PERFMON_CELL)	+= perfmon_cell.o
+--- /dev/null
++++ b/arch/powerpc/perfmon/perfmon.c
+@@ -0,0 +1,286 @@
++/*
++ * This file implements the powerpc specific
++ * support for the perfmon2 interface
++ *
++ * Copyright (c) 2005 David Gibson, IBM Corporation.
++ *
++ * based on versions for other architectures:
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/interrupt.h>
++#include <linux/perfmon.h>
++
++static void pfm_stop_active(struct task_struct *task,
++			    struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	BUG_ON(!arch_info->disable_counters || !arch_info->get_ovfl_pmds);
++
++	arch_info->disable_counters(ctx, set);
++
++	if (set->npend_ovfls)
++		return;
++
++	arch_info->get_ovfl_pmds(ctx, set);
++}
++
++/*
++ * Called from pfm_ctxsw(). Task is guaranteed to be current.
++ * Context is locked. Interrupts are masked. Monitoring is active.
++ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
++ *
++ * for per-thread:
++ * 	must stop monitoring for the task
++ * Return:
++ * 	non-zero : did not save PMDs (as part of stopping the PMU)
++ * 	       0 : saved PMDs (no need to save them in caller)
++ */
++int pfm_arch_ctxswout_thread(struct task_struct *task,
++			     struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	/*
++	 * disable lazy restore of PMC registers.
++	 */
++	set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
++
++	pfm_stop_active(task, ctx, set);
++
++	if (arch_info->ctxswout_thread)
++		arch_info->ctxswout_thread(task, ctx, set);
++
++	return 1;
++}
++
++/*
++ * Called from pfm_ctxsw
++ */
++void pfm_arch_ctxswin_thread(struct task_struct *task,
++			     struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	if (ctx->state != PFM_CTX_MASKED && ctx->flags.started == 1) {
++		BUG_ON(!arch_info->enable_counters);
++		arch_info->enable_counters(ctx, set);
++	}
++
++	if (arch_info->ctxswin_thread)
++		arch_info->ctxswin_thread(task, ctx, set);
++}
++
++/*
++ * Called from pfm_stop() and idle notifier
++ *
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-thread:
++ *   task is not necessarily current. If not current task, then
++ *   task is guaranteed stopped and off any cpu. Access to PMU
++ *   is not guaranteed. Interrupts are masked. Context is locked.
++ *   Set is the active set.
++ *
++ * For system-wide:
++ * 	task is current
++ *
++ * must disable active monitoring. ctx cannot be NULL
++ */
++void pfm_arch_stop(struct task_struct *task,
++		   struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	/*
++	 * no need to go through stop_save()
++	 * if we are already stopped
++	 */
++	if (!ctx->flags.started || ctx->state == PFM_CTX_MASKED)
++		return;
++
++	/*
++	 * stop live registers and collect pending overflow
++	 */
++	if (task == current)
++		pfm_stop_active(task, ctx, set);
++}
++
++/*
++ * Enable active monitoring. Called from pfm_start() and
++ * pfm_arch_unmask_monitoring().
++ *
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-thread:
++ * 	Task is not necessarily current. If not current task, then task
++ * 	is guaranteed stopped and off any cpu. No access to PMU if task
++ *	is not current.
++ *
++ * For system-wide:
++ * 	Task is always current
++ */
++void pfm_arch_start(struct task_struct *task,
++		    struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	if (task != current)
++		return;
++
++	BUG_ON(!arch_info->enable_counters);
++
++	arch_info->enable_counters(ctx, set);
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
++ * context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMD registers from set.
++ */
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	u64 *used_pmds;
++	u16 i, num;
++
++	/* The model-specific module can override the default
++	 * restore-PMD method.
++	 */
++	if (arch_info->restore_pmds)
++		return arch_info->restore_pmds(set);
++
++	num = set->nused_pmds;
++	used_pmds = set->used_pmds;
++
++	for (i = 0; num; i++) {
++		if (likely(test_bit(i, used_pmds))) {
++			pfm_write_pmd(ctx, i, set->pmds[i].value);
++			num--;
++		}
++	}
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
++ * context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMC registers from set, if needed.
++ */
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info;
++	u64 *impl_pmcs;
++	unsigned int i, max_pmc, reg;
++
++	/* The model-specific module can override the default
++	 * restore-PMC method.
++	 */
++	arch_info = pfm_pmu_conf->arch_info;
++	if (arch_info->restore_pmcs)
++		return arch_info->restore_pmcs(set);
++
++	/* The "common" powerpc model's enable the counters simply by writing
++	 * all the control registers. Therefore, if we're masked or stopped we
++	 * don't need to bother restoring the PMCs now.
++	 */
++	if (ctx->state == PFM_CTX_MASKED || ctx->flags.started == 0)
++		return;
++
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++	impl_pmcs = pfm_pmu_conf->regs.pmcs;
++
++	/*
++	 * Restore all pmcs in reverse order to ensure the counters aren't
++	 * enabled before their event selectors are set correctly.
++	 */
++	reg = max_pmc - 1;
++	for (i = 0; i < max_pmc; i++) {
++		if (test_bit(reg, impl_pmcs))
++			pfm_arch_write_pmc(ctx, reg, set->pmcs[reg]);
++		reg--;
++	}
++}
++
++char *pfm_arch_get_pmu_module_name(void)
++{
++	unsigned int pvr = mfspr(SPRN_PVR);
++
++	switch (PVR_VER(pvr)) {
++	case 0x0004: /* 604 */
++	case 0x0009: /* 604e;  */
++	case 0x000A: /* 604ev */
++	case 0x0008: /* 750/740 */
++	case 0x7000: /* 750FX */
++	case 0x7001:
++	case 0x7002: /* 750GX */
++	case 0x000C: /* 7400 */
++	case 0x800C: /* 7410 */
++	case 0x8000: /* 7451/7441 */
++	case 0x8001: /* 7455/7445 */
++	case 0x8002: /* 7457/7447 */
++	case 0x8003: /* 7447A */
++	case 0x8004: /* 7448 */
++		return("perfmon_ppc32");
++	case PV_POWER4:
++	case PV_POWER4p:
++		return "perfmon_power4";
++	case PV_POWER5:
++		return "perfmon_power5";
++	case PV_POWER5p:
++		return "perfmon_power5+";
++	case PV_POWER6:
++		return "perfmon_power6";
++	case PV_970:
++	case PV_970FX:
++	case PV_970MP:
++		return "perfmon_ppc970";
++	case PV_BE:
++		return "perfmon_cell";
++	}
++	return NULL;
++}
++
++void pfm_arch_init_percpu(void)
++{
++#ifdef CONFIG_PPC64
++	extern void ppc64_enable_pmcs(void);
++	ppc64_enable_pmcs();
++#endif
++}
++
++/**
++ * powerpc_irq_handler
++ *
++ * Get the perfmon context that belongs to the current CPU, and call the
++ * model-specific interrupt handler.
++ **/
++void powerpc_irq_handler(struct pt_regs *regs)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	struct pfm_context *ctx;
++
++	if (arch_info->irq_handler) {
++		ctx = __get_cpu_var(pmu_ctx);
++		if (likely(ctx))
++			arch_info->irq_handler(regs, ctx);
++	}
++}
+--- /dev/null
++++ b/arch/powerpc/perfmon/perfmon_cell.c
+@@ -0,0 +1,901 @@
++/*
++ * This file contains the Cell PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Copyright IBM Corporation 2007
++ * (C) Copyright 2007 TOSHIBA CORPORATION
++ *
++ * Based on other Perfmon2 PMU modules.
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <asm/cell-pmu.h>
++#include <asm/cell-regs.h>
++#include <asm/io.h>
++#include <asm/machdep.h>
++#include <asm/rtas.h>
++
++MODULE_AUTHOR("Kevin Corry <kevcorry@us.ibm.com>, "
++	      "Carl Love <carll@us.ibm.com>");
++MODULE_DESCRIPTION("Cell PMU description table");
++MODULE_LICENSE("GPL");
++
++/*
++ * Mapping from Perfmon logical control registers to Cell hardware registers.
++ */
++static struct pfm_regmap_desc pfm_cell_pmc_desc[] = {
++	/* Per-counter control registers. */
++	PMC_D(PFM_REG_I, "pm0_control",       0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm1_control",       0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm2_control",       0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm3_control",       0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm4_control",       0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm5_control",       0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm6_control",       0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm7_control",       0, 0, 0, 0),
++
++	/* Per-counter RTAS arguments. Each of these registers has three fields.
++	 *   bits 63-48: debug-bus word
++	 *   bits 47-32: sub-unit
++	 *   bits 31-0 : full signal number
++	 *   (MSB = 63, LSB = 0)
++	 */
++	PMC_D(PFM_REG_I, "pm0_event",         0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm1_event",         0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm2_event",         0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm3_event",         0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm4_event",         0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm5_event",         0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm6_event",         0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm7_event",         0, 0, 0, 0),
++
++	/* Global control registers. Same order as enum pm_reg_name. */
++	PMC_D(PFM_REG_I, "group_control",     0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "debug_bus_control", 0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "trace_address",     0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "ext_trace_timer",   0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm_status",         0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm_control",        0, 0, 0, 0),
++	PMC_D(PFM_REG_I, "pm_interval",       0, 0, 0, 0), /* FIX: Does user-space also need read access to this one? */
++	PMC_D(PFM_REG_I, "pm_start_stop",     0, 0, 0, 0),
++};
++#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_cell_pmc_desc)
++
++#define CELL_PMC_PM_STATUS 20
++/*
++ * Mapping from Perfmon logical data counters to Cell hardware counters.
++ */
++static struct pfm_regmap_desc pfm_cell_pmd_desc[] = {
++	PMD_D(PFM_REG_C, "pm0", 0),
++	PMD_D(PFM_REG_C, "pm1", 0),
++	PMD_D(PFM_REG_C, "pm2", 0),
++	PMD_D(PFM_REG_C, "pm3", 0),
++	PMD_D(PFM_REG_C, "pm4", 0),
++	PMD_D(PFM_REG_C, "pm5", 0),
++	PMD_D(PFM_REG_C, "pm6", 0),
++	PMD_D(PFM_REG_C, "pm7", 0),
++};
++#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_cell_pmd_desc)
++
++/*
++ * Debug-bus signal handling.
++ *
++ * Some Cell systems have firmware that can handle the debug-bus signal
++ * routing. For systems without this firmware, we have a minimal in-kernel
++ * implementation as well.
++ */
++
++/* The firmware only sees physical CPUs, so divide by 2 if SMT is on. */
++#ifdef CONFIG_SCHED_SMT
++#define RTAS_CPU(cpu) ((cpu) / 2)
++#else
++#define RTAS_CPU(cpu) (cpu)
++#endif
++#define RTAS_BUS_WORD(x)      (u16)(((x) >> 48) & 0x0000ffff)
++#define RTAS_SUB_UNIT(x)      (u16)(((x) >> 32) & 0x0000ffff)
++#define RTAS_SIGNAL_NUMBER(x) (s32)( (x)        & 0xffffffff)
++#define RTAS_SIGNAL_GROUP(x)  (RTAS_SIGNAL_NUMBER(x) / 100)
++
++#define subfunc_RESET		1
++#define subfunc_ACTIVATE	2
++
++#define passthru_ENABLE		1
++#define passthru_DISABLE	2
++
++/**
++ * struct cell_rtas_arg
++ *
++ * @cpu: Processor to modify. Linux numbers CPUs based on SMT IDs, but the
++ *       firmware only sees the physical CPUs. So this value should be the
++ *       SMT ID (from smp_processor_id() or get_cpu()) divided by 2.
++ * @sub_unit: Hardware subunit this applies to (if applicable).
++ * @signal_group: Signal group to enable/disable on the trace bus.
++ * @bus_word: For signal groups that propagate via the trace bus, this trace
++ *            bus word will be used. This is a mask of (1 << TraceBusWord).
++ *            For other signal groups, this specifies the trigger or event bus.
++ * @bit: Trigger/Event bit, if applicable for the signal group.
++ *
++ * An array of these structures are passed to rtas_call() to set up the
++ * signals on the debug bus.
++ **/
++struct cell_rtas_arg {
++	u16 cpu;
++	u16 sub_unit;
++	s16 signal_group;
++	u8 bus_word;
++	u8 bit;
++};
++
++/**
++ * rtas_reset_signals
++ *
++ * Use the firmware RTAS call to disable signal pass-thru and to reset the
++ * debug-bus signals.
++ **/
++static int rtas_reset_signals(u32 cpu)
++{
++	struct cell_rtas_arg signal;
++	u64 real_addr = virt_to_phys(&signal);
++	int rc;
++
++	memset(&signal, 0, sizeof(signal));
++	signal.cpu = RTAS_CPU(cpu);
++	rc = rtas_call(rtas_token("ibm,cbe-perftools"),
++		       5, 1, NULL,
++		       subfunc_RESET,
++		       passthru_DISABLE,
++		       real_addr >> 32,
++		       real_addr & 0xffffffff,
++		       sizeof(signal));
++
++	return rc;
++}
++
++/**
++ * rtas_activate_signals
++ *
++ * Use the firmware RTAS call to enable signal pass-thru and to activate the
++ * desired signal groups on the debug-bus.
++ **/
++static int rtas_activate_signals(struct cell_rtas_arg *signals,
++				 int num_signals)
++{
++	u64 real_addr = virt_to_phys(signals);
++	int rc;
++
++	rc = rtas_call(rtas_token("ibm,cbe-perftools"),
++		       5, 1, NULL,
++		       subfunc_ACTIVATE,
++		       passthru_ENABLE,
++		       real_addr >> 32,
++		       real_addr & 0xffffffff,
++		       num_signals * sizeof(*signals));
++
++	return rc;
++}
++
++#define HID1_RESET_MASK			(~0x00000001ffffffffUL)
++#define PPU_IU1_WORD0_HID1_EN_MASK	(~0x00000001f0c0802cUL)
++#define PPU_IU1_WORD0_HID1_EN_WORD	( 0x00000001f0400000UL)
++#define PPU_IU1_WORD1_HID1_EN_MASK	(~0x000000010fc08023UL)
++#define PPU_IU1_WORD1_HID1_EN_WORD	( 0x000000010f400001UL)
++#define PPU_XU_WORD0_HID1_EN_MASK	(~0x00000001f038402cUL)
++#define PPU_XU_WORD0_HID1_EN_WORD	( 0x00000001f0080008UL)
++#define PPU_XU_WORD1_HID1_EN_MASK	(~0x000000010f074023UL)
++#define PPU_XU_WORD1_HID1_EN_WORD	( 0x000000010f030002UL)
++
++/* The bus_word field in the cell_rtas_arg structure is a bit-mask
++ * indicating which debug-bus word(s) to use.
++ */
++enum {
++	BUS_WORD_0 = 1,
++	BUS_WORD_1 = 2,
++	BUS_WORD_2 = 4,
++	BUS_WORD_3 = 8,
++};
++
++/* Definitions of the signal-groups that the built-in signal-activation
++ * code can handle.
++ */
++enum {
++	SIG_GROUP_NONE = 0,
++
++	/* 2.x PowerPC Processor Unit (PPU) Signal Groups */
++	SIG_GROUP_PPU_IU1 = 21,
++	SIG_GROUP_PPU_XU = 22,
++};
++
++/**
++ * rmw_spr
++ *
++ * Read-modify-write for a special-purpose-register.
++ **/
++#define rmw_spr(spr_id, a_mask, o_mask) \
++	do { \
++		u64 value = mfspr(spr_id); \
++		value &= (u64)(a_mask); \
++		value |= (u64)(o_mask); \
++		mtspr((spr_id), value); \
++	} while (0)
++
++/**
++ * rmw_mmio_reg64
++ *
++ * Read-modify-write for a 64-bit MMIO register.
++ **/
++#define rmw_mmio_reg64(mem, a_mask, o_mask) \
++	do { \
++		u64 value = in_be64(&(mem)); \
++		value &= (u64)(a_mask); \
++		value |= (u64)(o_mask); \
++		out_be64(&(mem), value); \
++	} while (0)
++
++/**
++ * rmwb_mmio_reg64
++ *
++ * Set or unset a specified bit within a 64-bit MMIO register.
++ **/
++#define rmwb_mmio_reg64(mem, bit_num, set_bit) \
++	rmw_mmio_reg64((mem), ~(1UL << (63 - (bit_num))), \
++		       ((set_bit) << (63 - (bit_num))))
++
++/**
++ * passthru
++ *
++ * Enable or disable passthru mode in all the Cell signal islands.
++ **/
++static int passthru(u32 cpu, u64 enable)
++{
++	struct cbe_ppe_priv_regs __iomem *ppe_priv_regs;
++	struct cbe_pmd_regs __iomem *pmd_regs;
++	struct cbe_mic_tm_regs __iomem *mic_tm_regs;
++
++	ppe_priv_regs = cbe_get_cpu_ppe_priv_regs(cpu);
++	pmd_regs = cbe_get_cpu_pmd_regs(cpu);
++	mic_tm_regs = cbe_get_cpu_mic_tm_regs(cpu);
++
++	if (!ppe_priv_regs || !pmd_regs || !mic_tm_regs) {
++		PFM_ERR("Error getting Cell PPE, PMD, and MIC "
++			"register maps: 0x%p, 0x%p, 0x%p",
++			ppe_priv_regs, pmd_regs, mic_tm_regs);
++		return -EINVAL;
++	}
++
++	rmwb_mmio_reg64(ppe_priv_regs->L2_debug1, 61, enable);
++	rmwb_mmio_reg64(ppe_priv_regs->ciu_dr1, 5, enable);
++	rmwb_mmio_reg64(pmd_regs->on_ramp_trace, 39, enable);
++	rmwb_mmio_reg64(mic_tm_regs->MBL_debug, 20, enable);
++
++	return 0;
++}
++
++#define passthru_enable(cpu)  passthru(cpu, 1)
++#define passthru_disable(cpu) passthru(cpu, 0)
++
++static inline void reset_signal_registers(u32 cpu)
++{
++	rmw_spr(SPRN_HID1, HID1_RESET_MASK, 0);
++}
++
++/**
++ * celleb_reset_signals
++ *
++ * Non-rtas version of resetting the debug-bus signals.
++ **/
++static int celleb_reset_signals(u32 cpu)
++{
++	int rc;
++	rc = passthru_disable(cpu);
++	if (!rc)
++		reset_signal_registers(cpu);
++	return rc;
++}
++
++/**
++ * ppu_selection
++ *
++ * Write the HID1 register to connect the specified PPU signal-group to the
++ * debug-bus.
++ **/
++static int ppu_selection(struct cell_rtas_arg *signal)
++{
++	u64 hid1_enable_word = 0;
++	u64 hid1_enable_mask = 0;
++
++	switch (signal->signal_group) {
++
++	case SIG_GROUP_PPU_IU1: /* 2.1 PPU Instruction Unit - Group 1 */
++		switch (signal->bus_word) {
++		case BUS_WORD_0:
++			hid1_enable_mask = PPU_IU1_WORD0_HID1_EN_MASK;
++			hid1_enable_word = PPU_IU1_WORD0_HID1_EN_WORD;
++			break;
++		case BUS_WORD_1:
++			hid1_enable_mask = PPU_IU1_WORD1_HID1_EN_MASK;
++			hid1_enable_word = PPU_IU1_WORD1_HID1_EN_WORD;
++			break;
++		default:
++			PFM_ERR("Invalid bus-word (0x%x) for signal-group %d.",
++				signal->bus_word, signal->signal_group);
++			return -EINVAL;
++		}
++		break;
++
++	case SIG_GROUP_PPU_XU:  /* 2.2 PPU Execution Unit */
++		switch (signal->bus_word) {
++		case BUS_WORD_0:
++			hid1_enable_mask = PPU_XU_WORD0_HID1_EN_MASK;
++			hid1_enable_word = PPU_XU_WORD0_HID1_EN_WORD;
++			break;
++		case BUS_WORD_1:
++			hid1_enable_mask = PPU_XU_WORD1_HID1_EN_MASK;
++			hid1_enable_word = PPU_XU_WORD1_HID1_EN_WORD;
++			break;
++		default:
++			PFM_ERR("Invalid bus-word (0x%x) for signal-group %d.",
++				signal->bus_word, signal->signal_group);
++			return -EINVAL;
++		}
++		break;
++
++	default:
++		PFM_ERR("Signal-group %d not implemented.",
++			signal->signal_group);
++		return -EINVAL;
++	}
++
++	rmw_spr(SPRN_HID1, hid1_enable_mask, hid1_enable_word);
++
++	return 0;
++}
++
++/**
++ * celleb_activate_signals
++ *
++ * Non-rtas version of activating the debug-bus signals.
++ **/
++static int celleb_activate_signals(struct cell_rtas_arg *signals,
++				   int num_signals)
++{
++	int i, rc = -EINVAL;
++
++	for (i = 0; i < num_signals; i++) {
++		switch (signals[i].signal_group) {
++
++		/* 2.x PowerPC Processor Unit (PPU) Signal Selection */
++		case SIG_GROUP_PPU_IU1:
++		case SIG_GROUP_PPU_XU:
++			rc = ppu_selection(signals + i);
++			if (rc)
++				return rc;
++			break;
++
++		default:
++			PFM_ERR("Signal-group %d not implemented.",
++				signals[i].signal_group);
++			return -EINVAL;
++		}
++	}
++
++	if (0 < i)
++		rc = passthru_enable(signals[0].cpu);
++
++	return rc;
++}
++
++/**
++ * reset_signals
++ *
++ * Call to the firmware (if available) to reset the debug-bus signals.
++ * Otherwise call the built-in version.
++ **/
++int reset_signals(u32 cpu)
++{
++	int rc;
++
++	if (machine_is(celleb))
++		rc = celleb_reset_signals(cpu);
++	else
++		rc = rtas_reset_signals(cpu);
++
++	return rc;
++}
++
++/**
++ * activate_signals
++ *
++ * Call to the firmware (if available) to activate the debug-bus signals.
++ * Otherwise call the built-in version.
++ **/
++int activate_signals(struct cell_rtas_arg *signals, int num_signals)
++{
++	int rc;
++
++	if (machine_is(celleb))
++		rc = celleb_activate_signals(signals, num_signals);
++	else
++		rc = rtas_activate_signals(signals, num_signals);
++
++	return rc;
++}
++
++/**
++ *  pfm_cell_pmc_check
++ *
++ * Verify that we are going to write a valid value to the specified PMC.
++ **/
++int pfm_cell_pmc_check(struct pfm_context *ctx,
++		       struct pfm_event_set *set,
++		       struct pfarg_pmc *req)
++{
++	u16 cnum, reg_num = req->reg_num;
++	s16 signal_group = RTAS_SIGNAL_GROUP(req->reg_value);
++	u8 bus_word = RTAS_BUS_WORD(req->reg_value);
++
++	if (reg_num < NR_CTRS || reg_num >= (NR_CTRS * 2)) {
++		return -EINVAL;
++	}
++
++	switch (signal_group) {
++	case SIG_GROUP_PPU_IU1:
++	case SIG_GROUP_PPU_XU:
++		if ((bus_word != 0) && (bus_word != 1)) {
++			PFM_ERR("Invalid bus word (%d) for signal-group %d",
++				bus_word, signal_group);
++			return -EINVAL;
++		}
++		break;
++	default:
++		PFM_ERR("Signal-group %d not implemented.", signal_group);
++		return -EINVAL;
++	}
++
++	for (cnum = NR_CTRS; cnum < (NR_CTRS * 2); cnum++) {
++		if (test_bit(cnum, cast_ulp(set->used_pmcs)) &&
++		    bus_word == RTAS_BUS_WORD(set->pmcs[cnum]) &&
++		    signal_group != RTAS_SIGNAL_GROUP(set->pmcs[cnum])) {
++			PFM_ERR("Impossible signal-group combination: "
++				"(%u,%u,%d) (%u,%u,%d)",
++				reg_num, bus_word, signal_group, cnum,
++				RTAS_BUS_WORD(set->pmcs[cnum]),
++				RTAS_SIGNAL_GROUP(set->pmcs[cnum]));
++			return  -EBUSY;
++		}
++	}
++
++	return 0;
++}
++
++/**
++ * write_pm07_event
++ *
++ * Pull out the RTAS arguments from the 64-bit register value and make the
++ * RTAS activate-signals call.
++ **/
++static void write_pm07_event(int cpu, unsigned int ctr, u64 value)
++{
++	struct cell_rtas_arg signal;
++	s32 signal_number;
++	int rc;
++
++	signal_number = RTAS_SIGNAL_NUMBER(value);
++	if (!signal_number) {
++		/* Don't include counters that are counting cycles. */
++		return;
++	}
++
++	signal.cpu = RTAS_CPU(cpu);
++	signal.bus_word = 1 << RTAS_BUS_WORD(value);
++	signal.sub_unit = RTAS_SUB_UNIT(value);
++	signal.signal_group = signal_number / 100;
++	signal.bit = signal_number % 100;
++
++	rc = activate_signals(&signal, 1);
++	if (rc) {
++		PFM_WARN("%s(%d, %u, %lu): Error calling "
++			 "activate_signals(): %d\n", __FUNCTION__,
++			 cpu, ctr, (unsigned long)value, rc);
++		/* FIX: Could we change this routine to return an error? */
++	}
++}
++
++/**
++ * pfm_cell_probe_pmu
++ *
++ * Simply check the processor version register to see if we're currently
++ * on a Cell system.
++ **/
++static int pfm_cell_probe_pmu(void)
++{
++	unsigned long pvr = mfspr(SPRN_PVR);
++
++	if (PVR_VER(pvr) != PV_BE)
++		return -1;
++
++	return 0;
++}
++
++/**
++ * pfm_cell_write_pmc
++ **/
++static void pfm_cell_write_pmc(unsigned int cnum, u64 value)
++{
++	int cpu = smp_processor_id();
++
++	if (cnum < NR_CTRS) {
++		cbe_write_pm07_control(cpu, cnum, value);
++
++	} else if (cnum < NR_CTRS * 2) {
++		write_pm07_event(cpu, cnum - NR_CTRS, value);
++
++	} else if (cnum == CELL_PMC_PM_STATUS) {
++		/* The pm_status register must be treated separately from
++		 * the other "global" PMCs. This call will ensure that
++		 * the interrupts are routed to the correct CPU, as well
++		 * as writing the desired value to the pm_status register.
++		 */
++		cbe_enable_pm_interrupts(cpu, cbe_get_hw_thread_id(cpu), value);
++
++	} else if (cnum < PFM_PM_NUM_PMCS) {
++		cbe_write_pm(cpu, cnum - (NR_CTRS * 2), value);
++	}
++}
++
++/**
++ * pfm_cell_write_pmd
++ **/
++static void pfm_cell_write_pmd(unsigned int cnum, u64 value)
++{
++	int cpu = smp_processor_id();
++
++	if (cnum < NR_CTRS) {
++		cbe_write_ctr(cpu, cnum, value);
++	}
++}
++
++/**
++ * pfm_cell_read_pmd
++ **/
++static u64 pfm_cell_read_pmd(unsigned int cnum)
++{
++	int cpu = smp_processor_id();
++
++	if (cnum < NR_CTRS) {
++		return cbe_read_ctr(cpu, cnum);
++	}
++
++	return -EINVAL;
++}
++
++/**
++ * pfm_cell_enable_counters
++ *
++ * Just need to turn on the global disable bit in pm_control.
++ **/
++static void pfm_cell_enable_counters(struct pfm_context *ctx,
++				     struct pfm_event_set *set)
++{
++	cbe_enable_pm(smp_processor_id());
++}
++
++/**
++ * pfm_cell_disable_counters
++ *
++ * Just need to turn off the global disable bit in pm_control.
++ **/
++static void pfm_cell_disable_counters(struct pfm_context *ctx,
++				      struct pfm_event_set *set)
++{
++	cbe_disable_pm(smp_processor_id());
++}
++
++/**
++ * pfm_cell_restore_pmcs
++ *
++ * Write all control register values that are saved in the specified event
++ * set. We could use the pfm_arch_write_pmc() function to restore each PMC
++ * individually (as is done in other architectures), but that results in
++ * multiple RTAS calls. As an optimization, we will setup the RTAS argument
++ * array so we can do all event-control registers in one RTAS call.
++ **/
++void pfm_cell_restore_pmcs(struct pfm_event_set *set)
++{
++	struct cell_rtas_arg signals[NR_CTRS];
++	u64 value, *used_pmcs = set->used_pmcs;
++	int i, rc, num_used = 0, cpu = smp_processor_id();
++	s32 signal_number;
++
++	memset(signals, 0, sizeof(signals));
++
++	for (i = 0; i < NR_CTRS; i++) {
++		/* Write the per-counter control register. If the PMC is not
++		 * in use, then it will simply clear the register, which will
++		 * disable the associated counter.
++		 */
++		cbe_write_pm07_control(cpu, i, set->pmcs[i]);
++
++		/* Set up the next RTAS array entry for this counter. Only
++		 * include pm07_event registers that are in use by this set
++		 * so the RTAS call doesn't have to process blank array entries.
++		 */
++		if (!test_bit(i + NR_CTRS, used_pmcs)) {
++			continue;
++		}
++
++		value = set->pmcs[i + NR_CTRS];
++		signal_number = RTAS_SIGNAL_NUMBER(value);
++		if (!signal_number) {
++			/* Don't include counters that are counting cycles. */
++			continue;
++		}
++
++		signals[num_used].cpu = RTAS_CPU(cpu);
++		signals[num_used].sub_unit = RTAS_SUB_UNIT(value);
++		signals[num_used].bus_word = 1 << RTAS_BUS_WORD(value);
++		signals[num_used].signal_group = signal_number / 100;
++		signals[num_used].bit = signal_number % 100;
++		num_used++;
++	}
++
++	rc = activate_signals(signals, num_used);
++	if (rc) {
++		PFM_WARN("Error calling activate_signal(): %d\n", rc);
++		/* FIX: We will also need this routine to be able to return
++		 * an error if Stephane agrees to change pfm_arch_write_pmc
++		 * to return an error.
++		 */
++	}
++
++	/* Write all the global PMCs. Need to call pfm_cell_write_pmc()
++	 * instead of cbe_write_pm() due to special handling for the
++	 * pm_status register.
++	 */
++	for (i *= 2; i < PFM_PM_NUM_PMCS; i++)
++		pfm_cell_write_pmc(i, set->pmcs[i]);
++}
++
++/**
++ * pfm_cell_unload_context
++ *
++ * For system-wide contexts and self-monitored contexts, make the RTAS call
++ * to reset the debug-bus signals.
++ *
++ * For non-self-monitored contexts, the monitored thread will already have
++ * been taken off the CPU and we don't need to do anything additional.
++ **/
++static int pfm_cell_unload_context(struct pfm_context *ctx,
++				   struct task_struct *task)
++{
++	if (task == current || ctx->flags.system) {
++		reset_signals(smp_processor_id());
++	}
++	return 0;
++}
++
++/**
++ * pfm_cell_ctxswout_thread
++ *
++ * When a monitored thread is switched out (self-monitored or externally
++ * monitored) we need to reset the debug-bus signals so the next context that
++ * gets switched in can start from a clean set of signals.
++ **/
++int pfm_cell_ctxswout_thread(struct task_struct *task,
++			     struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	reset_signals(smp_processor_id());
++	return 0;
++}
++
++/**
++ * pfm_cell_get_ovfl_pmds
++ *
++ * Determine which counters in this set have overflowed and fill in the
++ * set->povfl_pmds mask and set->npend_ovfls count. On Cell, the pm_status
++ * register contains a bit for each counter to indicate overflow. However,
++ * those 8 bits are in the reverse order than what Perfmon2 is expecting,
++ * so we need to reverse the order of the overflow bits.
++ **/
++static void pfm_cell_get_ovfl_pmds(struct pfm_context *ctx,
++				   struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
++	u32 pm_status, ovfl_ctrs;
++	u64 povfl_pmds = 0;
++	int i;
++
++	if (!ctx_arch->last_read_updated)
++		/* This routine was not called via the interrupt handler.
++		 * Need to start by getting interrupts and updating
++		 * last_read_pm_status.
++		 */
++		ctx_arch->last_read_pm_status =
++			cbe_get_and_clear_pm_interrupts(smp_processor_id());
++
++	/* Reset the flag that the interrupt handler last read pm_status. */
++	ctx_arch->last_read_updated = 0;
++
++	pm_status = ctx_arch->last_read_pm_status &
++		    set->pmcs[CELL_PMC_PM_STATUS];
++	ovfl_ctrs = CBE_PM_OVERFLOW_CTRS(pm_status);
++
++	/* Reverse the order of the bits in ovfl_ctrs
++	 * and store the result in povfl_pmds.
++	 */
++	for (i = 0; i < PFM_PM_NUM_PMDS; i++) {
++		povfl_pmds = (povfl_pmds << 1) | (ovfl_ctrs & 1);
++		ovfl_ctrs >>= 1;
++	}
++
++	/* Mask povfl_pmds with set->used_pmds to get set->povfl_pmds.
++	 * Count the bits set in set->povfl_pmds to get set->npend_ovfls.
++	 */
++	bitmap_and(set->povfl_pmds, &povfl_pmds,
++		   set->used_pmds, PFM_PM_NUM_PMDS);
++	set->npend_ovfls = bitmap_weight(set->povfl_pmds, PFM_PM_NUM_PMDS);
++}
++
++/**
++ * handle_trace_buffer_interrupts
++ *
++ * This routine is for processing just the interval timer and trace buffer
++ * overflow interrupts. Performance counter interrupts are handled by the
++ * perf_irq_handler() routine, which reads and saves the pm_status register.
++ * This routine should not read the actual pm_status register, but rather
++ * the value passed in.
++ **/
++static void handle_trace_buffer_interrupts(unsigned long iip,
++					   struct pt_regs *regs,
++					   struct pfm_context *ctx,
++					   u32 pm_status)
++{
++	/* FIX: Currently ignoring trace-buffer interrupts. */
++	return;
++}
++
++/**
++ * pfm_cell_irq_handler
++ *
++ * Handler for all Cell performance-monitor interrupts.
++ **/
++static void pfm_cell_irq_handler(struct pt_regs *regs, struct pfm_context *ctx)
++{
++	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
++	u32 last_read_pm_status;
++	int cpu = smp_processor_id();
++
++	/* Need to disable and reenable the performance counters to get the
++	 * desired behavior from the hardware. This is specific to the Cell
++	 * PMU hardware.
++	 */
++	cbe_disable_pm(cpu);
++
++	/* Read the pm_status register to get the interrupt bits. If a
++	 * perfmormance counter overflow interrupt occurred, call the core
++	 * perfmon interrupt handler to service the counter overflow. If the
++	 * interrupt was for the interval timer or the trace_buffer,
++	 * call the interval timer and trace buffer interrupt handler.
++	 *
++	 * The value read from the pm_status register is stored in the
++	 * pmf_arch_context structure for use by other routines. Note that
++	 * reading the pm_status register resets the interrupt flags to zero.
++	 * Hence, it is important that the register is only read in one place.
++	 *
++	 * The pm_status reg interrupt reg format is:
++	 * [pmd0:pmd1:pmd2:pmd3:pmd4:pmd5:pmd6:pmd7:intt:tbf:tbu:]
++	 * - pmd0 to pm7 are the perf counter overflow interrupts.
++	 * - intt is the interval timer overflowed interrupt.
++	 * - tbf is the trace buffer full interrupt.
++	 * - tbu is the trace buffer underflow interrupt.
++	 * - The pmd0 bit is the MSB of the 32 bit register.
++	 */
++	ctx_arch->last_read_pm_status = last_read_pm_status =
++					cbe_get_and_clear_pm_interrupts(cpu);
++
++	/* Set flag for pfm_cell_get_ovfl_pmds() routine so it knows
++	 * last_read_pm_status was updated by the interrupt handler.
++	 */
++	ctx_arch->last_read_updated = 1;
++
++	if (last_read_pm_status & CBE_PM_ALL_OVERFLOW_INTR)
++		/* At least one counter overflowed. */
++		pfm_interrupt_handler(instruction_pointer(regs), regs);
++
++	if (last_read_pm_status & (CBE_PM_INTERVAL_INTR |
++				   CBE_PM_TRACE_BUFFER_FULL_INTR |
++				   CBE_PM_TRACE_BUFFER_UNDERFLOW_INTR))
++		/* Trace buffer or interval timer overflow. */
++		handle_trace_buffer_interrupts(instruction_pointer(regs),
++					       regs, ctx, last_read_pm_status);
++
++	/* The interrupt settings is the value written to the pm_status
++	 * register. It is saved in the context when the register is
++	 * written.
++	 */
++	cbe_enable_pm_interrupts(cpu, cbe_get_hw_thread_id(cpu),
++				 ctx->active_set->pmcs[CELL_PMC_PM_STATUS]);
++
++	/* The writes to the various performance counters only writes to a
++	 * latch. The new values (interrupt setting bits, reset counter value
++	 * etc.) are not copied to the actual registers until the performance
++	 * monitor is enabled. In order to get this to work as desired, the
++	 * permormance monitor needs to be disabled while writting to the
++	 * latches. This is a HW design issue.
++	 */
++	cbe_enable_pm(cpu);
++}
++
++static struct pfm_arch_pmu_info pfm_cell_pmu_info = {
++	.pmu_style        = PFM_POWERPC_PMU_CELL,
++	.write_pmc        = pfm_cell_write_pmc,
++	.write_pmd        = pfm_cell_write_pmd,
++	.read_pmd         = pfm_cell_read_pmd,
++	.enable_counters  = pfm_cell_enable_counters,
++	.disable_counters = pfm_cell_disable_counters,
++	.irq_handler      = pfm_cell_irq_handler,
++	.get_ovfl_pmds    = pfm_cell_get_ovfl_pmds,
++	.restore_pmcs     = pfm_cell_restore_pmcs,
++	.ctxswout_thread  = pfm_cell_ctxswout_thread,
++	.unload_context   = pfm_cell_unload_context,
++};
++
++static struct pfm_pmu_config pfm_cell_pmu_conf = {
++	.pmu_name = "Cell",
++	.version = "0.1",
++	.counter_width = 32,
++	.pmd_desc = pfm_cell_pmd_desc,
++	.pmc_desc = pfm_cell_pmc_desc,
++	.num_pmc_entries = PFM_PM_NUM_PMCS,
++	.num_pmd_entries = PFM_PM_NUM_PMDS,
++	.probe_pmu  = pfm_cell_probe_pmu,
++	.arch_info = &pfm_cell_pmu_info,
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++};
++
++/**
++ * pfm_cell_platform_probe
++ *
++ * If we're on a system without the firmware rtas call available, set up the
++ * PMC write-checker for all the pmX_event control registers.
++ **/
++static void pfm_cell_platform_probe(void)
++{
++	if (machine_is(celleb)) {
++		int cnum;
++		pfm_cell_pmu_conf.pmc_write_check = pfm_cell_pmc_check;
++		for (cnum = NR_CTRS; cnum < (NR_CTRS * 2); cnum++)
++			pfm_cell_pmc_desc[cnum].type |= PFM_REG_WC;
++	}
++}
++
++static int __init pfm_cell_pmu_init_module(void)
++{
++	pfm_cell_platform_probe();
++	return pfm_pmu_register(&pfm_cell_pmu_conf);
++}
++
++static void __exit pfm_cell_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_cell_pmu_conf);
++}
++
++module_init(pfm_cell_pmu_init_module);
++module_exit(pfm_cell_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/powerpc/perfmon/perfmon_power4.c
+@@ -0,0 +1,293 @@
++/*
++ * This file contains the POWER4 PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Copyright (c) 2007, IBM Corporation.
++ *
++ * Based on a simple modification of perfmon_power5.c for POWER4 by
++ * Corey Ashford <cjashfor@us.ibm.com>.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++MODULE_AUTHOR("Corey Ashford <cjashfor@us.ibm.com>");
++MODULE_DESCRIPTION("POWER4 PMU description table");
++MODULE_LICENSE("GPL");
++
++static struct pfm_regmap_desc pfm_power4_pmc_desc[]={
++/* mmcr0 */ PMC_D(PFM_REG_I, "MMCR0", MMCR0_FC, 0, 0, SPRN_MMCR0),
++/* mmcr1 */ PMC_D(PFM_REG_I, "MMCR1", 0, 0, 0, SPRN_MMCR1),
++/* mmcra */ PMC_D(PFM_REG_I, "MMCRA", 0, 0, 0, SPRN_MMCRA)
++};
++#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_power4_pmc_desc)
++
++/* The TB and PURR registers are read-only. Also, note that the TB register
++ * actually consists of both the 32-bit SPRN_TBRU and SPRN_TBRL registers.
++ * For Perfmon2's purposes, we'll treat it as a single 64-bit register.
++ */
++static struct pfm_regmap_desc pfm_power4_pmd_desc[]={
++/* tb    */ PMD_D((PFM_REG_I|PFM_REG_RO), "TB", SPRN_TBRL),
++/* pmd1  */ PMD_D(PFM_REG_C, "PMC1", SPRN_PMC1),
++/* pmd2  */ PMD_D(PFM_REG_C, "PMC2", SPRN_PMC2),
++/* pmd3  */ PMD_D(PFM_REG_C, "PMC3", SPRN_PMC3),
++/* pmd4  */ PMD_D(PFM_REG_C, "PMC4", SPRN_PMC4),
++/* pmd5  */ PMD_D(PFM_REG_C, "PMC5", SPRN_PMC5),
++/* pmd6  */ PMD_D(PFM_REG_C, "PMC6", SPRN_PMC6),
++/* pmd7  */ PMD_D(PFM_REG_C, "PMC7", SPRN_PMC7),
++/* pmd8  */ PMD_D(PFM_REG_C, "PMC8", SPRN_PMC8)
++};
++#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_power4_pmd_desc)
++
++static int pfm_power4_probe_pmu(void)
++{
++	unsigned long pvr = mfspr(SPRN_PVR);
++	int ver = PVR_VER(pvr);
++
++	if ((ver == PV_POWER4) || (ver == PV_POWER4p))
++		return 0;
++
++	return -1;
++}
++
++static void pfm_power4_write_pmc(unsigned int cnum, u64 value)
++{
++	switch (pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
++	case SPRN_MMCR0:
++		mtspr(SPRN_MMCR0, value);
++		break;
++	case SPRN_MMCR1:
++		mtspr(SPRN_MMCR1, value);
++		break;
++	case SPRN_MMCRA:
++		mtspr(SPRN_MMCRA, value);
++		break;
++	default:
++		BUG();
++	}
++}
++
++static void pfm_power4_write_pmd(unsigned int cnum, u64 value)
++{
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC1:
++		mtspr(SPRN_PMC1, value);
++		break;
++	case SPRN_PMC2:
++		mtspr(SPRN_PMC2, value);
++		break;
++	case SPRN_PMC3:
++		mtspr(SPRN_PMC3, value);
++		break;
++	case SPRN_PMC4:
++		mtspr(SPRN_PMC4, value);
++		break;
++	case SPRN_PMC5:
++		mtspr(SPRN_PMC5, value);
++		break;
++	case SPRN_PMC6:
++		mtspr(SPRN_PMC6, value);
++		break;
++	case SPRN_PMC7:
++		mtspr(SPRN_PMC7, value);
++		break;
++	case SPRN_PMC8:
++		mtspr(SPRN_PMC8, value);
++		break;
++	case SPRN_TBRL:
++	case SPRN_PURR:
++		/* Ignore writes to read-only registers. */
++		break;
++	default:
++		BUG();
++	}
++}
++
++static u64 pfm_power4_read_pmd(unsigned int cnum)
++{
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC1:
++		return mfspr(SPRN_PMC1);
++	case SPRN_PMC2:
++		return mfspr(SPRN_PMC2);
++	case SPRN_PMC3:
++		return mfspr(SPRN_PMC3);
++	case SPRN_PMC4:
++		return mfspr(SPRN_PMC4);
++	case SPRN_PMC5:
++		return mfspr(SPRN_PMC5);
++	case SPRN_PMC6:
++		return mfspr(SPRN_PMC6);
++	case SPRN_PMC7:
++		return mfspr(SPRN_PMC7);
++	case SPRN_PMC8:
++		return mfspr(SPRN_PMC8);
++	case SPRN_TBRL:
++		return ((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL);
++	case SPRN_PURR:
++		if (cpu_has_feature(CPU_FTR_PURR))
++			return mfspr(SPRN_PURR);
++		else
++			return 0;
++	default:
++		BUG();
++	}
++}
++
++/**
++ * pfm_power4_enable_counters
++ *
++ * Just need to load the current values into the control registers.
++ **/
++static void pfm_power4_enable_counters(struct pfm_context *ctx,
++				       struct pfm_event_set *set)
++{
++	unsigned int i, max_pmc;
++
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++
++	for (i = 0; i < max_pmc; i++)
++		if (test_bit(i, set->used_pmcs))
++			pfm_power4_write_pmc(i, set->pmcs[i]);
++}
++
++/**
++ * pfm_power4_disable_counters
++ *
++ * Just need to zero all the control registers.
++ **/
++static void pfm_power4_disable_counters(struct pfm_context *ctx,
++					struct pfm_event_set *set)
++{
++	unsigned int i, max;
++
++	max = pfm_pmu_conf->regs.max_pmc;
++
++	for (i = 0; i < max; i++)
++		if (test_bit(i, set->used_pmcs))
++			pfm_power4_write_pmc(i, 0);
++}
++
++/**
++ * pfm_power4_get_ovfl_pmds
++ *
++ * Determine which counters in this set have overflowed and fill in the
++ * set->povfl_pmds mask and set->npend_ovfls count.
++ **/
++static void pfm_power4_get_ovfl_pmds(struct pfm_context *ctx,
++				     struct pfm_event_set *set)
++{
++	unsigned int i;
++	unsigned int max_pmd = pfm_pmu_conf->regs.max_intr_pmd;
++	u64 *used_pmds = set->used_pmds;
++	u64 *cntr_pmds = pfm_pmu_conf->regs.cnt_pmds;
++	u64 width_mask = 1 << pfm_pmu_conf->counter_width;
++	u64 new_val, mask[PFM_PMD_BV];
++
++	bitmap_and(cast_ulp(mask), cast_ulp(cntr_pmds),
++		   cast_ulp(used_pmds), max_pmd);
++
++	for (i = 0; i < max_pmd; i++) {
++		if (test_bit(i, mask)) {
++			new_val = pfm_power4_read_pmd(i);
++			if (new_val & width_mask) {
++				set_bit(i, set->povfl_pmds);
++				set->npend_ovfls++;
++			}
++		}
++	}
++}
++
++static void pfm_power4_irq_handler(struct pt_regs *regs,
++				   struct pfm_context *ctx)
++{
++	u32 mmcr0;
++	u64 mmcra;
++
++	/* Disable the counters (set the freeze bit) to not polute
++	 * the counts.
++	 */
++	mmcr0 = mfspr(SPRN_MMCR0);
++	mtspr(SPRN_MMCR0, (mmcr0 | MMCR0_FC));
++	mmcra = mfspr(SPRN_MMCRA);
++
++	/* Set the PMM bit (see comment below). */
++	mtmsrd(mfmsr() | MSR_PMM);
++
++	pfm_interrupt_handler(instruction_pointer(regs), regs);
++
++	mmcr0 = mfspr(SPRN_MMCR0);
++	/* Reset the perfmon trigger. */
++	mmcr0 |= MMCR0_PMXE;
++
++	/*
++	 * We must clear the PMAO bit on some (GQ) chips. Just do it
++	 * all the time.
++	 */
++	mmcr0 &= ~MMCR0_PMAO;
++
++	/* Clear the appropriate bits in the MMCRA. */
++	mmcra &= POWER6_MMCRA_THRM | POWER6_MMCRA_OTHER;
++	mtspr(SPRN_MMCRA, mmcra);
++
++	/*
++	 * Now clear the freeze bit, counting will not start until we
++	 * rfid from this exception, because only at that point will
++	 * the PMM bit be cleared.
++	 */
++	mmcr0 &= ~MMCR0_FC;
++	mtspr(SPRN_MMCR0, mmcr0);
++}
++
++struct pfm_arch_pmu_info pfm_power4_pmu_info = {
++	.pmu_style        = PFM_POWERPC_PMU_POWER4,
++	.write_pmc        = pfm_power4_write_pmc,
++	.write_pmd        = pfm_power4_write_pmd,
++	.read_pmd         = pfm_power4_read_pmd,
++	.irq_handler      = pfm_power4_irq_handler,
++	.get_ovfl_pmds    = pfm_power4_get_ovfl_pmds,
++	.enable_counters  = pfm_power4_enable_counters,
++	.disable_counters = pfm_power4_disable_counters,
++};
++
++/*
++ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
++ */
++static struct pfm_pmu_config pfm_power4_pmu_conf = {
++	.pmu_name = "POWER4",
++	.counter_width = 31,
++	.pmd_desc = pfm_power4_pmd_desc,
++	.pmc_desc = pfm_power4_pmc_desc,
++	.num_pmc_entries = PFM_PM_NUM_PMCS,
++	.num_pmd_entries = PFM_PM_NUM_PMDS,
++	.probe_pmu  = pfm_power4_probe_pmu,
++	.arch_info = &pfm_power4_pmu_info,
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE
++};
++	
++static int __init pfm_power4_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_power4_pmu_conf);
++}
++
++static void __exit pfm_power4_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_power4_pmu_conf);
++}
++
++module_init(pfm_power4_pmu_init_module);
++module_exit(pfm_power4_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/powerpc/perfmon/perfmon_power5.c
+@@ -0,0 +1,292 @@
++/*
++ * This file contains the POWER5 PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Copyright (c) 2005 David Gibson, IBM Corporation.
++ *
++ * Based on perfmon_p6.c:
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++MODULE_AUTHOR("David Gibson <dwg@au1.ibm.com>");
++MODULE_DESCRIPTION("POWER5 PMU description table");
++MODULE_LICENSE("GPL");
++
++static struct pfm_regmap_desc pfm_power5_pmc_desc[]={
++/* mmcr0 */ PMC_D(PFM_REG_I, "MMCR0", MMCR0_FC, 0, 0, SPRN_MMCR0),
++/* mmcr1 */ PMC_D(PFM_REG_I, "MMCR1", 0, 0, 0, SPRN_MMCR1),
++/* mmcra */ PMC_D(PFM_REG_I, "MMCRA", 0, 0, 0, SPRN_MMCRA)
++};
++#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_power5_pmc_desc)
++
++/* The TB and PURR registers are read-only. Also, note that the TB register
++ * actually consists of both the 32-bit SPRN_TBRU and SPRN_TBRL registers.
++ * For Perfmon2's purposes, we'll treat it as a single 64-bit register.
++ */
++static struct pfm_regmap_desc pfm_power5_pmd_desc[]={
++/* tb    */ PMD_D((PFM_REG_I|PFM_REG_RO), "TB", SPRN_TBRL),
++/* pmd1  */ PMD_D(PFM_REG_C, "PMC1", SPRN_PMC1),
++/* pmd2  */ PMD_D(PFM_REG_C, "PMC2", SPRN_PMC2),
++/* pmd3  */ PMD_D(PFM_REG_C, "PMC3", SPRN_PMC3),
++/* pmd4  */ PMD_D(PFM_REG_C, "PMC4", SPRN_PMC4),
++/* pmd5  */ PMD_D(PFM_REG_C, "PMC5", SPRN_PMC5),
++/* pmd6  */ PMD_D(PFM_REG_C, "PMC6", SPRN_PMC6),
++/* purr  */ PMD_D((PFM_REG_I|PFM_REG_RO), "PURR", SPRN_PURR),
++};
++#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_power5_pmd_desc)
++
++static int pfm_power5_probe_pmu(void)
++{
++	unsigned long pvr = mfspr(SPRN_PVR);
++
++	if (PVR_VER(pvr) != PV_POWER5)
++		return -1;
++
++	return 0;
++}
++
++static void pfm_power5_write_pmc(unsigned int cnum, u64 value)
++{
++	switch (pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
++	case SPRN_MMCR0:
++		mtspr(SPRN_MMCR0, value);
++		break;
++	case SPRN_MMCR1:
++		mtspr(SPRN_MMCR1, value);
++		break;
++	case SPRN_MMCRA:
++		mtspr(SPRN_MMCRA, value);
++		break;
++	default:
++		BUG();
++	}
++}
++
++static void pfm_power5_write_pmd(unsigned int cnum, u64 value)
++{
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC1:
++		mtspr(SPRN_PMC1, value);
++		break;
++	case SPRN_PMC2:
++		mtspr(SPRN_PMC2, value);
++		break;
++	case SPRN_PMC3:
++		mtspr(SPRN_PMC3, value);
++		break;
++	case SPRN_PMC4:
++		mtspr(SPRN_PMC4, value);
++		break;
++	case SPRN_PMC5:
++		mtspr(SPRN_PMC5, value);
++		break;
++	case SPRN_PMC6:
++		mtspr(SPRN_PMC6, value);
++		break;
++	case SPRN_PMC7:
++		mtspr(SPRN_PMC7, value);
++		break;
++	case SPRN_PMC8:
++		mtspr(SPRN_PMC8, value);
++		break;
++	case SPRN_TBRL:
++	case SPRN_PURR:
++		/* Ignore writes to read-only registers. */
++		break;
++	default:
++		BUG();
++	}
++}
++
++static u64 pfm_power5_read_pmd(unsigned int cnum)
++{
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC1:
++		return mfspr(SPRN_PMC1);
++	case SPRN_PMC2:
++		return mfspr(SPRN_PMC2);
++	case SPRN_PMC3:
++		return mfspr(SPRN_PMC3);
++	case SPRN_PMC4:
++		return mfspr(SPRN_PMC4);
++	case SPRN_PMC5:
++		return mfspr(SPRN_PMC5);
++	case SPRN_PMC6:
++		return mfspr(SPRN_PMC6);
++	case SPRN_PMC7:
++		return mfspr(SPRN_PMC7);
++	case SPRN_PMC8:
++		return mfspr(SPRN_PMC8);
++	case SPRN_TBRL:
++		return ((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL);
++	case SPRN_PURR:
++		if (cpu_has_feature(CPU_FTR_PURR))
++			return mfspr(SPRN_PURR);
++		else
++			return 0;
++	default:
++		BUG();
++	}
++}
++
++/**
++ * pfm_power5_enable_counters
++ *
++ * Just need to load the current values into the control registers.
++ **/
++static void pfm_power5_enable_counters(struct pfm_context *ctx,
++				       struct pfm_event_set *set)
++{
++	unsigned int i, max_pmc;
++
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++
++	for (i = 0; i < max_pmc; i++)
++		if (test_bit(i, set->used_pmcs))
++			pfm_power5_write_pmc(i, set->pmcs[i]);
++}
++
++/**
++ * pfm_power5_disable_counters
++ *
++ * Just need to zero all the control registers.
++ **/
++static void pfm_power5_disable_counters(struct pfm_context *ctx,
++					struct pfm_event_set *set)
++{
++	unsigned int i, max;
++
++	max = pfm_pmu_conf->regs.max_pmc;
++
++	for (i = 0; i < max; i++)
++		if (test_bit(i, set->used_pmcs))
++			pfm_power5_write_pmc(i, 0);
++}
++
++/**
++ * pfm_power5_get_ovfl_pmds
++ *
++ * Determine which counters in this set have overflowed and fill in the
++ * set->povfl_pmds mask and set->npend_ovfls count.
++ **/
++static void pfm_power5_get_ovfl_pmds(struct pfm_context *ctx,
++				     struct pfm_event_set *set)
++{
++	unsigned int i;
++	unsigned int max = pfm_pmu_conf->regs.max_intr_pmd;
++	u64 *used_pmds = set->used_pmds;
++	u64 *intr_pmds = pfm_pmu_conf->regs.intr_pmds;
++	u64 width_mask = 1 << pfm_pmu_conf->counter_width;
++	u64 new_val, mask[PFM_PMD_BV];
++
++	bitmap_and(cast_ulp(mask), cast_ulp(intr_pmds),
++		   cast_ulp(used_pmds), max);
++
++	for (i = 0; i < max; i++) {
++		if (test_bit(i, mask)) {
++			new_val = pfm_power5_read_pmd(i);
++			if (new_val & width_mask) {
++				set_bit(i, set->povfl_pmds);
++				set->npend_ovfls++;
++			}
++		}
++	}
++}
++
++static void pfm_power5_irq_handler(struct pt_regs *regs,
++				   struct pfm_context *ctx)
++{
++	u32 mmcr0;
++	u64 mmcra;
++
++	/* Disable the counters (set the freeze bit) to not polute
++	 * the counts.
++	 */
++	mmcr0 = mfspr(SPRN_MMCR0);
++	mtspr(SPRN_MMCR0, (mmcr0 | MMCR0_FC));
++	mmcra = mfspr(SPRN_MMCRA);
++
++	/* Set the PMM bit (see comment below). */
++	mtmsrd(mfmsr() | MSR_PMM);
++
++	pfm_interrupt_handler(instruction_pointer(regs), regs);
++
++	mmcr0 = mfspr(SPRN_MMCR0);
++	/* Reset the perfmon trigger. */
++	mmcr0 |= MMCR0_PMXE;
++
++	/*
++	 * We must clear the PMAO bit on some (GQ) chips. Just do it
++	 * all the time.
++	 */
++	mmcr0 &= ~MMCR0_PMAO;
++
++	/* Clear the appropriate bits in the MMCRA. */
++	mmcra &= POWER6_MMCRA_THRM | POWER6_MMCRA_OTHER;
++	mtspr(SPRN_MMCRA, mmcra);
++
++	/*
++	 * Now clear the freeze bit, counting will not start until we
++	 * rfid from this exception, because only at that point will
++	 * the PMM bit be cleared.
++	 */
++	mmcr0 &= ~MMCR0_FC;
++	mtspr(SPRN_MMCR0, mmcr0);
++}
++
++struct pfm_arch_pmu_info pfm_power5_pmu_info = {
++	.pmu_style        = PFM_POWERPC_PMU_POWER5,
++	.write_pmc        = pfm_power5_write_pmc,
++	.write_pmd        = pfm_power5_write_pmd,
++	.read_pmd         = pfm_power5_read_pmd,
++	.irq_handler      = pfm_power5_irq_handler,
++	.get_ovfl_pmds    = pfm_power5_get_ovfl_pmds,
++	.enable_counters  = pfm_power5_enable_counters,
++	.disable_counters = pfm_power5_disable_counters,
++};
++
++/*
++ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
++ */
++static struct pfm_pmu_config pfm_power5_pmu_conf = {
++	.pmu_name = "POWER5",
++	.counter_width = 31,
++	.pmd_desc = pfm_power5_pmd_desc,
++	.pmc_desc = pfm_power5_pmc_desc,
++	.num_pmc_entries = PFM_PM_NUM_PMCS,
++	.num_pmd_entries = PFM_PM_NUM_PMDS,
++	.probe_pmu  = pfm_power5_probe_pmu,
++	.arch_info = &pfm_power5_pmu_info,
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE
++};
++
++static int __init pfm_power5_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_power5_pmu_conf);
++}
++
++static void __exit pfm_power5_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_power5_pmu_conf);
++}
++
++module_init(pfm_power5_pmu_init_module);
++module_exit(pfm_power5_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/powerpc/perfmon/perfmon_power6.c
+@@ -0,0 +1,495 @@
++/*
++ * This file contains the POWER6 PMU register description tables
++ * and pmc checker used by perfmon.c.  
++ *
++ * Copyright (c) 2007, IBM Corporation
++ *
++ * Based on perfmon_power5.c, and written by Carl Love <carll@us.ibm.com>
++ * and Kevin Corry <kevcorry@us.ibm.com>.  Some fixes and refinement by
++ * Corey Ashford <cjashfor@us.ibm.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++MODULE_AUTHOR("Corey Ashford <cjashfor@us.ibm.com>");
++MODULE_DESCRIPTION("POWER6 PMU description table");
++MODULE_LICENSE("GPL");
++
++static struct pfm_regmap_desc pfm_power6_pmc_desc[]={
++/* mmcr0 */ PMC_D(PFM_REG_I, "MMCR0", MMCR0_FC, 0, 0, SPRN_MMCR0),
++/* mmcr1 */ PMC_D(PFM_REG_I, "MMCR1", 0, 0, 0, SPRN_MMCR1),
++/* mmcra */ PMC_D(PFM_REG_I, "MMCRA", 0, 0, 0, SPRN_MMCRA)
++};
++#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_power6_pmc_desc)
++#define PFM_DELTA_TB    10000   /* Not a real registers */
++#define PFM_DELTA_PURR  10001
++
++/*
++ * counters wrap to zero at transition from 2^32-1 to 2^32.  Note:
++ * interrupt generated at transition from 2^31-1 to 2^31
++ */
++#define OVERFLOW_VALUE    0x100000000UL
++
++/* The TB and PURR registers are read-only. Also, note that the TB register
++ * actually consists of both the 32-bit SPRN_TBRU and SPRN_TBRL registers.
++ * For Perfmon2's purposes, we'll treat it as a single 64-bit register.
++ */
++static struct pfm_regmap_desc pfm_power6_pmd_desc[]={
++	/* On POWER 6 PMC5 and PMC6 are not writable, they do not
++	 * generate interrupts, and do not qualify their counts 
++	 * based on problem mode, supervisor mode or hypervisor mode.
++	 * These two counters are implemented as virtual counters
++	 * to make the appear to work like the other counters.  A 
++	 * kernel timer is used sample the real PMC5 and PMC6 and 
++	 * update the virtual counters.
++	 */
++/* tb    */ PMD_D((PFM_REG_I|PFM_REG_RO), "TB", SPRN_TBRL),
++/* pmd1  */ PMD_D(PFM_REG_C, "PMC1", SPRN_PMC1),
++/* pmd2  */ PMD_D(PFM_REG_C, "PMC2", SPRN_PMC2),
++/* pmd3  */ PMD_D(PFM_REG_C, "PMC3", SPRN_PMC3),
++/* pmd4  */ PMD_D(PFM_REG_C, "PMC4", SPRN_PMC4),
++/* pmd5  */ PMD_D((PFM_REG_I|PFM_REG_V), "PMC5", SPRN_PMC5),
++/* pmd6  */ PMD_D((PFM_REG_I|PFM_REG_V), "PMC6", SPRN_PMC6),
++/* purr  */ PMD_D((PFM_REG_I|PFM_REG_RO), "PURR", SPRN_PURR),
++/* delta purr */ PMD_D((PFM_REG_I|PFM_REG_V), "DELTA_TB", PFM_DELTA_TB),
++/* delta tb   */ PMD_D((PFM_REG_I|PFM_REG_V), "DELTA_PURR", PFM_DELTA_PURR),
++};
++
++#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_power6_pmd_desc)
++
++u32 pmc5_start_save[NR_CPUS];
++u32 pmc6_start_save[NR_CPUS];
++
++static struct timer_list pmc5_6_update[NR_CPUS];
++u64 enable_cntrs_cnt;
++u64 disable_cntrs_cnt;
++u64 call_delta;
++u64 pm5_6_interrupt;
++u64 pm1_4_interrupt;
++/* need ctx_arch for kernel timer.  Can't get it in context of the kernel
++ * timer.
++ */
++struct pfm_arch_context *pmc5_6_ctx_arch[NR_CPUS];
++long int update_time;
++
++static void delta(int cpu_num, struct pfm_arch_context *ctx_arch) {
++	u32 tmp5, tmp6;
++
++	call_delta++;
++
++	tmp5 = (u32) mfspr(SPRN_PMC5);
++	tmp6 = (u32) mfspr(SPRN_PMC6);
++
++	/*
++	 * The following difference calculation relies on 32-bit modular
++	 * arithmetic for the deltas to come out correct (especially in the
++	 * presence of a 32-bit counter wrap).
++	 */
++	ctx_arch->powergs_pmc5 += (u64)(tmp5 - pmc5_start_save[cpu_num]);
++	ctx_arch->powergs_pmc6 += (u64)(tmp6 - pmc6_start_save[cpu_num]);
++
++	pmc5_start_save[cpu_num] = tmp5;
++	pmc6_start_save[cpu_num] = tmp6;
++
++	return;
++}
++
++
++static void pmc5_6_updater(unsigned long cpu_num)
++{ 
++	/* update the virtual pmd 5 and pmd 6 counters */
++
++	delta(cpu_num, pmc5_6_ctx_arch[cpu_num]);
++	mod_timer(&pmc5_6_update[cpu_num], jiffies + update_time);
++}
++
++
++static int pfm_power6_probe_pmu(void)
++{
++	unsigned long pvr = mfspr(SPRN_PVR);
++
++	if (PVR_VER(pvr) != PV_POWER6)
++		return -1;
++	return 0;
++}
++
++static void pfm_power6_write_pmc(unsigned int cnum, u64 value)
++{
++	switch (pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
++	case SPRN_MMCR0:
++		mtspr(SPRN_MMCR0, value);
++		break;
++	case SPRN_MMCR1:
++		mtspr(SPRN_MMCR1, value);
++		break;
++	case SPRN_MMCRA:
++		mtspr(SPRN_MMCRA, value);
++		break;
++	default:
++		BUG();
++	}
++}
++
++static void pfm_power6_write_pmd(unsigned int cnum, u64 value)
++{
++	/* On POWER 6 PMC5 and PMC6 are implemented as 
++	 * virtual counters.  See comment in pfm_power6_pmd_desc 
++	 * definition.
++	 */
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC1:
++		mtspr(SPRN_PMC1, value);
++		break;
++	case SPRN_PMC2:
++		mtspr(SPRN_PMC2, value);
++		break;
++	case SPRN_PMC3:
++		mtspr(SPRN_PMC3, value);
++		break;
++	case SPRN_PMC4:
++		mtspr(SPRN_PMC4, value);
++		break;
++	case SPRN_TBRL:
++	case SPRN_PURR:
++		/* Ignore writes to read-only registers. */
++		break;
++	default:
++		BUG();
++	}
++}
++
++static u64 pfm_power6_sread(struct pfm_context *ctx, unsigned int cnum)
++{
++	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
++	int cpu_num = smp_processor_id();
++
++	/* On POWER 6 PMC5 and PMC6 are implemented as 
++	 * virtual counters.  See comment in pfm_power6_pmd_desc 
++	 * definition.
++	 */
++
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC5:
++		return ctx_arch->powergs_pmc5 + (u64)((u32)mfspr(SPRN_PMC5) - pmc5_start_save[cpu_num]);
++		break;
++
++	case SPRN_PMC6:
++		return ctx_arch->powergs_pmc6 + (u64)((u32)mfspr(SPRN_PMC6) - pmc6_start_save[cpu_num]);
++		break;
++
++	case PFM_DELTA_TB:
++		return ctx_arch->delta_tb + 
++			(((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL))
++			 - ctx_arch->delta_tb_start;
++		break;
++
++	case PFM_DELTA_PURR:
++		return ctx_arch->delta_purr + mfspr(SPRN_PURR) 
++			- ctx_arch->delta_purr_start;
++		break;
++
++	default:
++		BUG();
++	}
++}
++
++void pfm_power6_swrite(struct pfm_context *ctx, unsigned int cnum,
++	u64 val)
++{
++	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
++	int cpu_num = smp_processor_id();
++
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC5:
++		pmc5_start_save[cpu_num] = mfspr(SPRN_PMC5);
++		ctx_arch->powergs_pmc5 = val;
++		break;
++
++	case SPRN_PMC6:
++		pmc6_start_save[cpu_num] = mfspr(SPRN_PMC6);
++		ctx_arch->powergs_pmc6 = val;
++		break;
++
++	case PFM_DELTA_TB:
++		ctx_arch->delta_tb_start = 
++			(((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL));
++		ctx_arch->delta_tb = val;
++		break;
++		
++	case PFM_DELTA_PURR:
++		ctx_arch->delta_purr_start = mfspr(SPRN_PURR);
++		ctx_arch->delta_purr = val;
++		break;
++
++	default:
++		BUG();
++	}
++}
++
++static u64 pfm_power6_read_pmd(unsigned int cnum)
++{
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC1:
++		return mfspr(SPRN_PMC1);
++	case SPRN_PMC2:
++		return mfspr(SPRN_PMC2);
++	case SPRN_PMC3:
++		return mfspr(SPRN_PMC3);
++	case SPRN_PMC4:
++		return mfspr(SPRN_PMC4);
++	case SPRN_TBRL:
++		return ((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL);
++	case SPRN_PURR:
++		if (cpu_has_feature(CPU_FTR_PURR))
++			return mfspr(SPRN_PURR);
++		else
++			return 0;
++	default:
++		BUG();
++	}
++}
++
++/**
++ * pfm_power6_enable_counters
++ *
++ * Just need to load the current values into the control registers.
++ **/
++static void pfm_power6_enable_counters(struct pfm_context *ctx,
++				       struct pfm_event_set *set)
++{
++
++	unsigned int i, max_pmc;
++	int cpu_num = smp_processor_id();
++	struct pfm_arch_context *ctx_arch;
++	
++	enable_cntrs_cnt++;
++
++	/* need the ctx passed down to the routine */
++	ctx_arch = pfm_ctx_arch(ctx);
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++
++	for (i = 0; i < max_pmc; i++)
++		if (test_bit(i, set->used_pmcs))
++			pfm_power6_write_pmc(i, set->pmcs[i]);
++
++	/* save current free running HW event count */
++	pmc5_start_save[cpu_num] = mfspr(SPRN_PMC5);
++	pmc6_start_save[cpu_num] = mfspr(SPRN_PMC6);
++
++	ctx_arch->delta_purr_start = mfspr(SPRN_PURR);
++
++	if (cpu_has_feature(CPU_FTR_PURR))
++		ctx_arch->delta_tb_start =
++			((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL);
++	else
++		ctx_arch->delta_tb_start = 0;
++
++	/* Start kernel timer for this cpu to periodically update
++	 * the virtual counters.
++	 */
++	init_timer(&pmc5_6_update[cpu_num]);
++	pmc5_6_update[cpu_num].function = pmc5_6_updater;
++	pmc5_6_update[cpu_num].data = (unsigned long) cpu_num;
++	pmc5_6_update[cpu_num].expires = jiffies + update_time;
++	/* context for this timer, timer will be removed if context
++	 * is switched because the counters will be stopped first.
++	 * NEEDS WORK, I think this is all ok, a little concerned about a 
++	 * race between the kernel timer going off right as the counters
++	 * are being stopped and the context switching.  Need to think
++	 * about this.
++	 */
++	pmc5_6_ctx_arch[cpu_num] = ctx_arch;  
++	add_timer(&pmc5_6_update[cpu_num]);
++}
++
++/**
++ * pfm_power6_disable_counters
++ *
++ * Just need to zero all the control registers.
++ **/
++static void pfm_power6_disable_counters(struct pfm_context *ctx,
++					struct pfm_event_set *set)
++{
++	unsigned int i, max;
++	struct pfm_arch_context *ctx_arch;
++	int cpu_num = smp_processor_id();
++
++	disable_cntrs_cnt++;
++	ctx_arch = pfm_ctx_arch(ctx);
++	max = pfm_pmu_conf->regs.max_pmc;
++
++	/* delete kernel update timer */
++	del_timer_sync(&pmc5_6_update[cpu_num]);
++
++	for (i = 0; i < max; i++)
++		if (test_bit(i, set->used_pmcs))
++			pfm_power6_write_pmc(i, 0);
++
++	/* Update the virtual pmd 5 and 6 counters from the free running
++	 * HW counters
++	 */
++	delta(cpu_num, ctx_arch);
++
++	ctx_arch->delta_tb += 
++		(((u64)mfspr(SPRN_TBRU) << 32) | mfspr(SPRN_TBRL))
++		 - ctx_arch->delta_tb_start;
++
++	ctx_arch->delta_purr += mfspr(SPRN_PURR) 
++		- ctx_arch->delta_purr_start;
++}
++
++/**
++ * pfm_power6_get_ovfl_pmds
++ *
++ * Determine which counters in this set have overflowed and fill in the
++ * set->povfl_pmds mask and set->npend_ovfls count.
++ **/
++static void pfm_power6_get_ovfl_pmds(struct pfm_context *ctx,
++				     struct pfm_event_set *set)
++{
++	unsigned int i;
++	unsigned int first_intr_pmd = pfm_pmu_conf->regs.first_intr_pmd;
++	unsigned int max_intr_pmd = pfm_pmu_conf->regs.max_intr_pmd;
++	u64 *used_pmds = set->used_pmds;
++	u64 *cntr_pmds = pfm_pmu_conf->regs.cnt_pmds;
++	u64 width_mask = 1 << pfm_pmu_conf->counter_width;
++	u64 new_val, mask[PFM_PMD_BV];
++
++	bitmap_and(cast_ulp(mask), cast_ulp(cntr_pmds), cast_ulp(used_pmds), max_intr_pmd);
++
++	/* max_intr_pmd is actually the last interrupting pmd register + 1 */
++	for (i = first_intr_pmd; i < max_intr_pmd; i++) {
++		if (test_bit(i, mask)) {
++			new_val = pfm_power6_read_pmd(i);
++			if (new_val & width_mask) {
++				set_bit(i, set->povfl_pmds);
++				set->npend_ovfls++;
++			}
++		}
++	}
++}
++
++static void pfm_power6_irq_handler(struct pt_regs *regs,
++				   struct pfm_context *ctx)
++{
++	u32 mmcr0;
++	u64 mmcra;
++
++	/* Disable the counters (set the freeze bit) to not polute
++	 * the counts.
++	 */
++	mmcr0 = mfspr(SPRN_MMCR0);
++	mtspr(SPRN_MMCR0, (mmcr0 | MMCR0_FC));
++	mmcra = mfspr(SPRN_MMCRA);
++
++	/* Set the PMM bit (see comment below). */
++	mtmsrd(mfmsr() | MSR_PMM);
++
++	pm1_4_interrupt++;
++
++	pfm_interrupt_handler(instruction_pointer(regs), regs);
++
++	mmcr0 = mfspr(SPRN_MMCR0);
++	/* Reset the perfmon trigger. */
++	mmcr0 |= MMCR0_PMXE;
++
++	/*
++	 * We must clear the PMAO bit on some (GQ) chips. Just do it
++	 * all the time.
++	 */
++	mmcr0 &= ~MMCR0_PMAO;
++
++	/* Clear the appropriate bits in the MMCRA. */
++	mmcra &= POWER6_MMCRA_THRM | POWER6_MMCRA_OTHER;
++	mtspr(SPRN_MMCRA, mmcra);
++
++	/*
++	 * Now clear the freeze bit, counting will not start until we
++	 * rfid from this exception, because only at that point will
++	 * the PMM bit be cleared.
++	 */
++	mmcr0 &= ~MMCR0_FC;
++	mtspr(SPRN_MMCR0, mmcr0);
++}
++
++struct pfm_arch_pmu_info pfm_power6_pmu_info = {
++	.pmu_style        = PFM_POWERPC_PMU_POWER6,
++	.write_pmc        = pfm_power6_write_pmc,
++	.write_pmd        = pfm_power6_write_pmd,
++	.read_pmd         = pfm_power6_read_pmd,
++	.irq_handler      = pfm_power6_irq_handler,
++	.get_ovfl_pmds    = pfm_power6_get_ovfl_pmds,
++	.enable_counters  = pfm_power6_enable_counters,
++	.disable_counters = pfm_power6_disable_counters,
++};
++
++/*
++ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
++ */
++static struct pfm_pmu_config pfm_power6_pmu_conf = {
++	.pmu_name = "POWER6",
++	.counter_width = 31,
++	.pmd_desc = pfm_power6_pmd_desc,
++	.pmc_desc = pfm_power6_pmc_desc,
++	.num_pmc_entries = PFM_PM_NUM_PMCS,
++	.num_pmd_entries = PFM_PM_NUM_PMDS,
++	.probe_pmu  = pfm_power6_probe_pmu,
++	.arch_info = &pfm_power6_pmu_info,
++	.pmd_sread = pfm_power6_sread,
++	.pmd_swrite = pfm_power6_swrite,
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE
++};
++	
++static int __init pfm_power6_pmu_init_module(void)
++{
++	int ret;
++	disable_cntrs_cnt=0;
++	enable_cntrs_cnt=0;
++	call_delta=0;
++	pm5_6_interrupt=0;
++	pm1_4_interrupt=0;
++
++	/* calculate the time for updating counters 5 and 6 */
++
++	/*
++	 * MAX_EVENT_RATE assumes a max instruction issue rate of 2
++	 * instructions per clock cycle.  Experience shows that this factor
++	 * of 2 is more than adequate.
++	 */
++
++# define MAX_EVENT_RATE (ppc_proc_freq * 2)
++
++	/*
++	 * Calculate the time, in jiffies, it takes for event counter 5 or
++	 * 6 to completely wrap when counting at the max event rate, and
++	 * then figure on sampling at twice that rate.
++	 */
++	update_time = (((unsigned long)HZ * OVERFLOW_VALUE)
++		       / ((unsigned long)MAX_EVENT_RATE)) / 2;
++
++	ret =  pfm_pmu_register(&pfm_power6_pmu_conf);
++	return ret;
++}
++
++static void __exit pfm_power6_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_power6_pmu_conf);
++}
++
++module_init(pfm_power6_pmu_init_module);
++module_exit(pfm_power6_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/powerpc/perfmon/perfmon_ppc32.c
+@@ -0,0 +1,340 @@
++/*
++ * This file contains the PPC32 PMU register description tables
++ * and pmc checker used by perfmon.c.
++ *
++ * Philip Mucci, mucci@cs.utk.edu
++ *
++ * Based on code from:
++ * Copyright (c) 2005 David Gibson, IBM Corporation.
++ *
++ * Based on perfmon_p6.c:
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <asm/reg.h>
++
++MODULE_AUTHOR("Philip Mucci <mucci@cs.utk.edu>");
++MODULE_DESCRIPTION("PPC32 PMU description table");
++MODULE_LICENSE("GPL");
++
++static struct pfm_pmu_config pfm_ppc32_pmu_conf;
++
++static struct pfm_regmap_desc pfm_ppc32_pmc_desc[] = {
++/* mmcr0 */ PMC_D(PFM_REG_I, "MMCR0", 0x0, 0, 0, SPRN_MMCR0),
++/* mmcr1 */ PMC_D(PFM_REG_I, "MMCR1", 0x0, 0, 0, SPRN_MMCR1),
++/* mmcr2 */ PMC_D(PFM_REG_I, "MMCR2", 0x0, 0, 0, SPRN_MMCR2),
++};
++#define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_ppc32_pmc_desc)
++
++static struct pfm_regmap_desc pfm_ppc32_pmd_desc[] = {
++/* pmd0  */ PMD_D(PFM_REG_C, "PMC1", SPRN_PMC1),
++/* pmd1  */ PMD_D(PFM_REG_C, "PMC2", SPRN_PMC2),
++/* pmd2  */ PMD_D(PFM_REG_C, "PMC3", SPRN_PMC3),
++/* pmd3  */ PMD_D(PFM_REG_C, "PMC4", SPRN_PMC4),
++/* pmd4  */ PMD_D(PFM_REG_C, "PMC5", SPRN_PMC5),
++/* pmd5  */ PMD_D(PFM_REG_C, "PMC6", SPRN_PMC6),
++};
++#define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_ppc32_pmd_desc)
++
++static void perfmon_perf_irq(struct pt_regs *regs)
++{
++	u32 mmcr0;
++
++	/* BLATANTLY STOLEN FROM OPROFILE, then modified */
++
++	/* set the PMM bit (see comment below) */
++	mtmsr(mfmsr() | MSR_PMM);
++
++	pfm_interrupt_handler(instruction_pointer(regs), regs);
++
++	/* The freeze bit was set by the interrupt.
++	 * Clear the freeze bit, and reenable the interrupt.
++	 * The counters won't actually start until the rfi clears
++	 * the PMM bit.
++	 */
++
++	/* Unfreezes the counters on this CPU, enables the interrupt,
++	 * enables the counters to trigger the interrupt, and sets the
++	 * counters to only count when the mark bit is not set.
++	 */
++	mmcr0 = mfspr(SPRN_MMCR0);
++
++	mmcr0 &= ~(MMCR0_FC | MMCR0_FCM0);
++	mmcr0 |= (MMCR0_FCECE | MMCR0_PMC1CE | MMCR0_PMCnCE | MMCR0_PMXE);
++
++	mtspr(SPRN_MMCR0, mmcr0);
++}
++
++static int pfm_ppc32_probe_pmu(void)
++{
++	enum ppc32_pmu_type pm_type;
++	int nmmcr = 0, npmds = 0, intsok = 0, i;
++	unsigned int pvr;
++	char *str;
++
++	pvr = mfspr(SPRN_PVR);
++
++	switch (PVR_VER(pvr)) {
++	case 0x0004: /* 604 */
++		str = "PPC604";
++		pm_type = PFM_POWERPC_PMU_604;
++		nmmcr = 1;
++		npmds = 2;
++		break;
++	case 0x0009: /* 604e;  */
++	case 0x000A: /* 604ev */
++		str = "PPC604e";
++		pm_type = PFM_POWERPC_PMU_604e;
++		nmmcr = 2;
++		npmds = 4;
++		break;
++	case 0x0008: /* 750/740 */
++		str = "PPC750";
++		pm_type = PFM_POWERPC_PMU_750;
++		nmmcr = 2;
++		npmds = 4;
++		break;
++	case 0x7000: /* 750FX */
++	case 0x7001:
++		str = "PPC750";
++		pm_type = PFM_POWERPC_PMU_750;
++		nmmcr = 2;
++		npmds = 4;
++		if ((pvr & 0xFF0F) >= 0x0203)
++			intsok = 1;
++		break;
++	case 0x7002: /* 750GX */
++		str = "PPC750";
++		pm_type = PFM_POWERPC_PMU_750;
++		nmmcr = 2;
++		npmds = 4;
++		intsok = 1;
++	case 0x000C: /* 7400 */
++		str = "PPC7400";
++		pm_type = PFM_POWERPC_PMU_7400;
++		nmmcr = 3;
++		npmds = 4;
++		break;
++	case 0x800C: /* 7410 */
++		str = "PPC7410";
++		pm_type = PFM_POWERPC_PMU_7400;
++		nmmcr = 3;
++		npmds = 4;
++		if ((pvr & 0xFFFF) >= 0x01103)
++			intsok = 1;
++		break;
++	case 0x8000: /* 7451/7441 */
++	case 0x8001: /* 7455/7445 */
++	case 0x8002: /* 7457/7447 */
++	case 0x8003: /* 7447A */
++	case 0x8004: /* 7448 */
++		str = "PPC7450";
++		pm_type = PFM_POWERPC_PMU_7450;
++		nmmcr = 3; npmds = 6;
++		intsok = 1;
++		break;
++	default:
++		PFM_INFO("Unknown PVR_VER(0x%x)\n", PVR_VER(pvr));
++		return -1;
++	}
++
++	/*
++	 * deconfigure unimplemented registers
++	 */
++	for (i = npmds; i < PFM_PM_NUM_PMDS; i++)
++		pfm_ppc32_pmd_desc[i].type = PFM_REG_NA;
++
++	for (i = nmmcr; i < PFM_PM_NUM_PMCS; i++)
++		pfm_ppc32_pmc_desc[i].type = PFM_REG_NA;
++
++	/*
++	 * update PMU description structure
++	 */
++	pfm_ppc32_pmu_conf.pmu_name = str;
++	pfm_ppc32_pmu_info.pmu_style = pm_type;
++	pfm_ppc32_pmu_conf.num_pmc_entries = nmmcr;
++	pfm_ppc32_pmu_conf.num_pmd_entries = npmds;
++
++	if (intsok == 0)
++		PFM_INFO("Interrupts unlikely to work\n");
++
++	return reserve_pmc_hardware(perfmon_perf_irq);
++}
++
++static void pfm_ppc32_write_pmc(unsigned int cnum, u64 value)
++{
++	switch (pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
++	case SPRN_MMCR0:
++		mtspr(SPRN_MMCR0, value);
++		break;
++	case SPRN_MMCR1:
++		mtspr(SPRN_MMCR1, value);
++		break;
++	case SPRN_MMCR2:
++		mtspr(SPRN_MMCR2, value);
++		break;
++	default:
++		BUG();
++	}
++}
++
++static void pfm_ppc32_write_pmd(unsigned int cnum, u64 value)
++{
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC1:
++		mtspr(SPRN_PMC1, value);
++		break;
++	case SPRN_PMC2:
++		mtspr(SPRN_PMC2, value);
++		break;
++	case SPRN_PMC3:
++		mtspr(SPRN_PMC3, value);
++		break;
++	case SPRN_PMC4:
++		mtspr(SPRN_PMC4, value);
++		break;
++	case SPRN_PMC5:
++		mtspr(SPRN_PMC5, value);
++		break;
++	case SPRN_PMC6:
++		mtspr(SPRN_PMC6, value);
++		break;
++	default:
++		BUG();
++	}
++}
++
++static u64 pfm_ppc32_read_pmd(unsigned int cnum)
++{
++	switch (pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case SPRN_PMC1:
++		return mfspr(SPRN_PMC1);
++	case SPRN_PMC2:
++		return mfspr(SPRN_PMC2);
++	case SPRN_PMC3:
++		return mfspr(SPRN_PMC3);
++	case SPRN_PMC4:
++		return mfspr(SPRN_PMC4);
++	case SPRN_PMC5:
++		return mfspr(SPRN_PMC5);
++	case SPRN_PMC6:
++		return mfspr(SPRN_PMC6);
++	default:
++		BUG();
++	}
++}
++
++/**
++ * pfm_ppc32_enable_counters
++ *
++ * Just need to load the current values into the control registers.
++ **/
++static void pfm_ppc32_enable_counters(struct pfm_context *ctx,
++				      struct pfm_event_set *set)
++{
++	unsigned int i, max_pmc;
++
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++
++	for (i = 0; i < max_pmc; i++)
++		if (test_bit(i, set->used_pmcs))
++			pfm_ppc32_write_pmc(i, set->pmcs[i]);
++}
++
++/**
++ * pfm_ppc32_disable_counters
++ *
++ * Just need to zero all the control registers.
++ **/
++static void pfm_ppc32_disable_counters(struct pfm_context *ctx,
++				       struct pfm_event_set *set)
++{
++	unsigned int i, max;
++
++	max = pfm_pmu_conf->regs.max_pmc;
++
++	for (i = 0; i < max; i++)
++		if (test_bit(i, set->used_pmcs))
++			pfm_ppc32_write_pmc(ctx, 0);
++}
++
++/**
++ * pfm_ppc32_get_ovfl_pmds
++ *
++ * Determine which counters in this set have overflowed and fill in the
++ * set->povfl_pmds mask and set->npend_ovfls count.
++ **/
++static void pfm_ppc32_get_ovfl_pmds(struct pfm_context *ctx,
++				    struct pfm_event_set *set)
++{
++	unsigned int i;
++	unsigned int max_pmd = pfm_pmu_conf->regs.max_cnt_pmd;
++	u64 *used_pmds = set->used_pmds;
++	u64 *cntr_pmds = pfm_pmu_conf->regs.cnt_pmds;
++	u64 width_mask = 1 << pfm_pmu_conf->counter_width;
++	u64 new_val, mask[PFM_PMD_BV];
++
++	bitmap_and(cast_ulp(mask), cast_ulp(cntr_pmds),
++		   cast_ulp(used_pmds), max_pmd);
++
++	for (i = 0; i < max_pmd; i++) {
++		if (test_bit(i, mask)) {
++			new_val = pfm_ppc32_read_pmd(i);
++			if (new_val & width_mask) {
++				set_bit(i, set->povfl_pmds);
++				set->npend_ovfls++;
++			}
++		}
++	}
++}
++
++struct pfm_arch_pmu_info pfm_ppc32_pmu_info = {
++	.pmu_style        = PFM_POWERPC_PMU_NONE,
++	.write_pmc        = pfm_ppc32_write_pmc,
++	.write_pmd        = pfm_ppc32_write_pmd,
++	.read_pmd         = pfm_ppc32_read_pmd,
++	.get_ovfl_pmds    = pfm_ppc32_get_ovfl_pmds,
++	.enable_counters  = pfm_ppc32_enable_counters,
++	.disable_counters = pfm_ppc32_disable_counters,
++};
++
++static struct pfm_pmu_config pfm_ppc32_pmu_conf = {
++	.counter_width = 31,
++	.pmd_desc = pfm_ppc32_pmd_desc,
++	.pmc_desc = pfm_ppc32_pmc_desc,
++	.probe_pmu  = pfm_ppc32_probe_pmu,
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.version = "0.1",
++	.arch_info = &pfm_ppc32_pmu_info,
++};
++
++static int __init pfm_ppc32_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_ppc32_pmu_conf);
++}
++
++static void __exit pfm_ppc32_pmu_cleanup_module(void)
++{
++	release_pmc_hardware();
++	pfm_pmu_unregister(&pfm_ppc32_pmu_conf);
++}
++
++module_init(pfm_ppc32_pmu_init_module);
++module_exit(pfm_ppc32_pmu_cleanup_module);
+--- a/arch/powerpc/platforms/cell/cbe_regs.c
++++ b/arch/powerpc/platforms/cell/cbe_regs.c
+@@ -33,6 +33,7 @@ static struct cbe_regs_map
+ 	struct cbe_iic_regs __iomem *iic_regs;
+ 	struct cbe_mic_tm_regs __iomem *mic_tm_regs;
+ 	struct cbe_pmd_shadow_regs pmd_shadow_regs;
++	struct cbe_ppe_priv_regs __iomem *ppe_priv_regs;
+ } cbe_regs_maps[MAX_CBE];
+ static int cbe_regs_map_count;
+ 
+@@ -145,6 +146,23 @@ struct cbe_mic_tm_regs __iomem *cbe_get_
+ }
+ EXPORT_SYMBOL_GPL(cbe_get_cpu_mic_tm_regs);
+ 
++struct cbe_ppe_priv_regs __iomem *cbe_get_ppe_priv_regs(struct device_node *np)
++{
++	struct cbe_regs_map *map = cbe_find_map(np);
++	if (map == NULL)
++		return NULL;
++	return map->ppe_priv_regs;
++}
++
++struct cbe_ppe_priv_regs __iomem *cbe_get_cpu_ppe_priv_regs(int cpu)
++{
++	struct cbe_regs_map *map = cbe_thread_map[cpu].regs;
++	if (map == NULL)
++		return NULL;
++	return map->ppe_priv_regs;
++}
++EXPORT_SYMBOL_GPL(cbe_get_cpu_ppe_priv_regs);
++
+ u32 cbe_get_hw_thread_id(int cpu)
+ {
+ 	return cbe_thread_map[cpu].thread_id;
+@@ -206,6 +224,11 @@ void __init cbe_fill_regs_map(struct cbe
+ 		for_each_node_by_type(np, "mic-tm")
+ 			if (of_get_parent(np) == be)
+ 				map->mic_tm_regs = of_iomap(np, 0);
++
++		for_each_node_by_type(np, "ppe-mmio")
++			if (of_get_parent(np) == be)
++				map->ppe_priv_regs = of_iomap(np, 0);
++
+ 	} else {
+ 		struct device_node *cpu;
+ 		/* That hack must die die die ! */
+@@ -227,6 +250,10 @@ void __init cbe_fill_regs_map(struct cbe
+ 		prop = of_get_property(cpu, "mic-tm", NULL);
+ 		if (prop != NULL)
+ 			map->mic_tm_regs = ioremap(prop->address, prop->len);
++
++		prop = of_get_property(cpu, "ppe-mmio", NULL);
++		if (prop != NULL)
++			map->ppe_priv_regs = ioremap(prop->address, prop->len);
+ 	}
+ }
+ 
+--- a/arch/sparc/kernel/systbls.S
++++ b/arch/sparc/kernel/systbls.S
+@@ -80,6 +80,9 @@ sys_call_table:
+ /*300*/	.long sys_set_robust_list, sys_get_robust_list, sys_migrate_pages, sys_mbind, sys_get_mempolicy
+ /*305*/	.long sys_set_mempolicy, sys_kexec_load, sys_move_pages, sys_getcpu, sys_epoll_pwait
+ /*310*/	.long sys_utimensat, sys_signalfd, sys_timerfd, sys_eventfd, sys_fallocate
++/*315*/	.long sys_pfm_create_context, sys_pfm_write_pmcs, sys_pfm_write_pmds, sys_pfm_read_pmds, sys_pfm_load_context
++/*320*/	.long sys_pfm_start, sys_pfm_stop, sys_pfm_restart, sys_pfm_create_evtsets, sys_pfm_getinfo_evtsets
++/*325*/	.long sys_pfm_delete_evtsets, sys_pfm_unload_context
+ 
+ #ifdef CONFIG_SUNOS_EMUL
+ 	/* Now the SunOS syscall table. */
+@@ -197,6 +200,11 @@ sunos_sys_table:
+ 	.long sunos_nosys, sunos_nosys, sunos_nosys
+ 	.long sunos_nosys
+ /*310*/	.long sunos_nosys, sunos_nosys, sunos_nosys
+-	.long sunos_nosys, sunos_nosys
++	.long sunos_nosys, sunos_nosys, sunos_nosys
++	.long sunos_nosys, sunos_nosys, sunos_nosys
++	.long sunos_nosys
++/*320*/	.long sunos_nosys, sunos_nosys, sunos_nosys
++	.long sunos_nosys, sunos_nosys, sunos_nosys
++	.long sunos_nosys
+ 
+ #endif
+--- a/arch/sparc64/Kconfig
++++ b/arch/sparc64/Kconfig
+@@ -464,6 +464,8 @@ source "fs/Kconfig"
+ 
+ source "kernel/Kconfig.instrumentation"
+ 
++source "arch/sparc64/perfmon/Kconfig"
++
+ source "arch/sparc64/Kconfig.debug"
+ 
+ source "security/Kconfig"
+--- a/arch/sparc64/Makefile
++++ b/arch/sparc64/Makefile
+@@ -58,6 +58,8 @@ core-y				+= arch/sparc64/math-emu/
+ libs-y				+= arch/sparc64/prom/ arch/sparc64/lib/
+ drivers-$(CONFIG_OPROFILE)	+= arch/sparc64/oprofile/
+ 
++core-$(CONFIG_PERFMON)		+= arch/sparc64/perfmon/
++
+ boot := arch/sparc64/boot
+ 
+ image tftpboot.img vmlinux.aout: vmlinux
+--- a/arch/sparc64/kernel/cpu.c
++++ b/arch/sparc64/kernel/cpu.c
+@@ -18,52 +18,54 @@
+ DEFINE_PER_CPU(cpuinfo_sparc, __cpu_data) = { 0 };
+ 
+ struct cpu_iu_info {
+-  short manuf;
+-  short impl;
+-  char* cpu_name;   /* should be enough I hope... */
++	short manuf;
++	short impl;
++	char *cpu_name;
++	char *pmu_name;
+ };
+ 
+ struct cpu_fp_info {
+-  short manuf;
+-  short impl;
+-  char fpu_vers;
+-  char* fp_name;
++	short manuf;
++	short impl;
++	char fpu_vers;
++	char* fp_name;
+ };
+ 
+ struct cpu_fp_info linux_sparc_fpu[] = {
+-  { 0x17, 0x10, 0, "UltraSparc I integrated FPU"},
+-  { 0x22, 0x10, 0, "UltraSparc I integrated FPU"},
+-  { 0x17, 0x11, 0, "UltraSparc II integrated FPU"},
+-  { 0x17, 0x12, 0, "UltraSparc IIi integrated FPU"},
+-  { 0x17, 0x13, 0, "UltraSparc IIe integrated FPU"},
+-  { 0x3e, 0x14, 0, "UltraSparc III integrated FPU"},
+-  { 0x3e, 0x15, 0, "UltraSparc III+ integrated FPU"},
+-  { 0x3e, 0x16, 0, "UltraSparc IIIi integrated FPU"},
+-  { 0x3e, 0x18, 0, "UltraSparc IV integrated FPU"},
+-  { 0x3e, 0x19, 0, "UltraSparc IV+ integrated FPU"},
+-  { 0x3e, 0x22, 0, "UltraSparc IIIi+ integrated FPU"},
++	{ 0x17, 0x10, 0, "UltraSparc I integrated FPU" },
++	{ 0x22, 0x10, 0, "UltraSparc I integrated FPU" },
++	{ 0x17, 0x11, 0, "UltraSparc II integrated FPU" },
++	{ 0x17, 0x12, 0, "UltraSparc IIi integrated FPU" },
++	{ 0x17, 0x13, 0, "UltraSparc IIe integrated FPU" },
++	{ 0x3e, 0x14, 0, "UltraSparc III integrated FPU" },
++	{ 0x3e, 0x15, 0, "UltraSparc III+ integrated FPU" },
++	{ 0x3e, 0x16, 0, "UltraSparc IIIi integrated FPU" },
++	{ 0x3e, 0x18, 0, "UltraSparc IV integrated FPU" },
++	{ 0x3e, 0x19, 0, "UltraSparc IV+ integrated FPU" },
++	{ 0x3e, 0x22, 0, "UltraSparc IIIi+ integrated FPU" },
+ };
+ 
+ #define NSPARCFPU  ARRAY_SIZE(linux_sparc_fpu)
+ 
+ struct cpu_iu_info linux_sparc_chips[] = {
+-  { 0x17, 0x10, "TI UltraSparc I   (SpitFire)"},
+-  { 0x22, 0x10, "TI UltraSparc I   (SpitFire)"},
+-  { 0x17, 0x11, "TI UltraSparc II  (BlackBird)"},
+-  { 0x17, 0x12, "TI UltraSparc IIi (Sabre)"},
+-  { 0x17, 0x13, "TI UltraSparc IIe (Hummingbird)"},
+-  { 0x3e, 0x14, "TI UltraSparc III (Cheetah)"},
+-  { 0x3e, 0x15, "TI UltraSparc III+ (Cheetah+)"},
+-  { 0x3e, 0x16, "TI UltraSparc IIIi (Jalapeno)"},
+-  { 0x3e, 0x18, "TI UltraSparc IV (Jaguar)"},
+-  { 0x3e, 0x19, "TI UltraSparc IV+ (Panther)"},
+-  { 0x3e, 0x22, "TI UltraSparc IIIi+ (Serrano)"},
++	{ 0x17, 0x10, "TI UltraSparc I   (SpitFire)", "ultra12" },
++	{ 0x22, 0x10, "TI UltraSparc I   (SpitFire)", "ultra12" },
++	{ 0x17, 0x11, "TI UltraSparc II  (BlackBird)", "ultra12" },
++	{ 0x17, 0x12, "TI UltraSparc IIi (Sabre)", "ultra12" },
++	{ 0x17, 0x13, "TI UltraSparc IIe (Hummingbird)", "ultra12" },
++	{ 0x3e, 0x14, "TI UltraSparc III (Cheetah)", "ultra3" },
++	{ 0x3e, 0x15, "TI UltraSparc III+ (Cheetah+)", "ultra3+" },
++	{ 0x3e, 0x16, "TI UltraSparc IIIi (Jalapeno)", "ultra3i" },
++	{ 0x3e, 0x18, "TI UltraSparc IV (Jaguar)", "ultra3+" },
++	{ 0x3e, 0x19, "TI UltraSparc IV+ (Panther)", "ultra4+" },
++	{ 0x3e, 0x22, "TI UltraSparc IIIi+ (Serrano)", "ultra3i" },
+ };
+ 
+ #define NSPARCCHIPS  ARRAY_SIZE(linux_sparc_chips)
+ 
+ char *sparc_cpu_type;
+ char *sparc_fpu_type;
++char *sparc_pmu_type;
+ 
+ unsigned int fsr_storage;
+ 
+@@ -73,11 +75,13 @@ static void __init sun4v_cpu_probe(void)
+ 	case SUN4V_CHIP_NIAGARA1:
+ 		sparc_cpu_type = "UltraSparc T1 (Niagara)";
+ 		sparc_fpu_type = "UltraSparc T1 integrated FPU";
++		sparc_pmu_type = "niagara";
+ 		break;
+ 
+ 	case SUN4V_CHIP_NIAGARA2:
+ 		sparc_cpu_type = "UltraSparc T2 (Niagara2)";
+ 		sparc_fpu_type = "UltraSparc T2 integrated FPU";
++		sparc_pmu_type = "niagara2";
+ 		break;
+ 
+ 	default:
+@@ -85,6 +89,7 @@ static void __init sun4v_cpu_probe(void)
+ 		       prom_cpu_compatible);
+ 		sparc_cpu_type = "Unknown SUN4V CPU";
+ 		sparc_fpu_type = "Unknown SUN4V FPU";
++		sparc_pmu_type = "Unknown SUN4V PMU";
+ 		break;
+ 	}
+ }
+@@ -115,6 +120,8 @@ retry:
+ 			if (linux_sparc_chips[i].impl == impl) {
+ 				sparc_cpu_type =
+ 					linux_sparc_chips[i].cpu_name;
++				sparc_pmu_type =
++					linux_sparc_chips[i].pmu_name;
+ 				break;
+ 			}
+ 		}
+--- a/arch/sparc64/kernel/entry.S
++++ b/arch/sparc64/kernel/entry.S
+@@ -2605,3 +2605,43 @@ sun4v_mmu_demap_all:
+ 	retl
+ 	 nop
+ 	.size	sun4v_mmu_demap_all, .-sun4v_mmu_demap_all
++
++	.globl	sun4v_niagara_getperf
++	.type	sun4v_niagara_getperf,#function
++sun4v_niagara_getperf:
++	mov	%o0, %o4
++	mov	HV_FAST_GET_PERFREG, %o5
++	ta	HV_FAST_TRAP
++	stx	%o1, [%o4]
++	retl
++	 nop
++	.size	sun4v_niagara_getperf, .-sun4v_niagara_getperf
++
++	.globl	sun4v_niagara_setperf
++	.type	sun4v_niagara_setperf,#function
++sun4v_niagara_setperf:
++	mov	HV_FAST_SET_PERFREG, %o5
++	ta	HV_FAST_TRAP
++	retl
++	 nop
++	.size	sun4v_niagara_setperf, .-sun4v_niagara_setperf
++
++	.globl	sun4v_niagara2_getperf
++	.type	sun4v_niagara2_getperf,#function
++sun4v_niagara2_getperf:
++	mov	%o0, %o4
++	mov	HV_FAST_N2_GET_PERFREG, %o5
++	ta	HV_FAST_TRAP
++	stx	%o1, [%o4]
++	retl
++	 nop
++	.size	sun4v_niagara2_getperf, .-sun4v_niagara2_getperf
++
++	.globl	sun4v_niagara2_setperf
++	.type	sun4v_niagara2_setperf,#function
++sun4v_niagara2_setperf:
++	mov	HV_FAST_N2_SET_PERFREG, %o5
++	ta	HV_FAST_TRAP
++	retl
++	 nop
++	.size	sun4v_niagara2_setperf, .-sun4v_niagara2_setperf
+--- a/arch/sparc64/kernel/hvapi.c
++++ b/arch/sparc64/kernel/hvapi.c
+@@ -36,6 +36,7 @@ static struct api_info api_table[] = {
+ 	{ .group = HV_GRP_NCS,		.flags = FLAG_PRE_API	},
+ 	{ .group = HV_GRP_NIAG_PERF,	.flags = FLAG_PRE_API	},
+ 	{ .group = HV_GRP_FIRE_PERF,				},
++	{ .group = HV_GRP_NIAG2_PERF,				},
+ 	{ .group = HV_GRP_DIAG,		.flags = FLAG_PRE_API	},
+ };
+ 
+--- a/arch/sparc64/kernel/irq.c
++++ b/arch/sparc64/kernel/irq.c
+@@ -731,6 +731,70 @@ void handler_irq(int irq, struct pt_regs
+ 	set_irq_regs(old_regs);
+ }
+ 
++static void unhandled_perf_irq(struct pt_regs *regs)
++{
++	unsigned long pcr, pic;
++
++	read_pcr(pcr);
++	read_pic(pic);
++
++	write_pcr(0);
++
++	printk(KERN_EMERG "CPU %d: Got unexpected perf counter IRQ.\n",
++	       smp_processor_id());
++	printk(KERN_EMERG "CPU %d: PCR[%016lx] PIC[%016lx]\n",
++	       smp_processor_id(), pcr, pic);
++}
++
++/* Almost a direct copy of the powerpc PMC code.  */
++static DEFINE_SPINLOCK(perf_irq_lock);
++static void *perf_irq_owner_caller; /* mostly for debugging */
++static void (*perf_irq)(struct pt_regs *regs) = unhandled_perf_irq;
++
++/* Invoked from level 15 PIL handler in trap table.  */
++void perfctr_irq(int irq, struct pt_regs *regs)
++{
++	clear_softint(1 << irq);
++	perf_irq(regs);
++}
++
++int register_perfctr_intr(void (*handler)(struct pt_regs *))
++{
++	int ret;
++
++	if (!handler)
++		return -EINVAL;
++
++	spin_lock(&perf_irq_lock);
++	if (perf_irq != unhandled_perf_irq) {
++		printk(KERN_WARNING "register_perfctr_intr: "
++		       "perf IRQ busy (reserved by caller %p)\n",
++		       perf_irq_owner_caller);
++		ret = -EBUSY;
++		goto out;
++	}
++
++	perf_irq_owner_caller = __builtin_return_address(0);
++	perf_irq = handler;
++
++	ret = 0;
++out:
++	spin_unlock(&perf_irq_lock);
++
++	return ret;
++}
++EXPORT_SYMBOL_GPL(register_perfctr_intr);
++
++void release_perfctr_intr(void (*handler)(struct pt_regs *))
++{
++	spin_lock(&perf_irq_lock);
++	perf_irq_owner_caller = NULL;
++	perf_irq = unhandled_perf_irq;
++	spin_unlock(&perf_irq_lock);
++}
++EXPORT_SYMBOL_GPL(release_perfctr_intr);
++
++
+ #ifdef CONFIG_HOTPLUG_CPU
+ void fixup_irqs(void)
+ {
+--- a/arch/sparc64/kernel/process.c
++++ b/arch/sparc64/kernel/process.c
+@@ -31,6 +31,7 @@
+ #include <linux/tick.h>
+ #include <linux/init.h>
+ #include <linux/cpu.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/oplib.h>
+ #include <asm/uaccess.h>
+@@ -419,11 +420,7 @@ void exit_thread(void)
+ 			t->utraps[0]--;
+ 	}
+ 
+-	if (test_and_clear_thread_flag(TIF_PERFCTR)) {
+-		t->user_cntd0 = t->user_cntd1 = NULL;
+-		t->pcr_reg = 0;
+-		write_pcr(0);
+-	}
++	pfm_exit_thread(current);
+ }
+ 
+ void flush_thread(void)
+@@ -445,13 +442,6 @@ void flush_thread(void)
+ 
+ 	set_thread_wsaved(0);
+ 
+-	/* Turn off performance counters if on. */
+-	if (test_and_clear_thread_flag(TIF_PERFCTR)) {
+-		t->user_cntd0 = t->user_cntd1 = NULL;
+-		t->pcr_reg = 0;
+-		write_pcr(0);
+-	}
+-
+ 	/* Clear FPU register state. */
+ 	t->fpsaved[0] = 0;
+ 	
+@@ -637,16 +627,6 @@ int copy_thread(int nr, unsigned long cl
+ 	t->fpsaved[0] = 0;
+ 
+ 	if (regs->tstate & TSTATE_PRIV) {
+-		/* Special case, if we are spawning a kernel thread from
+-		 * a userspace task (via KMOD, NFS, or similar) we must
+-		 * disable performance counters in the child because the
+-		 * address space and protection realm are changing.
+-		 */
+-		if (t->flags & _TIF_PERFCTR) {
+-			t->user_cntd0 = t->user_cntd1 = NULL;
+-			t->pcr_reg = 0;
+-			t->flags &= ~_TIF_PERFCTR;
+-		}
+ 		t->kregs->u_regs[UREG_FP] = t->ksp;
+ 		t->flags |= ((long)ASI_P << TI_FLAG_CURRENT_DS_SHIFT);
+ 		flush_register_windows();
+@@ -684,6 +664,8 @@ int copy_thread(int nr, unsigned long cl
+ 	if (clone_flags & CLONE_SETTLS)
+ 		t->kregs->u_regs[UREG_G7] = regs->u_regs[UREG_I3];
+ 
++	pfm_copy_thread(p);
++
+ 	return 0;
+ }
+ 
+--- a/arch/sparc64/kernel/rtrap.S
++++ b/arch/sparc64/kernel/rtrap.S
+@@ -52,7 +52,7 @@ __handle_user_windows:
+ 		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
+ 		ldx			[%g6 + TI_FLAGS], %l0
+ 
+-1:		andcc			%l0, (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK), %g0
++1:		andcc			%l0, (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK | _TIF_PERFMON_WORK), %g0
+ 		be,pt			%xcc, __handle_user_windows_continue
+ 		 nop
+ 		mov			%l5, %o1
+@@ -73,57 +73,14 @@ __handle_user_windows:
+ 		ba,pt			%xcc, __handle_user_windows_continue
+ 
+ 		 andn			%l1, %l4, %l1
+-__handle_perfctrs:
+-		call			update_perfctrs
+-		 wrpr			%g0, RTRAP_PSTATE, %pstate
+-		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
+-		ldub			[%g6 + TI_WSAVED], %o2
+-		brz,pt			%o2, 1f
+-		 nop
+-		/* Redo userwin+sched+sig checks */
+-		call			fault_in_user_windows
+-
+-		 wrpr			%g0, RTRAP_PSTATE, %pstate
+-		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
+-		ldx			[%g6 + TI_FLAGS], %l0
+-		andcc			%l0, _TIF_NEED_RESCHED, %g0
+-		be,pt			%xcc, 1f
+-
+-		 nop
+-		call			schedule
+-		 wrpr			%g0, RTRAP_PSTATE, %pstate
+-		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
+-		ldx			[%g6 + TI_FLAGS], %l0
+-1:		andcc			%l0, (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK), %g0
+-
+-		be,pt			%xcc, __handle_perfctrs_continue
+-		 sethi			%hi(TSTATE_PEF), %o0
+-		mov			%l5, %o1
+-		mov			%l6, %o2
+-		add			%sp, PTREGS_OFF, %o0
+-		mov			%l0, %o3
+-		call			do_notify_resume
+-
+-		 wrpr			%g0, RTRAP_PSTATE, %pstate
+-		wrpr			%g0, RTRAP_PSTATE_IRQOFF, %pstate
+-		clr			%l6
+-		/* Signal delivery can modify pt_regs tstate, so we must
+-		 * reload it.
+-		 */
+-		ldx			[%sp + PTREGS_OFF + PT_V9_TSTATE], %l1
+-		sethi			%hi(0xf << 20), %l4
+-		and			%l1, %l4, %l4
+-		andn			%l1, %l4, %l1
+-		ba,pt			%xcc, __handle_perfctrs_continue
+-
+-		 sethi			%hi(TSTATE_PEF), %o0
+ __handle_userfpu:
+ 		rd			%fprs, %l5
+ 		andcc			%l5, FPRS_FEF, %g0
+ 		sethi			%hi(TSTATE_PEF), %o0
+ 		be,a,pn			%icc, __handle_userfpu_continue
+ 		 andn			%l1, %o0, %l1
+-		ba,a,pt			%xcc, __handle_userfpu_continue
++		ba,pt			%xcc, __handle_userfpu_continue
++		 nop
+ 
+ __handle_signal:
+ 		mov			%l5, %o1
+@@ -215,12 +172,8 @@ __handle_signal_continue:
+ 		brnz,pn			%o2, __handle_user_windows
+ 		 nop
+ __handle_user_windows_continue:
+-		ldx			[%g6 + TI_FLAGS], %l5
+-		andcc			%l5, _TIF_PERFCTR, %g0
+ 		sethi			%hi(TSTATE_PEF), %o0
+-		bne,pn			%xcc, __handle_perfctrs
+-__handle_perfctrs_continue:
+-		 andcc			%l1, %o0, %g0
++		andcc			%l1, %o0, %g0
+ 
+ 		/* This fpdepth clear is necessary for non-syscall rtraps only */
+ user_nowork:
+--- a/arch/sparc64/kernel/setup.c
++++ b/arch/sparc64/kernel/setup.c
+@@ -351,9 +351,6 @@ void __init setup_arch(char **cmdline_p)
+ 
+ /* BUFFER is PAGE_SIZE bytes long. */
+ 
+-extern char *sparc_cpu_type;
+-extern char *sparc_fpu_type;
+-
+ extern void smp_info(struct seq_file *);
+ extern void smp_bogo(struct seq_file *);
+ extern void mmu_info(struct seq_file *);
+@@ -368,6 +365,7 @@ static int show_cpuinfo(struct seq_file 
+ 	seq_printf(m, 
+ 		   "cpu\t\t: %s\n"
+ 		   "fpu\t\t: %s\n"
++		   "pmu\t\t: %s\n"
+ 		   "prom\t\t: %s\n"
+ 		   "type\t\t: %s\n"
+ 		   "ncpus probed\t: %d\n"
+@@ -380,6 +378,7 @@ static int show_cpuinfo(struct seq_file 
+ 		   ,
+ 		   sparc_cpu_type,
+ 		   sparc_fpu_type,
++		   sparc_pmu_type,
+ 		   prom_version,
+ 		   ((tlb_type == hypervisor) ?
+ 		    "sun4v" :
+--- a/arch/sparc64/kernel/signal.c
++++ b/arch/sparc64/kernel/signal.c
+@@ -22,6 +22,7 @@
+ #include <linux/tty.h>
+ #include <linux/binfmts.h>
+ #include <linux/bitops.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/uaccess.h>
+ #include <asm/ptrace.h>
+@@ -577,6 +578,9 @@ static void do_signal(struct pt_regs *re
+ void do_notify_resume(struct pt_regs *regs, unsigned long orig_i0, int restart_syscall,
+ 		      unsigned long thread_info_flags)
+ {
++	if (thread_info_flags & _TIF_PERFMON_WORK)
++		pfm_handle_work(regs);
++
+ 	if (thread_info_flags & (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK))
+ 		do_signal(regs, orig_i0, restart_syscall);
+ }
+--- a/arch/sparc64/kernel/smp.c
++++ b/arch/sparc64/kernel/smp.c
+@@ -860,6 +860,28 @@ int smp_call_function(void (*func)(void 
+ 				      cpu_online_map);
+ }
+ 
++int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
++			     int nonatomic, int wait)
++{
++	/* prevent preemption and reschedule on another processor */
++	int ret;
++	int me = get_cpu();
++	if (cpu == me) {
++		local_irq_disable();
++		func(info);
++		local_irq_enable();
++		put_cpu();
++		return 0;
++	}
++
++	ret = smp_call_function_mask(func, info, nonatomic, wait,
++				     cpumask_of_cpu(cpu));
++
++	put_cpu();
++	return ret;
++}
++EXPORT_SYMBOL(smp_call_function_single);
++
+ void smp_call_function_client(int irq, struct pt_regs *regs)
+ {
+ 	void (*func) (void *info) = call_data->func;
+--- a/arch/sparc64/kernel/sys_sparc.c
++++ b/arch/sparc64/kernel/sys_sparc.c
+@@ -27,7 +27,6 @@
+ 
+ #include <asm/uaccess.h>
+ #include <asm/utrap.h>
+-#include <asm/perfctr.h>
+ #include <asm/a.out.h>
+ #include <asm/unistd.h>
+ 
+@@ -854,106 +853,10 @@ asmlinkage long sys_rt_sigaction(int sig
+ 	return ret;
+ }
+ 
+-/* Invoked by rtrap code to update performance counters in
+- * user space.
+- */
+-asmlinkage void update_perfctrs(void)
+-{
+-	unsigned long pic, tmp;
+-
+-	read_pic(pic);
+-	tmp = (current_thread_info()->kernel_cntd0 += (unsigned int)pic);
+-	__put_user(tmp, current_thread_info()->user_cntd0);
+-	tmp = (current_thread_info()->kernel_cntd1 += (pic >> 32));
+-	__put_user(tmp, current_thread_info()->user_cntd1);
+-	reset_pic();
+-}
+-
+ asmlinkage long sys_perfctr(int opcode, unsigned long arg0, unsigned long arg1, unsigned long arg2)
+ {
+-	int err = 0;
+-
+-	switch(opcode) {
+-	case PERFCTR_ON:
+-		current_thread_info()->pcr_reg = arg2;
+-		current_thread_info()->user_cntd0 = (u64 __user *) arg0;
+-		current_thread_info()->user_cntd1 = (u64 __user *) arg1;
+-		current_thread_info()->kernel_cntd0 =
+-			current_thread_info()->kernel_cntd1 = 0;
+-		write_pcr(arg2);
+-		reset_pic();
+-		set_thread_flag(TIF_PERFCTR);
+-		break;
+-
+-	case PERFCTR_OFF:
+-		err = -EINVAL;
+-		if (test_thread_flag(TIF_PERFCTR)) {
+-			current_thread_info()->user_cntd0 =
+-				current_thread_info()->user_cntd1 = NULL;
+-			current_thread_info()->pcr_reg = 0;
+-			write_pcr(0);
+-			clear_thread_flag(TIF_PERFCTR);
+-			err = 0;
+-		}
+-		break;
+-
+-	case PERFCTR_READ: {
+-		unsigned long pic, tmp;
+-
+-		if (!test_thread_flag(TIF_PERFCTR)) {
+-			err = -EINVAL;
+-			break;
+-		}
+-		read_pic(pic);
+-		tmp = (current_thread_info()->kernel_cntd0 += (unsigned int)pic);
+-		err |= __put_user(tmp, current_thread_info()->user_cntd0);
+-		tmp = (current_thread_info()->kernel_cntd1 += (pic >> 32));
+-		err |= __put_user(tmp, current_thread_info()->user_cntd1);
+-		reset_pic();
+-		break;
+-	}
+-
+-	case PERFCTR_CLRPIC:
+-		if (!test_thread_flag(TIF_PERFCTR)) {
+-			err = -EINVAL;
+-			break;
+-		}
+-		current_thread_info()->kernel_cntd0 =
+-			current_thread_info()->kernel_cntd1 = 0;
+-		reset_pic();
+-		break;
+-
+-	case PERFCTR_SETPCR: {
+-		u64 __user *user_pcr = (u64 __user *)arg0;
+-
+-		if (!test_thread_flag(TIF_PERFCTR)) {
+-			err = -EINVAL;
+-			break;
+-		}
+-		err |= __get_user(current_thread_info()->pcr_reg, user_pcr);
+-		write_pcr(current_thread_info()->pcr_reg);
+-		current_thread_info()->kernel_cntd0 =
+-			current_thread_info()->kernel_cntd1 = 0;
+-		reset_pic();
+-		break;
+-	}
+-
+-	case PERFCTR_GETPCR: {
+-		u64 __user *user_pcr = (u64 __user *)arg0;
+-
+-		if (!test_thread_flag(TIF_PERFCTR)) {
+-			err = -EINVAL;
+-			break;
+-		}
+-		err |= __put_user(current_thread_info()->pcr_reg, user_pcr);
+-		break;
+-	}
+-
+-	default:
+-		err = -EINVAL;
+-		break;
+-	};
+-	return err;
++	/* Superceded by perfmon2 */
++	return -ENOSYS;
+ }
+ 
+ /*
+--- a/arch/sparc64/kernel/systbls.S
++++ b/arch/sparc64/kernel/systbls.S
+@@ -80,7 +80,10 @@ sys_call_table32:
+ 	.word sys_fchmodat, sys_faccessat, compat_sys_pselect6, compat_sys_ppoll, sys_unshare
+ /*300*/	.word compat_sys_set_robust_list, compat_sys_get_robust_list, compat_sys_migrate_pages, compat_sys_mbind, compat_sys_get_mempolicy
+ 	.word compat_sys_set_mempolicy, compat_sys_kexec_load, compat_sys_move_pages, sys_getcpu, compat_sys_epoll_pwait
+-/*310*/	.word compat_sys_utimensat, compat_sys_signalfd, compat_sys_timerfd, sys_eventfd, compat_sys_fallocate
++/*310*/	.word compat_sys_utimensat, compat_sys_signalfd, compat_sys_timerfd, sys_eventfd, compat_sys_fallocate, sys_pfm_create_context
++	.word sys_pfm_write_pmcs, sys_pfm_write_pmds, sys_pfm_read_pmds, sys_pfm_load_context
++/*320*/	.word sys_pfm_start, sys_pfm_stop, sys_pfm_restart, sys_pfm_create_evtsets, sys_pfm_getinfo_evtsets, sys_pfm_delete_evtsets
++	.word sys_pfm_unload_context
+ 
+ #endif /* CONFIG_COMPAT */
+ 
+@@ -152,7 +155,10 @@ sys_call_table:
+ 	.word sys_fchmodat, sys_faccessat, sys_pselect6, sys_ppoll, sys_unshare
+ /*300*/	.word sys_set_robust_list, sys_get_robust_list, sys_migrate_pages, sys_mbind, sys_get_mempolicy
+ 	.word sys_set_mempolicy, sys_kexec_load, sys_move_pages, sys_getcpu, sys_epoll_pwait
+-/*310*/	.word sys_utimensat, sys_signalfd, sys_timerfd, sys_eventfd, sys_fallocate
++/*310*/	.word sys_utimensat, sys_signalfd, sys_timerfd, sys_eventfd, sys_fallocate, sys_pfm_create_context
++	.word sys_pfm_write_pmcs, sys_pfm_write_pmds, sys_pfm_read_pmds, sys_pfm_load_context
++/*320*/	.word sys_pfm_start, sys_pfm_stop, sys_pfm_restart, sys_pfm_create_evtsets, sys_pfm_getinfo_evtsets, sys_pfm_delete_evtsets
++	.word sys_pfm_unload_context
+ 
+ #if defined(CONFIG_SUNOS_EMUL) || defined(CONFIG_SOLARIS_EMUL) || \
+     defined(CONFIG_SOLARIS_EMUL_MODULE)
+@@ -271,6 +277,11 @@ sunos_sys_table:
+ 	.word sunos_nosys, sunos_nosys, sunos_nosys
+ 	.word sunos_nosys
+ /*310*/	.word sunos_nosys, sunos_nosys, sunos_nosys
+-	.word sunos_nosys, sunos_nosys
++	.word sunos_nosys, sunos_nosys, sunos_nosys
++	.word sunos_nosys, sunos_nosys, sunos_nosys
++	.word sunos_nosys
++/*320*/	.word sunos_nosys, sunos_nosys, sunos_nosys
++	.word sunos_nosys, sunos_nosys, sunos_nosys
++	.word sunos_nosys
+ 
+ #endif
+--- a/arch/sparc64/kernel/traps.c
++++ b/arch/sparc64/kernel/traps.c
+@@ -2514,86 +2514,90 @@ extern void tsb_config_offsets_are_bolix
+ /* Only invoked on boot processor. */
+ void __init trap_init(void)
+ {
+-	/* Compile time sanity check. */
+-	if (TI_TASK != offsetof(struct thread_info, task) ||
+-	    TI_FLAGS != offsetof(struct thread_info, flags) ||
+-	    TI_CPU != offsetof(struct thread_info, cpu) ||
+-	    TI_FPSAVED != offsetof(struct thread_info, fpsaved) ||
+-	    TI_KSP != offsetof(struct thread_info, ksp) ||
+-	    TI_FAULT_ADDR != offsetof(struct thread_info, fault_address) ||
+-	    TI_KREGS != offsetof(struct thread_info, kregs) ||
+-	    TI_UTRAPS != offsetof(struct thread_info, utraps) ||
+-	    TI_EXEC_DOMAIN != offsetof(struct thread_info, exec_domain) ||
+-	    TI_REG_WINDOW != offsetof(struct thread_info, reg_window) ||
+-	    TI_RWIN_SPTRS != offsetof(struct thread_info, rwbuf_stkptrs) ||
+-	    TI_GSR != offsetof(struct thread_info, gsr) ||
+-	    TI_XFSR != offsetof(struct thread_info, xfsr) ||
+-	    TI_USER_CNTD0 != offsetof(struct thread_info, user_cntd0) ||
+-	    TI_USER_CNTD1 != offsetof(struct thread_info, user_cntd1) ||
+-	    TI_KERN_CNTD0 != offsetof(struct thread_info, kernel_cntd0) ||
+-	    TI_KERN_CNTD1 != offsetof(struct thread_info, kernel_cntd1) ||
+-	    TI_PCR != offsetof(struct thread_info, pcr_reg) ||
+-	    TI_PRE_COUNT != offsetof(struct thread_info, preempt_count) ||
+-	    TI_NEW_CHILD != offsetof(struct thread_info, new_child) ||
+-	    TI_SYS_NOERROR != offsetof(struct thread_info, syscall_noerror) ||
+-	    TI_RESTART_BLOCK != offsetof(struct thread_info, restart_block) ||
+-	    TI_KUNA_REGS != offsetof(struct thread_info, kern_una_regs) ||
+-	    TI_KUNA_INSN != offsetof(struct thread_info, kern_una_insn) ||
+-	    TI_FPREGS != offsetof(struct thread_info, fpregs) ||
+-	    (TI_FPREGS & (64 - 1)))
+-		thread_info_offsets_are_bolixed_dave();
+-
+-	if (TRAP_PER_CPU_THREAD != offsetof(struct trap_per_cpu, thread) ||
+-	    (TRAP_PER_CPU_PGD_PADDR !=
+-	     offsetof(struct trap_per_cpu, pgd_paddr)) ||
+-	    (TRAP_PER_CPU_CPU_MONDO_PA !=
+-	     offsetof(struct trap_per_cpu, cpu_mondo_pa)) ||
+-	    (TRAP_PER_CPU_DEV_MONDO_PA !=
+-	     offsetof(struct trap_per_cpu, dev_mondo_pa)) ||
+-	    (TRAP_PER_CPU_RESUM_MONDO_PA !=
+-	     offsetof(struct trap_per_cpu, resum_mondo_pa)) ||
+-	    (TRAP_PER_CPU_RESUM_KBUF_PA !=
+-	     offsetof(struct trap_per_cpu, resum_kernel_buf_pa)) ||
+-	    (TRAP_PER_CPU_NONRESUM_MONDO_PA !=
+-	     offsetof(struct trap_per_cpu, nonresum_mondo_pa)) ||
+-	    (TRAP_PER_CPU_NONRESUM_KBUF_PA !=
+-	     offsetof(struct trap_per_cpu, nonresum_kernel_buf_pa)) ||
+-	    (TRAP_PER_CPU_FAULT_INFO !=
+-	     offsetof(struct trap_per_cpu, fault_info)) ||
+-	    (TRAP_PER_CPU_CPU_MONDO_BLOCK_PA !=
+-	     offsetof(struct trap_per_cpu, cpu_mondo_block_pa)) ||
+-	    (TRAP_PER_CPU_CPU_LIST_PA !=
+-	     offsetof(struct trap_per_cpu, cpu_list_pa)) ||
+-	    (TRAP_PER_CPU_TSB_HUGE !=
+-	     offsetof(struct trap_per_cpu, tsb_huge)) ||
+-	    (TRAP_PER_CPU_TSB_HUGE_TEMP !=
+-	     offsetof(struct trap_per_cpu, tsb_huge_temp)) ||
+-	    (TRAP_PER_CPU_IRQ_WORKLIST_PA !=
+-	     offsetof(struct trap_per_cpu, irq_worklist_pa)) ||
+-	    (TRAP_PER_CPU_CPU_MONDO_QMASK !=
+-	     offsetof(struct trap_per_cpu, cpu_mondo_qmask)) ||
+-	    (TRAP_PER_CPU_DEV_MONDO_QMASK !=
+-	     offsetof(struct trap_per_cpu, dev_mondo_qmask)) ||
+-	    (TRAP_PER_CPU_RESUM_QMASK !=
+-	     offsetof(struct trap_per_cpu, resum_qmask)) ||
+-	    (TRAP_PER_CPU_NONRESUM_QMASK !=
+-	     offsetof(struct trap_per_cpu, nonresum_qmask)))
+-		trap_per_cpu_offsets_are_bolixed_dave();
+-
+-	if ((TSB_CONFIG_TSB !=
+-	     offsetof(struct tsb_config, tsb)) ||
+-	    (TSB_CONFIG_RSS_LIMIT !=
+-	     offsetof(struct tsb_config, tsb_rss_limit)) ||
+-	    (TSB_CONFIG_NENTRIES !=
+-	     offsetof(struct tsb_config, tsb_nentries)) ||
+-	    (TSB_CONFIG_REG_VAL !=
+-	     offsetof(struct tsb_config, tsb_reg_val)) ||
+-	    (TSB_CONFIG_MAP_VADDR !=
+-	     offsetof(struct tsb_config, tsb_map_vaddr)) ||
+-	    (TSB_CONFIG_MAP_PTE !=
+-	     offsetof(struct tsb_config, tsb_map_pte)))
+-		tsb_config_offsets_are_bolixed_dave();
+-
++	BUILD_BUG_ON(TI_TASK != offsetof(struct thread_info, task));
++	BUILD_BUG_ON(TI_FLAGS != offsetof(struct thread_info, flags));
++	BUILD_BUG_ON(TI_CPU != offsetof(struct thread_info, cpu));
++	BUILD_BUG_ON(TI_FPSAVED != offsetof(struct thread_info, fpsaved));
++	BUILD_BUG_ON(TI_KSP != offsetof(struct thread_info, ksp));
++	BUILD_BUG_ON(TI_FAULT_ADDR !=
++		     offsetof(struct thread_info, fault_address));
++	BUILD_BUG_ON(TI_KREGS != offsetof(struct thread_info, kregs));
++	BUILD_BUG_ON(TI_UTRAPS != offsetof(struct thread_info, utraps));
++	BUILD_BUG_ON(TI_EXEC_DOMAIN !=
++		     offsetof(struct thread_info, exec_domain));
++	BUILD_BUG_ON(TI_REG_WINDOW !=
++		     offsetof(struct thread_info, reg_window));
++	BUILD_BUG_ON(TI_RWIN_SPTRS !=
++		     offsetof(struct thread_info, rwbuf_stkptrs));
++	BUILD_BUG_ON(TI_GSR != offsetof(struct thread_info, gsr));
++	BUILD_BUG_ON(TI_XFSR != offsetof(struct thread_info, xfsr));
++	BUILD_BUG_ON(TI_PRE_COUNT !=
++		     offsetof(struct thread_info, preempt_count));
++	BUILD_BUG_ON(TI_NEW_CHILD !=
++		     offsetof(struct thread_info, new_child));
++	BUILD_BUG_ON(TI_SYS_NOERROR !=
++		     offsetof(struct thread_info, syscall_noerror));
++	BUILD_BUG_ON(TI_RESTART_BLOCK !=
++		     offsetof(struct thread_info, restart_block));
++	BUILD_BUG_ON(TI_KUNA_REGS !=
++		     offsetof(struct thread_info, kern_una_regs));
++	BUILD_BUG_ON(TI_KUNA_INSN !=
++		     offsetof(struct thread_info, kern_una_insn));
++	BUILD_BUG_ON(TI_FPREGS != offsetof(struct thread_info, fpregs));
++	BUILD_BUG_ON((TI_FPREGS & (64 - 1)));
++ 
++	BUILD_BUG_ON(TRAP_PER_CPU_THREAD !=
++		     offsetof(struct trap_per_cpu, thread));
++	BUILD_BUG_ON(TRAP_PER_CPU_PGD_PADDR !=
++		     offsetof(struct trap_per_cpu, pgd_paddr));
++	BUILD_BUG_ON(TRAP_PER_CPU_CPU_MONDO_PA !=
++		     offsetof(struct trap_per_cpu, cpu_mondo_pa));
++	BUILD_BUG_ON(TRAP_PER_CPU_DEV_MONDO_PA !=
++		     offsetof(struct trap_per_cpu, dev_mondo_pa));
++	BUILD_BUG_ON(TRAP_PER_CPU_RESUM_MONDO_PA !=
++		     offsetof(struct trap_per_cpu, resum_mondo_pa));
++	BUILD_BUG_ON(TRAP_PER_CPU_RESUM_KBUF_PA !=
++		     offsetof(struct trap_per_cpu, resum_kernel_buf_pa));
++	BUILD_BUG_ON(TRAP_PER_CPU_NONRESUM_MONDO_PA !=
++		     offsetof(struct trap_per_cpu, nonresum_mondo_pa));
++	BUILD_BUG_ON(TRAP_PER_CPU_NONRESUM_KBUF_PA !=
++		     offsetof(struct trap_per_cpu, nonresum_kernel_buf_pa));
++	BUILD_BUG_ON(TRAP_PER_CPU_FAULT_INFO !=
++		     offsetof(struct trap_per_cpu, fault_info));
++	BUILD_BUG_ON(TRAP_PER_CPU_CPU_MONDO_BLOCK_PA !=
++		     offsetof(struct trap_per_cpu, cpu_mondo_block_pa));
++	BUILD_BUG_ON(TRAP_PER_CPU_CPU_LIST_PA !=
++		     offsetof(struct trap_per_cpu, cpu_list_pa));
++	BUILD_BUG_ON(TRAP_PER_CPU_TSB_HUGE !=
++		     offsetof(struct trap_per_cpu, tsb_huge));
++	BUILD_BUG_ON(TRAP_PER_CPU_TSB_HUGE_TEMP !=
++		     offsetof(struct trap_per_cpu, tsb_huge_temp));
++#if 0
++	BUILD_BUG_ON(TRAP_PER_CPU_IRQ_WORKLIST !=
++		     offsetof(struct trap_per_cpu, irq_worklist));
++#endif
++	BUILD_BUG_ON(TRAP_PER_CPU_CPU_MONDO_QMASK !=
++		     offsetof(struct trap_per_cpu, cpu_mondo_qmask));
++	BUILD_BUG_ON(TRAP_PER_CPU_DEV_MONDO_QMASK !=
++		     offsetof(struct trap_per_cpu, dev_mondo_qmask));
++	BUILD_BUG_ON(TRAP_PER_CPU_RESUM_QMASK !=
++		     offsetof(struct trap_per_cpu, resum_qmask));
++	BUILD_BUG_ON(TRAP_PER_CPU_NONRESUM_QMASK !=
++		     offsetof(struct trap_per_cpu, nonresum_qmask));
++ 
++	BUILD_BUG_ON(TSB_CONFIG_TSB !=
++		      offsetof(struct tsb_config, tsb));
++	BUILD_BUG_ON(TSB_CONFIG_RSS_LIMIT !=
++		     offsetof(struct tsb_config, tsb_rss_limit));
++	BUILD_BUG_ON(TSB_CONFIG_NENTRIES !=
++		     offsetof(struct tsb_config, tsb_nentries));
++	BUILD_BUG_ON(TSB_CONFIG_REG_VAL !=
++		     offsetof(struct tsb_config, tsb_reg_val));
++	BUILD_BUG_ON(TSB_CONFIG_MAP_VADDR !=
++		     offsetof(struct tsb_config, tsb_map_vaddr));
++	BUILD_BUG_ON(TSB_CONFIG_MAP_PTE !=
++		     offsetof(struct tsb_config, tsb_map_pte));
++ 
+ 	/* Attach to the address space of init_task.  On SMP we
+ 	 * do this in smp.c:smp_callin for other cpus.
+ 	 */
+--- a/arch/sparc64/kernel/ttable.S
++++ b/arch/sparc64/kernel/ttable.S
+@@ -61,7 +61,7 @@ tl0_irq5:	TRAP_IRQ(handler_irq, 5)
+ tl0_irq6:	BTRAP(0x46) BTRAP(0x47) BTRAP(0x48) BTRAP(0x49)
+ tl0_irq10:	BTRAP(0x4a) BTRAP(0x4b) BTRAP(0x4c) BTRAP(0x4d)
+ tl0_irq14:	TRAP_IRQ(timer_interrupt, 14)
+-tl0_irq15:	TRAP_IRQ(handler_irq, 15)
++tl0_irq15:	TRAP_IRQ(perfctr_irq, 15)
+ tl0_resv050:	BTRAP(0x50) BTRAP(0x51) BTRAP(0x52) BTRAP(0x53) BTRAP(0x54) BTRAP(0x55)
+ tl0_resv056:	BTRAP(0x56) BTRAP(0x57) BTRAP(0x58) BTRAP(0x59) BTRAP(0x5a) BTRAP(0x5b)
+ tl0_resv05c:	BTRAP(0x5c) BTRAP(0x5d) BTRAP(0x5e) BTRAP(0x5f)
+--- /dev/null
++++ b/arch/sparc64/perfmon/Kconfig
+@@ -0,0 +1,16 @@
++menu "Hardware Performance Monitoring support"
++config PERFMON
++	bool "Perfmon2 performance monitoring interface"
++	default n
++	help
++	Enables the perfmon2 interface to access the hardware
++	performance counters. See <http://perfmon2.sf.net/> for
++	more details.
++
++config PERFMON_DEBUG
++	bool "Perfmon debugging"
++	depends on PERFMON
++	default n
++	help
++	Enables perfmon debugging support
++endmenu
+--- /dev/null
++++ b/arch/sparc64/perfmon/Makefile
+@@ -0,0 +1 @@
++obj-$(CONFIG_PERFMON)	+= perfmon.o
+--- /dev/null
++++ b/arch/sparc64/perfmon/perfmon.c
+@@ -0,0 +1,423 @@
++/* perfmon.c: sparc64 perfmon support
++ * 
++ * Copyright (C) 2007 David S. Miller (davem@davemloft.net)
++ */
++
++#include <linux/kernel.h>
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <linux/irq.h>
++
++#include <asm/system.h>
++#include <asm/spitfire.h>
++#include <asm/hypervisor.h>
++
++struct pcr_ops {
++	void (*write)(u64);
++	u64 (*read)(void);
++};
++
++static void direct_write_pcr(u64 val)
++{
++	write_pcr(val);
++}
++
++static u64 direct_read_pcr(void)
++{
++	u64 pcr;
++
++	read_pcr(pcr);
++
++	return pcr;
++}
++
++static struct pcr_ops direct_pcr_ops = {
++	.write	= direct_write_pcr,
++	.read	= direct_read_pcr,
++};
++
++/* Using the hypervisor call is needed so that we can set the
++ * hypervisor trace bit correctly, which is hyperprivileged.
++ */
++static void n2_write_pcr(u64 val)
++{
++	unsigned long ret;
++
++	ret = sun4v_niagara2_setperf(HV_N2_PERF_SPARC_CTL, val);
++	if (val != HV_EOK)
++		write_pcr(val);
++}
++
++static u64 n2_read_pcr(void)
++{
++	u64 pcr;
++
++	read_pcr(pcr);
++
++	return pcr;
++}
++
++static struct pcr_ops n2_pcr_ops = {
++	.write	= n2_write_pcr,
++	.read	= n2_read_pcr,
++};
++
++static struct pcr_ops *pcr_ops;
++
++void pfm_arch_write_pmc(struct pfm_context *ctx,
++			unsigned int cnum, u64 value)
++{
++	/*
++	 * we only write to the actual register when monitoring is
++	 * active (pfm_start was issued)
++	 */
++	if (ctx && ctx->flags.started == 0)
++		return;
++
++	pcr_ops->write(value);
++}
++
++u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
++{
++	return pcr_ops->read();
++}
++
++/*
++ * collect pending overflowed PMDs. Called from pfm_ctxsw()
++ * and from PMU interrupt handler. Must fill in set->povfl_pmds[]
++ * and set->npend_ovfls. Interrupts are masked
++ */
++static void __pfm_get_ovfl_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	unsigned int max = pfm_pmu_conf->regs.max_intr_pmd;
++	u64 wmask = 1ULL << pfm_pmu_conf->counter_width;
++	u64 *intr_pmds = pfm_pmu_conf->regs.intr_pmds;
++	u64 *used_mask = set->used_pmds;
++	u64 mask[PFM_PMD_BV];
++	unsigned int i;
++
++	bitmap_and(cast_ulp(mask),
++		   cast_ulp(intr_pmds),
++		   cast_ulp(used_mask),
++		   max);
++
++	/*
++	 * check all PMD that can generate interrupts
++	 * (that includes counters)
++	 */
++	for (i = 0; i < max; i++) {
++		if (test_bit(i, mask)) {
++			u64 new_val = pfm_arch_read_pmd(ctx, i);
++
++			PFM_DBG_ovfl("pmd%u new_val=0x%llx bit=%d\n",
++				     i, (unsigned long long)new_val,
++				     (new_val&wmask) ? 1 : 0);
++
++ 			if (new_val & wmask) {
++				__set_bit(i, set->povfl_pmds);
++				set->npend_ovfls++;
++			}
++		}
++	}
++}
++
++static void pfm_stop_active(struct task_struct *task, struct pfm_context *ctx,
++			       struct pfm_event_set *set)
++{
++	unsigned int i, max = pfm_pmu_conf->regs.max_pmc;
++
++	/*
++	 * clear enable bits, assume all pmcs are enable pmcs
++	 */
++	for (i = 0; i < max; i++) {
++		if (test_bit(i, set->used_pmcs))
++			pfm_arch_write_pmc(ctx, i, 0);
++	}
++
++	if (set->npend_ovfls)
++		return;
++
++	__pfm_get_ovfl_pmds(ctx, set);
++}
++
++/*
++ * Called from pfm_ctxsw(). Task is guaranteed to be current.
++ * Context is locked. Interrupts are masked. Monitoring is active.
++ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
++ *
++ * for per-thread:
++ * 	must stop monitoring for the task
++ *
++ * Return:
++ * 	non-zero : did not save PMDs (as part of stopping the PMU)
++ * 	       0 : saved PMDs (no need to save them in caller)
++ */
++int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
++			      struct pfm_event_set *set)
++{
++	/*
++	 * disable lazy restore of PMC registers.
++	 */
++	set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
++
++	pfm_stop_active(task, ctx, set);
++
++	return 1;
++}
++
++/*
++ * Called from pfm_stop() and idle notifier
++ *
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-thread:
++ *   task is not necessarily current. If not current task, then
++ *   task is guaranteed stopped and off any cpu. Access to PMU
++ *   is not guaranteed. Interrupts are masked. Context is locked.
++ *   Set is the active set.
++ *
++ * For system-wide:
++ * 	task is current
++ *
++ * must disable active monitoring. ctx cannot be NULL
++ */
++void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
++		   struct pfm_event_set *set)
++{
++	/*
++	 * no need to go through stop_save()
++	 * if we are already stopped
++	 */
++	if (!ctx->flags.started || ctx->state == PFM_CTX_MASKED)
++		return;
++
++	/*
++	 * stop live registers and collect pending overflow
++	 */
++	if (task == current)
++		pfm_stop_active(task, ctx, set);
++}
++
++/*
++ * Enable active monitoring. Called from pfm_start() and
++ * pfm_arch_unmask_monitoring().
++ *
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-trhead:
++ * 	Task is not necessarily current. If not current task, then task
++ * 	is guaranteed stopped and off any cpu. Access to PMU is not guaranteed.
++ *
++ * For system-wide:
++ * 	task is always current
++ *
++ * must enable active monitoring.
++ */
++void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
++	            struct pfm_event_set *set)
++{
++	unsigned int max_pmc = pfm_pmu_conf->regs.max_pmc;
++	unsigned int i;
++
++	if (task != current)
++		return;
++
++	for (i = 0; i < max_pmc; i++) {
++		if (test_bit(i, set->used_pmcs))
++			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
++	}
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
++ * context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMD registers from set.
++ */
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	unsigned int max_pmd = pfm_pmu_conf->regs.max_pmd;
++	u64 ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	u64 *impl_pmds = pfm_pmu_conf->regs.pmds;
++	unsigned int i;
++
++	/*
++	 * must restore all pmds to avoid leaking
++	 * information to user.
++	 */
++	for (i = 0; i < max_pmd; i++) {
++		u64 val;
++
++		if (test_bit(i, impl_pmds) == 0)
++			continue;
++
++		val = set->pmds[i].value;
++
++		/*
++		 * set upper bits for counter to ensure
++		 * overflow will trigger
++		 */
++		val &= ovfl_mask;
++
++		pfm_arch_write_pmd(ctx, i, val);
++	}
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw().
++ * Context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMC registers from set, if needed.
++ */
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	unsigned int max_pmc = pfm_pmu_conf->regs.max_pmc;
++	u64 *impl_pmcs = pfm_pmu_conf->regs.pmcs;
++	unsigned int i;
++
++	/* If we're masked or stopped we don't need to bother restoring
++	 * the PMCs now.
++	 */
++	if (ctx->state == PFM_CTX_MASKED || ctx->flags.started == 0)
++		return;
++
++	/*
++	 * restore all pmcs
++	 */
++	for (i = 0; i < max_pmc; i++)
++		if (test_bit(i, impl_pmcs))
++			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
++}
++
++char *pfm_arch_get_pmu_module_name(void)
++{
++	return NULL;
++}
++
++void perfmon_interrupt(struct pt_regs *regs)
++{
++	pfm_interrupt_handler(instruction_pointer(regs), regs);
++}
++
++static struct pfm_regmap_desc pfm_sparc64_pmc_desc[] = {
++	PMC_D(PFM_REG_I, "PCR", 0, 0, 0, 0),
++};
++
++static struct pfm_regmap_desc pfm_sparc64_pmd_desc[] = {
++	PMD_D(PFM_REG_C, "PIC0", 0),
++	PMD_D(PFM_REG_C, "PIC1", 0),
++};
++
++static int pfm_sparc64_probe(void)
++{
++	return 0;
++}
++
++static struct pfm_pmu_config pmu_sparc64_pmu_conf = {
++	.counter_width	= 31,
++	.pmd_desc	= pfm_sparc64_pmd_desc,
++	.num_pmd_entries= 2,
++	.pmc_desc	= pfm_sparc64_pmc_desc,
++	.num_pmc_entries= 1,
++	.probe_pmu	= pfm_sparc64_probe,
++	.flags		= PFM_PMU_BUILTIN_FLAG,
++	.owner		= THIS_MODULE,
++};
++
++static unsigned long perf_hsvc_group;
++static unsigned long perf_hsvc_major;
++static unsigned long perf_hsvc_minor;
++
++static int __init register_perf_hsvc(void)
++{
++	if (tlb_type == hypervisor) {
++		switch (sun4v_chip_type) {
++		case SUN4V_CHIP_NIAGARA1:
++			perf_hsvc_group = HV_GRP_NIAG_PERF;
++			break;
++
++		case SUN4V_CHIP_NIAGARA2:
++			perf_hsvc_group = HV_GRP_NIAG2_PERF;
++			break;
++
++		default:
++			return -ENODEV;
++		}
++
++
++		perf_hsvc_major = 1;
++		perf_hsvc_minor = 0;
++		if (sun4v_hvapi_register(perf_hsvc_group,
++					 perf_hsvc_major,
++					 &perf_hsvc_minor)) {
++			printk("perfmon: Could not register N2 hvapi.\n");
++			return -ENODEV;
++		}
++	}
++	return 0;
++}
++
++static void unregister_perf_hsvc(void)
++{
++	if (tlb_type != hypervisor)
++		return;
++	sun4v_hvapi_unregister(perf_hsvc_group);
++}
++
++static int __init pfm_sparc64_pmu_init(void)
++{
++	u64 mask;
++	int err;
++
++	err = register_perf_hsvc();
++	if (err)
++		return err;
++
++	if (tlb_type == hypervisor &&
++	    sun4v_chip_type == SUN4V_CHIP_NIAGARA2)
++		pcr_ops = &n2_pcr_ops;
++	else
++		pcr_ops = &direct_pcr_ops;
++
++	if (!strcmp(sparc_pmu_type, "ultra12"))
++		mask = (0xf << 11) | (0xf << 4) | 0x7;
++	else if (!strcmp(sparc_pmu_type, "ultra3") ||
++	    !strcmp(sparc_pmu_type, "ultra3i") ||
++	    !strcmp(sparc_pmu_type, "ultra3+") ||
++	    !strcmp(sparc_pmu_type, "ultra4+"))
++		mask = (0x3f << 11) | (0x3f << 4) | 0x7;
++	else if (!strcmp(sparc_pmu_type, "niagara2"))
++		mask = ((1UL << 63) | (1UL << 62) |
++			(1UL << 31) | (0xfUL << 27) | (0xffUL << 19) |
++			(1UL << 18) | (0xfUL << 14) | (0xff << 6) |
++			(0x3UL << 4) | 0x7UL);
++	else if (!strcmp(sparc_pmu_type, "niagara"))
++		mask = ((1UL << 9) | (1UL << 8) |
++			(0x7UL << 4) | 0x7UL);
++	else {
++		err = -ENODEV;
++		goto out_err;
++	}
++
++	pmu_sparc64_pmu_conf.pmu_name = sparc_pmu_type;
++	pfm_sparc64_pmc_desc[0].rsvd_msk = ~mask;
++
++	return pfm_pmu_register(&pmu_sparc64_pmu_conf);
++
++out_err:
++	unregister_perf_hsvc();
++	return err;
++}
++
++static void __exit pfm_sparc64_pmu_exit(void)
++{
++	unregister_perf_hsvc();
++	return pfm_pmu_unregister(&pmu_sparc64_pmu_conf);
++}
++
++module_init(pfm_sparc64_pmu_init);
++module_exit(pfm_sparc64_pmu_exit);
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1201,6 +1201,8 @@ config COMPAT_VDSO
+ 
+ 	  If unsure, say Y.
+ 
++source "arch/x86/perfmon/Kconfig"
++
+ endmenu
+ 
+ config ARCH_ENABLE_MEMORY_HOTPLUG
+--- a/arch/x86/Makefile_32
++++ b/arch/x86/Makefile_32
+@@ -115,6 +115,7 @@ core-y					+= arch/x86/kernel/ \
+ 					   arch/x86/mm/ \
+ 					   $(mcore-y)/ \
+ 					   arch/x86/crypto/
++core-$(CONFIG_PERFMON)			+= arch/x86/perfmon/
+ drivers-$(CONFIG_MATH_EMULATION)	+= arch/x86/math-emu/
+ drivers-$(CONFIG_PCI)			+= arch/x86/pci/
+ # must be linked after kernel/
+--- a/arch/x86/Makefile_64
++++ b/arch/x86/Makefile_64
+@@ -84,6 +84,7 @@ core-y					+= arch/x86/kernel/ \
+ 					   arch/x86/mm/ \
+ 					   arch/x86/crypto/ \
+ 					   arch/x86/vdso/
++core-$(CONFIG_PERFMON)			+= arch/x86/perfmon/
+ core-$(CONFIG_IA32_EMULATION)		+= arch/x86/ia32/
+ drivers-$(CONFIG_PCI)			+= arch/x86/pci/
+ drivers-$(CONFIG_OPROFILE)		+= arch/x86/oprofile/
+--- a/arch/x86/ia32/ia32entry.S
++++ b/arch/x86/ia32/ia32entry.S
+@@ -726,4 +726,17 @@ ia32_sys_call_table:
+ 	.quad compat_sys_timerfd
+ 	.quad sys_eventfd
+ 	.quad sys32_fallocate
++	.quad sys_pfm_create_context	/* 325 */
++	.quad sys_pfm_write_pmcs
++	.quad sys_pfm_write_pmds
++	.quad sys_pfm_read_pmds
++	.quad sys_pfm_load_context
++	.quad sys_pfm_start		/* 330 */
++	.quad sys_pfm_stop
++	.quad sys_pfm_restart
++	.quad sys_pfm_create_evtsets
++	.quad sys_pfm_getinfo_evtsets
++	.quad sys_pfm_delete_evtsets	/* 335 */
++	.quad sys_pfm_unload_context
++
+ ia32_syscall_end:
+--- a/arch/x86/kernel/apic_32.c
++++ b/arch/x86/kernel/apic_32.c
+@@ -28,6 +28,7 @@
+ #include <linux/acpi_pmtmr.h>
+ #include <linux/module.h>
+ #include <linux/dmi.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/atomic.h>
+ #include <asm/smp.h>
+@@ -135,9 +136,9 @@ void apic_wait_icr_idle(void)
+ 		cpu_relax();
+ }
+ 
+-unsigned long safe_apic_wait_icr_idle(void)
++unsigned int safe_apic_wait_icr_idle(void)
+ {
+-	unsigned long send_status;
++	unsigned int send_status;
+ 	int timeout;
+ 
+ 	timeout = 0;
+@@ -604,6 +605,37 @@ int setup_profiling_timer(unsigned int m
+ }
+ 
+ /*
++ * Setup extended LVT, AMD specific (K8, family 10h)
++ *
++ * Vector mappings are hard coded. On K8 only offset 0 (APIC500) and
++ * MCE interrupts are supported. Thus MCE offset must be set to 0.
++ */
++
++#define APIC_EILVT_LVTOFF_MCE 0
++#define APIC_EILVT_LVTOFF_IBS 1
++
++static void setup_APIC_eilvt(u8 lvt_off, u8 vector, u8 msg_type, u8 mask)
++{
++	unsigned long reg = (lvt_off << 4) + APIC_EILVT0;
++	unsigned int  v   = (mask << 16) | (msg_type << 8) | vector;
++	apic_write(reg, v);
++}
++
++u8 setup_APIC_eilvt_mce(u8 vector, u8 msg_type, u8 mask)
++{
++	setup_APIC_eilvt(APIC_EILVT_LVTOFF_MCE, vector, msg_type, mask);
++	return APIC_EILVT_LVTOFF_MCE;
++}
++EXPORT_SYMBOL(setup_APIC_eilvt_mce);
++
++u8 setup_APIC_eilvt_ibs(u8 vector, u8 msg_type, u8 mask)
++{
++	setup_APIC_eilvt(APIC_EILVT_LVTOFF_IBS, vector, msg_type, mask);
++	return APIC_EILVT_LVTOFF_IBS;
++}
++EXPORT_SYMBOL(setup_APIC_eilvt_ibs);
++
++/*
+  * Local APIC start and shutdown
+  */
+ 
+@@ -1330,6 +1362,9 @@ void __init apic_intr_init(void)
+ #ifdef CONFIG_X86_MCE_P4THERMAL
+ 	set_intr_gate(THERMAL_APIC_VECTOR, thermal_interrupt);
+ #endif
++#ifdef CONFIG_PERFMON
++	set_intr_gate(LOCAL_PERFMON_VECTOR, pmu_interrupt);
++#endif
+ }
+ 
+ /**
+--- a/arch/x86/kernel/apic_64.c
++++ b/arch/x86/kernel/apic_64.c
+@@ -160,7 +160,7 @@ void enable_NMI_through_LVT0 (void * dum
+ 	apic_write(APIC_LVT0, v);
+ }
+ 
+-int get_maxlvt(void)
++int lapic_get_maxlvt(void)
+ {
+ 	unsigned int v, maxlvt;
+ 
+@@ -194,7 +194,7 @@ void clear_local_APIC(void)
+ 	int maxlvt;
+ 	unsigned int v;
+ 
+-	maxlvt = get_maxlvt();
++	maxlvt = lapic_get_maxlvt();
+ 
+ 	/*
+ 	 * Masking an LVT entry can trigger a local APIC error
+@@ -333,7 +333,7 @@ int __init verify_local_APIC(void)
+ 	reg1 = GET_APIC_VERSION(reg0);
+ 	if (reg1 == 0x00 || reg1 == 0xff)
+ 		return 0;
+-	reg1 = get_maxlvt();
++	reg1 = lapic_get_maxlvt();
+ 	if (reg1 < 0x02 || reg1 == 0xff)
+ 		return 0;
+ 
+@@ -519,7 +519,7 @@ void __cpuinit setup_local_APIC (void)
+ 
+ 	{
+ 		unsigned oldvalue;
+-		maxlvt = get_maxlvt();
++		maxlvt = lapic_get_maxlvt();
+ 		oldvalue = apic_read(APIC_ESR);
+ 		value = ERROR_APIC_VECTOR;      // enables sending errors
+ 		apic_write(APIC_LVTERR, value);
+@@ -571,7 +571,7 @@ static int lapic_suspend(struct sys_devi
+ 	if (!apic_pm_state.active)
+ 		return 0;
+ 
+-	maxlvt = get_maxlvt();
++	maxlvt = lapic_get_maxlvt();
+ 
+ 	apic_pm_state.apic_id = apic_read(APIC_ID);
+ 	apic_pm_state.apic_taskpri = apic_read(APIC_TASKPRI);
+@@ -605,7 +605,7 @@ static int lapic_resume(struct sys_devic
+ 	if (!apic_pm_state.active)
+ 		return 0;
+ 
+-	maxlvt = get_maxlvt();
++	maxlvt = lapic_get_maxlvt();
+ 
+ 	local_irq_save(flags);
+ 	rdmsr(MSR_IA32_APICBASE, l, h);
+@@ -1011,14 +1011,36 @@ int setup_profiling_timer(unsigned int m
+ 	return -EINVAL;
+ }
+ 
+-void setup_APIC_extended_lvt(unsigned char lvt_off, unsigned char vector,
+-			     unsigned char msg_type, unsigned char mask)
++/*
++ * Setup extended LVT, AMD specific (K8, family 10h)
++ *
++ * Vector mappings are hard coded. On K8 only offset 0 (APIC500) and
++ * MCE interrupts are supported. Thus MCE offset must be set to 0.
++ */
++
++#define APIC_EILVT_LVTOFF_MCE 0
++#define APIC_EILVT_LVTOFF_IBS 1
++
++static void setup_APIC_eilvt(u8 lvt_off, u8 vector, u8 msg_type, u8 mask)
+ {
+-	unsigned long reg = (lvt_off << 4) + K8_APIC_EXT_LVT_BASE;
++	unsigned long reg = (lvt_off << 4) + APIC_EILVT0;
+ 	unsigned int  v   = (mask << 16) | (msg_type << 8) | vector;
+ 	apic_write(reg, v);
+ }
+ 
++u8 setup_APIC_eilvt_mce(u8 vector, u8 msg_type, u8 mask)
++{
++	setup_APIC_eilvt(APIC_EILVT_LVTOFF_MCE, vector, msg_type, mask);
++	return APIC_EILVT_LVTOFF_MCE;
++}
++
++u8 setup_APIC_eilvt_ibs(u8 vector, u8 msg_type, u8 mask)
++{
++	setup_APIC_eilvt(APIC_EILVT_LVTOFF_IBS, vector, msg_type, mask);
++	return APIC_EILVT_LVTOFF_IBS;
++}
++EXPORT_SYMBOL(setup_APIC_eilvt_ibs);
++
+ /*
+  * Local timer interrupt handler. It does both profiling and
+  * process statistics/rescheduling.
+@@ -1029,7 +1051,7 @@ void setup_APIC_extended_lvt(unsigned ch
+  * value into /proc/profile.
+  */
+ 
+-void smp_local_timer_interrupt(void)
++static void smp_local_timer_interrupt(void)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct clock_event_device *evt = &per_cpu(lapic_events, cpu);
+--- a/arch/x86/kernel/cpu/common.c
++++ b/arch/x86/kernel/cpu/common.c
+@@ -5,6 +5,7 @@
+ #include <linux/module.h>
+ #include <linux/percpu.h>
+ #include <linux/bootmem.h>
++#include <linux/perfmon.h>
+ #include <asm/semaphore.h>
+ #include <asm/processor.h>
+ #include <asm/i387.h>
+@@ -718,6 +719,8 @@ void __cpuinit cpu_init(void)
+ 	current_thread_info()->status = 0;
+ 	clear_used_math();
+ 	mxcsr_feature_mask_init();
++
++	pfm_init_percpu();
+ }
+ 
+ #ifdef CONFIG_HOTPLUG_CPU
+--- a/arch/x86/kernel/cpu/mcheck/mce_amd_64.c
++++ b/arch/x86/kernel/cpu/mcheck/mce_amd_64.c
+@@ -118,6 +118,7 @@ void __cpuinit mce_amd_feature_init(stru
+ {
+ 	unsigned int bank, block;
+ 	unsigned int cpu = smp_processor_id();
++	u8 lvt_off;
+ 	u32 low = 0, high = 0, address = 0;
+ 
+ 	for (bank = 0; bank < NR_BANKS; ++bank) {
+@@ -153,14 +154,13 @@ void __cpuinit mce_amd_feature_init(stru
+ 			if (shared_bank[bank] && c->cpu_core_id)
+ 				break;
+ #endif
++			lvt_off = setup_APIC_eilvt_mce(THRESHOLD_APIC_VECTOR,
++						       APIC_EILVT_MSG_FIX, 0);
++
+ 			high &= ~MASK_LVTOFF_HI;
+-			high |= K8_APIC_EXT_LVT_ENTRY_THRESHOLD << 20;
++			high |= lvt_off << 20;
+ 			wrmsr(address, low, high);
+ 
+-			setup_APIC_extended_lvt(K8_APIC_EXT_LVT_ENTRY_THRESHOLD,
+-						THRESHOLD_APIC_VECTOR,
+-						K8_APIC_EXT_INT_MSG_FIX, 0);
+-
+ 			threshold_defaults.address = address;
+ 			threshold_restart_bank(&threshold_defaults, 0, 0);
+ 		}
+--- a/arch/x86/kernel/entry_32.S
++++ b/arch/x86/kernel/entry_32.S
+@@ -466,7 +466,7 @@ ENDPROC(system_call)
+ 	ALIGN
+ 	RING0_PTREGS_FRAME		# can't unwind into user space anyway
+ work_pending:
+-	testb $_TIF_NEED_RESCHED, %cl
++	testw $(_TIF_NEED_RESCHED|_TIF_PERFMON_WORK), %cx
+ 	jz work_notifysig
+ work_resched:
+ 	call schedule
+--- a/arch/x86/kernel/entry_64.S
++++ b/arch/x86/kernel/entry_64.S
+@@ -283,7 +283,7 @@ sysret_careful:
+ sysret_signal:
+ 	TRACE_IRQS_ON
+ 	sti
+-	testl $(_TIF_SIGPENDING|_TIF_SINGLESTEP|_TIF_MCE_NOTIFY),%edx
++	testl $(_TIF_SIGPENDING|_TIF_SINGLESTEP|_TIF_MCE_NOTIFY|_TIF_PERFMON_WORK),%edx
+ 	jz    1f
+ 
+ 	/* Really a signal */
+@@ -377,7 +377,7 @@ int_very_careful:
+ 	jmp int_restore_rest
+ 	
+ int_signal:
+-	testl $(_TIF_SIGPENDING|_TIF_SINGLESTEP|_TIF_MCE_NOTIFY),%edx
++	testl $(_TIF_SIGPENDING|_TIF_SINGLESTEP|_TIF_MCE_NOTIFY|_TIF_PERFMON_WORK),%edx
+ 	jz 1f
+ 	movq %rsp,%rdi		# &ptregs -> arg1
+ 	xorl %esi,%esi		# oldset -> arg2
+@@ -603,7 +603,7 @@ retint_careful:
+ 	jmp retint_check
+ 	
+ retint_signal:
+-	testl $(_TIF_SIGPENDING|_TIF_SINGLESTEP|_TIF_MCE_NOTIFY),%edx
++	testl $(_TIF_SIGPENDING|_TIF_SINGLESTEP|_TIF_MCE_NOTIFY|_TIF_PERFMON_WORK),%edx
+ 	jz    retint_swapgs
+ 	TRACE_IRQS_ON
+ 	sti
+@@ -695,7 +695,13 @@ END(error_interrupt)
+ ENTRY(spurious_interrupt)
+ 	apicinterrupt SPURIOUS_APIC_VECTOR,smp_spurious_interrupt
+ END(spurious_interrupt)
+-				
++
++#ifdef CONFIG_PERFMON
++ENTRY(pmu_interrupt)
++	apicinterrupt LOCAL_PERFMON_VECTOR,smp_pmu_interrupt
++END(pmu_interrupt)
++#endif
++
+ /*
+  * Exception entry points.
+  */ 		
+--- a/arch/x86/kernel/i8259_64.c
++++ b/arch/x86/kernel/i8259_64.c
+@@ -11,6 +11,7 @@
+ #include <linux/kernel_stat.h>
+ #include <linux/sysdev.h>
+ #include <linux/bitops.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/acpi.h>
+ #include <asm/atomic.h>
+@@ -497,6 +498,9 @@ void __init init_IRQ(void)
+ 	set_intr_gate(SPURIOUS_APIC_VECTOR, spurious_interrupt);
+ 	set_intr_gate(ERROR_APIC_VECTOR, error_interrupt);
+ 
++#ifdef CONFIG_PERFMON
++	set_intr_gate(LOCAL_PERFMON_VECTOR, pmu_interrupt);
++#endif
+ 	if (!acpi_ioapic)
+ 		setup_irq(2, &irq2);
+ }
+--- a/arch/x86/kernel/io_apic_64.c
++++ b/arch/x86/kernel/io_apic_64.c
+@@ -1069,7 +1069,7 @@ void __apicdebuginit print_local_APIC(vo
+ 	v = apic_read(APIC_LVR);
+ 	printk(KERN_INFO "... APIC VERSION: %08x\n", v);
+ 	ver = GET_APIC_VERSION(v);
+-	maxlvt = get_maxlvt();
++	maxlvt = lapic_get_maxlvt();
+ 
+ 	v = apic_read(APIC_TASKPRI);
+ 	printk(KERN_DEBUG "... APIC TASKPRI: %08x (%02x)\n", v, v & APIC_TPRI_MASK);
+--- a/arch/x86/kernel/process_32.c
++++ b/arch/x86/kernel/process_32.c
+@@ -37,6 +37,7 @@
+ #include <linux/personality.h>
+ #include <linux/tick.h>
+ #include <linux/percpu.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/uaccess.h>
+ #include <asm/pgtable.h>
+@@ -418,6 +419,7 @@ void exit_thread(void)
+ 		tss->x86_tss.io_bitmap_base = INVALID_IO_BITMAP_OFFSET;
+ 		put_cpu();
+ 	}
++	pfm_exit_thread(current);
+ }
+ 
+ void flush_thread(void)
+@@ -469,6 +471,8 @@ int copy_thread(int nr, unsigned long cl
+ 
+ 	savesegment(gs,p->thread.gs);
+ 
++	pfm_copy_thread(p);
++
+ 	tsk = current;
+ 	if (unlikely(test_tsk_thread_flag(tsk, TIF_IO_BITMAP))) {
+ 		p->thread.io_bitmap_ptr = kmemdup(tsk->thread.io_bitmap_ptr,
+@@ -623,6 +627,10 @@ __switch_to_xtra(struct task_struct *pre
+ 	}
+ #endif
+ 
++	if (test_tsk_thread_flag(next_p, TIF_PERFMON_CTXSW)
++	    || test_tsk_thread_flag(prev_p, TIF_PERFMON_CTXSW))
++		pfm_ctxsw(prev_p, next_p);
++
+ 	if (!test_tsk_thread_flag(next_p, TIF_IO_BITMAP)) {
+ 		/*
+ 		 * Disable the bitmap via an invalid offset. We still cache
+--- a/arch/x86/kernel/process_64.c
++++ b/arch/x86/kernel/process_64.c
+@@ -37,6 +37,7 @@
+ #include <linux/kprobes.h>
+ #include <linux/kdebug.h>
+ #include <linux/tick.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/uaccess.h>
+ #include <asm/pgtable.h>
+@@ -392,6 +393,7 @@ void exit_thread(void)
+ 		t->io_bitmap_max = 0;
+ 		put_cpu();
+ 	}
++	pfm_exit_thread(me);
+ }
+ 
+ void flush_thread(void)
+@@ -500,6 +502,8 @@ int copy_thread(int nr, unsigned long cl
+ 	asm("mov %%es,%0" : "=m" (p->thread.es));
+ 	asm("mov %%ds,%0" : "=m" (p->thread.ds));
+ 
++	pfm_copy_thread(p);
++
+ 	if (unlikely(test_tsk_thread_flag(me, TIF_IO_BITMAP))) {
+ 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
+ 		if (!p->thread.io_bitmap_ptr) {
+@@ -570,6 +574,9 @@ static inline void __switch_to_xtra(stru
+ 		 */
+ 		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
+ 	}
++	if (test_tsk_thread_flag(next_p, TIF_PERFMON_CTXSW)
++	    || test_tsk_thread_flag(prev_p, TIF_PERFMON_CTXSW))
++		pfm_ctxsw(prev_p, next_p);
+ }
+ 
+ /*
+@@ -674,7 +681,7 @@ __switch_to(struct task_struct *prev_p, 
+ 	 * Now maybe reload the debug registers and handle I/O bitmaps
+ 	 */
+ 	if (unlikely((task_thread_info(next_p)->flags & _TIF_WORK_CTXSW))
+-	    || test_tsk_thread_flag(prev_p, TIF_IO_BITMAP))
++	    || (task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW))
+ 		__switch_to_xtra(prev_p, next_p, tss);
+ 
+ 	/* If the task has used fpu the last 5 timeslices, just do a full
+--- a/arch/x86/kernel/setup64.c
++++ b/arch/x86/kernel/setup64.c
+@@ -11,6 +11,7 @@
+ #include <linux/bootmem.h>
+ #include <linux/bitops.h>
+ #include <linux/module.h>
++#include <linux/perfmon.h>
+ #include <asm/pda.h>
+ #include <asm/pgtable.h>
+ #include <asm/processor.h>
+@@ -292,4 +293,6 @@ void __cpuinit cpu_init (void)
+ 	fpu_init(); 
+ 
+ 	raw_local_save_flags(kernel_eflags);
++
++	pfm_init_percpu();
+ }
+--- a/arch/x86/kernel/signal_32.c
++++ b/arch/x86/kernel/signal_32.c
+@@ -19,6 +19,7 @@
+ #include <linux/ptrace.h>
+ #include <linux/elf.h>
+ #include <linux/binfmts.h>
++#include <linux/perfmon.h>
+ #include <asm/processor.h>
+ #include <asm/ucontext.h>
+ #include <asm/uaccess.h>
+@@ -655,6 +656,10 @@ void do_notify_resume(struct pt_regs *re
+ 		clear_thread_flag(TIF_SINGLESTEP);
+ 	}
+ 
++	/* process perfmon asynchronous work (e.g. block thread or reset) */
++	if (thread_info_flags & _TIF_PERFMON_WORK)
++		pfm_handle_work(regs);
++
+ 	/* deal with pending signal delivery */
+ 	if (thread_info_flags & (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK))
+ 		do_signal(regs);
+--- a/arch/x86/kernel/signal_64.c
++++ b/arch/x86/kernel/signal_64.c
+@@ -19,6 +19,7 @@
+ #include <linux/stddef.h>
+ #include <linux/personality.h>
+ #include <linux/compiler.h>
++#include <linux/perfmon.h>
+ #include <asm/ucontext.h>
+ #include <asm/uaccess.h>
+ #include <asm/i387.h>
+@@ -471,6 +472,10 @@ do_notify_resume(struct pt_regs *regs, v
+ 		clear_thread_flag(TIF_SINGLESTEP);
+ 	}
+ 
++	/* process perfmon asynchronous work (e.g. block thread or reset) */
++	if (thread_info_flags & _TIF_PERFMON_WORK)
++		pfm_handle_work(regs);
++
+ #ifdef CONFIG_X86_MCE
+ 	/* notify userspace of pending MCEs */
+ 	if (thread_info_flags & _TIF_MCE_NOTIFY)
+--- a/arch/x86/kernel/smpboot_32.c
++++ b/arch/x86/kernel/smpboot_32.c
+@@ -36,6 +36,7 @@
+ #include <linux/module.h>
+ #include <linux/init.h>
+ #include <linux/kernel.h>
++#include <linux/perfmon.h>
+ 
+ #include <linux/mm.h>
+ #include <linux/sched.h>
+@@ -1212,6 +1213,7 @@ int __cpu_disable(void)
+ 	fixup_irqs(map);
+ 	/* It's now safe to remove this processor from the online map */
+ 	cpu_clear(cpu, cpu_online_map);
++	pfm_cpu_disable();
+ 	return 0;
+ }
+ 
+--- a/arch/x86/kernel/smpboot_64.c
++++ b/arch/x86/kernel/smpboot_64.c
+@@ -49,6 +49,7 @@
+ #include <linux/mc146818rtc.h>
+ #include <linux/smp.h>
+ #include <linux/kdebug.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/mtrr.h>
+ #include <asm/pgalloc.h>
+@@ -388,7 +389,7 @@ static void inquire_remote_apic(int apic
+ 
+ 	printk(KERN_INFO "Inquiring remote APIC #%d...\n", apicid);
+ 
+-	for (i = 0; i < ARRAY_SIZE(regs); i++) {
++	for (i = 0; i < sizeof(regs) / sizeof(*regs); i++) {
+ 		printk("... APIC #%d %s: ", apicid, names[i]);
+ 
+ 		/*
+@@ -466,7 +467,7 @@ static int __cpuinit wakeup_secondary_vi
+ 	 */
+ 	Dprintk("#startup loops: %d.\n", num_starts);
+ 
+-	maxlvt = get_maxlvt();
++	maxlvt = lapic_get_maxlvt();
+ 
+ 	for (j = 1; j <= num_starts; j++) {
+ 		Dprintk("Sending STARTUP #%d.\n",j);
+@@ -1057,6 +1058,7 @@ int __cpu_disable(void)
+ 	spin_unlock(&vector_lock);
+ 	remove_cpu_from_maps();
+ 	fixup_irqs(cpu_online_map);
++	pfm_cpu_disable();
+ 	return 0;
+ }
+ 
+--- a/arch/x86/kernel/syscall_table_32.S
++++ b/arch/x86/kernel/syscall_table_32.S
+@@ -324,3 +324,15 @@ ENTRY(sys_call_table)
+ 	.long sys_timerfd
+ 	.long sys_eventfd
+ 	.long sys_fallocate
++	.long sys_pfm_create_context	/* 325 */
++	.long sys_pfm_write_pmcs
++	.long sys_pfm_write_pmds
++	.long sys_pfm_read_pmds
++	.long sys_pfm_load_context
++	.long sys_pfm_start		/* 330 */
++	.long sys_pfm_stop
++	.long sys_pfm_restart
++	.long sys_pfm_create_evtsets
++	.long sys_pfm_getinfo_evtsets
++	.long sys_pfm_delete_evtsets	/* 335 */
++	.long sys_pfm_unload_context
+--- a/arch/x86/oprofile/nmi_int.c
++++ b/arch/x86/oprofile/nmi_int.c
+@@ -15,6 +15,7 @@
+ #include <linux/slab.h>
+ #include <linux/moduleparam.h>
+ #include <linux/kdebug.h>
++#include <linux/perfmon.h>
+ #include <asm/nmi.h>
+ #include <asm/msr.h>
+ #include <asm/apic.h>
+@@ -199,8 +200,12 @@ static int nmi_setup(void)
+ 	if (!allocate_msrs())
+ 		return -ENOMEM;
+ 
++	if (pfm_reserve_allcpus())
++		return -EBUSY;
++
+ 	if ((err = register_die_notifier(&profile_exceptions_nb))){
+ 		free_msrs();
++		pfm_release_allcpus();
+ 		return err;
+ 	}
+ 
+@@ -279,6 +284,7 @@ static void nmi_shutdown(void)
+ 	unregister_die_notifier(&profile_exceptions_nb);
+ 	model->shutdown(cpu_msrs);
+ 	free_msrs();
++	pfm_release_allcpus();
+ }
+ 
+  
+--- a/arch/x86/pci/common.c
++++ b/arch/x86/pci/common.c
+@@ -55,6 +55,7 @@ int pcibios_scanned;
+  * configuration space.
+  */
+ DEFINE_SPINLOCK(pci_config_lock);
++EXPORT_SYMBOL(pci_config_lock);
+ 
+ /*
+  * Several buggy motherboards address only 16 devices and mirror
+--- /dev/null
++++ b/arch/x86/perfmon/Kconfig
+@@ -0,0 +1,72 @@
++menu "Hardware Performance Monitoring support"
++config PERFMON
++	bool "Perfmon2 performance monitoring interface"
++	select X86_LOCAL_APIC
++	default n
++	help
++	Enables the perfmon2 interface to access the hardware
++	performance counters. See <http://perfmon2.sf.net/> for
++	more details.
++
++config PERFMON_DEBUG
++	bool "Perfmon debugging"
++	default n
++	depends on PERFMON
++	help
++	Enables perfmon debugging support
++
++config X86_PERFMON_P6
++	tristate "Support for Intel P6/Pentium M processor hardware performance counters"
++	depends on PERFMON && X86_32
++	default n
++	help
++	Enables support for Intel P6-style hardware performance counters.
++	To be used for with Intel Pentium III, PentiumPro, Pentium M processors.
++
++config X86_PERFMON_P4
++	tristate "Support for Intel Pentium 4/Xeon hardware performance counters"
++	depends on PERFMON
++	default n
++	help
++	Enables support for Intel Pentium 4/Xeon (Netburst) hardware performance
++	counters.
++
++config	X86_PERFMON_PEBS_P4
++	tristate "Support for Intel Netburst Precise Event-Based Sampling (PEBS)"
++	depends on PERFMON && X86_PERFMON_P4
++	default n
++	help
++	Enables support for Precise Event-Based Sampling (PEBS) on the Intel
++	Netburst processors such as Pentium 4, Xeon which support it.
++
++config  X86_PERFMON_CORE
++	tristate "Support for Intel Core-based performance counters"
++	depends on PERFMON
++	default n
++	help
++	Enables support for Intel Core-based performance counters. Enable
++	this option to support Intel Core 2 processors.
++
++config	X86_PERFMON_PEBS_CORE
++	tristate "Support for Intel Core Precise Event-Based Sampling (PEBS)"
++	depends on PERFMON && X86_PERFMON_CORE
++	default n
++	help
++	Enables support for Precise Event-Based Sampling (PEBS) on the Intel
++	Core processors.
++
++config  X86_PERFMON_INTEL_ARCH
++	tristate "Support for Intel architectural perfmon v1/v2"
++	depends on PERFMON
++	default n
++	help
++	Enables support for Intel architectural performance counters.
++	This feature was introduced with Intel Core Solo/Core Duo processors.
++
++config	X86_PERFMON_AMD64
++	tristate "Support AMD Athlon64/Opteron64 hardware performance counters"
++	depends on PERFMON
++	default n
++	help
++	Enables support for Athlon64/Opterton64 hardware performance counters.
++endmenu
+--- /dev/null
++++ b/arch/x86/perfmon/Makefile
+@@ -0,0 +1,12 @@
++#
++# Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
++# Contributed by Stephane Eranian <eranian@hpl.hp.com>
++#
++obj-$(CONFIG_PERFMON)			+= perfmon.o
++obj-$(CONFIG_X86_PERFMON_P6)		+= perfmon_p6.o
++obj-$(CONFIG_X86_PERFMON_P4)		+= perfmon_p4.o
++obj-$(CONFIG_X86_PERFMON_CORE)		+= perfmon_intel_core.o
++obj-$(CONFIG_X86_PERFMON_INTEL_ARCH)	+= perfmon_intel_arch.o
++obj-$(CONFIG_X86_PERFMON_PEBS_P4)	+= perfmon_pebs_p4_smpl.o
++obj-$(CONFIG_X86_PERFMON_PEBS_CORE)	+= perfmon_pebs_core_smpl.o
++obj-$(CONFIG_X86_PERFMON_AMD64)   	+= perfmon_amd64.o
+--- /dev/null
++++ b/arch/x86/perfmon/perfmon.c
+@@ -0,0 +1,1456 @@
++/*
++ * This file implements the X86 specific support for the perfmon2 interface
++ *
++ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * Copyright (c) 2007 Advanced Micro Devices, Inc.
++ * Contributed by Robert Richter <robert.richter@amd.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/interrupt.h>
++#include <linux/perfmon.h>
++#include <linux/kprobes.h>
++#include <linux/kdebug.h>
++
++#include <asm/nmi.h>
++#include <asm/apic.h>
++
++DEFINE_PER_CPU(unsigned long, real_iip);
++DEFINE_PER_CPU(int, pfm_using_nmi);
++
++struct pfm_ds_area_p4 {
++	unsigned long	bts_buf_base;
++	unsigned long	bts_index;
++	unsigned long	bts_abs_max;
++	unsigned long	bts_intr_thres;
++	unsigned long	pebs_buf_base;
++	unsigned long	pebs_index;
++	unsigned long	pebs_abs_max;
++	unsigned long	pebs_intr_thres;
++	u64		pebs_cnt_reset;
++};
++
++struct pfm_ds_area_intel_core {
++	u64	bts_buf_base;
++	u64	bts_index;
++	u64	bts_abs_max;
++	u64	bts_intr_thres;
++	u64	pebs_buf_base;
++	u64	pebs_index;
++	u64	pebs_abs_max;
++	u64	pebs_intr_thres;
++	u64	pebs_cnt_reset;
++};
++
++
++static int (*pfm_has_ovfl)(struct pfm_context *);
++static int (*pfm_stop_save)(struct pfm_context *ctx,
++			    struct pfm_event_set *set);
++
++static inline int get_smt_id(void)
++{
++#ifdef CONFIG_SMP
++	int cpu = smp_processor_id();
++	return (cpu != first_cpu(__get_cpu_var(cpu_sibling_map)));
++#else
++	return 0;
++#endif
++}
++
++void __pfm_write_reg_p4(const struct pfm_arch_ext_reg *xreg, u64 val)
++{
++	u64 pmi;
++	int smt_id;
++
++	smt_id = get_smt_id();
++	/*
++	 * HT is only supported by P4-style PMU
++	 *
++	 * Adjust for T1 if necessary:
++	 *
++	 * - move the T0_OS/T0_USR bits into T1 slots
++	 * - move the OVF_PMI_T0 bits into T1 slot
++	 *
++	 * The P4/EM64T T1 is cleared by description table.
++	 * User only works with T0.
++	 */
++	if (smt_id) {
++		if (xreg->reg_type & PFM_REGT_ESCR) {
++
++			/* copy T0_USR & T0_OS to T1 */
++			val |= ((val & 0xc) >> 2);
++
++			/* clear bits T0_USR & T0_OS */
++			val &= ~0xc;
++
++		} else if (xreg->reg_type & PFM_REGT_CCCR) {
++			pmi = (val >> 26) & 0x1;
++			if (pmi) {
++				val &=~(1UL<<26);
++				val |= 1UL<<27;
++			}
++		}
++	}
++	if (xreg->addrs[smt_id])
++		wrmsrl(xreg->addrs[smt_id], val);
++}
++
++void __pfm_read_reg_p4(const struct pfm_arch_ext_reg *xreg, u64 *val)
++{
++	int smt_id;
++
++	smt_id = get_smt_id();
++
++	if (likely(xreg->addrs[smt_id])) {
++		rdmsrl(xreg->addrs[smt_id], *val);
++		/*
++		 * HT is only supported by P4-style PMU
++		 *
++		 * move the Tx_OS and Tx_USR bits into
++		 * T0 slots setting the T1 slots to zero
++		 */
++		if (xreg->reg_type & PFM_REGT_ESCR) {
++			if (smt_id)
++				*val |= (((*val) & 0x3) << 2);
++
++			/*
++			 * zero out bits that are reserved
++			 * (including T1_OS and T1_USR)
++			 */
++			*val &= PFM_ESCR_RSVD;
++		}
++	} else {
++		*val = 0;
++	}
++}
++
++/*
++ * called from NMI interrupt handler
++ */
++static void __kprobes __pfm_arch_quiesce_pmu_percpu(void)
++{
++	struct pfm_arch_pmu_info *arch_info;
++	unsigned int i;
++
++	arch_info = pfm_pmu_conf->arch_info;
++
++	/*
++	 * quiesce PMU by clearing registers that have enable bits
++	 * (start/stop capabilities).
++	 */
++	for (i = 0; i < arch_info->max_ena; i++)
++		if (test_bit(i, cast_ulp(arch_info->enable_mask)))
++			pfm_arch_write_pmc(NULL, i, 0);
++}
++
++/*
++ * Called from pfm_ctxsw(). Task is guaranteed to be current.
++ * set cannot be NULL. Context is locked. Interrupts are masked.
++ *
++ * Caller has already restored all PMD and PMC registers, if
++ * necessary (i.e., lazy restore scheme).
++ *
++ * on X86, there is nothing else to do. Even with PEBS, the
++ * DS area is already restore by pfm_arch_restore_pmcs() which
++ * is systematically called as the lazy restore scheme does not
++ * work for PMCs (stopping is a destructive operation for PMC).
++ */
++void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx,
++			     struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	if (set->npend_ovfls)
++		__get_cpu_var(real_iip) = ctx_arch->saved_real_iip;
++
++	/*
++	 * enable RDPMC on this CPU
++	 */
++	if (ctx_arch->flags.insecure)
++		set_in_cr4(X86_CR4_PCE);
++}
++
++static int pfm_stop_save_p6(struct pfm_context *ctx,
++			    struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	u64 used_mask[PFM_PMC_BV];
++	u64 *cnt_pmds;
++	u64 val, wmask, ovfl_mask;
++	u32 i, count;
++
++	wmask = 1ULL << pfm_pmu_conf->counter_width;
++
++	bitmap_and(cast_ulp(used_mask),
++		   cast_ulp(set->used_pmcs),
++		   cast_ulp(arch_info->enable_mask),
++		   arch_info->max_ena);
++
++	count = bitmap_weight(cast_ulp(used_mask), pfm_pmu_conf->regs.max_pmc);
++
++	/*
++	 * stop monitoring
++	 * Unfortunately, this is very expensive!
++	 * wrmsrl() is serializing.
++	 */
++	for (i = 0; count; i++) {
++		if (test_bit(i, cast_ulp(used_mask))) {
++			wrmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, 0);
++			count--;
++		}
++	}
++
++	/*
++	 * if we already having a pending overflow condition, we simply
++	 * return to take care of this first.
++	 */
++	if (set->npend_ovfls)
++		return 1;
++
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
++
++	/*
++	 * check for pending overflows and save PMDs (combo)
++	 * we employ used_pmds because we also need to save
++	 * and not just check for pending interrupts.
++	 *
++	 * Must check for counting PMDs because of virtual PMDs
++	 */
++	count = set->nused_pmds;
++	for (i = 0; count; i++) {
++		if (test_bit(i, cast_ulp(set->used_pmds))) {
++			val = pfm_arch_read_pmd(ctx, i);
++			if (likely(test_bit(i, cast_ulp(cnt_pmds)))) {
++				if (!(val & wmask)) {
++					__set_bit(i, cast_ulp(set->povfl_pmds));
++					set->npend_ovfls++;
++				}
++				val = (set->pmds[i].value & ~ovfl_mask) | (val & ovfl_mask);
++			}
++			set->pmds[i].value = val;
++			count--;
++		}
++	}
++	/* 0 means: no need to save PMDs at upper level */
++	return 0;
++}
++
++#define PFM_AMD64_IBSFETCHVAL	(1ULL<<49) /* valid fetch sample */
++#define PFM_AMD64_IBSFETCHEN	(1ULL<<48) /* fetch sampling enabled */
++#define PFM_AMD64_IBSOPVAL	(1ULL<<18) /* valid execution sample */
++#define PFM_AMD64_IBSOPEN	(1ULL<<17) /* execution sampling enabled */
++
++/*
++ * Must check for IBS event BEFORE stop_save_p6 because
++ * stopping monitoring does destroy IBS state information
++ * in IBSFETCHCTL/IBSOPCTL because they are tagged as enable
++ * registers.
++ */
++static int pfm_stop_save_amd64(struct pfm_context *ctx,
++			       struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	u64 used_mask[PFM_PMC_BV];
++	u64 *cnt_pmds;
++	u64 val, wmask, ovfl_mask;
++	u32 i, count, use_ibs;
++
++	/*
++	 * IBS used if:
++	 *   - on family 10h processor with IBS
++	 *   - at least one of the IBS PMD registers is used
++	 */
++	use_ibs = (arch_info->flags & PFM_X86_FL_IBS)
++		&& (test_bit(arch_info->ibsfetchctl_pmd, cast_ulp(set->used_pmds))
++		    ||test_bit(arch_info->ibsopctl_pmd, cast_ulp(set->used_pmds)));
++
++	wmask = 1ULL << pfm_pmu_conf->counter_width;
++
++	bitmap_and(cast_ulp(used_mask),
++		   cast_ulp(set->used_pmcs),
++		   cast_ulp(arch_info->enable_mask),
++		   arch_info->max_ena);
++
++	count = bitmap_weight(cast_ulp(used_mask), pfm_pmu_conf->regs.max_pmc);
++
++	/*
++	 * stop monitoring
++	 * Unfortunately, this is very expensive!
++	 * wrmsrl() is serializing.
++	 *
++	 * With IBS, we need to do read-modify-write to preserve the content
++	 * for OpsCTL and FetchCTL because they are also used as PMDs and saved
++	 * below
++	 */
++	if (use_ibs) {
++		for (i = 0; count; i++) {
++			if (test_bit(i, cast_ulp(used_mask))) {
++				if (i == arch_info->ibsfetchctl_pmc) {
++					rdmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, val);
++					val &= ~PFM_AMD64_IBSFETCHEN;
++				} else if (i == arch_info->ibsopctl_pmc) {
++					rdmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, val);
++					val &= ~PFM_AMD64_IBSOPEN;
++				} else
++					val = 0;
++				wrmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, val);
++				count--;
++			}
++		}
++	} else {
++		for (i = 0; count; i++) {
++			if (test_bit(i, cast_ulp(used_mask))) {
++				wrmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, 0);
++				count--;
++			}
++		}
++	}
++
++	/*
++	 * if we already having a pending overflow condition, we simply
++	 * return to take care of this first.
++	 */
++	if (set->npend_ovfls)
++		return 1;
++
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
++
++	/*
++	 * check for pending overflows and save PMDs (combo)
++	 * we employ used_pmds because we also need to save
++	 * and not just check for pending interrupts.
++	 *
++	 * Must check for counting PMDs because of virtual PMDs and IBS
++	 */
++	count = set->nused_pmds;
++	for (i = 0; count; i++) {
++		if (test_bit(i, cast_ulp(set->used_pmds))) {
++			val = pfm_arch_read_pmd(ctx, i);
++			if (likely(test_bit(i, cast_ulp(cnt_pmds)))) {
++				if (!(val & wmask)) {
++					__set_bit(i, cast_ulp(set->povfl_pmds));
++					set->npend_ovfls++;
++				}
++				val = (set->pmds[i].value & ~ovfl_mask) | (val & ovfl_mask);
++			}
++			set->pmds[i].value = val;
++			count--;
++		}
++	}
++
++	/*
++	 * check if IBS contains valid data, and mark the corresponding
++	 * PMD has overflowed
++	 */
++	if (use_ibs) {
++		i = arch_info->ibsfetchctl_pmd;
++		if (set->pmds[i].value & PFM_AMD64_IBSFETCHVAL) {
++			__set_bit(i, cast_ulp(set->povfl_pmds));
++			set->npend_ovfls++;
++		}
++		i = arch_info->ibsopctl_pmd;
++		if (set->pmds[i].value & PFM_AMD64_IBSOPVAL) {
++			__set_bit(i, cast_ulp(set->povfl_pmds));
++			set->npend_ovfls++;
++		}
++	}
++	/* 0 means: no need to save PMDs at upper level */
++	return 0;
++}
++
++static int pfm_stop_save_intel_core(struct pfm_context *ctx,
++				    struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	struct pfm_arch_context *ctx_arch;
++	struct pfm_ds_area_intel_core *ds = NULL;
++	u64 used_mask[PFM_PMC_BV];
++	u64 *cnt_mask;
++	u64 val, wmask, ovfl_mask;
++	u16 count, has_ovfl;
++	u16 i, pebs_idx = ~0;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	wmask = 1ULL << pfm_pmu_conf->counter_width;
++
++	/*
++	 * used enable pmc bitmask
++	 */
++	bitmap_and(cast_ulp(used_mask),
++			cast_ulp(set->used_pmcs),
++			cast_ulp(arch_info->enable_mask),
++			arch_info->max_ena);
++
++	count = bitmap_weight(cast_ulp(used_mask), arch_info->max_ena);
++	/*
++	 * stop monitoring
++	 * Unfortunately, this is very expensive!
++	 * wrmsrl() is serializing.
++	 */
++	for (i = 0; count; i++) {
++		if (test_bit(i, cast_ulp(used_mask))) {
++			wrmsrl(pfm_pmu_conf->pmc_desc[i].hw_addr, 0);
++			count--;
++		}
++	}
++	/*
++	 * if we already having a pending overflow condition, we simply
++	 * return to take care of this first.
++	 */
++	if (set->npend_ovfls)
++		return 1;
++
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	cnt_mask = pfm_pmu_conf->regs.cnt_pmds;
++
++	if (ctx_arch->flags.use_pebs) {
++		ds = ctx_arch->ds_area;
++		pebs_idx = arch_info->pebs_ctr_idx;
++		PFM_DBG("ds=%p pebs_idx=0x%llx thres=0x%llx",
++			ds,
++			(unsigned long long)ds->pebs_index,
++			(unsigned long long)ds->pebs_intr_thres);
++	}
++
++	/*
++	 * Check for pending overflows and save PMDs (combo)
++	 * We employ used_pmds and not intr_pmds because we must
++	 * also saved on PMD registers.
++	 * Must check for counting PMDs because of virtual PMDs
++	 *
++	 * XXX: should use the ovf_status register instead, yet
++	 *      we would have to check if NMI is used and fallback
++	 *      to individual pmd inspection.
++	 */
++	count = set->nused_pmds;
++
++	for (i = 0; count; i++) {
++		if (test_bit(i, cast_ulp(set->used_pmds))) {
++			val = pfm_arch_read_pmd(ctx, i);
++			if (likely(test_bit(i, cast_ulp(cnt_mask)))) {
++				if (i == pebs_idx)
++					has_ovfl = (ds->pebs_index >= ds->pebs_intr_thres);
++				else
++					has_ovfl = !(val & wmask);
++				if (has_ovfl) {
++					__set_bit(i, cast_ulp(set->povfl_pmds));
++					set->npend_ovfls++;
++				}
++				val = (set->pmds[i].value & ~ovfl_mask) | (val & ovfl_mask);
++			}
++			set->pmds[i].value = val;
++			count--;
++		}
++	}
++	/* 0 means: no need to save PMDs at upper level */
++	return 0;
++}
++
++static int pfm_stop_save_p4(struct pfm_context *ctx,
++			    struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	struct pfm_arch_context *ctx_arch;
++	struct pfm_arch_ext_reg *xrc, *xrd;
++	struct pfm_ds_area_p4 *ds = NULL;
++	u64 used_mask[PFM_PMC_BV];
++	u16 i, j, count, pebs_idx = ~0;
++	u16 max_pmc;
++	u64 cccr, ctr1, ctr2, ovfl_mask;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++	xrc = arch_info->pmc_addrs;
++	xrd = arch_info->pmd_addrs;
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++
++	/*
++	 * build used enable PMC bitmask
++	 * if user did not set any CCCR, then mask is
++	 * empty and there is nothing to do because nothing
++	 * was started
++	 */
++	bitmap_and(cast_ulp(used_mask),
++		   cast_ulp(set->used_pmcs),
++		   cast_ulp(arch_info->enable_mask),
++		   arch_info->max_ena);
++
++	count = bitmap_weight(cast_ulp(used_mask), arch_info->max_ena);
++
++	PFM_DBG_ovfl("npend=%u ena_mask=0x%llx u_pmcs=0x%llx count=%u num=%u",
++		set->npend_ovfls,
++		(unsigned long long)arch_info->enable_mask[0],
++		(unsigned long long)set->used_pmcs[0],
++		count, arch_info->max_ena);
++
++
++	/*
++	 * ensures we do not destroy pending overflow
++	 * information. If pended interrupts are already
++	 * known, then we just stop monitoring.
++	 */
++	if (set->npend_ovfls) {
++		/*
++		 * clear enable bit
++		 * unfortunately, this is very expensive!
++		 */
++		for (i = 0; count; i++) {
++			if (test_bit(i, cast_ulp(used_mask))) {
++				__pfm_write_reg_p4(xrc+i, 0);
++				count--;
++			}
++		}
++		/* need save PMDs at upper level */
++		return 1;
++	}
++
++	if (ctx_arch->flags.use_pebs) {
++		ds = ctx_arch->ds_area;
++		pebs_idx = arch_info->pebs_ctr_idx;
++		PFM_DBG("ds=%p pebs_idx=0x%llx thres=0x%llx",
++			ds,
++			(unsigned long long)ds->pebs_index,
++			(unsigned long long)ds->pebs_intr_thres);
++	}
++
++	/*
++	 * stop monitoring AND collect pending overflow information AND
++	 * save pmds.
++	 *
++	 * We need to access the CCCR twice, once to get overflow info
++	 * and a second to stop monitoring (which destroys the OVF flag)
++	 * Similarly, we need to read the counter twice to check whether
++	 * it did overflow between the CCR read and the CCCR write.
++	 */
++	for (i = 0; count; i++) {
++		if (i != pebs_idx && test_bit(i, cast_ulp(used_mask))) {
++			/*
++			 * controlled counter
++			 */
++			j = xrc[i].ctr;
++
++			/* read CCCR (PMC) value */
++			__pfm_read_reg_p4(xrc+i, &cccr);
++
++			/* read counter (PMD) controlled by PMC */
++			__pfm_read_reg_p4(xrd+j, &ctr1);
++
++			/* clear CCCR value: stop counter but destroy OVF */
++			__pfm_write_reg_p4(xrc+i, 0);
++
++			/* read counter controlled by CCCR again */
++			__pfm_read_reg_p4(xrd+j, &ctr2);
++
++			/*
++			 * there is an overflow if either:
++			 * 	- CCCR.ovf is set (and we just cleared it)
++			 * 	- ctr2 < ctr1
++			 * in that case we set the bit corresponding to the
++			 * overflowed PMD  in povfl_pmds.
++			 */
++			if ((cccr & (1ULL<<31)) || (ctr2 < ctr1)) {
++				__set_bit(j, cast_ulp(set->povfl_pmds));
++				set->npend_ovfls++;
++			}
++			ctr2 = (set->pmds[j].value & ~ovfl_mask) | (ctr2 & ovfl_mask);
++			set->pmds[j].value = ctr2;
++			count--;
++		}
++	}
++	/*
++	 * check for PEBS buffer full and set the corresponding PMD overflow
++	 */
++	if (ctx_arch->flags.use_pebs) {
++		PFM_DBG("ds=%p pebs_idx=0x%lx thres=0x%lx", ds, ds->pebs_index, ds->pebs_intr_thres);
++		if (ds->pebs_index >= ds->pebs_intr_thres
++		    && test_bit(arch_info->pebs_ctr_idx, cast_ulp(set->used_pmds))) {
++			__set_bit(arch_info->pebs_ctr_idx, cast_ulp(set->povfl_pmds));
++			set->npend_ovfls++;
++		}
++	}
++	/* 0 means: no need to save the PMD at higher level */
++	return 0;
++}
++
++/*
++ * Called from pfm_stop() and idle notifier
++ *
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-thread:
++ *   task is not necessarily current. If not current task, then
++ *   task is guaranteed stopped and off any cpu. Access to PMU
++ *   is not guaranteed.
++ *
++ * For system-wide:
++ * 	task is current
++ *
++ * must disable active monitoring. ctx cannot be NULL
++ */
++void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
++		   struct pfm_event_set *set)
++{
++	/*
++	 * no need to go through stop_save()
++	 * if we are already stopped
++	 */
++	if (!ctx->flags.started || ctx->state == PFM_CTX_MASKED)
++		return;
++
++	if (task == current)
++		pfm_stop_save(ctx, set);
++}
++
++/*
++ * Called from pfm_ctxsw(). Task is guaranteed to be current.
++ * Context is locked. Interrupts are masked. Monitoring may be active.
++ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
++ *
++ * Must stop monitoring, save pending overflow information
++ *
++ * Return:
++ * 	non-zero : did not save PMDs (as part of stopping the PMU)
++ * 	       0 : saved PMDs (no need to save them in caller)
++ */
++int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
++		             struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * disable lazy restore of PMCS on ctxswin because
++	 * we modify some of them.
++	 */
++	set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
++
++	if (set->npend_ovfls) {
++		ctx_arch->saved_real_iip = __get_cpu_var(real_iip);
++	}
++	/*
++	 * disable RDPMC on this CPU
++	 */
++	if (ctx_arch->flags.insecure)
++		clear_in_cr4(X86_CR4_PCE);
++
++	if (ctx->state == PFM_CTX_MASKED)
++		return 1;
++
++	return pfm_stop_save(ctx, set);
++}
++
++/*
++ * called from pfm_start() and idle notifier
++ *
++ * Interrupts are masked. Context is locked. Set is the active set.
++ *
++ * For per-thread:
++ * 	Task is not necessarily current. If not current task, then task
++ * 	is guaranteed stopped and off any cpu. No access to PMU is task
++ *	is not current.
++ *
++ * For system-wide:
++ * 	task is always current
++ *
++ * must enable active monitoring.
++ */
++void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
++		    struct pfm_event_set *set)
++{
++	struct pfm_arch_context *ctx_arch;
++	u64 *mask;
++	u16 i, num;
++
++	/*
++	 * pfm_start issue while context is masked as no effect.
++	 * This comes from the fact that on x86, masking and stopping
++	 * use the same mechanism, i.e., clearing the enable bits
++	 * of the PMC registers.
++	 */
++	if (ctx->state == PFM_CTX_MASKED)
++		return;
++
++	/*
++	 * cannot restore PMC if no access to PMU. Will be done
++	 * when the thread is switched back in
++	 */
++	if (task != current)
++		return;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * reload DS area pointer.
++	 * Must be done before we restore the PMCs
++	 * avoid a race condition
++	 */
++	if (ctx_arch->flags.use_ds)
++		wrmsrl(MSR_IA32_DS_AREA, (unsigned long)ctx_arch->ds_area);
++	/*
++	 * we must actually install all implemented pmcs registers because
++	 * until started, we do not write any PMC registers.
++	 * Note that registers used  by other subsystems (e.g. NMI) are
++	 * removed from pmcs.
++	 *
++	 * The available registers that are actually not used get their default
++	 * value such that counters do not count anything. As such, we can
++	 * afford to write all of them but then stop only the one we use.
++	 *
++	 * XXX: we may be able to optimize this for non-P4 PMU as pmcs are
++	 * independent from each others.
++	 */
++	num = pfm_pmu_conf->regs.num_pmcs;
++	mask = pfm_pmu_conf->regs.pmcs;
++	for (i = 0; num; i++) {
++		if (test_bit(i, cast_ulp(mask))) {
++			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
++			num--;
++		}
++	}
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw()
++ *
++ * context is locked. Interrupts are masked. Set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore PMD registers
++ */
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	u64 *used_pmds;
++	u16 i, num;
++
++	used_pmds = set->used_pmds;
++	num = set->nused_pmds;
++
++	/*
++	 * we can restore only the PMD we use because:
++	 * 	- you can only read with pfm_read_pmds() the registers
++	 * 	  declared used via pfm_write_pmds(), smpl_pmds, reset_pmds
++	 *
++	 * 	- if cr4.pce=1, only counters are exposed to user. No
++	 * 	  address is ever exposed by counters.
++	 */
++	for (i = 0; num; i++) {
++		if (likely(test_bit(i, cast_ulp(used_pmds)))) {
++			pfm_write_pmd(ctx, i, set->pmds[i].value);
++			num--;
++		}
++	}
++}
++
++/*
++ * function called from pfm_switch_sets(), pfm_context_load_thread(),
++ * pfm_context_load_sys(), pfm_ctxsw().
++ * Context is locked. Interrupts are masked. set cannot be NULL.
++ * Access to the PMU is guaranteed.
++ *
++ * function must restore all PMC registers from set
++ */
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	struct pfm_arch_context *ctx_arch;
++	u64 *mask;
++	u16 i, num;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	/*
++	 * we need to restore PMCs only when:
++	 * 	- context is not masked
++	 * 	- monitoring was activated
++	 *
++	 * Masking monitoring after an overflow does not change the
++	 * value of flags.started
++	 */
++	if (ctx->state == PFM_CTX_MASKED || !ctx->flags.started)
++		return;
++
++	/*
++	 * must restore DS pointer before restoring PMCs
++	 * as this can potentially reactivate monitoring
++	 */
++	if (ctx_arch->flags.use_ds)
++		wrmsrl(MSR_IA32_DS_AREA, (unsigned long)ctx_arch->ds_area);
++
++	/*
++	 * In general, writing MSRs is very expensive, so try to be smart.
++	 *
++	 * P6-style, Core-style:
++	 * 	- pmc are totally independent of each other, there is
++	 * 	  possible side-effect from stale pmcs. Therefore we only
++	 * 	  restore the registers we use
++	 * P4-style:
++	 * 	- must restore everything because there are some dependencies
++	 * 	(e.g., ESCR and CCCR)
++	 */
++	if (arch_info->pmu_style == PFM_X86_PMU_P4) {
++		num = pfm_pmu_conf->regs.num_pmcs;
++		mask = pfm_pmu_conf->regs.pmcs;
++	} else {
++		num = set->nused_pmcs;
++		mask = set->used_pmcs;
++	}
++	for (i = 0; num; i++) {
++		if (test_bit(i, cast_ulp(mask))) {
++			pfm_arch_write_pmc(ctx, i, set->pmcs[i]);
++			num--;
++		}
++	}
++}
++
++/*
++ * invoked only when NMI is used. Called from the LOCAL_PERFMON_VECTOR
++ * handler to copy P4 overflow state captured when the NMI triggered.
++ * Given that on P4, stopping monitoring destroy the overflow information
++ * we save it in pfm_has_ovfl_p4() where monitoring is also stopped.
++ *
++ * Here we propagate the overflow state to current active set. The
++ * freeze_pmu() call we not overwrite this state because npend_ovfls
++ * is non-zero.
++ */
++static void pfm_p4_copy_nmi_state(void)
++{
++	struct pfm_context *ctx;
++	struct pfm_arch_context *ctx_arch;
++	struct pfm_event_set *set;
++
++	ctx = __get_cpu_var(pmu_ctx);
++	if (!ctx)
++		return;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	set = ctx->active_set;
++
++	if (ctx_arch->p4->npend_ovfls) {
++		set->npend_ovfls = ctx_arch->p4->npend_ovfls;
++
++		bitmap_copy(cast_ulp(set->povfl_pmds),
++			    cast_ulp(ctx_arch->p4->povfl_pmds),
++			    pfm_pmu_conf->regs.max_pmd);
++
++		ctx_arch->p4->npend_ovfls = 0;
++	}
++}
++
++/*
++ * The PMU interrupt is handled through an interrupt gate, therefore
++ * the CPU automatically clears the EFLAGS.IF, i.e., masking interrupts.
++ *
++ * The perfmon interrupt handler MUST run with interrupts disabled due
++ * to possible race with other, higher priority interrupts, such as timer
++ * or IPI function calls.
++ *
++ * See description in IA-32 architecture manual, Vol 3 section 5.8.1
++ */
++fastcall void smp_pmu_interrupt(struct pt_regs *regs)
++{
++	struct pfm_arch_pmu_info *arch_info;
++	unsigned long iip;
++	int using_nmi;
++
++	using_nmi = __get_cpu_var(pfm_using_nmi);
++
++	ack_APIC_irq();
++
++	irq_enter();
++
++	/*
++	 * when using NMI, pfm_handle_nmi() gets called
++	 * first. It stops monitoring and record the
++	 * iip into real_iip, then it repost the interrupt
++	 * using the lower priority vector LOCAL_PERFMON_VECTOR
++	 *
++	 * On P4, due to the difficulty of detecting overflows
++	 * and stoppping the PMU, pfm_handle_nmi() needs to
++	 * record npend_ovfl and ovfl_pmds in ctx_arch. So
++	 * here we simply copy them back to the set.
++	 */
++	if (using_nmi) {
++		arch_info = pfm_pmu_conf->arch_info;
++		iip = __get_cpu_var(real_iip);
++		if (arch_info->pmu_style == PFM_X86_PMU_P4)
++			pfm_p4_copy_nmi_state();
++	} else
++		iip = instruction_pointer(regs);
++
++	pfm_interrupt_handler(iip, regs);
++
++	/*
++	 * On Intel P6, Pentium M, P4, Intel Core:
++	 * 	- it is necessary to clear the MASK field for the LVTPC
++	 * 	  vector. Otherwise interrupts remain masked. See
++	 * 	  section 8.5.1
++	 * AMD X86-64:
++	 * 	- the documentation does not stipulate the behavior.
++	 * 	  To be safe, we also rewrite the vector to clear the
++	 * 	  mask field
++	 */
++	if (!using_nmi && current_cpu_data.x86_vendor == X86_VENDOR_INTEL)
++		apic_write(APIC_LVTPC, LOCAL_PERFMON_VECTOR);
++
++	irq_exit();
++}
++
++/*
++ * detect is counters have overflowed.
++ * return:
++ * 	0 : no overflow
++ * 	1 : at least one overflow
++ *
++ * used by AMD64 and Intel architectural PMU
++ */
++static int __kprobes pfm_has_ovfl_p6(struct pfm_context *ctx)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	struct pfm_arch_ext_reg *xrd;
++	u64 *cnt_mask;
++	u64 wmask, val;
++	u16 i, num;
++
++	cnt_mask = pfm_pmu_conf->regs.cnt_pmds;
++	num = pfm_pmu_conf->regs.num_counters;
++	wmask = 1ULL << pfm_pmu_conf->counter_width;
++	xrd = arch_info->pmd_addrs;
++
++	for (i = 0; num; i++) {
++		if (test_bit(i, cast_ulp(cnt_mask))) {
++			rdmsrl(xrd[i].addrs[0], val);
++			if (!(val & wmask))
++				return 1;
++			num--;
++		}
++	}
++	return 0;
++}
++
++static int __kprobes pfm_has_ovfl_amd64(struct pfm_context *ctx)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	u64 val;
++	/*
++	 * Check for IBS events
++	 */
++	if (arch_info->flags & PFM_X86_FL_IBS) {
++		rdmsrl(pfm_pmu_conf->pmc_desc[arch_info->ibsfetchctl_pmc].hw_addr, val);
++		if (val & PFM_AMD64_IBSFETCHVAL)
++			return 1;
++		rdmsrl(pfm_pmu_conf->pmc_desc[arch_info->ibsopctl_pmc].hw_addr, val);
++		if (val & PFM_AMD64_IBSOPVAL)
++			return 1;
++	}
++	return pfm_has_ovfl_p6(ctx);
++}
++
++/*
++ * detect is counters have overflowed.
++ * return:
++ * 	0 : no overflow
++ * 	1 : at least one overflow
++ *
++ * used by Intel P4
++ */
++static int __kprobes pfm_has_ovfl_p4(struct pfm_context *ctx)
++{	
++	struct pfm_arch_ext_reg *xrc, *xrd;
++	struct pfm_arch_context *ctx_arch;
++	struct pfm_arch_p4_context *p4;
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	u64 ena_mask[PFM_PMC_BV];
++	u64 cccr, ctr1, ctr2;
++	int n, i, j;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++	xrc = arch_info->pmc_addrs;
++	xrd = arch_info->pmd_addrs;
++	p4 = ctx_arch->p4;
++
++	bitmap_and(cast_ulp(ena_mask),
++			cast_ulp(pfm_pmu_conf->regs.pmcs),
++			cast_ulp(arch_info->enable_mask),
++			arch_info->max_ena);
++
++	n = bitmap_weight(cast_ulp(ena_mask), arch_info->max_ena);
++
++	for(i=0; n; i++) {
++		if (!test_bit(i, cast_ulp(ena_mask)))
++			continue;
++		/*
++		 * controlled counter
++		 */
++		j = xrc[i].ctr;
++
++		/* read CCCR (PMC) value */
++		__pfm_read_reg_p4(xrc+i, &cccr);
++
++		/* read counter (PMD) controlled by PMC */
++		__pfm_read_reg_p4(xrd+j, &ctr1);
++
++		/* clear CCCR value: stop counter but destroy OVF */
++		__pfm_write_reg_p4(xrc+i, 0);
++
++		/* read counter controlled by CCCR again */
++		__pfm_read_reg_p4(xrd+j, &ctr2);
++
++		/*
++		 * there is an overflow if either:
++		 * 	- CCCR.ovf is set (and we just cleared it)
++		 * 	- ctr2 < ctr1
++		 * in that case we set the bit corresponding to the
++		 * overflowed PMD in povfl_pmds.
++		 */
++		if ((cccr & (1ULL<<31)) || (ctr2 < ctr1)) {
++			__set_bit(j, cast_ulp(ctx_arch->p4->povfl_pmds));
++			ctx_arch->p4->npend_ovfls++;
++		}
++		p4->saved_cccrs[i] = cccr;
++		n--;
++	}
++	/*
++	 * if there was no overflow, then it means the NMI was not really
++	 * for us, so we have to resume monitoring
++	 */
++	if (unlikely(!ctx_arch->p4->npend_ovfls)) {
++		for(i=0; n; i++) {
++			if (!test_bit(i, cast_ulp(ena_mask)))
++				continue;
++			__pfm_write_reg_p4(xrc+i, ctx_arch->p4->saved_cccrs[i]);
++		}
++	}
++	return 0;
++}
++
++/*
++ * detect is counters have overflowed.
++ * return:
++ * 	0 : no overflow
++ * 	1 : at least one overflow
++ *
++ * used by Intel Core-based processors
++ */
++static int __kprobes pfm_has_ovfl_intel_core(struct pfm_context *ctx)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	struct pfm_arch_ext_reg *xrd;
++	u64 *cnt_mask;
++	u64 wmask, val;
++	u16 i, num;
++
++	cnt_mask = pfm_pmu_conf->regs.cnt_pmds;
++	num = pfm_pmu_conf->regs.num_counters;
++	wmask = 1ULL << pfm_pmu_conf->counter_width;
++	xrd = arch_info->pmd_addrs;
++
++	for (i = 0; num; i++) {
++		if (test_bit(i, cast_ulp(cnt_mask))) {
++			rdmsrl(xrd[i].addrs[0], val);
++			if (!(val & wmask))
++				return 1;
++			num--;
++		}
++	}
++	return 0;
++}
++
++/*
++ * called from notify_die() notifier from an trap handler path. We only
++ * care about NMI related callbacks, and ignore everything else.
++ *
++ * Cannot grab any locks, include the perfmon context lock
++ *
++ * Must detect if NMI interrupt comes from perfmon, and if so it must
++ * stop the PMU and repost a lower-priority interrupt. The perfmon interrupt
++ * handler needs to grab the context lock, thus is cannot be run directly
++ * from the NMI interrupt call path.
++ */
++static int __kprobes pfm_handle_nmi(struct notifier_block *nb, unsigned long val,
++			      void *data)
++{
++	struct die_args *args = data;
++	struct pfm_context *ctx;
++
++	/*
++	 * only NMI related calls
++	 */
++	if (val != DIE_NMI_IPI)
++		return NOTIFY_DONE;
++
++	if (!__get_cpu_var(pfm_using_nmi))
++		return NOTIFY_DONE;
++
++	/*
++	 * perfmon not active on this processor
++	 */
++	ctx = __get_cpu_var(pmu_ctx);
++	if (ctx == NULL) {
++		PFM_DBG_ovfl("ctx NULL");
++		return NOTIFY_DONE;
++	}
++
++	/*
++	 * detect if we have overflows, i.e., NMI interrupt
++	 * caused by PMU
++	 */
++	if (!pfm_has_ovfl(ctx)) {
++		PFM_DBG_ovfl("no ovfl");
++		return NOTIFY_DONE;
++	}
++
++	/*
++	 * we stop the PMU to avoid further overflow before this
++	 * one is treated by lower priority interrupt handler
++	 */
++	__pfm_arch_quiesce_pmu_percpu();
++
++	/*
++	 * record actual instruction pointer
++	 */
++	__get_cpu_var(real_iip) = instruction_pointer(args->regs);
++
++	/*
++	 * post lower priority interrupt (LOCAL_PERFMON_VECTOR)
++	 */
++	pfm_arch_resend_irq();
++
++	pfm_stats_get(ovfl_intr_nmi_count)++;
++
++	/*
++	 * we need to rewrite the APIC vector on Intel
++	 */
++	if (current_cpu_data.x86_vendor == X86_VENDOR_INTEL)
++		apic_write(APIC_LVTPC, APIC_DM_NMI);
++
++	/*
++	 * the notification was for us
++	 */
++	return NOTIFY_STOP;
++}
++
++static struct notifier_block pfm_nmi_nb = {
++	.notifier_call = pfm_handle_nmi
++};
++
++/*
++ * called from pfm_register_pmu_config() after the new
++ * config has been validated. The pfm_session_lock
++ * is held.
++ *
++ * return:
++ * 	< 0 : if error
++ * 	  0 : if success
++ */
++int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
++{
++	struct pfm_arch_pmu_info *arch_info = cfg->arch_info;
++
++	/*
++	 * adust stop routine based on PMU model
++	 *
++	 * P6  : P6, Pentium M, Intel architectural perfmon
++	 * P4  : Xeon, EM64T, P4
++	 * Core: Core 2,
++	 * AMD64: AMD64 (K8, family 10h)
++	 */
++	switch(arch_info->pmu_style) {
++	case PFM_X86_PMU_P4:
++		pfm_stop_save = pfm_stop_save_p4;
++		pfm_has_ovfl  = pfm_has_ovfl_p4;
++		break;
++	case PFM_X86_PMU_P6:
++		pfm_stop_save = pfm_stop_save_p6;
++		pfm_has_ovfl  = pfm_has_ovfl_p6;
++		break;
++	case PFM_X86_PMU_CORE:
++		pfm_stop_save = pfm_stop_save_intel_core;
++		pfm_has_ovfl  = pfm_has_ovfl_intel_core;
++		break;
++	case PFM_X86_PMU_AMD64:
++		pfm_stop_save = pfm_stop_save_amd64;
++		pfm_has_ovfl  = pfm_has_ovfl_amd64;
++		break;
++	default:
++		PFM_INFO("unknown pmu_style=%d", arch_info->pmu_style);
++		return -EINVAL;
++	}
++	return 0;
++}
++
++void pfm_arch_pmu_config_remove(void)
++{
++}
++
++char *pfm_arch_get_pmu_module_name(void)
++{
++	switch(current_cpu_data.x86) {
++	case 6:
++		switch(current_cpu_data.x86_model) {
++		case 3: /* Pentium II */
++		case 7 ... 11:
++		case 13:
++			return "perfmon_p6";
++		case 15: /* Merom */
++		case 23: /* Penryn */
++			return "perfmon_intel_core";
++		default:
++			goto try_arch;
++		}
++	case 15:
++	case 16:
++		/* All Opteron processors */
++		if (current_cpu_data.x86_vendor == X86_VENDOR_AMD)
++			return "perfmon_amd64";
++
++		switch(current_cpu_data.x86_model) {
++		case 0 ... 6:
++			return "perfmon_p4";
++		}
++		/* FALL THROUGH */
++	default:
++try_arch:
++		if (boot_cpu_has(X86_FEATURE_ARCH_PERFMON))
++			return "perfmon_intel_arch";
++		return NULL;
++	}
++	return NULL;
++}
++
++void pfm_arch_resend_irq(void)
++{
++	unsigned long val, dest;
++	/*
++	 * we cannot use hw_resend_irq() because it goes to
++	 * the I/O APIC. We need to go to the Local APIC.
++	 *
++	 * The "int vec" is not the right solution either
++	 * because it triggers a software intr. We need
++	 * to regenerate the interrupt and have it pended
++	 * until we unmask interrupts.
++	 *
++	 * Instead we send ourself an IPI on the perfmon
++	 * vector.
++	 */
++	val  = APIC_DEST_SELF|APIC_INT_ASSERT|
++	       APIC_DM_FIXED|LOCAL_PERFMON_VECTOR;
++
++	dest = apic_read(APIC_ID);
++	apic_write(APIC_ICR2, dest);
++	apic_write(APIC_ICR, val);
++}
++
++DEFINE_PER_CPU(unsigned long, saved_lvtpc);
++
++static void pfm_arch_pmu_acquire_percpu(void *data)
++{
++
++	unsigned int tmp, vec;
++	unsigned long flags = (unsigned long)data;
++	unsigned long lvtpc;
++
++	/*
++	 * we only reprogram the LVTPC vector if we have detected
++	 * no sharing, otherwise it means the APIC is already program
++	 * and we use whatever vector (likely NMI) was used
++	 */
++	if (!(flags & PFM_X86_FL_SHARING)) {
++		vec = flags & PFM_X86_FL_USE_NMI ? APIC_DM_NMI : LOCAL_PERFMON_VECTOR;
++		tmp = apic_read(APIC_LVTERR);
++		apic_write(APIC_LVTERR, tmp | APIC_LVT_MASKED);
++		apic_write(APIC_LVTPC, vec);
++		apic_write(APIC_LVTERR, tmp);
++		PFM_DBG("written LVTPC=0x%x", vec);
++	}
++	lvtpc = (unsigned long)apic_read(APIC_LVTPC);
++	__get_cpu_var(pfm_using_nmi) = lvtpc == APIC_DM_NMI;
++	PFM_DBG("LTVPC=0x%lx using_nmi=%d", lvtpc, __get_cpu_var(pfm_using_nmi));
++}
++
++/*
++ * called from pfm_pmu_acquire() with
++ * pfm_pmu_conf.regs copied from pfm_pmu_conf.full_regs
++ * needs to adjust regs to match current PMU availabilityy
++ *
++ * Caller does recalculate all max/num/first limits on the
++ * pfm_pmu_conf.regs structure.
++ *
++ * interrupts are not masked
++ *
++ *
++ * XXX: until reserve_*_nmi() get fixed by Bjorn to work
++ * correctly whenever the NMI watchdog is not used. We skip
++ * the allocation. Yet we do the percpu initialization.
++ */
++int pfm_arch_pmu_acquire(void)
++{
++	struct pfm_arch_pmu_info *arch_info;
++	struct pfm_regmap_desc *d;
++	struct pfm_arch_ext_reg *pc;
++	u16 i, n, ena = 0, nlost;
++
++	arch_info = pfm_pmu_conf->arch_info;
++	pc = arch_info->pmc_addrs;
++
++	bitmap_zero(cast_ulp(arch_info->enable_mask), PFM_MAX_PMCS);
++	arch_info->flags &= ~PFM_X86_FL_SHARING;
++
++	d = pfm_pmu_conf->pmc_desc;
++	n = pfm_pmu_conf->regs.num_pmcs;
++	nlost = 0;
++	for(i=0; n; i++, d++) {
++		/*
++		 * skip not implemented registers (including those
++		 * already removed by the module)
++		 */
++		if (!(d->type & PFM_REG_I))
++			continue;
++
++		n--;
++
++		if (d->type & PFM_REG_V)
++			continue;
++
++		/*
++		 * reserve register with lower-level allocator
++		 */
++		if (!reserve_evntsel_nmi(d->hw_addr)) {
++			PFM_DBG("pmc%d (%s) in use elsewhere, disabling", i, d->desc);
++			__clear_bit(i, cast_ulp(pfm_pmu_conf->regs.pmcs));
++			nlost++;
++			continue;
++		}
++
++		if (!(pc[i].reg_type & PFM_REGT_EN))
++			continue;
++		__set_bit(i, cast_ulp(arch_info->enable_mask));
++		ena++;
++		arch_info->max_ena = i + 1;
++	}
++
++	PFM_DBG("%u PMCs with enable capability", ena);
++
++	if (!ena) {
++		PFM_INFO("no registers with start/stop capability,"
++			 "try rebooting with nmi_watchdog=0, or check that Oprofile is not running");
++		goto undo;
++	}
++	PFM_DBG("nlost=%d info_flags=0x%x\n", nlost, arch_info->flags);
++	/*
++	 * some PMU models (e.g., P6) do not support sharing
++	 * so check if we found less than the expected number of PMC registers
++	 */
++	if (nlost) {
++		if (arch_info->flags & PFM_X86_FL_NO_SHARING) {
++			PFM_INFO("PMU already used by another subsystem, "
++				 "PMU does not support sharing, "
++				 "try disabling Oprofile or "
++				 "reboot with nmi_watchdog=0");
++			goto undo;
++		}
++		arch_info->flags |= PFM_X86_FL_SHARING;
++	}
++
++	d = pfm_pmu_conf->pmd_desc;
++	n = pfm_pmu_conf->regs.num_pmds;
++	for(i=0; n; i++, d++) {
++		if (!(d->type & PFM_REG_I))
++			continue;
++		n--;
++
++		if (d->type & PFM_REG_V)
++			continue;
++
++		if (!reserve_perfctr_nmi(d->hw_addr)) {
++			PFM_DBG("pmd%d (%s) in use elsewhere, disabling", i, d->desc);
++			__clear_bit(i, cast_ulp(pfm_pmu_conf->regs.pmds));
++			__clear_bit(i, cast_ulp(pfm_pmu_conf->regs.cnt_pmds));
++			__clear_bit(i, cast_ulp(pfm_pmu_conf->regs.rw_pmds));
++		}
++	}
++	/*
++	 * program APIC on each CPU
++	 */
++	on_each_cpu(pfm_arch_pmu_acquire_percpu,
++		    (void *)(unsigned long)arch_info->flags , 0, 1);
++
++	return 0;
++undo:
++	/*
++	 * must undo reservation in case of error
++	 */
++	n = pfm_pmu_conf->regs.max_pmc;
++	d = pfm_pmu_conf->pmc_desc;
++	for(i=0; i < n; i++, d++) {
++		if (!test_bit(i, cast_ulp(pfm_pmu_conf->regs.pmcs)))
++			continue;
++		release_evntsel_nmi(d->hw_addr);
++	}
++	return -EBUSY;
++}
++
++static void pfm_arch_pmu_release_percpu(void *data)
++{
++	__get_cpu_var(pfm_using_nmi) = 0;
++}
++
++/*
++ * called from pfm_pmu_release()
++ * interrupts are not masked
++ */
++void pfm_arch_pmu_release(void)
++{
++	struct pfm_regmap_desc *d;
++	u16 i, n;
++
++	d = pfm_pmu_conf->pmc_desc;
++	n = pfm_pmu_conf->regs.num_pmcs;
++	for(i=0; n; i++, d++) {
++		if (!test_bit(i, cast_ulp(pfm_pmu_conf->regs.pmcs)))
++			continue;
++		release_evntsel_nmi(d->hw_addr);
++		n--;
++		PFM_DBG("pmc%u released", i);
++	}
++	d = pfm_pmu_conf->pmd_desc;
++	n = pfm_pmu_conf->regs.num_pmds;
++	for(i=0; n; i++, d++) {
++		if (!test_bit(i, cast_ulp(pfm_pmu_conf->regs.pmds)))
++			continue;
++		release_perfctr_nmi(d->hw_addr);
++		n--;
++		PFM_DBG("pmd%u released", i);
++	}
++	on_each_cpu(pfm_arch_pmu_release_percpu, NULL , 0, 1);
++}
++
++int pfm_arch_init(void)
++{
++	/*
++	 * we need to register our NMI handler when the kernels boots
++	 * to avoid a deadlock condition with the NMI watchdog or Oprofile
++	 * if we were to try and register/unregister on-demand.
++	 */
++	register_die_notifier(&pfm_nmi_nb);
++	return 0;
++}
+--- /dev/null
++++ b/arch/x86/perfmon/perfmon_amd64.c
+@@ -0,0 +1,641 @@
++/*
++ * This file contains the PMU description for the Athlon64 and Opteron64
++ * processors. It supports 32 and 64-bit modes.
++ *
++ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * Copyright (c) 2007 Advanced Micro Devices, Inc.
++ * Contributed by Robert Richter <robert.richter@amd.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <linux/vmalloc.h>
++#include <linux/pci.h>
++#include <asm/apic.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_AUTHOR("Robert Richter <robert.richter@amd.com>");
++MODULE_DESCRIPTION("AMD64 PMU description table");
++MODULE_LICENSE("GPL");
++
++static int force_nmi;
++MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
++module_param(force_nmi, bool, 0600);
++
++static struct pfm_arch_pmu_info pfm_amd64_pmu_info = {
++	.pmc_addrs = {
++/* pmc0  */	{{MSR_K7_EVNTSEL0, 0}, 0, PFM_REGT_EN},
++/* pmc1  */	{{MSR_K7_EVNTSEL1, 0}, 1, PFM_REGT_EN},
++/* pmc2  */	{{MSR_K7_EVNTSEL2, 0}, 2, PFM_REGT_EN},
++/* pmc3  */	{{MSR_K7_EVNTSEL3, 0}, 3, PFM_REGT_EN},
++/* pmc4  */	{{MSR_AMD64_IBSFETCHCTL, 0}, 0, PFM_REGT_EN|PFM_REGT_IBS},
++/* pmc5  */	{{MSR_AMD64_IBSOPCTL, 0}, 0, PFM_REGT_EN|PFM_REGT_IBS},
++	},
++	.pmd_addrs = {
++/* pmd0  */	{{MSR_K7_PERFCTR0, 0}, 0, PFM_REGT_CTR},
++/* pmd1  */	{{MSR_K7_PERFCTR1, 0}, 0, PFM_REGT_CTR},
++/* pmd2  */	{{MSR_K7_PERFCTR2, 0}, 0, PFM_REGT_CTR},
++/* pmd3  */	{{MSR_K7_PERFCTR3, 0}, 0, PFM_REGT_CTR},
++/* pmd4  */	{{MSR_AMD64_IBSFETCHCTL, 0}, 0, PFM_REGT_IBS},
++/* pmd5  */	{{MSR_AMD64_IBSFETCHLINAD, 0}, 0, PFM_REGT_IBS},
++/* pmd6  */	{{MSR_AMD64_IBSFETCHPHYSAD, 0}, 0, PFM_REGT_IBS},
++/* pmd7  */	{{MSR_AMD64_IBSOPCTL, 0}, 0, PFM_REGT_IBS},
++/* pmd8  */	{{MSR_AMD64_IBSOPRIP, 0}, 0, PFM_REGT_IBS},
++/* pmd9  */	{{MSR_AMD64_IBSOPDATA, 0}, 0, PFM_REGT_IBS},
++/* pmd10 */	{{MSR_AMD64_IBSOPDATA2, 0}, 0, PFM_REGT_IBS},
++/* pmd11 */	{{MSR_AMD64_IBSOPDATA3, 0}, 0, PFM_REGT_IBS_EXT},
++/* pmd12 */	{{MSR_AMD64_IBSDCLINAD, 0}, 0, PFM_REGT_IBS_EXT},
++/* pmd13 */	{{MSR_AMD64_IBSDCPHYSAD, 0}, 0, PFM_REGT_IBS_EXT},
++	},
++	.ibsfetchctl_pmc = 4,
++	.ibsfetchctl_pmd = 4,
++	.ibsopctl_pmc = 5,
++	.ibsopctl_pmd = 7,
++	.pmu_style = PFM_X86_PMU_AMD64,
++};
++
++/*
++ * force Local APIC interrupt on overflow
++ */
++#define PFM_K8_VAL	(1ULL<<20)
++#define PFM_K8_NO64	(1ULL<<20)
++
++/*
++ * for performance counter control registers:
++ *
++ * reserved bits must be zero
++ *
++ * for family 15:
++ * - upper 32 bits are reserved
++ *
++ * for family 16:
++ * - bits 36-39 are reserved
++ * - bits 42-63 are reserved
++ */
++#define PFM_K8_RSVD ((~((1ULL<<32)-1)) | (1ULL<<20) | (1ULL<<21))
++#define PFM_16_RSVD ((0x3fffffULL<<42) | (0xfULL<<36) | (1ULL<<20) | (1ULL<<21))
++
++/*
++ * for IBS registers:
++ * 	IBSFETCHCTL: all bits are reserved except bits 57, 48, 15:0
++ * 	IBSOPSCTL  : all bits are reserved except bits 17, 15:0
++ */
++#define PFM_AMD64_IBSFETCHCTL_RSVD	(~((1ULL<<48)|(1ULL<<57)|0xffffULL))
++#define PFM_AMD64_IBSOPCTL_RSVD		(~((1ULL<<17)|0xffffULL))
++
++#define IBSCTL				0x1cc
++#define IBSCTL_LVTOFFSETVAL		(1 << 8)
++
++#define ENABLE_CF8_EXT_CFG		(1ULL << 46)
++
++static struct pfm_regmap_desc pfm_amd64_pmc_desc[] = {
++/* pmc0  */ PMC_D(PFM_REG_I64, "PERFSEL0", PFM_K8_VAL, PFM_K8_RSVD, PFM_K8_NO64, MSR_K7_EVNTSEL0),
++/* pmc1  */ PMC_D(PFM_REG_I64, "PERFSEL1", PFM_K8_VAL, PFM_K8_RSVD, PFM_K8_NO64, MSR_K7_EVNTSEL1),
++/* pmc2  */ PMC_D(PFM_REG_I64, "PERFSEL2", PFM_K8_VAL, PFM_K8_RSVD, PFM_K8_NO64, MSR_K7_EVNTSEL2),
++/* pmc3  */ PMC_D(PFM_REG_I64, "PERFSEL3", PFM_K8_VAL, PFM_K8_RSVD, PFM_K8_NO64, MSR_K7_EVNTSEL3),
++/* pmc4  */ PMC_D(PFM_REG_I,   "IBSFETCHCTL", 0, PFM_AMD64_IBSFETCHCTL_RSVD, 0, MSR_AMD64_IBSFETCHCTL),
++/* pmc5  */ PMC_D(PFM_REG_I,   "IBSOPCTL",    0, PFM_AMD64_IBSOPCTL_RSVD,    0, MSR_AMD64_IBSOPCTL),
++};
++#define PFM_AMD_NUM_PMCS ARRAY_SIZE(pfm_amd64_pmc_desc)
++
++#define PFM_REG_IBS (PFM_REG_I|PFM_REG_INTR)
++static struct pfm_regmap_desc pfm_amd64_pmd_desc[] = {
++/* pmd0  */ PMD_D(PFM_REG_C,   "PERFCTR0",	MSR_K7_PERFCTR0),
++/* pmd1  */ PMD_D(PFM_REG_C,   "PERFCTR1",	MSR_K7_PERFCTR1),
++/* pmd2  */ PMD_D(PFM_REG_C,   "PERFCTR2",	MSR_K7_PERFCTR2),
++/* pmd3  */ PMD_D(PFM_REG_C,   "PERFCTR3",	MSR_K7_PERFCTR3),
++/* pmd4  */ PMD_D(PFM_REG_IBS, "IBSFETCHCTL",	MSR_AMD64_IBSFETCHCTL),
++/* pmd5  */ PMD_D(PFM_REG_IRO, "IBSFETCHLINAD",	MSR_AMD64_IBSFETCHLINAD),
++/* pmd6  */ PMD_D(PFM_REG_IRO, "IBSFETCHPHYSAD", MSR_AMD64_IBSFETCHPHYSAD),
++/* pmd7  */ PMD_D(PFM_REG_IBS, "IBSOPCTL",	MSR_AMD64_IBSOPCTL),
++/* pmd8  */ PMD_D(PFM_REG_IRO, "IBSOPRIP",	MSR_AMD64_IBSOPRIP),
++/* pmd9  */ PMD_D(PFM_REG_IRO, "IBSOPDATA",	MSR_AMD64_IBSOPDATA),
++/* pmd10 */ PMD_D(PFM_REG_IRO, "IBSOPDATA2",	MSR_AMD64_IBSOPDATA2),
++/* pmd11 */ PMD_D(PFM_REG_IRO, "IBSOPDATA3",	MSR_AMD64_IBSOPDATA3),
++/* pmd12 */ PMD_D(PFM_REG_IRO, "IBSDCLINAD",	MSR_AMD64_IBSDCLINAD),
++/* pmd13 */ PMD_D(PFM_REG_IRO, "IBSDCPHYSAD",	MSR_AMD64_IBSDCPHYSAD),
++};
++#define PFM_AMD_NUM_PMDS ARRAY_SIZE(pfm_amd64_pmd_desc)
++
++static struct pfm_context **pfm_nb_sys_owners;
++static struct pfm_context *pfm_nb_task_owner;
++
++static struct pfm_pmu_config pfm_amd64_pmu_conf;
++
++/* Functions for accessing extended PCI config space. Can be removed
++   when Kernel API exists. */
++extern spinlock_t pci_config_lock;
++
++#define PCI_CONF1_ADDRESS(bus, devfn, reg) \
++	(0x80000000 | ((reg & 0xF00) << 16) | ((bus & 0xFF) << 16) \
++	| (devfn << 8) | (reg & 0xFC))
++
++#define is_ibs(x) (pfm_amd64_pmu_info.pmc_addrs[x].reg_type & PFM_REGT_IBS)
++
++static int pci_read(unsigned int seg, unsigned int bus,
++		    unsigned int devfn, int reg, int len, u32 *value)
++{
++	unsigned long flags;
++
++	if ((bus > 255) || (devfn > 255) || (reg > 4095)) {
++		*value = -1;
++		return -EINVAL;
++	}
++
++	spin_lock_irqsave(&pci_config_lock, flags);
++
++	outl(PCI_CONF1_ADDRESS(bus, devfn, reg), 0xCF8);
++
++	switch (len) {
++	case 1:
++		*value = inb(0xCFC + (reg & 3));
++		break;
++	case 2:
++		*value = inw(0xCFC + (reg & 2));
++		break;
++	case 4:
++		*value = inl(0xCFC);
++		break;
++	}
++
++	spin_unlock_irqrestore(&pci_config_lock, flags);
++
++	return 0;
++}
++
++static int pci_write(unsigned int seg, unsigned int bus,
++		     unsigned int devfn, int reg, int len, u32 value)
++{
++	unsigned long flags;
++
++	if ((bus > 255) || (devfn > 255) || (reg > 4095))
++		return -EINVAL;
++
++	spin_lock_irqsave(&pci_config_lock, flags);
++
++	outl(PCI_CONF1_ADDRESS(bus, devfn, reg), 0xCF8);
++
++	switch (len) {
++	case 1:
++		outb((u8)value, 0xCFC + (reg & 3));
++		break;
++	case 2:
++		outw((u16)value, 0xCFC + (reg & 2));
++		break;
++	case 4:
++		outl((u32)value, 0xCFC);
++		break;
++	}
++
++	spin_unlock_irqrestore(&pci_config_lock, flags);
++
++	return 0;
++}
++
++static inline int
++pci_read_ext_config_dword(struct pci_dev *dev, int where, u32 *val)
++{
++	return pci_read(0, dev->bus->number, dev->devfn, where, 4, val);
++}
++
++static inline int
++pci_write_ext_config_dword(struct pci_dev *dev, int where, u32 val)
++{
++	return pci_write(0, dev->bus->number, dev->devfn, where, 4, val);
++}
++
++static void pfm_amd64_enable_pci_ecs_per_cpu(void)
++{
++	u64 reg;
++	/* enable PCI extended config space */
++	rdmsrl(MSR_AMD64_NB_CFG, reg);
++	if (reg & ENABLE_CF8_EXT_CFG)
++		return;
++	reg |= ENABLE_CF8_EXT_CFG;
++	wrmsrl(MSR_AMD64_NB_CFG, reg);
++}
++
++static void pfm_amd64_setup_eilvt_per_cpu(void *info)
++{
++	u8 lvt_off;
++
++	pfm_amd64_enable_pci_ecs_per_cpu();
++
++	/* program the IBS vector to the perfmon vector */
++	lvt_off =  setup_APIC_eilvt_ibs(LOCAL_PERFMON_VECTOR,
++					APIC_EILVT_MSG_FIX, 0);
++	PFM_DBG("APIC_EILVT%d set to 0x%x", lvt_off, LOCAL_PERFMON_VECTOR);
++	pfm_amd64_pmu_info.ibs_eilvt_off = lvt_off;
++}
++
++static int pfm_amd64_setup_eilvt(void)
++{
++	struct pci_dev *cpu_cfg;
++	int nodes;
++	u32 value = 0;
++
++	/* per CPU setup */
++	on_each_cpu(pfm_amd64_setup_eilvt_per_cpu, NULL, 0, 1);
++
++	nodes = 0;
++	cpu_cfg = NULL;
++	do {
++		cpu_cfg = pci_get_device(PCI_VENDOR_ID_AMD,
++					 PCI_DEVICE_ID_AMD_10H_NB_MISC,
++					 cpu_cfg);
++		if (!cpu_cfg)
++			break;
++		++nodes;
++		pci_write_ext_config_dword(cpu_cfg, IBSCTL,
++					   pfm_amd64_pmu_info.ibs_eilvt_off
++					   | IBSCTL_LVTOFFSETVAL);
++		pci_read_ext_config_dword(cpu_cfg, IBSCTL, &value);
++		if (value != (pfm_amd64_pmu_info.ibs_eilvt_off
++			      | IBSCTL_LVTOFFSETVAL)) {
++			PFM_DBG("Failed to setup IBS LVT offset, "
++				"IBSCTL = 0x%08x", value);
++			return 1;
++		}
++	} while (1);
++
++	if (!nodes) {
++		PFM_DBG("No CPU node configured for IBS");
++		return 1;
++	}
++
++#ifdef CONFIG_X86_64
++	/* Sanity check */
++	/* Works only for 64bit with proper numa implementation. */
++	if (nodes != num_possible_nodes()) {
++		PFM_DBG("Failed to setup CPU node(s) for IBS, "
++			"found: %d, expected %d",
++			nodes, num_possible_nodes());
++		return 1;
++	}
++#endif
++	return 0;
++}
++
++/*
++ * There can only be one user per socket for the Northbridge (NB) events,
++ * so we enforce mutual exclusion as follows:
++ * 	- per-thread : only one context machine-wide can use NB events
++ * 	- system-wide: only one context per processor socket
++ *
++ * Exclusion is enforced at:
++ * 	- pfm_load_context()
++ * 	- pfm_write_pmcs() for attached contexts
++ *
++ * Exclusion is released at:
++ * 	- pfm_unload_context() or any calls that implicitely uses it
++ *
++ * return:
++ * 	0  : successfully acquire NB access
++ * 	< 0:  errno, failed to acquire NB access
++ */
++static int pfm_amd64_acquire_nb(struct pfm_context *ctx)
++{
++	struct pfm_context **entry, *old;
++	int proc_id;
++
++#ifdef CONFIG_SMP
++	proc_id = topology_physical_package_id(smp_processor_id());
++#else
++	proc_id = 0;
++#endif
++
++	if (ctx->flags.system)
++		entry = &pfm_nb_sys_owners[proc_id];
++	else
++		entry = &pfm_nb_task_owner;
++
++	old = cmpxchg(entry, NULL, ctx);
++	if (!old) {
++		if (ctx->flags.system)
++			PFM_DBG("acquired Northbridge event access on socket %u", proc_id);
++		else
++			PFM_DBG("acquired Northbridge event access globally");
++	} else if (old != ctx) {
++		if (ctx->flags.system)
++			PFM_DBG("NorthBridge event conflict on socket %u", proc_id);
++		else
++			PFM_DBG("global NorthBridge event conflict");
++		return -EBUSY;
++	}
++	return 0;
++}
++
++/*
++ * invoked from pfm_write_pmcs() when pfm_nb_sys_owners is not NULL,i.e.,
++ * when we have detected a multi-core processor.
++ *
++ * context is locked, interrupts are masked
++ */
++static int pfm_amd64_pmc_write_check(struct pfm_context *ctx,
++			     struct pfm_event_set *set,
++			     struct pfarg_pmc *req)
++{
++	unsigned int event;
++
++	/*
++	 * delay checking NB event until we load the context
++	 */
++	if (ctx->state == PFM_CTX_UNLOADED)
++		return 0;
++
++	/*
++	 * check event is NB event
++	 */
++	event = (unsigned int)(req->reg_value & 0xff);
++	if (event < 0xee)
++		return 0;
++
++	return pfm_amd64_acquire_nb(ctx);
++}
++
++/*
++ * invoked on pfm_load_context().
++ * context is locked, interrupts are masked
++ */
++static int pfm_amd64_load_context(struct pfm_context *ctx)
++{
++	struct pfm_event_set *set;
++	unsigned int i, n;
++
++	/*
++	 * scan all sets for NB events
++	 */
++	list_for_each_entry(set, &ctx->set_list, list) {
++		n = set->nused_pmcs;
++		for (i = 0; n; i++) {
++			if (!test_bit(i, cast_ulp(set->used_pmcs)))
++				continue;
++
++			if (!is_ibs(i) && (set->pmcs[i] & 0xff) >= 0xee)
++				goto found;
++			n--;
++		}
++	}
++	return 0;
++found:
++	return pfm_amd64_acquire_nb(ctx);
++}
++
++/*
++ * invoked on pfm_unload_context()
++ */
++static int pfm_amd64_unload_context(struct pfm_context *ctx)
++{
++	struct pfm_context **entry, *old;
++	int proc_id;
++
++#ifdef CONFIG_SMP
++	proc_id = topology_physical_package_id(smp_processor_id());
++#else
++	proc_id = 0;
++#endif
++
++	/*
++	 * unload always happens on the monitored CPU in system-wide
++	 */
++	if (ctx->flags.system)
++		entry = &pfm_nb_sys_owners[proc_id];
++	else
++		entry = &pfm_nb_task_owner;
++
++	old = cmpxchg(entry, ctx, NULL);
++	if (old == ctx) {
++		if (ctx->flags.system)
++			PFM_DBG("released NorthBridge on socket %u", proc_id);
++		else
++			PFM_DBG("released NorthBridge events globally");
++	}
++	return 0;
++}
++
++/*
++ * detect if we need to activate NorthBridge event access control
++ */
++static int pfm_amd64_setup_nb_event_control(void)
++{
++	unsigned int c, n = 0;
++	unsigned int max_phys = 0;
++
++#ifdef CONFIG_SMP
++	for_each_possible_cpu(c) {
++		if (cpu_data(c).phys_proc_id > max_phys)
++			max_phys = cpu_data(c).phys_proc_id;
++	}
++#else
++	max_phys = 0;
++#endif
++	if (max_phys > 255) {
++		PFM_INFO("socket id %d is too big to handle", max_phys);
++		return -ENOMEM;
++	}
++
++	n = max_phys + 1;
++	if (n < 2)
++		return 0;
++
++	pfm_nb_sys_owners = vmalloc(n * sizeof(*pfm_nb_sys_owners));
++	if (!pfm_nb_sys_owners)
++		return -ENOMEM;
++
++	memset(pfm_nb_sys_owners, 0, n * sizeof(*pfm_nb_sys_owners));
++	pfm_nb_task_owner = NULL;
++
++	/*
++	 * activate write-checker for PMC registers
++	 */
++	for (c = 0; c < PFM_AMD_NUM_PMCS; c++) {
++		if (!is_ibs(c))
++			pfm_amd64_pmc_desc[c].type |= PFM_REG_WC;
++	}
++
++	pfm_amd64_pmu_info.load_context = pfm_amd64_load_context;
++	pfm_amd64_pmu_info.unload_context = pfm_amd64_unload_context;
++
++	pfm_amd64_pmu_conf.pmc_write_check = pfm_amd64_pmc_write_check;
++
++	PFM_INFO("NorthBridge event access control enabled");
++
++	return 0;
++}
++
++/*
++ * disable registers which are not available on
++ * the host (applies to IBS registers)
++ */
++static void pfm_amd64_check_registers(void)
++{
++	struct pfm_arch_ext_reg *ext_reg;
++	u16 i, has_ibs, has_ibsext;
++
++	has_ibs = pfm_amd64_pmu_info.flags & PFM_X86_FL_IBS;
++	has_ibsext = pfm_amd64_pmu_info.flags & PFM_X86_FL_IBS_EXT;
++
++	PFM_DBG("has_ibs=%d has_ibs_ext=%d", has_ibs, has_ibsext);
++
++	/*
++	 * Scan PMC registers
++	 */
++	ext_reg = pfm_amd64_pmu_info.pmc_addrs;
++	for (i = 0; i < PFM_AMD_NUM_PMCS;  i++, ext_reg++) {
++		if (!has_ibs && ext_reg->reg_type & PFM_REGT_IBS) {
++			ext_reg->reg_type = PFM_REGT_NA;
++			pfm_amd64_pmc_desc[i].type = PFM_REG_NA;
++			PFM_DBG("pmc%u not available", i);
++		}
++		if (!has_ibsext && ext_reg->reg_type & PFM_REGT_IBS_EXT) {
++			ext_reg->reg_type = PFM_REGT_NA;
++			pfm_amd64_pmc_desc[i].type = PFM_REG_NA;
++			PFM_DBG("pmc%u not available", i);
++		}
++	}
++
++	/*
++	 * Scan PMD registers
++	 */
++	ext_reg = pfm_amd64_pmu_info.pmd_addrs;
++	for (i = 0; i < PFM_AMD_NUM_PMDS;  i++, ext_reg++) {
++		if (!has_ibs && ext_reg->reg_type & PFM_REGT_IBS) {
++			ext_reg->reg_type = PFM_REGT_NA;
++			pfm_amd64_pmd_desc[i].type = PFM_REG_NA;
++			PFM_DBG("pmd%u not available", i);
++		}
++		if (!has_ibsext && ext_reg->reg_type & PFM_REGT_IBS_EXT) {
++			ext_reg->reg_type = PFM_REGT_NA;
++			pfm_amd64_pmd_desc[i].type = PFM_REG_NA;
++			PFM_DBG("pmd%u not available", i);
++		}
++
++		/*
++		 * adjust reserved mask for counters
++		 */
++		if (ext_reg->reg_type & PFM_REGT_CTR)
++			pfm_amd64_pmd_desc[i].rsvd_msk = ~((1ULL<<48)-1);
++	}
++	/*
++	 * adjust reserved bit fields for family 16
++	 */
++	if (current_cpu_data.x86 == 16) {
++		for(i=0; i < PFM_AMD_NUM_PMCS; i++)
++			if (pfm_amd64_pmc_desc[i].rsvd_msk == PFM_K8_RSVD)
++				pfm_amd64_pmc_desc[i].rsvd_msk = PFM_16_RSVD;
++	}
++}
++
++static int pfm_amd64_probe_pmu(void)
++{
++	u64 val = 0;
++	if (current_cpu_data.x86_vendor != X86_VENDOR_AMD) {
++		PFM_INFO("not an AMD processor");
++		return -1;
++	}
++
++	switch (current_cpu_data.x86) {
++	case 16:
++		if (current_cpu_data.x86_model >= 2) {
++			/* Family 10h, RevB and later */
++			pfm_amd64_pmu_info.flags |= PFM_X86_FL_IBS_EXT
++				| PFM_X86_FL_USE_EI;
++		}
++		pfm_amd64_pmu_info.flags |= PFM_X86_FL_IBS;
++		rdmsrl(MSR_AMD64_IBSCTL, val);
++	case 15:
++	case  6:
++		PFM_INFO("found family=%d VAL=0x%llx", current_cpu_data.x86, (unsigned long long)val);
++		break;
++	default:
++		PFM_INFO("unsupported family=%d", current_cpu_data.x86);
++		return -1;
++	}
++
++	/*
++	 * check for local APIC (required)
++	 */
++	if (!cpu_has_apic) {
++		PFM_INFO("no local APIC, unsupported");
++		return -1;
++	}
++
++	if (current_cpu_data.x86_max_cores > 1
++	    && pfm_amd64_setup_nb_event_control())
++		return -1;
++
++	if (force_nmi)
++		pfm_amd64_pmu_info.flags |= PFM_X86_FL_USE_NMI;
++
++	/* Setup extended interrupt */
++	if (pfm_amd64_pmu_info.flags & PFM_X86_FL_USE_EI) {
++		if (pfm_amd64_setup_eilvt()) {
++			PFM_INFO("Failed to initialize extended interrupts "
++				 "for IBS");
++			pfm_amd64_pmu_info.flags &= ~(PFM_X86_FL_IBS
++					      | PFM_X86_FL_IBS_EXT
++					      | PFM_X86_FL_USE_EI);
++			PFM_INFO("Unable to use IBS");
++		}
++	}
++
++	if (pfm_amd64_pmu_info.flags & PFM_X86_FL_IBS)
++		PFM_INFO("IBS supported");
++
++	if (pfm_amd64_pmu_info.flags & PFM_X86_FL_IBS_EXT)
++		PFM_INFO("IBS extended registers supported");
++
++	if (pfm_amd64_pmu_info.flags & PFM_X86_FL_USE_EI)
++		PFM_INFO("Using extended interrupts for IBS");
++	else if (pfm_amd64_pmu_info.flags & (PFM_X86_FL_IBS|PFM_X86_FL_IBS_EXT))
++		PFM_INFO("Using performance counter interrupts for IBS");
++
++	pfm_amd64_check_registers();
++
++	return 0;
++}
++
++static struct pfm_pmu_config pfm_amd64_pmu_conf = {
++	.pmu_name = "AMD64",
++	.counter_width = 47,
++	.pmd_desc = pfm_amd64_pmd_desc,
++	.pmc_desc = pfm_amd64_pmc_desc,
++	.num_pmc_entries = PFM_AMD_NUM_PMCS,
++	.num_pmd_entries = PFM_AMD_NUM_PMDS,
++	.probe_pmu = pfm_amd64_probe_pmu,
++	.version = "1.2",
++	.arch_info = &pfm_amd64_pmu_info,
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++};
++
++static int __init pfm_amd64_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_amd64_pmu_conf);
++}
++
++static void __exit pfm_amd64_pmu_cleanup_module(void)
++{
++	if (pfm_nb_sys_owners)
++		vfree(pfm_nb_sys_owners);
++
++	pfm_pmu_unregister(&pfm_amd64_pmu_conf);
++}
++
++module_init(pfm_amd64_pmu_init_module);
++module_exit(pfm_amd64_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/x86/perfmon/perfmon_intel_arch.c
+@@ -0,0 +1,387 @@
++/*
++ * This file contains the Intel architectural perfmon v1 or v2
++ * description tables.
++ *
++ * Architectural perfmon was introduced with Intel Core Solo/Duo
++ * processors.
++ *
++ * Copyright (c) 2006-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <asm/msr.h>
++#include <asm/apic.h>
++#include <asm/nmi.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("Intel architectural perfmon v1");
++MODULE_LICENSE("GPL");
++
++static int force, force_nmi;
++MODULE_PARM_DESC(force, "bool: force module to load succesfully");
++MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
++module_param(force, bool, 0600);
++module_param(force_nmi, bool, 0600);
++
++/*
++ * - upper 32 bits are reserved
++ * - INT: APIC enable bit is reserved (forced to 1)
++ * - bit 21 is reserved
++ *
++ * RSVD: reserved bits are 1
++ */
++#define PFM_IA_PMC_RSVD	((~((1ULL<<32)-1)) \
++			| (1ULL<<20) \
++			| (1ULL<<21))
++
++/*
++ * force Local APIC interrupt on overflow
++ * disable with NO_EMUL64
++ */
++#define PFM_IA_PMC_VAL	(1ULL<<20)
++#define PFM_IA_NO64	(1ULL<<20)
++
++/*
++ * architectuture specifies that:
++ * IA32_PMCx MSR        : starts at 0x0c1 & occupy a contiguous block of MSR
++ * IA32_PERFEVTSELx MSR : starts at 0x186 & occupy a contiguous block of MSR
++ * MSR_GEN_FIXED_CTR0   : starts at 0x309 & occupy a contiguous block of MSR
++ */
++#define MSR_GEN_SEL_BASE	MSR_P6_EVNTSEL0
++#define MSR_GEN_PMC_BASE	MSR_P6_PERFCTR0
++#define MSR_GEN_FIXED_PMC_BASE	MSR_CORE_PERF_FIXED_CTR0
++
++#define PFM_IA_SEL(n)	{ 			\
++	.addrs[0] = MSR_GEN_SEL_BASE+(n),	\
++	.ctr = n,				\
++	.reg_type = PFM_REGT_EN}
++
++#define PFM_IA_CTR(n) {				\
++	.addrs[0] = MSR_GEN_PMC_BASE+(n),	\
++	.ctr = n,				\
++	.reg_type = PFM_REGT_CTR}
++
++#define PFM_IA_FCTR(n) {			\
++	.addrs[0] = MSR_GEN_FIXED_PMC_BASE+(n),	\
++	.ctr = n,				\
++	.reg_type = PFM_REGT_CTR}
++
++/*
++ * layout of EAX for CPUID.0xa leaf function
++ */
++struct pmu_eax {
++        unsigned int version:8;		/* architectural perfmon version */
++        unsigned int num_cnt:8; 	/* number of generic counters */
++        unsigned int cnt_width:8;	/* width of generic counters */
++        unsigned int ebx_length:8;	/* number of architected events */
++};
++
++/*
++ * layout of EDX for CPUID.0xa leaf function when perfmon v2 is detected
++ */
++struct pmu_edx {
++        unsigned int num_cnt:5;		/* number of fixed counters */
++        unsigned int cnt_width:8;	/* width of fixed counters */
++        unsigned int reserved:19;
++};
++
++
++/*
++ * physical addresses of MSR controlling the perfevtsel and counter registers
++ */
++struct pfm_arch_pmu_info pfm_intel_arch_pmu_info={
++	.pmc_addrs = {
++/* pmc0  */	PFM_IA_SEL(0) ,  PFM_IA_SEL(1),  PFM_IA_SEL(2),  PFM_IA_SEL(3),
++/* pmc4  */	PFM_IA_SEL(4) ,  PFM_IA_SEL(5),  PFM_IA_SEL(6),  PFM_IA_SEL(7),
++/* pmc8  */	PFM_IA_SEL(8) ,  PFM_IA_SEL(9), PFM_IA_SEL(10), PFM_IA_SEL(11),
++/* pmc12 */	PFM_IA_SEL(12), PFM_IA_SEL(13), PFM_IA_SEL(14), PFM_IA_SEL(15),
++
++/* pmc16 */	{
++			.addrs[0] = MSR_CORE_PERF_FIXED_CTR_CTRL,
++			.reg_type = PFM_REGT_EN
++		}
++	},
++
++	.pmd_addrs = {
++/* pmd0  */	PFM_IA_CTR(0) ,  PFM_IA_CTR(1),  PFM_IA_CTR(2),  PFM_IA_CTR(3),
++/* pmd4  */	PFM_IA_CTR(4) ,  PFM_IA_CTR(5),  PFM_IA_CTR(6),  PFM_IA_CTR(7),
++/* pmd8  */	PFM_IA_CTR(8) ,  PFM_IA_CTR(9), PFM_IA_CTR(10), PFM_IA_CTR(11),
++/* pmd12 */	PFM_IA_CTR(12), PFM_IA_CTR(13), PFM_IA_CTR(14), PFM_IA_CTR(15),
++
++/* pmd16 */	PFM_IA_FCTR(0), PFM_IA_FCTR(1), PFM_IA_FCTR(2), PFM_IA_FCTR(3),
++/* pmd20 */	PFM_IA_FCTR(4), PFM_IA_FCTR(5), PFM_IA_FCTR(6), PFM_IA_FCTR(7),
++/* pmd24 */	PFM_IA_FCTR(8), PFM_IA_FCTR(9), PFM_IA_FCTR(10), PFM_IA_FCTR(11),
++/* pmd28 */	PFM_IA_FCTR(12), PFM_IA_FCTR(13), PFM_IA_FCTR(14), PFM_IA_FCTR(15)
++	},
++	.pmu_style = PFM_X86_PMU_P6
++};
++
++#define PFM_IA_C(n) {                   \
++	.type = PFM_REG_I64,            \
++	.desc = "PERFEVTSEL"#n,         \
++	.dfl_val = PFM_IA_PMC_VAL,      \
++	.rsvd_msk = PFM_IA_PMC_RSVD,    \
++	.no_emul64_msk = PFM_IA_NO64,   \
++	.hw_addr = MSR_GEN_SEL_BASE+(n) \
++	}
++
++#define PFM_IA_D(n) PMD_D(PFM_REG_C, "PMC"#n, MSR_P6_PERFCTR0+n)
++#define PFM_IA_FD(n) PMD_D(PFM_REG_C, "FIXED_CTR"#n, MSR_CORE_PERF_FIXED_CTR0+n)
++
++static struct pfm_regmap_desc pfm_intel_arch_pmc_desc[]={
++/* pmc0  */ PFM_IA_C(0),  PFM_IA_C(1),   PFM_IA_C(2),  PFM_IA_C(3),
++/* pmc4  */ PFM_IA_C(4),  PFM_IA_C(5),   PFM_IA_C(6),  PFM_IA_C(7),
++/* pmc8  */ PFM_IA_C(8),  PFM_IA_C(9),  PFM_IA_C(10), PFM_IA_C(11),
++/* pmc12 */ PFM_IA_C(12), PFM_IA_C(13), PFM_IA_C(14), PFM_IA_C(15),
++
++/* pmc16 */ { .type = PFM_REG_I,
++	      .desc = "FIXED_CTRL",
++	      .dfl_val = 0x8888888888888888ULL,
++	      .rsvd_msk = 0xccccccccccccccccULL,
++	      .no_emul64_msk = 0,
++	      .hw_addr = MSR_CORE_PERF_FIXED_CTR_CTRL
++	    },
++};
++#define PFM_IA_MAX_PMCS	ARRAY_SIZE(pfm_intel_arch_pmc_desc)
++
++static struct pfm_regmap_desc pfm_intel_arch_pmd_desc[]={
++/* pmd0  */  PFM_IA_D(0),  PFM_IA_D(1),  PFM_IA_D(2),  PFM_IA_D(3),
++/* pmd4  */  PFM_IA_D(4),  PFM_IA_D(5),  PFM_IA_D(6),  PFM_IA_D(7),
++/* pmd8  */  PFM_IA_D(8),  PFM_IA_D(9), PFM_IA_D(10), PFM_IA_D(11),
++/* pmd12 */ PFM_IA_D(12), PFM_IA_D(13), PFM_IA_D(14), PFM_IA_D(15),
++
++/* pmd16 */ PFM_IA_FD(0), PFM_IA_FD(1), PFM_IA_FD(2), PFM_IA_FD(3),
++/* pmd20 */ PFM_IA_FD(4), PFM_IA_FD(5), PFM_IA_FD(6), PFM_IA_FD(7),
++/* pmd24 */ PFM_IA_FD(8), PFM_IA_FD(9), PFM_IA_FD(10), PFM_IA_FD(11),
++/* pmd28 */ PFM_IA_FD(16), PFM_IA_FD(17), PFM_IA_FD(18), PFM_IA_FD(19)
++};
++#define PFM_IA_MAX_PMDS	ARRAY_SIZE(pfm_intel_arch_pmd_desc)
++
++#define PFM_IA_MAX_CNT		16 /* maximum # of generic counters in mapping table */
++#define PFM_IA_MAX_FCNT		16 /* maximum # of fixed counters in mapping table */
++#define PFM_IA_FCNT_BASE	16 /* base index of fixed counters PMD */
++
++static struct pfm_pmu_config pfm_intel_arch_pmu_conf;
++
++static int pfm_intel_arch_check_errata(void)
++{
++	/*
++	 * Core Duo errata AE49 (no fix). Both counters share a single
++	 * enable bit in PERFEVTSEL0
++	 */
++	if (current_cpu_data.x86 == 6 && current_cpu_data.x86_model == 14) {
++		pfm_intel_arch_pmu_info.flags |= PFM_X86_FL_NO_SHARING;
++	}
++	return 0;
++}
++
++static int pfm_intel_arch_probe_pmu(void)
++{
++	union {
++		unsigned int val;
++		struct pmu_eax eax;
++		struct pmu_edx edx;
++	} eax, edx;
++	unsigned int ebx, ecx;
++	unsigned int num_cnt, i;
++	u64 dfl, rsvd;
++
++	edx.val = 0;
++
++	if (!cpu_has_arch_perfmon && force == 0) {
++		PFM_INFO("no support for Intel architectural PMU");
++		return -1;
++	}
++
++	if (!cpu_has_apic) {
++		PFM_INFO("no Local APIC, try rebooting with lapic option");
++		return -1;
++	}
++
++	if (pfm_intel_arch_check_errata())
++		return -1;
++
++	if (force == 0) {
++		/* cpuid() call protected by cpu_has_arch_perfmon */
++		cpuid(0xa, &eax.val, &ebx, &ecx, &edx.val);
++	} else {
++		/* lowest common denominator */
++		eax.eax.version = 1;
++		eax.eax.num_cnt = 2;
++		eax.eax.cnt_width = 40;
++		edx.val = 0;
++	}
++	/*
++	 * reject processors supported by perfmon_intel_core
++	 *
++	 * We need to do this explicitely to avoid depending
++	 * on the link order in case, the modules are compiled as
++	 * builtin.
++	 *
++	 * non Intel processors are rejected by cpu_has_arch_perfmon
++	 */
++	if (current_cpu_data.x86 == 6) {
++		switch(current_cpu_data.x86_model) {
++			case 15: /* Merom: use perfmon_intel_core  */
++				return -1;
++			default:
++				break;
++		}
++	}
++
++	/*
++	 * some 6/15 models have buggy BIOS
++	 */
++	if (eax.eax.version == 0
++	    && current_cpu_data.x86 == 6 && current_cpu_data.x86_model == 15) {
++		PFM_INFO("buggy v2 BIOS, adjusting for 2 generic counters");
++		eax.eax.version = 2;
++		eax.eax.num_cnt = 2;
++		eax.eax.cnt_width = 40;
++	}
++
++	/*
++	 * some v2 BIOSes are incomplete
++	 */
++	if (eax.eax.version == 2 && !edx.edx.num_cnt) {
++		PFM_INFO("buggy v2 BIOS, adjusting for 3 fixed counters");
++		edx.edx.num_cnt = 3;
++		edx.edx.cnt_width = 40;
++	}
++
++	/*
++	 * no fixed counters on earlier versions
++	 */
++	if (eax.eax.version < 2)
++		edx.val = 0;
++
++	PFM_INFO("detected architecural perfmon v%d", eax.eax.version);
++	PFM_INFO("num_gen=%d width=%d num_fixed=%d width=%d",
++		  eax.eax.num_cnt,
++		  eax.eax.cnt_width,
++		  edx.edx.num_cnt,
++		  edx.edx.cnt_width);
++
++	/* number of generic counters */
++	num_cnt = eax.eax.num_cnt;
++
++	if (num_cnt >= PFM_IA_MAX_CNT) {
++		printk(KERN_INFO "perfmon: Limiting number of generic counters to %zu,"
++				 "HW supports %u", PFM_IA_MAX_PMCS, num_cnt);
++		num_cnt = PFM_IA_MAX_CNT;
++
++	}
++
++	/*
++	 * adjust rsvd_msk for generic counters based on actual width
++	 */
++	for(i=0; i < num_cnt; i++)
++		pfm_intel_arch_pmd_desc[i].rsvd_msk = ~((1ULL<<eax.eax.cnt_width)-1);
++
++	/*
++	 * mark unused generic counters as not available
++	 */
++	for(i=num_cnt; i < PFM_IA_MAX_CNT; i++) {
++		pfm_intel_arch_pmd_desc[i].type = PFM_REG_NA;
++		pfm_intel_arch_pmc_desc[i].type = PFM_REG_NA;
++	}
++
++	/*
++	 * now process fixed counters (if any)
++	 */
++	num_cnt = edx.edx.num_cnt;
++
++	/*
++	 * adjust rsvd_msk for fixed counters based on actual width
++	 */
++	for(i=0; i < num_cnt; i++)
++		pfm_intel_arch_pmd_desc[PFM_IA_FCNT_BASE+i].rsvd_msk = ~((1ULL<<edx.edx.cnt_width)-1);
++
++	/*
++	 * mark unused fixed counters as
++	 * unavailable.
++	 * update the rsvd_msk, dfl_val for
++	 * FIXED_CTRL:
++	 * 	rsvd_msk: set all 4 bits
++	 *	dfl_val : clear all 4 bits
++	 */
++	dfl = pfm_intel_arch_pmc_desc[16].dfl_val;
++	rsvd = pfm_intel_arch_pmc_desc[16].rsvd_msk;
++
++	for(i=num_cnt; i < PFM_IA_MAX_FCNT; i++) {
++		pfm_intel_arch_pmd_desc[PFM_IA_FCNT_BASE+i].type = PFM_REG_NA;
++		rsvd |= 0xfULL << (i<<2);
++		dfl &= ~(0xfULL << (i<<2));
++	}
++
++	/*
++	 * FIXED_CTR_CTRL unavailable when no fixed counters are defined
++	 */
++	if (!num_cnt) {
++		pfm_intel_arch_pmc_desc[16].type = PFM_REG_NA;
++	} else {
++		pfm_intel_arch_pmc_desc[16].rsvd_msk = rsvd;
++		pfm_intel_arch_pmc_desc[16].dfl_val = dfl;
++	}
++
++	/*
++	 * Maximum number of entries in both tables (some maybe NA)
++	 */
++	pfm_intel_arch_pmu_conf.num_pmc_entries = PFM_IA_MAX_PMCS;
++	pfm_intel_arch_pmu_conf.num_pmd_entries = PFM_IA_MAX_PMDS;
++
++	if (force_nmi)
++		pfm_intel_arch_pmu_info.flags |= PFM_X86_FL_USE_NMI;
++
++
++	return 0;
++}
++
++/*
++ * Counters may have model-specific width. Yet the documentation says
++ * that only the lower 32 bits can be written to due to the specification
++ * of wrmsr. bits [32-(w-1)] are sign extensions of bit 31. Bits [w-63] must
++ * not be set (see rsvd_msk for PMDs). As such the effective width of a
++ * counter is 31 bits only regardless of what CPUID.0xa returns.
++ *
++ * See IA-32 Intel Architecture Software developer manual Vol 3B chapter 18
++ */
++static struct pfm_pmu_config pfm_intel_arch_pmu_conf={
++	.pmu_name = "Intel architectural",
++	.pmd_desc = pfm_intel_arch_pmd_desc,
++	.counter_width   = 31,
++	.pmc_desc = pfm_intel_arch_pmc_desc,
++	.probe_pmu = pfm_intel_arch_probe_pmu,
++	.version = "1.0",
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.arch_info = &pfm_intel_arch_pmu_info
++};
++
++static int __init pfm_intel_arch_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_intel_arch_pmu_conf);
++}
++
++static void __exit pfm_intel_arch_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_intel_arch_pmu_conf);
++}
++
++module_init(pfm_intel_arch_pmu_init_module);
++module_exit(pfm_intel_arch_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/x86/perfmon/perfmon_intel_core.c
+@@ -0,0 +1,248 @@
++/*
++ * This file contains the Intel Core PMU registers description tables.
++ * Intel Core-based processors support architectural perfmon v2 + PEBS
++ *
++ * Copyright (c) 2006-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <asm/nmi.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("Intel Core");
++MODULE_LICENSE("GPL");
++
++static int force_nmi;
++MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
++module_param(force_nmi, bool, 0600);
++
++/*
++ * - upper 32 bits are reserved
++ * - INT: APIC enable bit is reserved (forced to 1)
++ * - bit 21 is reserved
++ *
++ *   RSVD: reserved bits must be 1
++ */
++#define PFM_CORE_PMC_RSVD ((~((1ULL<<32)-1)) \
++			| (1ULL<<20)   \
++			| (1ULL<<21))
++
++/*
++ * force Local APIC interrupt on overflow
++ * disable with NO_EMUL64
++ */
++#define PFM_CORE_PMC_VAL	(1ULL<<20)
++#define PFM_CORE_NO64		(1ULL<<20)
++
++#define PFM_CORE_NA { .reg_type = PFM_REGT_NA}
++
++#define PFM_CORE_CA(m, c, t) \
++	{ \
++	  .addrs[0] = m, \
++	  .ctr = c, \
++	  .reg_type = t \
++	}
++/*
++ * physical addresses of MSR for evntsel and perfctr registers
++ *
++ * IMPORTANT:
++ * 	The mapping  was chosen to be compatible with the Intel
++ * 	architectural perfmon, so that applications which only
++ * 	know about the architectural perfmon can work on Core
++ * 	without any changes.
++ *
++ * 	We do not expose the GLOBAL_* registers because:
++ * 	- would be incompatible with architectural perfmon v1
++ * 	  (unless default means, measures everything for GLOBAL_CTRL)
++ *	- would cause a conflict when NMI watchdog is enabled.
++ *
++ */
++struct pfm_arch_pmu_info pfm_core_pmu_info={
++	.pmc_addrs = {
++/* pmc0  */	PFM_CORE_CA(MSR_P6_EVNTSEL0, 0, PFM_REGT_EN),
++/* pmc1  */	PFM_CORE_CA(MSR_P6_EVNTSEL1, 1, PFM_REGT_EN),
++/* pmc2  */	PFM_CORE_NA, PFM_CORE_NA,
++/* pmc4  */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
++/* pmc8  */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
++/* pmc12 */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
++/* pmc16 */	PFM_CORE_CA(MSR_CORE_PERF_FIXED_CTR_CTRL, 0, PFM_REGT_EN),
++/* pmc17 */	PFM_CORE_CA(MSR_IA32_PEBS_ENABLE, 0, PFM_REGT_EN)
++	},
++	.pmd_addrs = {
++/* pmd0  */	PFM_CORE_CA(MSR_P6_PERFCTR0, 0, PFM_REGT_CTR),
++/* pmd1  */	PFM_CORE_CA(MSR_P6_PERFCTR1, 0, PFM_REGT_CTR),
++/* pmd2  */	PFM_CORE_NA, PFM_CORE_NA,
++/* pmd4  */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
++/* pmd8  */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
++/* pmd12 */	PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA, PFM_CORE_NA,
++/* pmd16 */	PFM_CORE_CA(MSR_CORE_PERF_FIXED_CTR0, 0, PFM_REGT_CTR),
++/* pmd17 */	PFM_CORE_CA(MSR_CORE_PERF_FIXED_CTR1, 0, PFM_REGT_CTR),
++/* pmd18 */	PFM_CORE_CA(MSR_CORE_PERF_FIXED_CTR2, 0, PFM_REGT_CTR)
++	},
++	.pebs_ctr_idx = 0, /* IA32_PMC0 */
++	.pmu_style = PFM_X86_PMU_CORE
++};
++
++static struct pfm_regmap_desc pfm_core_pmc_desc[]={
++/* pmc0  */ {
++	      .type = PFM_REG_I64,
++	      .desc = "PERFEVTSEL0",
++	      .dfl_val = PFM_CORE_PMC_VAL,
++	      .rsvd_msk = PFM_CORE_PMC_RSVD,
++	      .no_emul64_msk = PFM_CORE_NO64,
++	      .hw_addr = MSR_P6_EVNTSEL0
++	    },
++/* pmc1  */ {
++	      .type = PFM_REG_I64,
++	      .desc = "PERFEVTSEL1",
++	      .dfl_val = PFM_CORE_PMC_VAL,
++	      .rsvd_msk = PFM_CORE_PMC_RSVD,
++	      .no_emul64_msk = PFM_CORE_NO64,
++	      .hw_addr = MSR_P6_EVNTSEL1
++	    },
++/* pmc2  */ PMX_NA, PMX_NA,
++/* pmc4  */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
++/* pmc8  */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
++/* pmc12 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
++/* pmc16 */ { .type = PFM_REG_I,
++	      .desc = "FIXED_CTRL",
++	      .dfl_val = 0x888ULL,
++	      .rsvd_msk = 0xfffffffffffffcccULL,
++	      .no_emul64_msk = 0,
++	      .hw_addr = MSR_CORE_PERF_FIXED_CTR_CTRL
++	    },
++/* pmc17  */ { .type = PFM_REG_W,
++	      .desc = "PEBS_ENABLE",
++	      .dfl_val = 0,
++	      .rsvd_msk = 0xfffffffffffffffeULL,
++	      .no_emul64_msk = 0,
++	      .hw_addr = MSR_IA32_PEBS_ENABLE
++	    }
++};
++
++#define PFM_CORE_D(n) PMD_D(PFM_REG_C, "PMC"#n, MSR_P6_PERFCTR0+n)
++#define PFM_CORE_FD(n) PMD_D(PFM_REG_C, "FIXED_CTR"#n, MSR_CORE_PERF_FIXED_CTR0+n)
++
++static struct pfm_regmap_desc pfm_core_pmd_desc[]={
++/* pmd0  */ PFM_CORE_D(0),
++/* pmd1  */ PFM_CORE_D(1),
++/* pmd2  */ PMX_NA, PMX_NA,
++/* pmd4  */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
++/* pmd8  */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
++/* pmd12 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA,
++/* pmd16 */ PFM_CORE_FD(0),
++/* pmd17 */ PFM_CORE_FD(1),
++/* pmd18 */ PFM_CORE_FD(2)
++};
++#define PFM_CORE_NUM_PMCS	ARRAY_SIZE(pfm_core_pmc_desc)
++#define PFM_CORE_NUM_PMDS	ARRAY_SIZE(pfm_core_pmd_desc)
++
++static struct pfm_pmu_config pfm_core_pmu_conf;
++
++static int pfm_core_probe_pmu(void)
++{
++	unsigned int i;
++
++	/*
++	 * Check for Intel Core processor explicitely
++	 * Checking for cpu_has_perfmon is not enough as this
++	 * matches intel Core Duo/Core Solo but none supports
++	 * PEBS.
++	 *
++	 * Intel Core = arch perfmon v2 + PEBS
++	 */
++	if (current_cpu_data.x86 != 6)
++		return -1;
++
++	switch(current_cpu_data.x86_model) {
++		case 15: /* Merom */
++			break;
++		case 23: /* Penryn */
++			break;
++		default:
++			return -1;
++	}
++
++	if (!cpu_has_apic) {
++		PFM_INFO("no Local APIC, unsupported");
++		return -1;
++	}
++
++	PFM_INFO("nmi_watchdog=%d nmi_active=%d force_nmi=%d",
++		nmi_watchdog, atomic_read(&nmi_active), force_nmi);
++
++	/*
++	 * Intel Core processors implement DS and PEBS, no need to check
++	 */
++	pfm_core_pmu_info.flags |= PFM_X86_FL_PMU_DS|PFM_X86_FL_PMU_PEBS;
++	PFM_INFO("PEBS supported, enabled");
++
++	/*
++	 * Core 2 have 40-bit counters (generic, fixed)
++	 */
++	for(i=0; i < PFM_CORE_NUM_PMDS; i++)
++		pfm_core_pmd_desc[i].rsvd_msk = ~((1ULL<<40)-1);
++
++	if (force_nmi)
++		pfm_core_pmu_info.flags |= PFM_X86_FL_USE_NMI;
++
++	return 0;
++}
++
++static int pfm_core_pmc17_check(struct pfm_context *ctx,
++			     struct pfm_event_set *set,
++			     struct pfarg_pmc *req)
++{
++	struct pfm_arch_context *ctx_arch;
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * if user activates PEBS_ENABLE, then we need to have a valid
++	 * DS Area setup. This only happens when the PEBS sampling format is used
++	 * in which case PFM_X86_USE_PEBS is set. We must reject all other requests.
++	 * Otherwise we may pickup stale MSR_IA32_DS_AREA values. It appears
++	 * that a value of 0 for this MSR does crash the system with PEBS_ENABLE=1.
++	 */
++	if (!ctx_arch->flags.use_pebs && req->reg_value) {
++		PFM_DBG("pmc17 (PEBS_ENABLE) can only be used with PEBS sampling format");
++		return -EINVAL;
++	}
++	return 0;
++}
++
++/*
++ * Counters may have model-specific width which can be probed using
++ * the CPUID.0xa leaf. Yet, the documentation says: "
++ * In the initial implementation, only the read bit width is reported
++ * by CPUID, write operations are limited to the low 32 bits.
++ * Bits [w-32] are sign extensions of bit 31. As such the effective width
++ * of a counter is 31 bits only.
++ */
++static struct pfm_pmu_config pfm_core_pmu_conf={
++	.pmu_name = "Intel Core",
++	.pmd_desc = pfm_core_pmd_desc,
++	.counter_width = 31,
++	.num_pmc_entries = PFM_CORE_NUM_PMCS,
++	.num_pmd_entries = PFM_CORE_NUM_PMDS,
++	.pmc_desc = pfm_core_pmc_desc,
++	.probe_pmu = pfm_core_probe_pmu,
++	.version = "1.2",
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.arch_info = &pfm_core_pmu_info,
++	.pmc_write_check = pfm_core_pmc17_check
++};
++
++static int __init pfm_core_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_core_pmu_conf);
++}
++
++static void __exit pfm_core_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_core_pmu_conf);
++}
++
++module_init(pfm_core_pmu_init_module);
++module_exit(pfm_core_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/x86/perfmon/perfmon_p4.c
+@@ -0,0 +1,411 @@
++/*
++ * This file contains the P4/Xeon PMU register description tables
++ * for both 32 and 64 bit modes.
++ *
++ * Copyright (c) 2005 Intel Corporation
++ * Contributed by Bryan Wilkerson <bryan.p.wilkerson@intel.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <asm/msr.h>
++#include <asm/apic.h>
++#include <asm/nmi.h>
++
++MODULE_AUTHOR("Bryan Wilkerson <bryan.p.wilkerson@intel.com>");
++MODULE_DESCRIPTION("P4/Xeon/EM64T PMU description table");
++MODULE_LICENSE("GPL");
++
++static int force;
++MODULE_PARM_DESC(force, "bool: force module to load succesfully");
++module_param(force, bool, 0600);
++
++static int force_nmi;
++MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
++module_param(force_nmi, bool, 0600);
++
++/*
++ * CCCR default value:
++ * 	- OVF_PMI_T0=1 (bit 26)
++ * 	- OVF_PMI_T1=0 (bit 27) (set if necessary in pfm_write_reg())
++ * 	- all other bits are zero
++ *
++ * OVF_PMI is forced to zero if PFM_REGFL_NO_EMUL64 is set on CCCR
++ */
++#define PFM_CCCR_DFL	(1ULL<<26) | (3ULL<<16)
++
++/*
++ * CCCR reserved fields:
++ * 	- bits 0-11, 25-29, 31-63
++ * 	- OVF_PMI (26-27), override with REGFL_NO_EMUL64
++ *
++ * RSVD: reserved bits must be 1
++ */
++#define PFM_CCCR_RSVD     ~((0xfull<<12)  \
++			| (0x7full<<18) \
++			| (0x1ull<<30))
++
++#define PFM_P4_NO64	(3ULL<<26) /* use 3 even in non HT mode */
++
++/*
++ * With HyperThreading enabled:
++ *
++ *  The ESCRs and CCCRs are divided in half with the top half
++ *  belonging to logical processor 0 and the bottom half going to
++ *  logical processor 1. Thus only half of the PMU resources are
++ *  accessible to applications.
++ *
++ *  PEBS is not available due to the fact that:
++ *  	- MSR_PEBS_MATRIX_VERT is shared between the threads
++ *      - IA32_PEBS_ENABLE is shared between the threads
++ *
++ * With HyperThreading disabled:
++ *
++ * The full set of PMU resources is exposed to applications.
++ *
++ * The mapping is chosen such that PMCxx -> MSR is the same
++ * in HT and non HT mode, if register is present in HT mode.
++ *
++ */
++#define PFM_REGT_NHTESCR (PFM_REGT_ESCR|PFM_REGT_NOHT)
++#define PFM_REGT_NHTCCCR (PFM_REGT_CCCR|PFM_REGT_NOHT|PFM_REGT_EN)
++#define PFM_REGT_NHTPEBS (PFM_REGT_PEBS|PFM_REGT_NOHT|PFM_REGT_EN)
++#define PFM_REGT_NHTCTR  (PFM_REGT_CTR|PFM_REGT_NOHT)
++#define PFM_REGT_ENAC    (PFM_REGT_CCCR|PFM_REGT_EN)
++
++static struct pfm_arch_pmu_info pfm_p4_pmu_info={
++ .pmc_addrs = {
++	/*pmc 0 */    {{MSR_P4_BPU_ESCR0, MSR_P4_BPU_ESCR1}, 0, PFM_REGT_ESCR}, /*   BPU_ESCR0,1 */
++	/*pmc 1 */    {{MSR_P4_IS_ESCR0, MSR_P4_IS_ESCR1}, 0, PFM_REGT_ESCR}, /*    IS_ESCR0,1 */
++	/*pmc 2 */    {{MSR_P4_MOB_ESCR0, MSR_P4_MOB_ESCR1}, 0, PFM_REGT_ESCR}, /*   MOB_ESCR0,1 */
++	/*pmc 3 */    {{MSR_P4_ITLB_ESCR0, MSR_P4_ITLB_ESCR1}, 0, PFM_REGT_ESCR}, /*  ITLB_ESCR0,1 */
++	/*pmc 4 */    {{MSR_P4_PMH_ESCR0, MSR_P4_PMH_ESCR1}, 0, PFM_REGT_ESCR}, /*   PMH_ESCR0,1 */
++	/*pmc 5 */    {{MSR_P4_IX_ESCR0, MSR_P4_IX_ESCR1}, 0, PFM_REGT_ESCR}, /*    IX_ESCR0,1 */
++	/*pmc 6 */    {{MSR_P4_FSB_ESCR0, MSR_P4_FSB_ESCR1}, 0, PFM_REGT_ESCR}, /*   FSB_ESCR0,1 */
++	/*pmc 7 */    {{MSR_P4_BSU_ESCR0, MSR_P4_BSU_ESCR1}, 0, PFM_REGT_ESCR}, /*   BSU_ESCR0,1 */
++	/*pmc 8 */    {{MSR_P4_MS_ESCR0, MSR_P4_MS_ESCR1}, 0, PFM_REGT_ESCR}, /*    MS_ESCR0,1 */
++	/*pmc 9 */    {{MSR_P4_TC_ESCR0, MSR_P4_TC_ESCR1}, 0, PFM_REGT_ESCR}, /*    TC_ESCR0,1 */
++	/*pmc 10*/    {{MSR_P4_TBPU_ESCR0, MSR_P4_TBPU_ESCR1}, 0, PFM_REGT_ESCR}, /*  TBPU_ESCR0,1 */
++	/*pmc 11*/    {{MSR_P4_FLAME_ESCR0, MSR_P4_FLAME_ESCR1}, 0, PFM_REGT_ESCR}, /* FLAME_ESCR0,1 */
++	/*pmc 12*/    {{MSR_P4_FIRM_ESCR0, MSR_P4_FIRM_ESCR1}, 0, PFM_REGT_ESCR}, /*  FIRM_ESCR0,1 */
++	/*pmc 13*/    {{MSR_P4_SAAT_ESCR0, MSR_P4_SAAT_ESCR1}, 0, PFM_REGT_ESCR}, /*  SAAT_ESCR0,1 */
++	/*pmc 14*/    {{MSR_P4_U2L_ESCR0, MSR_P4_U2L_ESCR1}, 0, PFM_REGT_ESCR}, /*   U2L_ESCR0,1 */
++	/*pmc 15*/    {{MSR_P4_DAC_ESCR0, MSR_P4_DAC_ESCR1}, 0, PFM_REGT_ESCR}, /*   DAC_ESCR0,1 */
++	/*pmc 16*/    {{MSR_P4_IQ_ESCR0, MSR_P4_IQ_ESCR1}, 0, PFM_REGT_ESCR}, /*    IQ_ESCR0,1 (only model 1 and 2) */
++	/*pmc 17*/    {{MSR_P4_ALF_ESCR0, MSR_P4_ALF_ESCR1}, 0, PFM_REGT_ESCR}, /*   ALF_ESCR0,1 */
++	/*pmc 18*/    {{MSR_P4_RAT_ESCR0, MSR_P4_RAT_ESCR1}, 0, PFM_REGT_ESCR}, /*   RAT_ESCR0,1 */
++	/*pmc 19*/    {{MSR_P4_SSU_ESCR0, 0}, 0, PFM_REGT_ESCR}, /*   SSU_ESCR0   */
++	/*pmc 20*/    {{MSR_P4_CRU_ESCR0, MSR_P4_CRU_ESCR1}, 0, PFM_REGT_ESCR}, /*   CRU_ESCR0,1 */
++	/*pmc 21*/    {{MSR_P4_CRU_ESCR2, MSR_P4_CRU_ESCR3}, 0, PFM_REGT_ESCR}, /*   CRU_ESCR2,3 */
++	/*pmc 22*/    {{MSR_P4_CRU_ESCR4, MSR_P4_CRU_ESCR5}, 0, PFM_REGT_ESCR}, /*   CRU_ESCR4,5 */
++
++	/*pmc 23*/    {{MSR_P4_BPU_CCCR0, MSR_P4_BPU_CCCR2}, 0, PFM_REGT_ENAC}, /*   BPU_CCCR0,2 */
++	/*pmc 24*/    {{MSR_P4_BPU_CCCR1, MSR_P4_BPU_CCCR3}, 1, PFM_REGT_ENAC}, /*   BPU_CCCR1,3 */
++	/*pmc 25*/    {{MSR_P4_MS_CCCR0, MSR_P4_MS_CCCR2}, 2, PFM_REGT_ENAC}, /*    MS_CCCR0,2 */
++	/*pmc 26*/    {{MSR_P4_MS_CCCR1, MSR_P4_MS_CCCR3}, 3, PFM_REGT_ENAC}, /*    MS_CCCR1,3 */
++	/*pmc 27*/    {{MSR_P4_FLAME_CCCR0, MSR_P4_FLAME_CCCR2}, 4, PFM_REGT_ENAC}, /* FLAME_CCCR0,2 */
++	/*pmc 28*/    {{MSR_P4_FLAME_CCCR1, MSR_P4_FLAME_CCCR3}, 5, PFM_REGT_ENAC}, /* FLAME_CCCR1,3 */
++	/*pmc 29*/    {{MSR_P4_IQ_CCCR0, MSR_P4_IQ_CCCR2}, 6, PFM_REGT_ENAC}, /*    IQ_CCCR0,2 */
++	/*pmc 30*/    {{MSR_P4_IQ_CCCR1, MSR_P4_IQ_CCCR3}, 7, PFM_REGT_ENAC}, /*    IQ_CCCR1,3 */
++	/*pmc 31*/    {{MSR_P4_IQ_CCCR4, MSR_P4_IQ_CCCR5}, 8, PFM_REGT_ENAC}, /*    IQ_CCCR4,5 */
++	/* non HT extensions */
++	/*pmc 32*/    {{MSR_P4_BPU_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   BPU_ESCR1   */
++	/*pmc 33*/    {{MSR_P4_IS_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    IS_ESCR1   */
++	/*pmc 34*/    {{MSR_P4_MOB_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   MOB_ESCR1   */
++	/*pmc 35*/    {{MSR_P4_ITLB_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*  ITLB_ESCR1   */
++	/*pmc 36*/    {{MSR_P4_PMH_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   PMH_ESCR1   */
++	/*pmc 37*/    {{MSR_P4_IX_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    IX_ESCR1   */
++	/*pmc 38*/    {{MSR_P4_FSB_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   FSB_ESCR1   */
++	/*pmc 39*/    {{MSR_P4_BSU_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   BSU_ESCR1   */
++	/*pmc 40*/    {{MSR_P4_MS_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    MS_ESCR1   */
++	/*pmc 41*/    {{MSR_P4_TC_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    TC_ESCR1   */
++	/*pmc 42*/    {{MSR_P4_TBPU_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*  TBPU_ESCR1   */
++	/*pmc 43*/    {{MSR_P4_FLAME_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /* FLAME_ESCR1   */
++	/*pmc 44*/    {{MSR_P4_FIRM_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*  FIRM_ESCR1   */
++	/*pmc 45*/    {{MSR_P4_SAAT_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*  SAAT_ESCR1   */
++	/*pmc 46*/    {{MSR_P4_U2L_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   U2L_ESCR1   */
++	/*pmc 47*/    {{MSR_P4_DAC_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   DAC_ESCR1   */
++	/*pmc 48*/    {{MSR_P4_IQ_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*    IQ_ESCR1   (only model 1 and 2) */
++	/*pmc 49*/    {{MSR_P4_ALF_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   ALF_ESCR1   */
++	/*pmc 50*/    {{MSR_P4_RAT_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   RAT_ESCR1   */
++	/*pmc 51*/    {{MSR_P4_CRU_ESCR1,     0}, 0, PFM_REGT_NHTESCR}, /*   CRU_ESCR1   */
++	/*pmc 52*/    {{MSR_P4_CRU_ESCR3,     0}, 0, PFM_REGT_NHTESCR}, /*   CRU_ESCR3   */
++	/*pmc 53*/    {{MSR_P4_CRU_ESCR5,     0}, 0, PFM_REGT_NHTESCR}, /*   CRU_ESCR5   */
++	/*pmc 54*/    {{MSR_P4_BPU_CCCR1,     0}, 9, PFM_REGT_NHTCCCR}, /*   BPU_CCCR1   */
++	/*pmc 55*/    {{MSR_P4_BPU_CCCR3,     0},10, PFM_REGT_NHTCCCR}, /*   BPU_CCCR3   */
++	/*pmc 56*/    {{MSR_P4_MS_CCCR1,     0},11, PFM_REGT_NHTCCCR}, /*    MS_CCCR1   */
++	/*pmc 57*/    {{MSR_P4_MS_CCCR3,     0},12, PFM_REGT_NHTCCCR}, /*    MS_CCCR3   */
++	/*pmc 58*/    {{MSR_P4_FLAME_CCCR1,     0},13, PFM_REGT_NHTCCCR}, /* FLAME_CCCR1   */
++	/*pmc 59*/    {{MSR_P4_FLAME_CCCR3,     0},14, PFM_REGT_NHTCCCR}, /* FLAME_CCCR3   */
++	/*pmc 60*/    {{MSR_P4_IQ_CCCR2,     0},15, PFM_REGT_NHTCCCR}, /*    IQ_CCCR2   */
++	/*pmc 61*/    {{MSR_P4_IQ_CCCR3,     0},16, PFM_REGT_NHTCCCR}, /*    IQ_CCCR3   */
++	/*pmc 62*/    {{MSR_P4_IQ_CCCR5,     0},17, PFM_REGT_NHTCCCR}, /*    IQ_CCCR5   */
++	/*pmc 63*/    {{0x3f2,     0}, 0, PFM_REGT_NHTPEBS},/* PEBS_MATRIX_VERT */
++	/*pmc 64*/    {{0x3f1,     0}, 0, PFM_REGT_NHTPEBS} /* PEBS_ENABLE   */
++},
++
++.pmd_addrs = {
++	/*pmd 0 */    {{MSR_P4_BPU_PERFCTR0, MSR_P4_BPU_PERFCTR2}, 0, PFM_REGT_CTR},  /*   BPU_CTR0,2  */
++	/*pmd 1 */    {{MSR_P4_BPU_PERFCTR1, MSR_P4_BPU_PERFCTR3}, 0, PFM_REGT_CTR},  /*   BPU_CTR1,3  */
++	/*pmd 2 */    {{MSR_P4_MS_PERFCTR0, MSR_P4_MS_PERFCTR2}, 0, PFM_REGT_CTR},  /*    MS_CTR0,2  */
++	/*pmd 3 */    {{MSR_P4_MS_PERFCTR1, MSR_P4_MS_PERFCTR3}, 0, PFM_REGT_CTR},  /*    MS_CTR1,3  */
++	/*pmd 4 */    {{MSR_P4_FLAME_PERFCTR0, MSR_P4_FLAME_PERFCTR2}, 0, PFM_REGT_CTR},  /* FLAME_CTR0,2  */
++	/*pmd 5 */    {{MSR_P4_FLAME_PERFCTR1, MSR_P4_FLAME_PERFCTR3}, 0, PFM_REGT_CTR},  /* FLAME_CTR1,3  */
++	/*pmd 6 */    {{MSR_P4_IQ_PERFCTR0, MSR_P4_IQ_PERFCTR2}, 0, PFM_REGT_CTR},  /*    IQ_CTR0,2  */
++	/*pmd 7 */    {{MSR_P4_IQ_PERFCTR1, MSR_P4_IQ_PERFCTR3}, 0, PFM_REGT_CTR},  /*    IQ_CTR1,3  */
++	/*pmd 8 */    {{MSR_P4_IQ_PERFCTR4, MSR_P4_IQ_PERFCTR5}, 0, PFM_REGT_CTR},  /*    IQ_CTR4,5  */
++	/*
++	 * non HT extensions
++	 */
++	/*pmd 9 */    {{MSR_P4_BPU_PERFCTR2,     0}, 0, PFM_REGT_NHTCTR},  /*   BPU_CTR2    */
++	/*pmd 10*/    {{MSR_P4_BPU_PERFCTR3,     0}, 0, PFM_REGT_NHTCTR},  /*   BPU_CTR3    */
++	/*pmd 11*/    {{MSR_P4_MS_PERFCTR2,     0}, 0, PFM_REGT_NHTCTR},  /*    MS_CTR2    */
++	/*pmd 12*/    {{MSR_P4_MS_PERFCTR3,     0}, 0, PFM_REGT_NHTCTR},  /*    MS_CTR3    */
++	/*pmd 13*/    {{MSR_P4_FLAME_PERFCTR2,     0}, 0, PFM_REGT_NHTCTR},  /* FLAME_CTR2    */
++	/*pmd 14*/    {{MSR_P4_FLAME_PERFCTR3,     0}, 0, PFM_REGT_NHTCTR},  /* FLAME_CTR3    */
++	/*pmd 15*/    {{MSR_P4_IQ_PERFCTR2,     0}, 0, PFM_REGT_NHTCTR},  /*    IQ_CTR2    */
++	/*pmd 16*/    {{MSR_P4_IQ_PERFCTR3,     0}, 0, PFM_REGT_NHTCTR},  /*    IQ_CTR3    */
++	/*pmd 17*/    {{MSR_P4_IQ_PERFCTR5,     0}, 0, PFM_REGT_NHTCTR},  /*    IQ_CTR5    */
++},
++.pebs_ctr_idx = 8, /* thread0: IQ_CTR4, thread1: IQ_CTR5 */
++.pmu_style = PFM_X86_PMU_P4
++};
++
++static struct pfm_regmap_desc pfm_p4_pmc_desc[]={
++/* pmc0  */ PMC_D(PFM_REG_I, "BPU_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_BPU_ESCR0),
++/* pmc1  */ PMC_D(PFM_REG_I, "IS_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IQ_ESCR0),
++/* pmc2  */ PMC_D(PFM_REG_I, "MOB_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_MOB_ESCR0),
++/* pmc3  */ PMC_D(PFM_REG_I, "ITLB_ESCR0" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_ITLB_ESCR0),
++/* pmc4  */ PMC_D(PFM_REG_I, "PMH_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_PMH_ESCR0),
++/* pmc5  */ PMC_D(PFM_REG_I, "IX_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IX_ESCR0),
++/* pmc6  */ PMC_D(PFM_REG_I, "FSB_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FSB_ESCR0),
++/* pmc7  */ PMC_D(PFM_REG_I, "BSU_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_BSU_ESCR0),
++/* pmc8  */ PMC_D(PFM_REG_I, "MS_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_MS_ESCR0),
++/* pmc9  */ PMC_D(PFM_REG_I, "TC_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_TC_ESCR0),
++/* pmc10 */ PMC_D(PFM_REG_I, "TBPU_ESCR0" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_TBPU_ESCR0),
++/* pmc11 */ PMC_D(PFM_REG_I, "FLAME_ESCR0", 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FLAME_ESCR0),
++/* pmc12 */ PMC_D(PFM_REG_I, "FIRM_ESCR0" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FIRM_ESCR0),
++/* pmc13 */ PMC_D(PFM_REG_I, "SAAT_ESCR0" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_SAAT_ESCR0),
++/* pmc14 */ PMC_D(PFM_REG_I, "U2L_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_U2L_ESCR0),
++/* pmc15 */ PMC_D(PFM_REG_I, "DAC_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_DAC_ESCR0),
++/* pmc16 */ PMC_D(PFM_REG_I, "IQ_ESCR0"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IQ_ESCR0), /* only model 1 and 2*/
++/* pmc17 */ PMC_D(PFM_REG_I, "ALF_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_ALF_ESCR0),
++/* pmc18 */ PMC_D(PFM_REG_I, "RAT_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_RAT_ESCR0),
++/* pmc19 */ PMC_D(PFM_REG_I, "SSU_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_SSU_ESCR0),
++/* pmc20 */ PMC_D(PFM_REG_I, "CRU_ESCR0"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR0),
++/* pmc21 */ PMC_D(PFM_REG_I, "CRU_ESCR2"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR2),
++/* pmc22 */ PMC_D(PFM_REG_I, "CRU_ESCR4"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR4),
++/* pmc23 */ PMC_D(PFM_REG_I64, "BPU_CCCR0"  , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_BPU_CCCR0),
++/* pmc24 */ PMC_D(PFM_REG_I64, "BPU_CCCR1"  , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_BPU_CCCR1),
++/* pmc25 */ PMC_D(PFM_REG_I64, "MS_CCCR0"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_MS_CCCR0),
++/* pmc26 */ PMC_D(PFM_REG_I64, "MS_CCCR1"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_MS_CCCR1),
++/* pmc27 */ PMC_D(PFM_REG_I64, "FLAME_CCCR0", PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_FLAME_CCCR0),
++/* pmc28 */ PMC_D(PFM_REG_I64, "FLAME_CCCR1", PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_FLAME_CCCR1),
++/* pmc29 */ PMC_D(PFM_REG_I64, "IQ_CCCR0"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR0),
++/* pmc30 */ PMC_D(PFM_REG_I64, "IQ_CCCR1"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR1),
++/* pmc31 */ PMC_D(PFM_REG_I64, "IQ_CCCR4"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR4),
++		/* No HT extension */
++/* pmc32 */ PMC_D(PFM_REG_I, "BPU_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_BPU_ESCR1),
++/* pmc33 */ PMC_D(PFM_REG_I, "IS_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IS_ESCR1),
++/* pmc34 */ PMC_D(PFM_REG_I, "MOB_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_MOB_ESCR1),
++/* pmc35 */ PMC_D(PFM_REG_I, "ITLB_ESCR1" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_ITLB_ESCR1),
++/* pmc36 */ PMC_D(PFM_REG_I, "PMH_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_PMH_ESCR1),
++/* pmc37 */ PMC_D(PFM_REG_I, "IX_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IX_ESCR1),
++/* pmc38 */ PMC_D(PFM_REG_I, "FSB_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FSB_ESCR1),
++/* pmc39 */ PMC_D(PFM_REG_I, "BSU_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_BSU_ESCR1),
++/* pmc40 */ PMC_D(PFM_REG_I, "MS_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_MS_ESCR1),
++/* pmc41 */ PMC_D(PFM_REG_I, "TC_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_TC_ESCR1),
++/* pmc42 */ PMC_D(PFM_REG_I, "TBPU_ESCR1" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_TBPU_ESCR1),
++/* pmc43 */ PMC_D(PFM_REG_I, "FLAME_ESCR1", 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FLAME_ESCR1),
++/* pmc44 */ PMC_D(PFM_REG_I, "FIRM_ESCR1" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_FIRM_ESCR1),
++/* pmc45 */ PMC_D(PFM_REG_I, "SAAT_ESCR1" , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_SAAT_ESCR1),
++/* pmc46 */ PMC_D(PFM_REG_I, "U2L_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_U2L_ESCR1),
++/* pmc47 */ PMC_D(PFM_REG_I, "DAC_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_DAC_ESCR1),
++/* pmc48 */ PMC_D(PFM_REG_I, "IQ_ESCR1"   , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_IQ_ESCR1), /* only model 1 and 2 */
++/* pmc49 */ PMC_D(PFM_REG_I, "ALF_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_ALF_ESCR1),
++/* pmc50 */ PMC_D(PFM_REG_I, "RAT_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_RAT_ESCR1),
++/* pmc51 */ PMC_D(PFM_REG_I, "CRU_ESCR1"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR1),
++/* pmc52 */ PMC_D(PFM_REG_I, "CRU_ESCR3"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR3),
++/* pmc53 */ PMC_D(PFM_REG_I, "CRU_ESCR5"  , 0x0, PFM_ESCR_RSVD, 0, MSR_P4_CRU_ESCR5),
++/* pmc54 */ PMC_D(PFM_REG_I64, "BPU_CCCR2"  , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_BPU_CCCR2),
++/* pmc55 */ PMC_D(PFM_REG_I64, "BPU_CCCR3"  , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_BPU_CCCR3),
++/* pmc56 */ PMC_D(PFM_REG_I64, "MS_CCCR2"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_MS_CCCR2),
++/* pmc57 */ PMC_D(PFM_REG_I64, "MS_CCCR3"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_MS_CCCR3),
++/* pmc58 */ PMC_D(PFM_REG_I64, "FLAME_CCCR2", PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_FLAME_CCCR2),
++/* pmc59 */ PMC_D(PFM_REG_I64, "FLAME_CCCR3", PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_FLAME_CCCR3),
++/* pmc60 */ PMC_D(PFM_REG_I64, "IQ_CCCR2"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR2),
++/* pmc61 */ PMC_D(PFM_REG_I64, "IQ_CCCR3"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR3),
++/* pmc62 */ PMC_D(PFM_REG_I64, "IQ_CCCR5"   , PFM_CCCR_DFL, PFM_CCCR_RSVD, PFM_P4_NO64, MSR_P4_IQ_CCCR5),
++/* pmc63 */ PMC_D(PFM_REG_I, "PEBS_MATRIX_VERT", 0, 0xffffffffffffffecULL, 0, 0x3f2),
++/* pmc64 */ PMC_D(PFM_REG_I, "PEBS_ENABLE", 0, 0xfffffffff8ffe000ULL, 0, 0x3f1)
++};
++#define PFM_P4_NUM_PMCS ARRAY_SIZE(pfm_p4_pmc_desc)
++
++/*
++ * See section 15.10.6.6 for details about the IQ block
++ */
++static struct pfm_regmap_desc pfm_p4_pmd_desc[]={
++/* pmd0  */ PMD_D(PFM_REG_C, "BPU_CTR0", MSR_P4_BPU_PERFCTR0),
++/* pmd1  */ PMD_D(PFM_REG_C, "BPU_CTR1", MSR_P4_BPU_PERFCTR1),
++/* pmd2  */ PMD_D(PFM_REG_C, "MS_CTR0", MSR_P4_MS_PERFCTR0),
++/* pmd3  */ PMD_D(PFM_REG_C, "MS_CTR1", MSR_P4_MS_PERFCTR1),
++/* pmd4  */ PMD_D(PFM_REG_C, "FLAME_CTR0", MSR_P4_FLAME_PERFCTR0),
++/* pmd5  */ PMD_D(PFM_REG_C, "FLAME_CTR1", MSR_P4_FLAME_PERFCTR1),
++/* pmd6  */ PMD_D(PFM_REG_C, "IQ_CTR0", MSR_P4_IQ_PERFCTR0),
++/* pmd7  */ PMD_D(PFM_REG_C, "IQ_CTR1", MSR_P4_IQ_PERFCTR1),
++/* pmd8  */ PMD_D(PFM_REG_C, "IQ_CTR4", MSR_P4_IQ_PERFCTR4),
++		/* no HT extension */
++/* pmd9  */ PMD_D(PFM_REG_C, "BPU_CTR2", MSR_P4_BPU_PERFCTR2),
++/* pmd10 */ PMD_D(PFM_REG_C, "BPU_CTR3", MSR_P4_BPU_PERFCTR3),
++/* pmd11 */ PMD_D(PFM_REG_C, "MS_CTR2", MSR_P4_MS_PERFCTR2),
++/* pmd12 */ PMD_D(PFM_REG_C, "MS_CTR3", MSR_P4_MS_PERFCTR3),
++/* pmd13 */ PMD_D(PFM_REG_C, "FLAME_CTR2", MSR_P4_FLAME_PERFCTR2),
++/* pmd14 */ PMD_D(PFM_REG_C, "FLAME_CTR3", MSR_P4_FLAME_PERFCTR3),
++/* pmd15 */ PMD_D(PFM_REG_C, "IQ_CTR2", MSR_P4_IQ_PERFCTR2),
++/* pmd16 */ PMD_D(PFM_REG_C, "IQ_CTR3", MSR_P4_IQ_PERFCTR3),
++/* pmd17 */ PMD_D(PFM_REG_C, "IQ_CTR5", MSR_P4_IQ_PERFCTR5)
++};
++#define PFM_P4_NUM_PMDS ARRAY_SIZE(pfm_p4_pmd_desc)
++
++/*
++ * Due to hotplug CPU support, threads may not necessarily
++ * be activated at the time the module is inserted. We need
++ * to check whether  they could be activated by looking at
++ * the present CPU (present != online).
++ */
++static int pfm_p4_probe_pmu(void)
++{
++	unsigned int i;
++	int ht_enabled;
++
++	/*
++	 * only works on Intel processors
++	 */
++	if (current_cpu_data.x86_vendor != X86_VENDOR_INTEL) {
++		PFM_INFO("not running on Intel processor");
++		return -1;
++	}
++
++	if (current_cpu_data.x86 != 15) {
++		PFM_INFO("unsupported family=%d", current_cpu_data.x86);
++		return -1;
++	}
++
++	switch(current_cpu_data.x86_model) {
++		case 0 ... 2:
++			break;
++		case 3 ... 6:
++			/*
++			 * IQ_ESCR0, IQ_ESCR1 only present on model 1, 2
++			 */
++			pfm_p4_pmc_desc[16].type = PFM_REG_NA;
++			pfm_p4_pmc_desc[48].type = PFM_REG_NA;
++			break;
++		default:
++			/*
++			 * do not know if they all work the same, so reject
++			 * for now
++			 */
++			if (!force) {
++				PFM_INFO("unsupported model %d",
++					 current_cpu_data.x86_model);
++				return -1;
++			}
++	}
++
++	/*
++	 * check for local APIC (required)
++	 */
++	if (!cpu_has_apic) {
++		PFM_INFO("no local APIC, unsupported");
++		return -1;
++	}
++#ifdef CONFIG_SMP
++	ht_enabled = (cpus_weight(__get_cpu_var(cpu_core_map))
++		   / current_cpu_data.x86_max_cores) > 1;
++#else
++	ht_enabled = 0;
++#endif
++	if (cpu_has_ht) {
++
++		PFM_INFO("HyperThreading supported, status %s",
++			 ht_enabled ? "on": "off");
++		/*
++		 * disable registers not supporting HT
++		 */
++		if (ht_enabled) {
++			PFM_INFO("disabling half the registers for HT");
++			for (i = 0; i < PFM_P4_NUM_PMCS; i++) {
++				if (pfm_p4_pmu_info.pmc_addrs[(i)].reg_type &
++				    PFM_REGT_NOHT)
++					pfm_p4_pmc_desc[i].type = PFM_REG_NA;
++			}
++			for (i = 0; i < PFM_P4_NUM_PMDS; i++) {
++				if (pfm_p4_pmu_info.pmd_addrs[(i)].reg_type &
++				    PFM_REGT_NOHT)
++					pfm_p4_pmd_desc[i].type = PFM_REG_NA;
++			}
++		}
++	}
++
++	if (cpu_has_ds) {
++		PFM_INFO("Data Save Area (DS) supported");
++
++		pfm_p4_pmu_info.flags = PFM_X86_FL_PMU_DS;
++
++		if (cpu_has_pebs) {
++			/*
++			 * PEBS does not work with HyperThreading enabled
++			 */
++	                if (ht_enabled) {
++				PFM_INFO("PEBS supported, status off (because of HT)");
++			} else {
++				pfm_p4_pmu_info.flags |= PFM_X86_FL_PMU_PEBS;
++				PFM_INFO("PEBS supported, status on");
++			}
++		}
++	}
++	if (force_nmi)
++		pfm_p4_pmu_info.flags |= PFM_X86_FL_USE_NMI;
++	return 0;
++}
++
++static struct pfm_pmu_config pfm_p4_pmu_conf={
++	.pmu_name = "Intel P4",
++	.counter_width = 40,
++	.pmd_desc = pfm_p4_pmd_desc,
++	.pmc_desc = pfm_p4_pmc_desc,
++	.num_pmc_entries = PFM_P4_NUM_PMCS,
++	.num_pmd_entries = PFM_P4_NUM_PMDS,
++	.probe_pmu = pfm_p4_probe_pmu,
++	.version = "1.0",
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.arch_info = &pfm_p4_pmu_info
++};
++
++static int __init pfm_p4_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_p4_pmu_conf);
++}
++
++static void __exit pfm_p4_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_p4_pmu_conf);
++}
++
++module_init(pfm_p4_pmu_init_module);
++module_exit(pfm_p4_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/x86/perfmon/perfmon_p6.c
+@@ -0,0 +1,164 @@
++/*
++ * This file contains the P6 family processor PMU register description tables
++ *
++ * This module supports original P6 processors
++ * (Pentium II, Pentium Pro, Pentium III) and Pentium M.
++ *
++ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++#include <asm/msr.h>
++#include <asm/nmi.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("P6 PMU description table");
++MODULE_LICENSE("GPL");
++
++static int force_nmi;
++MODULE_PARM_DESC(force_nmi, "bool: force use of NMI for PMU interrupt");
++module_param(force_nmi, bool, 0600);
++
++/*
++ * - upper 32 bits are reserved
++ * - INT: APIC enable bit is reserved (forced to 1)
++ * - bit 21 is reserved
++ * - bit 22 is reserved on PEREVNTSEL1
++ *
++ * RSVD: reserved bits are 1
++ */
++#define PFM_P6_PMC0_RSVD ((~((1ULL<<32)-1)) | (1ULL<<20) | (1ULL<<21))
++#define PFM_P6_PMC1_RSVD ((~((1ULL<<32)-1)) | (1ULL<<20) | (3ULL<<21))
++
++/*
++ * force Local APIC interrupt on overflow
++ * disable with NO_EMUL64
++ */
++#define PFM_P6_PMC_VAL  (1ULL<<20)
++#define PFM_P6_NO64	(1ULL<<20)
++
++/*
++ * PFM_X86_FL_NO_SHARING: because of the single enable bit on MSR_P6_EVNTSEL0
++ * the PMU cannot be shared with NMI watchdog or Oprofile
++ */
++struct pfm_arch_pmu_info pfm_p6_pmu_info={
++	.pmc_addrs = {
++		{{MSR_P6_EVNTSEL0, 0}, 0, PFM_REGT_EN}, /* has enable bit */
++		{{MSR_P6_EVNTSEL1, 0}, 1, PFM_REGT_OTH} /* no enable bit  */
++	},
++	.pmd_addrs = {
++		{{MSR_P6_PERFCTR0, 0}, 0, PFM_REGT_CTR},
++		{{MSR_P6_PERFCTR1, 0}, 0, PFM_REGT_CTR}
++	},
++	.flags = PFM_X86_FL_NO_SHARING,
++	.pmu_style = PFM_X86_PMU_P6
++};
++
++static struct pfm_regmap_desc pfm_p6_pmc_desc[]={
++/* pmc0  */ PMC_D(PFM_REG_I64, "PERFEVTSEL0", PFM_P6_PMC_VAL, PFM_P6_PMC0_RSVD, PFM_P6_NO64, MSR_P6_EVNTSEL0),
++/* pmc1  */ PMC_D(PFM_REG_I64, "PERFEVTSEL1", PFM_P6_PMC_VAL, PFM_P6_PMC1_RSVD, PFM_P6_NO64, MSR_P6_EVNTSEL1)
++};
++#define PFM_P6_NUM_PMCS	ARRAY_SIZE(pfm_p6_pmc_desc)
++
++static struct pfm_regmap_desc pfm_p6_pmd_desc[]={
++/* pmd0  */ PMD_D(PFM_REG_C  , "PERFCTR0", MSR_P6_PERFCTR0),
++/* pmd1  */ PMD_D(PFM_REG_C  , "PERFCTR1", MSR_P6_PERFCTR1)
++};
++#define PFM_P6_NUM_PMDS ARRAY_SIZE(pfm_p6_pmd_desc)
++
++static int pfm_p6_probe_pmu(void)
++{
++	int high, low;
++
++	if (current_cpu_data.x86_vendor != X86_VENDOR_INTEL) {
++		PFM_INFO("not an Intel processor");
++		return -1;
++	}
++
++	/*
++	 * check for P6 processor family
++	 */
++	if (current_cpu_data.x86 != 6) {
++		PFM_INFO("unsupported family=%d", current_cpu_data.x86);
++		return -1;
++	}
++
++	switch(current_cpu_data.x86_model) {
++		case 1: /* Pentium Pro */
++		case 3:
++		case 5: /* Pentium II Deschutes */
++		case 7 ... 11:
++			break;
++		case 13:
++			/* for Pentium M, we need to check if PMU exist */
++			rdmsr(MSR_IA32_MISC_ENABLE, low, high);
++			if (low & (1U << 7))
++				break;
++		default:
++			PFM_INFO("unsupported CPU model %d",
++				 current_cpu_data.x86_model);
++			return -1;
++
++	}
++
++	if (!cpu_has_apic) {
++		PFM_INFO("no Local APIC, try rebooting with lapic");
++		return -1;
++	}
++	/*
++	 * force NMI interrupt?
++	 */
++	if (force_nmi)
++		pfm_p6_pmu_info.flags |= PFM_X86_FL_USE_NMI;
++
++	return 0;
++}
++
++/*
++ * Counters have 40 bits implemented. However they are designed such
++ * that bits [32-39] are sign extensions of bit 31. As such the
++ * effective width of a counter for P6-like PMU is 31 bits only.
++ *
++ * See IA-32 Intel Architecture Software developer manual Vol 3B
++ */
++static struct pfm_pmu_config pfm_p6_pmu_conf={
++	.pmu_name = "Intel P6 processor Family",
++	.counter_width = 31,
++	.pmd_desc = pfm_p6_pmd_desc,
++	.pmc_desc = pfm_p6_pmc_desc,
++	.num_pmc_entries = PFM_P6_NUM_PMCS,
++	.num_pmd_entries = PFM_P6_NUM_PMDS,
++	.probe_pmu = pfm_p6_probe_pmu,
++	.version = "1.0",
++	.flags = PFM_PMU_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++	.arch_info = &pfm_p6_pmu_info
++};
++
++static int __init pfm_p6_pmu_init_module(void)
++{
++	return pfm_pmu_register(&pfm_p6_pmu_conf);
++}
++
++static void __exit pfm_p6_pmu_cleanup_module(void)
++{
++	pfm_pmu_unregister(&pfm_p6_pmu_conf);
++}
++
++module_init(pfm_p6_pmu_init_module);
++module_exit(pfm_p6_pmu_cleanup_module);
+--- /dev/null
++++ b/arch/x86/perfmon/perfmon_pebs_core_smpl.c
+@@ -0,0 +1,252 @@
++/*
++ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file implements the Precise Event Based Sampling (PEBS)
++ * sampling format for Intel Core-based processors.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/kernel.h>
++#include <linux/types.h>
++#include <linux/module.h>
++#include <linux/init.h>
++#include <linux/smp.h>
++#include <asm/msr.h>
++
++#include <linux/perfmon.h>
++#include <asm/perfmon_pebs_core_smpl.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("Intel Core Precise Event-Based Sampling (PEBS)");
++MODULE_LICENSE("GPL");
++
++#define ALIGN_PEBS(a, order) \
++		((a)+(1UL<<(order))-1) & ~((1UL<<(order))-1)
++
++#define PEBS_PADDING_ORDER 8 /* log2(256) padding for PEBS alignment constraint */
++
++static int pfm_pebs_core_fmt_validate(u32 flags, u16 npmds, void *data)
++{
++	struct pfm_pebs_core_smpl_arg *arg = data;
++	size_t min_buf_size;
++
++	/*
++	 * need to define at least the size of the buffer
++	 */
++	if (data == NULL) {
++		PFM_DBG("no argument passed");
++		return -EINVAL;
++	}
++
++	/*
++	 * compute min buf size. npmds is the maximum number
++	 * of implemented PMD registers.
++	 */
++	min_buf_size = sizeof(struct pfm_pebs_core_smpl_hdr)
++		     + sizeof(struct pfm_pebs_core_smpl_entry)
++		     + (1UL<<PEBS_PADDING_ORDER); /* padding for alignment */
++
++	PFM_DBG("validate flags=0x%x min_buf_size=%zu buf_size=%zu",
++		  flags,
++		  min_buf_size,
++		  arg->buf_size);
++
++	/*
++	 * must hold at least the buffer header + one minimally sized entry
++	 */
++	if (arg->buf_size < min_buf_size)
++		return -EINVAL;
++
++	return 0;
++}
++
++static int pfm_pebs_core_fmt_get_size(unsigned int flags, void *data, size_t *size)
++{
++	struct pfm_pebs_core_smpl_arg *arg = data;
++
++	/*
++	 * size has been validated in pfm_pebs_core_fmt_validate()
++	 */
++	*size = arg->buf_size + (1UL<<PEBS_PADDING_ORDER);
++
++	return 0;
++}
++
++static int pfm_pebs_core_fmt_init(struct pfm_context *ctx, void *buf,
++			     u32 flags, u16 npmds, void *data)
++{
++	struct pfm_arch_context *ctx_arch;
++	struct pfm_pebs_core_smpl_hdr *hdr;
++	struct pfm_pebs_core_smpl_arg *arg = data;
++	u64 pebs_start, pebs_end;
++	struct pfm_ds_area_core *ds;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	hdr = buf;
++	ds = &hdr->ds;
++
++	/*
++	 * align PEBS buffer base
++	 */
++	pebs_start = ALIGN_PEBS((unsigned long)(hdr+1), PEBS_PADDING_ORDER);
++	pebs_end = pebs_start + arg->buf_size + 1;
++
++	hdr->version = PFM_PEBS_CORE_SMPL_VERSION;
++	hdr->buf_size = arg->buf_size;
++	hdr->overflows = 0;
++
++	/*
++	 * express PEBS buffer base as offset from the end of the header
++	 */
++	hdr->start_offs = pebs_start - (unsigned long)(hdr+1);
++
++	/*
++	 * PEBS buffer boundaries
++	 */
++	ds->pebs_buf_base = pebs_start;
++	ds->pebs_abs_max = pebs_end;
++
++	/*
++	 * PEBS starting position
++	 */
++	ds->pebs_index = pebs_start;
++
++	/*
++	 * PEBS interrupt threshold
++	 */
++	ds->pebs_intr_thres = pebs_start
++			    + arg->intr_thres
++			    * sizeof(struct pfm_pebs_core_smpl_entry);
++
++	/*
++	 * save counter reset value for PEBS counter
++	 */
++	ds->pebs_cnt_reset = arg->cnt_reset;
++
++	/*
++	 * keep track of DS AREA
++	 */
++	ctx_arch->ds_area = ds;
++	ctx_arch->flags.use_ds = 1;
++	ctx_arch->flags.use_pebs = 1;
++
++	PFM_DBG("buffer=%p buf_size=%llu offs=%llu pebs_start=0x%llx "
++		  "pebs_end=0x%llx ds=%p pebs_thres=0x%llx cnt_reset=0x%llx",
++		  buf,
++		  (unsigned long long)hdr->buf_size,
++		  (unsigned long long)hdr->start_offs,
++		  (unsigned long long)pebs_start,
++		  (unsigned long long)pebs_end,
++		  ds,
++		  (unsigned long long)ds->pebs_intr_thres,
++		  (unsigned long long)ds->pebs_cnt_reset);
++
++	return 0;
++}
++
++static int pfm_pebs_core_fmt_handler(void *buf, struct pfm_ovfl_arg *arg,
++			       unsigned long ip, u64 tstamp, void *data)
++{
++	struct pfm_pebs_core_smpl_hdr *hdr;
++
++	hdr = buf;
++
++	PFM_DBG_ovfl("buffer full");
++	/*
++	 * increment number of buffer overflows.
++	 * important to detect duplicate set of samples.
++	 */
++	hdr->overflows++;
++
++	/*
++	 * request notification and masking of monitoring.
++	 * Notification is still subject to the overflowed
++	 * register having the FL_NOTIFY flag set.
++	 */
++	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY| PFM_OVFL_CTRL_MASK;
++
++	return -ENOBUFS; /* we are full, sorry */
++}
++
++static int pfm_pebs_core_fmt_restart(int is_active, u32 *ovfl_ctrl,
++				void *buf)
++{
++	struct pfm_pebs_core_smpl_hdr *hdr = buf;
++
++	/*
++	 * reset index to base of buffer
++	 */
++	hdr->ds.pebs_index = hdr->ds.pebs_buf_base;
++
++	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
++
++	return 0;
++}
++
++static int pfm_pebs_core_fmt_exit(void *buf)
++{
++	return 0;
++}
++
++static struct pfm_smpl_fmt pebs_core_fmt={
++	.fmt_name = PFM_PEBS_CORE_SMPL_NAME,
++	.fmt_version = 0x1,
++	.fmt_arg_size = sizeof(struct pfm_pebs_core_smpl_arg),
++	.fmt_validate = pfm_pebs_core_fmt_validate,
++	.fmt_getsize = pfm_pebs_core_fmt_get_size,
++	.fmt_init = pfm_pebs_core_fmt_init,
++	.fmt_handler = pfm_pebs_core_fmt_handler,
++	.fmt_restart = pfm_pebs_core_fmt_restart,
++	.fmt_exit = pfm_pebs_core_fmt_exit,
++	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++};
++
++static int __init pfm_pebs_core_fmt_init_module(void)
++{
++	if (!cpu_has_pebs) {
++		PFM_INFO("processor does not have PEBS support");
++		return -1;
++	}
++	/*
++	 * cpu_has_pebs is not enough to identify Intel Core PEBS
++	 * which is different fro Pentium 4 PEBS. Therefore we do
++	 * a more detailed check here
++	 */
++	if (current_cpu_data.x86 != 6) {
++		PFM_INFO("not an Intel Core processor");
++		return -1;
++	}
++
++	switch(current_cpu_data.x86_model) {
++		case 15: /* Merom */
++		case 23: /* Penryn */
++			break;
++		default:
++			PFM_INFO("not an Intel Core processor");
++			return -1;
++	}
++	return pfm_fmt_register(&pebs_core_fmt);
++}
++
++static void __exit pfm_pebs_core_fmt_cleanup_module(void)
++{
++	pfm_fmt_unregister(&pebs_core_fmt);
++}
++
++module_init(pfm_pebs_core_fmt_init_module);
++module_exit(pfm_pebs_core_fmt_cleanup_module);
+--- /dev/null
++++ b/arch/x86/perfmon/perfmon_pebs_p4_smpl.c
+@@ -0,0 +1,252 @@
++/*
++ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file implements the Precise Event Based Sampling (PEBS)
++ * sampling format. It supports the following processors:
++ *	- 32-bit Pentium 4 or other Netburst-based processors
++ *	- 64-bit Pentium 4 or other Netburst-based processors
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#include <linux/kernel.h>
++#include <linux/types.h>
++#include <linux/module.h>
++#include <linux/init.h>
++#include <linux/smp.h>
++#include <asm/msr.h>
++
++#include <linux/perfmon.h>
++#include <asm/perfmon_pebs_p4_smpl.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("Intel P4 Precise Event-Based Sampling (PEBS)");
++MODULE_LICENSE("GPL");
++
++#define ALIGN_PEBS(a, order) \
++		((a)+(1UL<<(order))-1) & ~((1UL<<(order))-1)
++
++#define PEBS_PADDING_ORDER 8 /* log2(256) padding for PEBS alignment constraint */
++
++static int pfm_pebs_p4_fmt_validate(u32 flags, u16 npmds, void *data)
++{
++	struct pfm_pebs_p4_smpl_arg *arg = data;
++	size_t min_buf_size;
++
++	/*
++	 * need to define at least the size of the buffer
++	 */
++	if (data == NULL) {
++		PFM_DBG("no argument passed");
++		return -EINVAL;
++	}
++
++	/*
++	 * compute min buf size. npmds is the maximum number
++	 * of implemented PMD registers.
++	 */
++	min_buf_size = sizeof(struct pfm_pebs_p4_smpl_hdr)
++		     + sizeof(struct pfm_pebs_p4_smpl_entry)
++		     + (1UL<<PEBS_PADDING_ORDER); /* padding for alignment */
++
++	PFM_DBG("validate flags=0x%x min_buf_size=%zu buf_size=%zu",
++		  flags,
++		  min_buf_size,
++		  arg->buf_size);
++
++	/*
++	 * must hold at least the buffer header + one minimally sized entry
++	 */
++	if (arg->buf_size < min_buf_size)
++		return -EINVAL;
++
++	return 0;
++}
++
++static int pfm_pebs_p4_fmt_get_size(unsigned int flags, void *data, size_t *size)
++{
++	struct pfm_pebs_p4_smpl_arg *arg = data;
++
++	/*
++	 * size has been validated in pfm_pebs_p4_fmt_validate()
++	 */
++	*size = arg->buf_size + (1UL<<PEBS_PADDING_ORDER);
++
++	return 0;
++}
++
++static int pfm_pebs_p4_fmt_init(struct pfm_context *ctx, void *buf,
++			        u32 flags, u16 npmds, void *data)
++{
++	struct pfm_arch_context *ctx_arch;
++	struct pfm_pebs_p4_smpl_hdr *hdr;
++	struct pfm_pebs_p4_smpl_arg *arg = data;
++	unsigned long pebs_start, pebs_end;
++	struct pfm_ds_area_p4 *ds;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	hdr = buf;
++	ds = &hdr->ds;
++
++	/*
++	 * align PEBS buffer base
++	 */
++	pebs_start = ALIGN_PEBS((unsigned long)(hdr+1), PEBS_PADDING_ORDER);
++	pebs_end = pebs_start + arg->buf_size + 1;
++
++	hdr->version = PFM_PEBS_P4_SMPL_VERSION;
++	hdr->buf_size = arg->buf_size;
++	hdr->overflows = 0;
++
++	/*
++	 * express PEBS buffer base as offset from the end of the header
++	 */
++	hdr->start_offs = pebs_start - (unsigned long)(hdr+1);
++
++	/*
++	 * PEBS buffer boundaries
++	 */
++	ds->pebs_buf_base = pebs_start;
++	ds->pebs_abs_max = pebs_end;
++
++	/*
++	 * PEBS starting position
++	 */
++	ds->pebs_index = pebs_start;
++
++	/*
++	 * PEBS interrupt threshold
++	 */
++	ds->pebs_intr_thres = pebs_start
++			    + arg->intr_thres
++			    * sizeof(struct pfm_pebs_p4_smpl_entry);
++
++	/*
++	 * save counter reset value for PEBS counter
++	 */
++	ds->pebs_cnt_reset = arg->cnt_reset;
++
++	/*
++	 * keep track of DS AREA
++	 */
++	ctx_arch->ds_area = ds;
++	ctx_arch->flags.use_pebs = 1;
++	ctx_arch->flags.use_ds = 1;
++
++	PFM_DBG("buffer=%p buf_size=%llu offs=%llu pebs_start=0x%lx "
++		  "pebs_end=0x%lx ds=%p pebs_thres=0x%lx cnt_reset=0x%llx",
++		  buf,
++		  (unsigned long long)hdr->buf_size,
++		  (unsigned long long)hdr->start_offs,
++		  pebs_start,
++		  pebs_end,
++		  ds,
++		  ds->pebs_intr_thres,
++		  (unsigned long long)ds->pebs_cnt_reset);
++
++	return 0;
++}
++
++static int pfm_pebs_p4_fmt_handler(void *buf, struct pfm_ovfl_arg *arg,
++			       unsigned long ip, u64 tstamp, void *data)
++{
++	struct pfm_pebs_p4_smpl_hdr *hdr;
++
++	hdr = buf;
++
++	PFM_DBG_ovfl("buffer full");
++	/*
++	 * increment number of buffer overflows.
++	 * important to detect duplicate set of samples.
++	 */
++	hdr->overflows++;
++
++	/*
++	 * request notification and masking of monitoring.
++	 * Notification is still subject to the overflowed
++	 * register having the FL_NOTIFY flag set.
++	 */
++	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY| PFM_OVFL_CTRL_MASK;
++
++	return -ENOBUFS; /* we are full, sorry */
++}
++
++static int pfm_pebs_p4_fmt_restart(int is_active, u32 *ovfl_ctrl,
++				void *buf)
++{
++	struct pfm_pebs_p4_smpl_hdr *hdr = buf;
++
++	/*
++	 * reset index to base of buffer
++	 */
++	hdr->ds.pebs_index = hdr->ds.pebs_buf_base;
++
++	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
++
++	return 0;
++}
++
++static int pfm_pebs_p4_fmt_exit(void *buf)
++{
++	return 0;
++}
++
++static struct pfm_smpl_fmt pebs_p4_fmt={
++	.fmt_name = PFM_PEBS_P4_SMPL_NAME,
++	.fmt_version = 0x1,
++	.fmt_arg_size = sizeof(struct pfm_pebs_p4_smpl_arg),
++	.fmt_validate = pfm_pebs_p4_fmt_validate,
++	.fmt_getsize = pfm_pebs_p4_fmt_get_size,
++	.fmt_init = pfm_pebs_p4_fmt_init,
++	.fmt_handler = pfm_pebs_p4_fmt_handler,
++	.fmt_restart = pfm_pebs_p4_fmt_restart,
++	.fmt_exit = pfm_pebs_p4_fmt_exit,
++	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
++	.owner = THIS_MODULE,
++};
++
++static int __init pfm_pebs_p4_fmt_init_module(void)
++{
++	int ht_enabled;
++
++	if (!cpu_has_pebs) {
++		PFM_INFO("processor does not have PEBS support");
++		return -1;
++	}
++	if (current_cpu_data.x86 != 15) {
++		PFM_INFO("not an Intel Pentium 4");
++		return -1;
++	}
++#ifdef CONFIG_SMP
++	ht_enabled = (cpus_weight(__get_cpu_var(cpu_core_map))
++		   / current_cpu_data.x86_max_cores) > 1;
++#else
++	ht_enabled = 0;
++#endif
++	if (ht_enabled) {
++		PFM_INFO("PEBS not available because HyperThreading is on");
++		return -1;
++	}
++	return pfm_fmt_register(&pebs_p4_fmt);
++}
++
++static void __exit pfm_pebs_p4_fmt_cleanup_module(void)
++{
++	pfm_fmt_unregister(&pebs_p4_fmt);
++}
++
++module_init(pfm_pebs_p4_fmt_init_module);
++module_exit(pfm_pebs_p4_fmt_cleanup_module);
+--- a/drivers/oprofile/oprofile_files.c
++++ b/drivers/oprofile/oprofile_files.c
+@@ -117,7 +117,7 @@ static ssize_t dump_write(struct file * 
+ static const struct file_operations dump_fops = {
+ 	.write		= dump_write,
+ };
+- 
++
+ void oprofile_create_files(struct super_block * sb, struct dentry * root)
+ {
+ 	oprofilefs_create_file(sb, root, "enable", &enable_fops);
+--- a/include/asm-ia64/hw_irq.h
++++ b/include/asm-ia64/hw_irq.h
+@@ -63,9 +63,9 @@ extern int ia64_last_device_vector;
+ #define IA64_NUM_DEVICE_VECTORS		(IA64_LAST_DEVICE_VECTOR - IA64_FIRST_DEVICE_VECTOR + 1)
+ 
+ #define IA64_MCA_RENDEZ_VECTOR		0xe8	/* MCA rendez interrupt */
+-#define IA64_PERFMON_VECTOR		0xee	/* performance monitor interrupt vector */
+ #define IA64_TIMER_VECTOR		0xef	/* use highest-prio group 15 interrupt for timer */
+ #define	IA64_MCA_WAKEUP_VECTOR		0xf0	/* MCA wakeup (must be >MCA_RENDEZ_VECTOR) */
++#define IA64_PERFMON_VECTOR		0xf1	/* performance monitor interrupt vector */
+ #define IA64_IPI_LOCAL_TLB_FLUSH	0xfc	/* SMP flush local TLB */
+ #define IA64_IPI_RESCHEDULE		0xfd	/* SMP reschedule */
+ #define IA64_IPI_VECTOR			0xfe	/* inter-processor interrupt vector */
+--- a/include/asm-ia64/perfmon.h
++++ b/include/asm-ia64/perfmon.h
+@@ -1,279 +1,321 @@
+ /*
+- * Copyright (C) 2001-2003 Hewlett-Packard Co
+- *               Stephane Eranian <eranian@hpl.hp.com>
+- */
++ * Copyright (c) 2001-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file contains Itanium Processor Family specific definitions
++ * for the perfmon interface.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#ifndef _ASM_IA64_PERFMON_H_
++#define _ASM_IA64_PERFMON_H_
+ 
+-#ifndef _ASM_IA64_PERFMON_H
+-#define _ASM_IA64_PERFMON_H
++#ifdef __KERNEL__
+ 
+ /*
+- * perfmon comamnds supported on all CPU models
++ * compatibility for version v2.0 of the interface
+  */
+-#define PFM_WRITE_PMCS		0x01
+-#define PFM_WRITE_PMDS		0x02
+-#define PFM_READ_PMDS		0x03
+-#define PFM_STOP		0x04
+-#define PFM_START		0x05
+-#define PFM_ENABLE		0x06 /* obsolete */
+-#define PFM_DISABLE		0x07 /* obsolete */
+-#define PFM_CREATE_CONTEXT	0x08
+-#define PFM_DESTROY_CONTEXT	0x09 /* obsolete use close() */
+-#define PFM_RESTART		0x0a
+-#define PFM_PROTECT_CONTEXT	0x0b /* obsolete */
+-#define PFM_GET_FEATURES	0x0c
+-#define PFM_DEBUG		0x0d
+-#define PFM_UNPROTECT_CONTEXT	0x0e /* obsolete */
+-#define PFM_GET_PMC_RESET_VAL	0x0f
+-#define PFM_LOAD_CONTEXT	0x10
+-#define PFM_UNLOAD_CONTEXT	0x11
++#include <asm/perfmon_compat.h>
++#include <asm/delay.h>
+ 
+ /*
+- * PMU model specific commands (may not be supported on all PMU models)
++ * describe the content of the pfm_syst_info field
++ * layout:
++ * bits[00-15] : generic flags
++ * bits[16-31] : arch-specific flags
+  */
+-#define PFM_WRITE_IBRS		0x20
+-#define PFM_WRITE_DBRS		0x21
++#define PFM_ITA_CPUINFO_IDLE_EXCL 0x10000 /* stop monitoring in idle loop */
+ 
+ /*
+- * context flags
++ * For some CPUs, the upper bits of a counter must be set in order for the
++ * overflow interrupt to happen. On overflow, the counter has wrapped around,
++ * and the upper bits are cleared. This function may be used to set them back.
+  */
+-#define PFM_FL_NOTIFY_BLOCK    	 0x01	/* block task on user level notifications */
+-#define PFM_FL_SYSTEM_WIDE	 0x02	/* create a system wide context */
+-#define PFM_FL_OVFL_NO_MSG	 0x80   /* do not post overflow/end messages for notification */
++static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx, unsigned int cnum)
++{}
+ 
+-/*
+- * event set flags
+- */
+-#define PFM_SETFL_EXCL_IDLE      0x01   /* exclude idle task (syswide only) XXX: DO NOT USE YET */
++static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
++{
++	return 0;
++}
+ 
+-/*
+- * PMC flags
+- */
+-#define PFM_REGFL_OVFL_NOTIFY	0x1	/* send notification on overflow */
+-#define PFM_REGFL_RANDOM	0x2	/* randomize sampling interval   */
++static inline void pfm_arch_pmu_config_remove(void)
++{}
+ 
+ /*
+- * PMD/PMC/IBR/DBR return flags (ignored on input)
++ * called from __pfm_interrupt_handler(). ctx is not NULL.
++ * ctx is locked. PMU interrupt is masked.
+  *
+- * Those flags are used on output and must be checked in case EAGAIN is returned
+- * by any of the calls using a pfarg_reg_t or pfarg_dbreg_t structure.
+- */
+-#define PFM_REG_RETFL_NOTAVAIL	(1UL<<31) /* set if register is implemented but not available */
+-#define PFM_REG_RETFL_EINVAL	(1UL<<30) /* set if register entry is invalid */
+-#define PFM_REG_RETFL_MASK	(PFM_REG_RETFL_NOTAVAIL|PFM_REG_RETFL_EINVAL)
+-
+-#define PFM_REG_HAS_ERROR(flag)	(((flag) & PFM_REG_RETFL_MASK) != 0)
+-
+-typedef unsigned char pfm_uuid_t[16];	/* custom sampling buffer identifier type */
+-
+-/*
+- * Request structure used to define a context
+- */
+-typedef struct {
+-	pfm_uuid_t     ctx_smpl_buf_id;	 /* which buffer format to use (if needed) */
+-	unsigned long  ctx_flags;	 /* noblock/block */
+-	unsigned short ctx_nextra_sets;	 /* number of extra event sets (you always get 1) */
+-	unsigned short ctx_reserved1;	 /* for future use */
+-	int	       ctx_fd;		 /* return arg: unique identification for context */
+-	void	       *ctx_smpl_vaddr;	 /* return arg: virtual address of sampling buffer, is used */
+-	unsigned long  ctx_reserved2[11];/* for future use */
+-} pfarg_context_t;
++ * must stop all monitoring to ensure handler has consistent view.
++ * must collect overflowed PMDs bitmask  into povfls_pmds and
++ * npend_ovfls. If no interrupt detected then npend_ovfls
++ * must be set to zero.
++ */
++static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
++					    struct pfm_event_set *set)
++{
++	u64 tmp;
++
++	/*
++	 * do not overwrite existing value, must
++	 * process those first (coming from context switch replay)
++	 */
++	if (set->npend_ovfls)
++		return;
++
++	ia64_srlz_d();
++
++	tmp =  ia64_get_pmc(0) & ~0xf;
++
++	set->povfl_pmds[0] = tmp;
++
++	set->npend_ovfls = ia64_popcnt(tmp);
++}
++
++static inline int pfm_arch_init_pmu_config(void)
++{
++	return 0;
++}
++
++static inline void pfm_arch_resend_irq(void)
++{
++	ia64_resend_irq(IA64_PERFMON_VECTOR);
++}
++
++static inline void pfm_arch_serialize(void)
++{
++	ia64_srlz_d();
++}
++
++static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
++{
++	PFM_DBG_ovfl("state=%d", ctx->state);
++	ia64_set_pmc(0, 0);
++	/* no serialization */
++}
++
++static inline void pfm_arch_write_pmc(struct pfm_context *ctx, unsigned int cnum, u64 value)
++{
++	if (cnum < 256) {
++		ia64_set_pmc(pfm_pmu_conf->pmc_desc[cnum].hw_addr, value);
++	} else if (cnum < 264) {
++		ia64_set_ibr(cnum-256, value);
++		ia64_dv_serialize_instruction();
++	} else {
++		ia64_set_dbr(cnum-264, value);
++		ia64_dv_serialize_instruction();
++	}
++}
++
++/*
++ * On IA-64, for per-thread context which have the ITA_FL_INSECURE
++ * flag, it is possible to start/stop monitoring directly from user evel
++ * without calling pfm_start()/pfm_stop. This allows very lightweight
++ * control yet the kernel sometimes needs to know if monitoring is actually
++ * on or off.
++ *
++ * Tracking of this information is normally done by pfm_start/pfm_stop
++ * in flags.started. Here we need to compensate by checking actual
++ * psr bit.
++ */
++static inline int pfm_arch_is_active(struct pfm_context *ctx)
++{
++	return ctx->flags.started || ia64_getreg(_IA64_REG_PSR) & (IA64_PSR_UP|IA64_PSR_PP);
++}
++
++static inline void pfm_arch_write_pmd(struct pfm_context *ctx, unsigned int cnum, u64 value)
++{
++	/*
++	 * for a counting PMD, overflow bit must be cleared
++	 */
++	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_C64)
++		value &= pfm_pmu_conf->ovfl_mask;
++
++	/*
++	 * for counters, write to upper bits are ignored, no need to mask
++	 */
++	ia64_set_pmd(pfm_pmu_conf->pmd_desc[cnum].hw_addr, value);
++}
++
++static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
++{
++	return ia64_get_pmd(pfm_pmu_conf->pmd_desc[cnum].hw_addr);
++}
++
++static inline u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
++{
++	return ia64_get_pmc(pfm_pmu_conf->pmc_desc[cnum].hw_addr);
++}
++
++static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
++					 struct pfm_context *ctx,
++					 struct pfm_event_set *set)
++{
++	struct pt_regs *regs;
++
++	regs = task_pt_regs(task);
++	ia64_psr(regs)->pp = 0;
++}
++
++static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
++					struct pfm_context *ctx,
++					struct pfm_event_set *set)
++{
++	struct pt_regs *regs;
++
++	if (!(set->flags & PFM_ITA_SETFL_INTR_ONLY)) {
++		regs = task_pt_regs(task);
++		ia64_psr(regs)->pp = 1;
++	}
++}
++
++/*
++ * On IA-64, the PMDs are NOT saved by pfm_arch_freeze_pmu()
++ * when entering the PMU interrupt handler, thus, we need
++ * to save them in pfm_switch_sets_from_intr()
++ */
++static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
++					   struct pfm_event_set *set)
++{
++	pfm_save_pmds(ctx, set);
++}
++
++int pfm_arch_context_create(struct pfm_context *ctx, u32 ctx_flags);
++
++static inline void pfm_arch_context_free(struct pfm_context *ctx)
++{}
++
++int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
++			     struct pfm_event_set *set);
++void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx,
++			     struct pfm_event_set *set);
++
++int pfm_arch_unload_context(struct pfm_context *ctx, struct task_struct *task);
++int pfm_arch_load_context(struct pfm_context *ctx, struct pfm_event_set *set,
++			  struct task_struct *task);
++int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags);
++
++void pfm_arch_mask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set);
++void pfm_arch_unmask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set);
++
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
++
++void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
++		   struct pfm_event_set *set);
++void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
++		    struct pfm_event_set *set);
++
++int  pfm_arch_init(void);
++void pfm_arch_init_percpu(void);
++char *pfm_arch_get_pmu_module_name(void);
++
++int __pfm_use_dbregs(struct task_struct *task);
++int  __pfm_release_dbregs(struct task_struct *task);
++int pfm_ia64_mark_dbregs_used(struct pfm_context *ctx,
++			      struct pfm_event_set *set);
++
++void pfm_arch_show_session(struct seq_file *m);
++
++static inline int pfm_arch_pmu_acquire(void)
++{
++	return 0;
++}
++
++static inline void pfm_arch_pmu_release(void)
++{}
++
++/* not necessary on IA-64 */
++static inline void pfm_cacheflush(void *addr, unsigned int len)
++{}
+ 
+ /*
+- * Request structure used to write/read a PMC or PMD
++ * miscellaneous architected definitions
+  */
+-typedef struct {
+-	unsigned int	reg_num;	   /* which register */
+-	unsigned short	reg_set;	   /* event set for this register */
+-	unsigned short	reg_reserved1;	   /* for future use */
+-
+-	unsigned long	reg_value;	   /* initial pmc/pmd value */
+-	unsigned long	reg_flags;	   /* input: pmc/pmd flags, return: reg error */
+-
+-	unsigned long	reg_long_reset;	   /* reset after buffer overflow notification */
+-	unsigned long	reg_short_reset;   /* reset after counter overflow */
+-
+-	unsigned long	reg_reset_pmds[4]; /* which other counters to reset on overflow */
+-	unsigned long	reg_random_seed;   /* seed value when randomization is used */
+-	unsigned long	reg_random_mask;   /* bitmask used to limit random value */
+-	unsigned long   reg_last_reset_val;/* return: PMD last reset value */
+-
+-	unsigned long	reg_smpl_pmds[4];  /* which pmds are accessed when PMC overflows */
+-	unsigned long	reg_smpl_eventid;  /* opaque sampling event identifier */
+-
+-	unsigned long   reg_reserved2[3];   /* for future use */
+-} pfarg_reg_t;
+-
+-typedef struct {
+-	unsigned int	dbreg_num;		/* which debug register */
+-	unsigned short	dbreg_set;		/* event set for this register */
+-	unsigned short	dbreg_reserved1;	/* for future use */
+-	unsigned long	dbreg_value;		/* value for debug register */
+-	unsigned long	dbreg_flags;		/* return: dbreg error */
+-	unsigned long	dbreg_reserved2[1];	/* for future use */
+-} pfarg_dbreg_t;
+-
+-typedef struct {
+-	unsigned int	ft_version;	/* perfmon: major [16-31], minor [0-15] */
+-	unsigned int	ft_reserved;	/* reserved for future use */
+-	unsigned long	reserved[4];	/* for future use */
+-} pfarg_features_t;
+-
+-typedef struct {
+-	pid_t		load_pid;	   /* process to load the context into */
+-	unsigned short	load_set;	   /* first event set to load */
+-	unsigned short	load_reserved1;	   /* for future use */
+-	unsigned long	load_reserved2[3]; /* for future use */
+-} pfarg_load_t;
+-
+-typedef struct {
+-	int		msg_type;		/* generic message header */
+-	int		msg_ctx_fd;		/* generic message header */
+-	unsigned long	msg_ovfl_pmds[4];	/* which PMDs overflowed */
+-	unsigned short  msg_active_set;		/* active set at the time of overflow */
+-	unsigned short  msg_reserved1;		/* for future use */
+-	unsigned int    msg_reserved2;		/* for future use */
+-	unsigned long	msg_tstamp;		/* for perf tuning/debug */
+-} pfm_ovfl_msg_t;
+-
+-typedef struct {
+-	int		msg_type;		/* generic message header */
+-	int		msg_ctx_fd;		/* generic message header */
+-	unsigned long	msg_tstamp;		/* for perf tuning */
+-} pfm_end_msg_t;
+-
+-typedef struct {
+-	int		msg_type;		/* type of the message */
+-	int		msg_ctx_fd;		/* unique identifier for the context */
+-	unsigned long	msg_tstamp;		/* for perf tuning */
+-} pfm_gen_msg_t;
+-
+-#define PFM_MSG_OVFL	1	/* an overflow happened */
+-#define PFM_MSG_END	2	/* task to which context was attached ended */
+-
+-typedef union {
+-	pfm_ovfl_msg_t	pfm_ovfl_msg;
+-	pfm_end_msg_t	pfm_end_msg;
+-	pfm_gen_msg_t	pfm_gen_msg;
+-} pfm_msg_t;
++#define PFM_ITA_FCNTR	4 /* first counting monitor (PMC/PMD) */
+ 
+ /*
+- * Define the version numbers for both perfmon as a whole and the sampling buffer format.
++ * private event set flags  (set_priv_flags)
+  */
+-#define PFM_VERSION_MAJ		 2U
+-#define PFM_VERSION_MIN		 0U
+-#define PFM_VERSION		 (((PFM_VERSION_MAJ&0xffff)<<16)|(PFM_VERSION_MIN & 0xffff))
+-#define PFM_VERSION_MAJOR(x)	 (((x)>>16) & 0xffff)
+-#define PFM_VERSION_MINOR(x)	 ((x) & 0xffff)
++#define PFM_ITA_SETFL_USE_DBR	0x1000000 /* set uses debug registers */
+ 
+ 
+ /*
+- * miscellaneous architected definitions
++ * Itanium-specific data structures
+  */
+-#define PMU_FIRST_COUNTER	4	/* first counting monitor (PMC/PMD) */
+-#define PMU_MAX_PMCS		256	/* maximum architected number of PMC registers */
+-#define PMU_MAX_PMDS		256	/* maximum architected number of PMD registers */
+-
+-#ifdef __KERNEL__
+-
+-extern long perfmonctl(int fd, int cmd, void *arg, int narg);
++struct pfm_ia64_context_flags {
++	unsigned int use_dbr:1;	 /* use range restrictions (debug registers) */
++	unsigned int insecure:1; /* insecure monitoring for non-self session */
++	unsigned int reserved:30;/* for future use */
++};
+ 
+-typedef struct {
+-	void (*handler)(int irq, void *arg, struct pt_regs *regs);
+-} pfm_intr_handler_desc_t;
++struct pfm_arch_context {
++	struct pfm_ia64_context_flags flags;	/* arch specific ctx flags */
++	u64			 ctx_saved_psr_up;/* storage for ctxsw psr_up */
++#ifdef CONFIG_IA64_PERFMON_COMPAT
++	void			*ctx_smpl_vaddr; /* vaddr of user mapping */
++#endif
++};
+ 
+-extern void pfm_save_regs (struct task_struct *);
+-extern void pfm_load_regs (struct task_struct *);
++#ifdef CONFIG_IA64_PERFMON_COMPAT
++ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
++			     char __user *buf,
++			     int non_block,
++			     size_t size);
++int pfm_ia64_compat_init(void);
++int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
++			         size_t rsize, struct file *filp);
++#else
++static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
++			     char __user *buf,
++			     int non_block,
++			     size_t size)
++{
++	return -EINVAL;
++}
+ 
+-extern void pfm_exit_thread(struct task_struct *);
+-extern int  pfm_use_debug_registers(struct task_struct *);
+-extern int  pfm_release_debug_registers(struct task_struct *);
+-extern void pfm_syst_wide_update_task(struct task_struct *, unsigned long info, int is_ctxswin);
+-extern void pfm_inherit(struct task_struct *task, struct pt_regs *regs);
+-extern void pfm_init_percpu(void);
+-extern void pfm_handle_work(void);
+-extern int  pfm_install_alt_pmu_interrupt(pfm_intr_handler_desc_t *h);
+-extern int  pfm_remove_alt_pmu_interrupt(pfm_intr_handler_desc_t *h);
++static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
++					       size_t rsize, struct file *filp)
++{
++	return -EINVAL;
++}
++#endif
+ 
++extern struct pfm_ia64_pmu_info *pfm_ia64_pmu_info;
+ 
++#define PFM_ARCH_CTX_SIZE	(sizeof(struct pfm_arch_context))
+ 
+ /*
+- * Reset PMD register flags
++ * IA-64 does not need extra alignment requirements for the sampling buffer
+  */
+-#define PFM_PMD_SHORT_RESET	0
+-#define PFM_PMD_LONG_RESET	1
++#define PFM_ARCH_SMPL_ALIGN_SIZE	0
+ 
+-typedef union {
+-	unsigned int val;
+-	struct {
+-		unsigned int notify_user:1;	/* notify user program of overflow */
+-		unsigned int reset_ovfl_pmds:1;	/* reset overflowed PMDs */
+-		unsigned int block_task:1;	/* block monitored task on kernel exit */
+-		unsigned int mask_monitoring:1; /* mask monitors via PMCx.plm */
+-		unsigned int reserved:28;	/* for future use */
+-	} bits;
+-} pfm_ovfl_ctrl_t;
+-
+-typedef struct {
+-	unsigned char	ovfl_pmd;			/* index of overflowed PMD  */
+-	unsigned char   ovfl_notify;			/* =1 if monitor requested overflow notification */
+-	unsigned short  active_set;			/* event set active at the time of the overflow */
+-	pfm_ovfl_ctrl_t ovfl_ctrl;			/* return: perfmon controls to set by handler */
+-
+-	unsigned long   pmd_last_reset;			/* last reset value of of the PMD */
+-	unsigned long	smpl_pmds[4];			/* bitmask of other PMD of interest on overflow */
+-	unsigned long   smpl_pmds_values[PMU_MAX_PMDS]; /* values for the other PMDs of interest */
+-	unsigned long   pmd_value;			/* current 64-bit value of the PMD */
+-	unsigned long	pmd_eventid;			/* eventid associated with PMD */
+-} pfm_ovfl_arg_t;
+-
+-
+-typedef struct {
+-	char		*fmt_name;
+-	pfm_uuid_t	fmt_uuid;
+-	size_t		fmt_arg_size;
+-	unsigned long	fmt_flags;
+-
+-	int		(*fmt_validate)(struct task_struct *task, unsigned int flags, int cpu, void *arg);
+-	int		(*fmt_getsize)(struct task_struct *task, unsigned int flags, int cpu, void *arg, unsigned long *size);
+-	int 		(*fmt_init)(struct task_struct *task, void *buf, unsigned int flags, int cpu, void *arg);
+-	int		(*fmt_handler)(struct task_struct *task, void *buf, pfm_ovfl_arg_t *arg, struct pt_regs *regs, unsigned long stamp);
+-	int		(*fmt_restart)(struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs);
+-	int		(*fmt_restart_active)(struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs);
+-	int		(*fmt_exit)(struct task_struct *task, void *buf, struct pt_regs *regs);
+ 
+-	struct list_head fmt_list;
+-} pfm_buffer_fmt_t;
++static inline void pfm_release_dbregs(struct task_struct *task)
++{
++	if (task->thread.flags & IA64_THREAD_DBG_VALID)
++		__pfm_release_dbregs(task);
++}
+ 
+-extern int pfm_register_buffer_fmt(pfm_buffer_fmt_t *fmt);
+-extern int pfm_unregister_buffer_fmt(pfm_uuid_t uuid);
++#define pfm_use_dbregs(_t)     __pfm_use_dbregs(_t)
+ 
+-/*
+- * perfmon interface exported to modules
+- */
+-extern int pfm_mod_read_pmds(struct task_struct *, void *req, unsigned int nreq, struct pt_regs *regs);
+-extern int pfm_mod_write_pmcs(struct task_struct *, void *req, unsigned int nreq, struct pt_regs *regs);
+-extern int pfm_mod_write_ibrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs);
+-extern int pfm_mod_write_dbrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs);
+-
+-/*
+- * describe the content of the local_cpu_date->pfm_syst_info field
+- */
+-#define PFM_CPUINFO_SYST_WIDE	0x1	/* if set a system wide session exists */
+-#define PFM_CPUINFO_DCR_PP	0x2	/* if set the system wide session has started */
+-#define PFM_CPUINFO_EXCL_IDLE	0x4	/* the system wide session excludes the idle task */
+-
+-/*
+- * sysctl control structure. visible to sampling formats
+- */
+-typedef struct {
+-	int	debug;		/* turn on/off debugging via syslog */
+-	int	debug_ovfl;	/* turn on/off debug printk in overflow handler */
+-	int	fastctxsw;	/* turn on/off fast (unsecure) ctxsw */
+-	int	expert_mode;	/* turn on/off value checking */
+-} pfm_sysctl_t;
+-extern pfm_sysctl_t pfm_sysctl;
+ 
++struct pfm_arch_pmu_info {
++	unsigned long mask_pmcs[PFM_PMC_BV]; /* PMC to modify when masking monitoring */
++};
+ 
++DECLARE_PER_CPU(u32, pfm_syst_info);
+ #endif /* __KERNEL__ */
+-
+-#endif /* _ASM_IA64_PERFMON_H */
++#endif /* _ASM_IA64_PERFMON_H_ */
+--- /dev/null
++++ b/include/asm-ia64/perfmon_compat.h
+@@ -0,0 +1,168 @@
++/*
++ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This header file contains perfmon interface definition
++ * that are now obsolete and should be dropped in favor
++ * of their equivalent functions as explained below.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++
++#ifndef _ASM_IA64_PERFMON_COMPAT_H_
++#define _ASM_IA64_PERFMON_COMPAT_H_
++
++/*
++ * custom sampling buffer identifier type
++ */
++typedef __u8 pfm_uuid_t[16];
++
++/*
++ * obsolete perfmon commands. Supported only on IA-64 for
++ * backward compatiblity reasons with perfmon v2.0.
++ */
++#define PFM_WRITE_PMCS		0x01 /* use pfm_write_pmcs */
++#define PFM_WRITE_PMDS		0x02 /* use pfm_write_pmds */
++#define PFM_READ_PMDS		0x03 /* use pfm_read_pmds */
++#define PFM_STOP		0x04 /* use pfm_stop */
++#define PFM_START		0x05 /* use pfm_start */
++#define PFM_ENABLE		0x06 /* obsolete */
++#define PFM_DISABLE		0x07 /* obsolete */
++#define PFM_CREATE_CONTEXT	0x08 /* use pfm_create_context */
++#define PFM_DESTROY_CONTEXT	0x09 /* use close() */
++#define PFM_RESTART		0x0a /* use pfm_restart */
++#define PFM_PROTECT_CONTEXT	0x0b /* obsolete */
++#define PFM_GET_FEATURES	0x0c /* use /proc/sys/perfmon */
++#define PFM_DEBUG		0x0d /* /proc/sys/kernel/perfmon/debug */
++#define PFM_UNPROTECT_CONTEXT	0x0e /* obsolete */
++#define PFM_GET_PMC_RESET_VAL	0x0f /* use /proc/perfmon_map */
++#define PFM_LOAD_CONTEXT	0x10 /* use pfm_load_context */
++#define PFM_UNLOAD_CONTEXT	0x11 /* use pfm_unload_context */
++
++/*
++ * PMU model specific commands (may not be supported on all PMU models)
++ */
++#define PFM_WRITE_IBRS		0x20 /* obsolete: use PFM_WRITE_PMCS[256-263] */
++#define PFM_WRITE_DBRS		0x21 /* obsolete: use PFM_WRITE_PMCS[264-271] */
++
++/*
++ * argument to PFM_CREATE_CONTEXT
++ */
++struct pfarg_context {
++	pfm_uuid_t     ctx_smpl_buf_id;	 /* buffer format to use */
++	unsigned long  ctx_flags;	 /* noblock/block */
++	unsigned int   ctx_reserved1;	 /* for future use */
++	int	       ctx_fd;		 /* return: fildesc */
++	void	       *ctx_smpl_vaddr;	 /* return: vaddr of buffer */
++	unsigned long  ctx_reserved3[11];/* for future use */
++};
++
++/*
++ * argument structure for PFM_WRITE_PMCS/PFM_WRITE_PMDS/PFM_WRITE_PMDS
++ */
++struct pfarg_reg {
++	unsigned int	reg_num;	   /* which register */
++	unsigned short	reg_set;	   /* event set for this register */
++	unsigned short	reg_reserved1;	   /* for future use */
++
++	unsigned long	reg_value;	   /* initial pmc/pmd value */
++	unsigned long	reg_flags;	   /* input: pmc/pmd flags, return: reg error */
++
++	unsigned long	reg_long_reset;	   /* reset after buffer overflow notification */
++	unsigned long	reg_short_reset;   /* reset after counter overflow */
++
++	unsigned long	reg_reset_pmds[4]; /* which other counters to reset on overflow */
++	unsigned long	reg_random_seed;   /* seed for randomization */
++	unsigned long	reg_random_mask;   /* random range limit */
++	unsigned long   reg_last_reset_val;/* return: PMD last reset value */
++
++	unsigned long	reg_smpl_pmds[4];  /* pmds to be saved on overflow */
++	unsigned long	reg_smpl_eventid;  /* opaque sampling event id */
++	unsigned long   reg_ovfl_switch_cnt;/* #overflows to switch */
++
++	unsigned long   reg_reserved2[2];   /* for future use */
++};
++
++/*
++ * argument to PFM_WRITE_IBRS/PFM_WRITE_DBRS
++ */
++struct pfarg_dbreg {
++	unsigned int	dbreg_num;		/* which debug register */
++	unsigned short	dbreg_set;		/* event set */
++	unsigned short	dbreg_reserved1;	/* for future use */
++	unsigned long	dbreg_value;		/* value for debug register */
++	unsigned long	dbreg_flags;		/* return: dbreg error */
++	unsigned long	dbreg_reserved2[1];	/* for future use */
++};
++
++/*
++ * argument to PFM_GET_FEATURES
++ */
++struct pfarg_features {
++	unsigned int	ft_version;	/* major [16-31], minor [0-15] */
++	unsigned int	ft_reserved;	/* reserved for future use */
++	unsigned long	reserved[4];	/* for future use */
++};
++
++typedef struct {
++	int		msg_type;		/* generic message header */
++	int		msg_ctx_fd;		/* generic message header */
++	unsigned long	msg_ovfl_pmds[4];	/* which PMDs overflowed */
++	unsigned short  msg_active_set;		/* active set at the time of overflow */
++	unsigned short  msg_reserved1;		/* for future use */
++	unsigned int    msg_reserved2;		/* for future use */
++	unsigned long	msg_tstamp;		/* for perf tuning/debug */
++} pfm_ovfl_msg_t;
++
++typedef struct {
++	int		msg_type;		/* generic message header */
++	int		msg_ctx_fd;		/* generic message header */
++	unsigned long	msg_tstamp;		/* for perf tuning */
++} pfm_end_msg_t;
++
++typedef struct {
++	int		msg_type;		/* type of the message */
++	int		msg_ctx_fd;		/* unique identifier for the context */
++	unsigned long	msg_tstamp;		/* for perf tuning */
++} pfm_gen_msg_t;
++
++typedef union {
++	int type;
++	pfm_ovfl_msg_t	pfm_ovfl_msg;
++	pfm_end_msg_t	pfm_end_msg;
++	pfm_gen_msg_t	pfm_gen_msg;
++} pfm_msg_t;
++
++/*
++ * PMD/PMC return flags in case of error (ignored on input)
++ *
++ * reg_flags layout:
++ * bit 00-15 : generic flags
++ * bits[16-23] : arch-specific flags (see asm/perfmon.h)
++ * bit 24-31 : error codes
++ *
++ * Those flags are used on output and must be checked in case EINVAL is
++ * returned by a command accepting a vector of values and each has a flag
++ * field, such as pfarg_reg or pfarg_reg
++ */
++#define PFM_REG_RETFL_NOTAVAIL	(1<<31) /* not implemented or unaccessible */
++#define PFM_REG_RETFL_EINVAL	(1<<30) /* entry is invalid */
++#define PFM_REG_RETFL_MASK	(PFM_REG_RETFL_NOTAVAIL|\
++				 PFM_REG_RETFL_EINVAL)
++
++#define PFM_REG_HAS_ERROR(flag)	(((flag) & PFM_REG_RETFL_MASK) != 0)
++
++
++#endif /* _ASM_IA64_PERFMON_COMPAT_H_ */
+--- /dev/null
++++ b/include/asm-ia64/perfmon_const.h
+@@ -0,0 +1,52 @@
++/*
++ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file contains ia64 specific definitions for the perfmon
++ * interface.
++ *
++ * This file MUST never be included directly. Use linux/perfmon.h.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#ifndef _ASM_IA64_PERFMON_CONST_H_
++#define _ASM_IA64_PERFMON_CONST_H_
++
++#define PFM_ARCH_MAX_PMCS	(256+64)
++#define PFM_ARCH_MAX_PMDS	(256+64)
++
++#define PFM_ARCH_PMD_STK_ARG	8
++#define PFM_ARCH_PMC_STK_ARG	8
++
++/*
++ * Itanium specific context flags
++ *
++ * bits[00-15]: generic flags (see asm/perfmon.h)
++ * bits[16-31]: arch-specific flags
++ */
++#define PFM_ITA_FL_INSECURE 0x10000 /* clear psr.sp on non system, non self-monitoring */
++
++/*
++ * Itanium specific public event set flags (set_flags)
++ *
++ * event set flags layout:
++ * bits[00-15] : generic flags
++ * bits[16-31] : arch-specific flags
++ */
++#define PFM_ITA_SETFL_EXCL_INTR	0x10000	 /* exclude interrupt execution */
++#define PFM_ITA_SETFL_INTR_ONLY	0x20000	 /* include only interrupt execution */
++#define PFM_ITA_SETFL_IDLE_EXCL 0x40000  /* stop monitoring in idle loop */
++
++#endif /* _ASM_IA64_PERFMON_CONST_H_ */
+--- a/include/asm-ia64/perfmon_default_smpl.h
++++ b/include/asm-ia64/perfmon_default_smpl.h
+@@ -1,83 +1,106 @@
+ /*
+- * Copyright (C) 2002-2003 Hewlett-Packard Co
+- *               Stephane Eranian <eranian@hpl.hp.com>
++ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+  *
+- * This file implements the default sampling buffer format
+- * for Linux/ia64 perfmon subsystem.
+- */
+-#ifndef __PERFMON_DEFAULT_SMPL_H__
+-#define __PERFMON_DEFAULT_SMPL_H__ 1
++ * This file implements the old default sampling buffer format
++ * for the perfmon2 subsystem. For IA-64 only.
++ *
++ * It requires the use of the perfmon_compat.h header. It is recommended
++ * that applications be ported to the new format instead.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#ifndef __ASM_IA64_PERFMON_DEFAULT_SMPL_H__
++#define __ASM_IA64_PERFMON_DEFAULT_SMPL_H__ 1
++
++#ifndef __ia64__
++#error "this file must be used for compatibility reasons only on IA-64"
++#endif
+ 
+ #define PFM_DEFAULT_SMPL_UUID { \
+-		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82, 0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
++		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82,\
++		0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
+ 
+ /*
+  * format specific parameters (passed at context creation)
+  */
+-typedef struct {
++struct pfm_default_smpl_arg {
+ 	unsigned long buf_size;		/* size of the buffer in bytes */
+ 	unsigned int  flags;		/* buffer specific flags */
+ 	unsigned int  res1;		/* for future use */
+ 	unsigned long reserved[2];	/* for future use */
+-} pfm_default_smpl_arg_t;
++};
+ 
+ /*
+  * combined context+format specific structure. Can be passed
+- * to PFM_CONTEXT_CREATE
++ * to PFM_CONTEXT_CREATE (not PFM_CONTEXT_CREATE2)
+  */
+-typedef struct {
+-	pfarg_context_t		ctx_arg;
+-	pfm_default_smpl_arg_t	buf_arg;
+-} pfm_default_smpl_ctx_arg_t;
++struct pfm_default_smpl_ctx_arg {
++	struct pfarg_context		ctx_arg;
++	struct pfm_default_smpl_arg	buf_arg;
++};
+ 
+ /*
+  * This header is at the beginning of the sampling buffer returned to the user.
+  * It is directly followed by the first record.
+  */
+-typedef struct {
+-	unsigned long	hdr_count;		/* how many valid entries */
+-	unsigned long	hdr_cur_offs;		/* current offset from top of buffer */
+-	unsigned long	hdr_reserved2;		/* reserved for future use */
+-
+-	unsigned long	hdr_overflows;		/* how many times the buffer overflowed */
+-	unsigned long   hdr_buf_size;		/* how many bytes in the buffer */
+-
+-	unsigned int	hdr_version;		/* contains perfmon version (smpl format diffs) */
+-	unsigned int	hdr_reserved1;		/* for future use */
+-	unsigned long	hdr_reserved[10];	/* for future use */
+-} pfm_default_smpl_hdr_t;
++struct pfm_default_smpl_hdr {
++	u64	hdr_count;	/* how many valid entries */
++	u64	hdr_cur_offs;	/* current offset from top of buffer */
++	u64	dr_reserved2;	/* reserved for future use */
++
++	u64	hdr_overflows;	/* how many times the buffer overflowed */
++	u64	hdr_buf_size;	/* how many bytes in the buffer */
++
++	u32	hdr_version;	/* smpl format version*/
++	u32	hdr_reserved1;		/* for future use */
++	u64	hdr_reserved[10];	/* for future use */
++};
+ 
+ /*
+  * Entry header in the sampling buffer.  The header is directly followed
+- * with the values of the PMD registers of interest saved in increasing 
+- * index order: PMD4, PMD5, and so on. How many PMDs are present depends 
++ * with the values of the PMD registers of interest saved in increasing
++ * index order: PMD4, PMD5, and so on. How many PMDs are present depends
+  * on how the session was programmed.
+  *
+  * In the case where multiple counters overflow at the same time, multiple
+  * entries are written consecutively.
+  *
+- * last_reset_value member indicates the initial value of the overflowed PMD. 
++ * last_reset_value member indicates the initial value of the overflowed PMD.
+  */
+-typedef struct {
+-        int             pid;                    /* thread id (for NPTL, this is gettid()) */
+-        unsigned char   reserved1[3];           /* reserved for future use */
+-        unsigned char   ovfl_pmd;               /* index of overflowed PMD */
+-
+-        unsigned long   last_reset_val;         /* initial value of overflowed PMD */
+-        unsigned long   ip;                     /* where did the overflow interrupt happened  */
+-        unsigned long   tstamp;                 /* ar.itc when entering perfmon intr. handler */
+-
+-        unsigned short  cpu;                    /* cpu on which the overfow occured */
+-        unsigned short  set;                    /* event set active when overflow ocurred   */
+-        int    		tgid;              	/* thread group id (for NPTL, this is getpid()) */
+-} pfm_default_smpl_entry_t;
+-
+-#define PFM_DEFAULT_MAX_PMDS		64 /* how many pmds supported by data structures (sizeof(unsigned long) */
+-#define PFM_DEFAULT_MAX_ENTRY_SIZE	(sizeof(pfm_default_smpl_entry_t)+(sizeof(unsigned long)*PFM_DEFAULT_MAX_PMDS))
+-#define PFM_DEFAULT_SMPL_MIN_BUF_SIZE	(sizeof(pfm_default_smpl_hdr_t)+PFM_DEFAULT_MAX_ENTRY_SIZE)
++struct pfm_default_smpl_entry {
++	pid_t	pid;		/* thread id (for NPTL, this is gettid()) */
++	uint8_t	reserved1[3];	/* for future use */
++	uint8_t	ovfl_pmd;	/* overflow pmd for this sample */
++	u64	last_reset_val;	/* initial value of overflowed PMD */
++	unsigned long ip;	/* where did the overflow interrupt happened */
++	u64	tstamp; 	/* overflow timetamp */
++	u16	cpu;  		/* cpu on which the overfow occured */
++	u16	set;  		/* event set active when overflow ocurred   */
++	pid_t	tgid;		/* thread group id (for NPTL, this is getpid()) */
++};
++
++#define PFM_DEFAULT_MAX_PMDS		64 /* #pmds supported  */
++#define PFM_DEFAULT_MAX_ENTRY_SIZE	(sizeof(struct pfm_default_smpl_entry)+\
++					 (sizeof(u64)*PFM_DEFAULT_MAX_PMDS))
++#define PFM_DEFAULT_SMPL_MIN_BUF_SIZE	(sizeof(struct pfm_default_smpl_hdr)+\
++					 PFM_DEFAULT_MAX_ENTRY_SIZE)
+ 
+ #define PFM_DEFAULT_SMPL_VERSION_MAJ	2U
+-#define PFM_DEFAULT_SMPL_VERSION_MIN	0U
+-#define PFM_DEFAULT_SMPL_VERSION	(((PFM_DEFAULT_SMPL_VERSION_MAJ&0xffff)<<16)|(PFM_DEFAULT_SMPL_VERSION_MIN & 0xffff))
++#define PFM_DEFAULT_SMPL_VERSION_MIN 1U
++#define PFM_DEFAULT_SMPL_VERSION (((PFM_DEFAULT_SMPL_VERSION_MAJ&0xffff)<<16)|\
++				    (PFM_DEFAULT_SMPL_VERSION_MIN & 0xffff))
+ 
+-#endif /* __PERFMON_DEFAULT_SMPL_H__ */
++#endif /* __ASM_IA64_PERFMON_DEFAULT_SMPL_H__ */
+--- a/include/asm-ia64/processor.h
++++ b/include/asm-ia64/processor.h
+@@ -41,7 +41,6 @@
+ 
+ #define IA64_THREAD_FPH_VALID	(__IA64_UL(1) << 0)	/* floating-point high state valid? */
+ #define IA64_THREAD_DBG_VALID	(__IA64_UL(1) << 1)	/* debug registers valid? */
+-#define IA64_THREAD_PM_VALID	(__IA64_UL(1) << 2)	/* performance registers valid? */
+ #define IA64_THREAD_UAC_NOPRINT	(__IA64_UL(1) << 3)	/* don't log unaligned accesses */
+ #define IA64_THREAD_UAC_SIGBUS	(__IA64_UL(1) << 4)	/* generate SIGBUS on unaligned acc. */
+ #define IA64_THREAD_MIGRATION	(__IA64_UL(1) << 5)	/* require migration
+@@ -257,14 +256,6 @@ struct thread_struct {
+ #else
+ # define INIT_THREAD_IA32
+ #endif /* CONFIG_IA32_SUPPORT */
+-#ifdef CONFIG_PERFMON
+-	void *pfm_context;		     /* pointer to detailed PMU context */
+-	unsigned long pfm_needs_checking;    /* when >0, pending perfmon work on kernel exit */
+-# define INIT_THREAD_PM		.pfm_context =		NULL,     \
+-				.pfm_needs_checking =	0UL,
+-#else
+-# define INIT_THREAD_PM
+-#endif
+ 	__u64 dbr[IA64_NUM_DBG_REGS];
+ 	__u64 ibr[IA64_NUM_DBG_REGS];
+ 	struct ia64_fpreg fph[96];	/* saved/loaded on demand */
+@@ -279,7 +270,6 @@ struct thread_struct {
+ 	.task_size =	DEFAULT_TASK_SIZE,			\
+ 	.last_fph_cpu =  -1,					\
+ 	INIT_THREAD_IA32					\
+-	INIT_THREAD_PM						\
+ 	.dbr =		{0, },					\
+ 	.ibr =		{0, },					\
+ 	.fph =		{{{{0}}}, }				\
+--- a/include/asm-ia64/system.h
++++ b/include/asm-ia64/system.h
+@@ -210,22 +210,18 @@ struct task_struct;
+ extern void ia64_save_extra (struct task_struct *task);
+ extern void ia64_load_extra (struct task_struct *task);
+ 
+-#ifdef CONFIG_PERFMON
+-  DECLARE_PER_CPU(unsigned long, pfm_syst_info);
+-# define PERFMON_IS_SYSWIDE() (__get_cpu_var(pfm_syst_info) & 0x1)
+-#else
+-# define PERFMON_IS_SYSWIDE() (0)
+-#endif
+-
+-#define IA64_HAS_EXTRA_STATE(t)							\
+-	((t)->thread.flags & (IA64_THREAD_DBG_VALID|IA64_THREAD_PM_VALID)	\
+-	 || IS_IA32_PROCESS(task_pt_regs(t)) || PERFMON_IS_SYSWIDE())
++#define IA64_HAS_EXTRA_STATE(t)						\
++	(((t)->thread.flags & IA64_THREAD_DBG_VALID)			\
++	 || IS_IA32_PROCESS(task_pt_regs(t)))
+ 
+ #define __switch_to(prev,next,last) do {							 \
+ 	if (IA64_HAS_EXTRA_STATE(prev))								 \
+ 		ia64_save_extra(prev);								 \
+ 	if (IA64_HAS_EXTRA_STATE(next))								 \
+ 		ia64_load_extra(next);								 \
++	if (test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW)					 \
++	    || test_tsk_thread_flag(next, TIF_PERFMON_CTXSW))					 \
++		pfm_ctxsw(prev, next);								 \
+ 	ia64_psr(task_pt_regs(next))->dfh = !ia64_is_local_fpu_owner(next);			 \
+ 	(last) = ia64_switch_to((next));							 \
+ } while (0)
+--- a/include/asm-ia64/thread_info.h
++++ b/include/asm-ia64/thread_info.h
+@@ -91,24 +91,27 @@ struct thread_info {
+ #define TIF_MCA_INIT		18	/* this task is processing MCA or INIT */
+ #define TIF_DB_DISABLED		19	/* debug trap disabled for fsyscall */
+ #define TIF_FREEZE		20	/* is freezing for suspend */
++#define TIF_PERFMON_CTXSW	21	/* perfmon needs ctxsw calls */
+ 
+ #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
+ #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
+ #define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
+ #define _TIF_SYSCALL_TRACEAUDIT	(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SINGLESTEP)
+ #define _TIF_RESTORE_SIGMASK	(1 << TIF_RESTORE_SIGMASK)
+-#define _TIF_PERFMON_WORK	(1 << TIF_PERFMON_WORK)
+ #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
+ #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
+ #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
+ #define _TIF_MCA_INIT		(1 << TIF_MCA_INIT)
+ #define _TIF_DB_DISABLED	(1 << TIF_DB_DISABLED)
+ #define _TIF_FREEZE		(1 << TIF_FREEZE)
++#define _TIF_PERFMON_CTXSW	(1 << TIF_PERFMON_CTXSW)
++#define _TIF_PERFMON_WORK	(1 << TIF_PERFMON_WORK)
+ 
+ /* "work to do on user-return" bits */
+-#define TIF_ALLWORK_MASK	(_TIF_SIGPENDING|_TIF_PERFMON_WORK|_TIF_SYSCALL_AUDIT|\
+-				 _TIF_NEED_RESCHED| _TIF_SYSCALL_TRACE|\
+-				 _TIF_RESTORE_SIGMASK)
++#define TIF_ALLWORK_MASK	(_TIF_SIGPENDING|_TIF_NEED_RESCHED|\
++				 _TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|\
++				 _TIF_RESTORE_SIGMASK|_TIF_PERFMON_WORK)
++
+ /* like TIF_ALLWORK_BITS but sans TIF_SYSCALL_TRACE or TIF_SYSCALL_AUDIT */
+ #define TIF_WORK_MASK		(TIF_ALLWORK_MASK&~(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT))
+ 
+--- a/include/asm-ia64/unistd.h
++++ b/include/asm-ia64/unistd.h
+@@ -299,11 +299,23 @@
+ #define __NR_signalfd			1307
+ #define __NR_timerfd			1308
+ #define __NR_eventfd			1309
++#define __NR_pfm_create_context		1310
++#define __NR_pfm_write_pmcs		(__NR_pfm_create_context+1)
++#define __NR_pfm_write_pmds		(__NR_pfm_create_context+2)
++#define __NR_pfm_read_pmds		(__NR_pfm_create_context+3)
++#define __NR_pfm_load_context		(__NR_pfm_create_context+4)
++#define __NR_pfm_start			(__NR_pfm_create_context+5)
++#define __NR_pfm_stop			(__NR_pfm_create_context+6)
++#define __NR_pfm_restart		(__NR_pfm_create_context+7)
++#define __NR_pfm_create_evtsets		(__NR_pfm_create_context+8)
++#define __NR_pfm_getinfo_evtsets 	(__NR_pfm_create_context+9)
++#define __NR_pfm_delete_evtsets 	(__NR_pfm_create_context+10)
++#define __NR_pfm_unload_context		(__NR_pfm_create_context+11)
+ 
+ #ifdef __KERNEL__
+ 
+ 
+-#define NR_syscalls			286 /* length of syscall table */
++#define NR_syscalls			298 /* length of syscall table */
+ 
+ /*
+  * The following defines stop scripts/checksyscalls.sh from complaining about
+--- /dev/null
++++ b/include/asm-mips/perfmon.h
+@@ -0,0 +1,421 @@
++/*
++ * Copyright (c) 2005 Philip Mucci.
++ *
++ * Based on other versions:
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file contains mips64 specific definitions for the perfmon
++ * interface.
++ *
++ * This file MUST never be included directly. Use linux/perfmon.h.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#ifndef _ASM_MIPS64_PERFMON_H_
++#define _ASM_MIPS64_PERFMON_H_
++
++#ifdef __KERNEL__
++
++#include <asm/cacheflush.h>
++
++#define PFM_ARCH_PMD_STK_ARG	2
++#define PFM_ARCH_PMC_STK_ARG	2
++
++struct pfm_arch_pmu_info {
++	u32 pmu_style;
++};
++
++#define MIPS64_CONFIG_PMC_MASK (1 << 4)
++#define MIPS64_PMC_INT_ENABLE_MASK (1 << 4)
++#define MIPS64_PMC_CNT_ENABLE_MASK (0xf)
++#define MIPS64_PMC_EVT_MASK (0x7 << 6)
++#define MIPS64_PMC_CTR_MASK (1 << 31)
++#define MIPS64_PMD_INTERRUPT (1 << 31)
++
++/* Coprocessor register 25 contains the PMU interface. */
++/* Sel 0 is control for counter 0 */
++/* Sel 1 is count for counter 0. */
++/* Sel 2 is control for counter 1. */
++/* Sel 3 is count for counter 1. */
++
++/*
++
++31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4  3 2 1 0
++M  0--------------------------------------------------------------0 Event-- IE U S K EXL
++
++M 31 If this bit is one, another pair of Performance Control
++and Counter registers is implemented at a MTC0
++
++Event 8:5 Counter event enabled for this counter. Possible events
++are listed in Table 6-30. R/W Undefined
++
++IE 4 Counter Interrupt Enable. This bit masks bit 31 of the
++associated count register from the interrupt exception
++request output. R/W 0
++
++U 3 Count in User Mode. When this bit is set, the specified
++event is counted in User Mode. R/W Undefined
++
++S 2 Count in Supervisor Mode. When this bit is set, the
++specified event is counted in Supervisor Mode. R/W Undefined
++
++K 1 Count in Kernel Mode. When this bit is set, count the
++event in Kernel Mode when EXL and ERL both are 0. R/W Undefined
++
++EXL 0 Count when EXL. When this bit is set, count the event
++when EXL = 1 and ERL = 0. R/W Undefined
++*/
++
++static inline void pfm_arch_resend_irq(void)
++{}
++
++static inline void pfm_arch_serialize(void)
++{}
++
++
++static inline void pfm_arch_unfreeze_pmu(void)
++{}
++
++/*
++ * MIPS does not save the PMDs during pfm_arch_intr_freeze_pmu(), thus
++ * this routine needs to do it when switching sets on overflow
++ */
++static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
++					   struct pfm_event_set *set)
++{
++	pfm_save_pmds(ctx, set);
++}
++
++static inline void pfm_arch_write_pmc(struct pfm_context *ctx,
++				      unsigned int cnum, u64 value)
++{
++  /*
++   * we only write to the actual register when monitoring is
++   * active (pfm_start was issued)
++   */
++  if (ctx && (ctx->flags.started == 0))
++	  return;
++
++  switch(pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
++  case 0:
++    write_c0_perfctrl0(value);
++    break;
++  case 1:
++    write_c0_perfctrl1(value);
++    break;
++  case 2:
++    write_c0_perfctrl2(value);
++    break;
++  case 3:
++    write_c0_perfctrl3(value);
++    break;
++  default:
++    BUG();
++  }
++}
++
++static inline void pfm_arch_write_pmd(struct pfm_context *ctx,
++				      unsigned int cnum, u64 value)
++{
++  value &= pfm_pmu_conf->ovfl_mask;
++
++  switch(pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++  case 0:
++    write_c0_perfcntr0(value);
++    break;
++  case 1:
++    write_c0_perfcntr1(value);
++    break;
++  case 2:
++    write_c0_perfcntr2(value);
++    break;
++  case 3:
++    write_c0_perfcntr3(value);
++    break;
++  default:
++    BUG();
++  }
++}
++
++static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
++{
++	switch(pfm_pmu_conf->pmd_desc[cnum].hw_addr) {
++	case 0:
++		return read_c0_perfcntr0();
++		break;
++	case 1:
++		return read_c0_perfcntr1();
++		break;
++	case 2:
++		return read_c0_perfcntr2();
++		break;
++	case 3:
++		return read_c0_perfcntr3();
++		break;
++	default:
++		BUG();
++		return 0;
++	}
++}
++
++static inline u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
++{
++	switch(pfm_pmu_conf->pmc_desc[cnum].hw_addr) {
++	case 0:
++		return read_c0_perfctrl0();
++		break;
++	case 1:
++		return read_c0_perfctrl1();
++		break;
++	case 2:
++		return read_c0_perfctrl2();
++		break;
++	case 3:
++		return read_c0_perfctrl3();
++		break;
++	default:
++		BUG();
++		return 0;
++	}
++}
++
++/*
++ * For some CPUs, the upper bits of a counter must be set in order for the
++ * overflow interrupt to happen. On overflow, the counter has wrapped around,
++ * and the upper bits are cleared. This function may be used to set them back.
++ */
++static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx,
++					   unsigned int cnum)
++{
++  u64 val;
++  val = pfm_arch_read_pmd(ctx, cnum);
++  /* This masks out overflow bit 31 */
++  pfm_arch_write_pmd(ctx, cnum, val);
++}
++
++/*
++ * At certain points, perfmon needs to know if monitoring has been
++ * explicitely started/stopped by user via pfm_start/pfm_stop. The
++ * information is tracked in ctx.flags.started. However on certain
++ * architectures, it may be possible to start/stop directly from
++ * user level with a single assembly instruction bypassing
++ * the kernel. This function must be used to determine by
++ * an arch-specific mean if monitoring is actually started/stopped.
++ */
++static inline int pfm_arch_is_active(struct pfm_context *ctx)
++{
++	return ctx->flags.started;
++}
++
++static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
++					 struct pfm_context *ctx,
++					 struct pfm_event_set *set)
++{}
++
++static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
++                                         struct pfm_context *ctx,
++                                         struct pfm_event_set *set)
++{}
++
++static inline void pfm_arch_ctxswin_thread(struct task_struct *task,
++                                         struct pfm_context *ctx,
++                                         struct pfm_event_set *set)
++{}
++
++int  pfm_arch_is_monitoring_active(struct pfm_context *ctx);
++int  pfm_arch_ctxswout_thread(struct task_struct *task,
++			      struct pfm_context *ctx, struct pfm_event_set *set);
++void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
++		   struct pfm_event_set *set);
++void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
++		    struct pfm_event_set *set);
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
++char *pfm_arch_get_pmu_module_name(void);
++
++static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
++					    struct pfm_event_set *set)
++{
++	pfm_arch_stop(current, ctx, set);
++	/*
++	 * we mark monitoring as stopped to avoid
++	 * certain side effects especially in
++	 * pfm_switch_sets_from_intr() on
++	 * pfm_arch_restore_pmcs()
++	 */
++	ctx->flags.started = 0;
++}
++
++/*
++ * unfreeze PMU from pfm_do_interrupt_handler()
++ * ctx may be NULL for spurious
++ */
++static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
++{
++	if (!ctx)
++		return;
++
++	PFM_DBG_ovfl("state=%d", ctx->state);
++
++	ctx->flags.started = 1;
++
++	if (ctx->state == PFM_CTX_MASKED)
++		return;
++
++	pfm_arch_restore_pmcs(ctx, ctx->active_set);
++}
++
++static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
++{
++	return 0;
++}
++
++/*
++ * this function is called from the PMU interrupt handler ONLY.
++ * On MIPS, the PMU is frozen via arch_stop, masking would be implemented
++ * via arch-stop as well. Given that the PMU is already stopped when
++ * entering the interrupt handler, we do not need to stop it again, so
++ * this function is a nop.
++ */
++static inline void pfm_arch_mask_monitoring(struct pfm_context *ctx,
++					    struct pfm_event_set *set)
++{}
++
++/*
++ * on MIPS masking/unmasking uses the start/stop mechanism, so we simply
++ * need to start here.
++ */
++static inline void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
++					      struct pfm_event_set *set)
++{
++	pfm_arch_start(current, ctx, set);
++}
++
++static inline void pfm_arch_pmu_config_remove(void)
++{}
++
++static inline int pfm_arch_context_create(struct pfm_context *ctx,
++					  u32 ctx_flags)
++{
++	return 0;
++}
++
++static inline void pfm_arch_context_free(struct pfm_context *ctx)
++{}
++
++
++
++
++
++/*
++ * function called from pfm_setfl_sane(). Context is locked
++ * and interrupts are masked.
++ * The value of flags is the value of ctx_flags as passed by
++ * user.
++ *
++ * function must check arch-specific set flags.
++ * Return:
++ * 	1 when flags are valid
++ *      0 on error
++ */
++static inline int
++pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
++{
++	return 0;
++}
++
++static inline int pfm_arch_init(void)
++{
++	return 0;
++}
++
++static inline void pfm_arch_init_percpu(void)
++{}
++
++static inline int pfm_arch_load_context(struct pfm_context *ctx,
++					struct pfm_event_set *set,
++					struct task_struct *task)
++{
++	return 0;
++}
++
++static inline int pfm_arch_unload_context(struct pfm_context *ctx,
++					  struct task_struct *task)
++{
++	return 0;
++}
++
++static inline int pfm_arch_pmu_acquire(void)
++{
++	return 0;
++}
++
++static inline void pfm_arch_pmu_release(void)
++{}
++
++#ifdef CONFIG_PERFMON_FLUSH
++/*
++ * due to cache aliasing problem on MIPS, it is necessary to flush
++ * pages out of the cache when they are modified.
++ */
++static inline void pfm_cacheflush(void *addr, unsigned int len)
++{
++	unsigned long start, end;
++
++	start = (unsigned long)addr & PAGE_MASK;
++	end = ((unsigned long)addr + len + PAGE_SIZE - 1) & PAGE_MASK;
++
++	while(start < end) {
++		flush_data_cache_page(start);
++		start += PAGE_SIZE;
++	}
++}
++#else
++static inline void pfm_cacheflush(void *addr, unsigned int len)
++{}
++#endif
++
++/*
++ * not used for mips
++ */
++static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
++					       size_t rsize, struct file *filp)
++{
++	return -EINVAL;
++}
++static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
++			     char __user *buf,
++			     int non_block,
++			     size_t size)
++{
++	return -EINVAL;
++}
++struct pfm_arch_context {
++	/* empty */
++};
++
++#define PFM_ARCH_CTX_SIZE	sizeof(struct pfm_arch_context)
++/*
++ * MIPS may need extra alignment requirements for the sampling buffer
++ */
++#ifdef CONFIG_PERFMON_SMPL_ALIGN
++#define PFM_ARCH_SMPL_ALIGN_SIZE	0x4000
++#else
++#define PFM_ARCH_SMPL_ALIGN_SIZE	0
++#endif
++
++#endif /* __KERNEL__ */
++#endif /* _ASM_MIPS64_PERFMON_H_ */
+--- /dev/null
++++ b/include/asm-mips/perfmon_const.h
+@@ -0,0 +1,30 @@
++/*
++ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file contains mips64 specific definitions for the perfmon
++ * interface.
++ *
++ * This file MUST never be included directly. Use linux/perfmon.h.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#ifndef _ASM_MIPS64_PERFMON_CONST_H_
++#define _ASM_MIPS64_PERFMON_CONST_H_
++
++#define PFM_ARCH_MAX_PMCS	(256+64) /* 256 HW 64 SW */
++#define PFM_ARCH_MAX_PMDS	(256+64) /* 256 HW 64 SW */
++
++#endif /* _ASM_MIPS64_PERFMON_CONST_H_ */
+--- a/include/asm-mips/system.h
++++ b/include/asm-mips/system.h
+@@ -67,6 +67,9 @@ do {									\
+ 	__mips_mt_fpaff_switch_to(prev);				\
+ 	if (cpu_has_dsp)						\
+ 		__save_dsp(prev);					\
++	if (test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW)		\
++	    || test_tsk_thread_flag(next, TIF_PERFMON_CTXSW))		\
++		pfm_ctxsw(prev, next);					\
+ 	(last) = resume(prev, next, task_thread_info(next));		\
+ } while (0)
+ 
+--- a/include/asm-mips/thread_info.h
++++ b/include/asm-mips/thread_info.h
+@@ -112,6 +112,7 @@ register struct thread_info *__current_t
+ #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
+ #define TIF_SYSCALL_AUDIT	3	/* syscall auditing active */
+ #define TIF_SECCOMP		4	/* secure computing */
++#define TIF_PERFMON_WORK	5	/* work for pfm_handle_work() */
+ #define TIF_RESTORE_SIGMASK	9	/* restore signal mask in do_signal() */
+ #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
+ #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
+@@ -122,6 +123,7 @@ register struct thread_info *__current_t
+ #define TIF_32BIT_REGS		22	/* also implies 16/32 fprs */
+ #define TIF_32BIT_ADDR		23	/* 32-bit address space (o32/n32) */
+ #define TIF_FPUBOUND		24	/* thread bound to FPU-full CPU set */
++#define TIF_PERFMON_CTXSW	25	/* perfmon needs ctxsw calls */
+ #define TIF_SYSCALL_TRACE	31	/* syscall trace active */
+ 
+ #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
+@@ -138,6 +140,8 @@ register struct thread_info *__current_t
+ #define _TIF_32BIT_REGS		(1<<TIF_32BIT_REGS)
+ #define _TIF_32BIT_ADDR		(1<<TIF_32BIT_ADDR)
+ #define _TIF_FPUBOUND		(1<<TIF_FPUBOUND)
++#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
++#define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
+ 
+ /* work to do on interrupt/exception return */
+ #define _TIF_WORK_MASK		(0x0000ffef & ~_TIF_SECCOMP)
+--- a/include/asm-mips/unistd.h
++++ b/include/asm-mips/unistd.h
+@@ -341,11 +341,23 @@
+ #define __NR_timerfd			(__NR_Linux + 318)
+ #define __NR_eventfd			(__NR_Linux + 319)
+ #define __NR_fallocate			(__NR_Linux + 320)
++#define __NR_pfm_create_context         (__NR_Linux + 321)
++#define __NR_pfm_write_pmcs		(__NR_pfm_create_context+1)
++#define __NR_pfm_write_pmds		(__NR_pfm_create_context+2)
++#define __NR_pfm_read_pmds		(__NR_pfm_create_context+3)
++#define __NR_pfm_load_context		(__NR_pfm_create_context+4)
++#define __NR_pfm_start			(__NR_pfm_create_context+5)
++#define __NR_pfm_stop			(__NR_pfm_create_context+6)
++#define __NR_pfm_restart		(__NR_pfm_create_context+7)
++#define __NR_pfm_create_evtsets		(__NR_pfm_create_context+8)
++#define __NR_pfm_getinfo_evtsets 	(__NR_pfm_create_context+9)
++#define __NR_pfm_delete_evtsets 	(__NR_pfm_create_context+10)
++#define __NR_pfm_unload_context		(__NR_pfm_create_context+11)
+ 
+ /*
+  * Offset of the last Linux o32 flavoured syscall
+  */
+-#define __NR_Linux_syscalls		320
++#define __NR_Linux_syscalls		332
+ 
+ #endif /* _MIPS_SIM == _MIPS_SIM_ABI32 */
+ 
+@@ -638,16 +650,28 @@
+ #define __NR_timerfd			(__NR_Linux + 277)
+ #define __NR_eventfd			(__NR_Linux + 278)
+ #define __NR_fallocate			(__NR_Linux + 279)
++#define __NR_pfm_create_context         (__NR_Linux + 280)
++#define __NR_pfm_write_pmcs		(__NR_pfm_create_context+1)
++#define __NR_pfm_write_pmds		(__NR_pfm_create_context+2)
++#define __NR_pfm_read_pmds		(__NR_pfm_create_context+3)
++#define __NR_pfm_load_context		(__NR_pfm_create_context+4)
++#define __NR_pfm_start			(__NR_pfm_create_context+5)
++#define __NR_pfm_stop			(__NR_pfm_create_context+6)
++#define __NR_pfm_restart		(__NR_pfm_create_context+7)
++#define __NR_pfm_create_evtsets		(__NR_pfm_create_context+8)
++#define __NR_pfm_getinfo_evtsets 	(__NR_pfm_create_context+9)
++#define __NR_pfm_delete_evtsets 	(__NR_pfm_create_context+10)
++#define __NR_pfm_unload_context		(__NR_pfm_create_context+11)
+ 
+ /*
+  * Offset of the last Linux 64-bit flavoured syscall
+  */
+-#define __NR_Linux_syscalls		279
++#define __NR_Linux_syscalls		291
+ 
+ #endif /* _MIPS_SIM == _MIPS_SIM_ABI64 */
+ 
+ #define __NR_64_Linux			5000
+-#define __NR_64_Linux_syscalls		279
++#define __NR_64_Linux_syscalls		291
+ 
+ #if _MIPS_SIM == _MIPS_SIM_NABI32
+ 
+@@ -939,16 +963,28 @@
+ #define __NR_timerfd			(__NR_Linux + 281)
+ #define __NR_eventfd			(__NR_Linux + 282)
+ #define __NR_fallocate			(__NR_Linux + 283)
++#define __NR_pfm_create_context         (__NR_Linux + 284)
++#define __NR_pfm_write_pmcs		(__NR_pfm_create_context+1)
++#define __NR_pfm_write_pmds		(__NR_pfm_create_context+2)
++#define __NR_pfm_read_pmds		(__NR_pfm_create_context+3)
++#define __NR_pfm_load_context		(__NR_pfm_create_context+4)
++#define __NR_pfm_start			(__NR_pfm_create_context+5)
++#define __NR_pfm_stop			(__NR_pfm_create_context+6)
++#define __NR_pfm_restart		(__NR_pfm_create_context+7)
++#define __NR_pfm_create_evtsets		(__NR_pfm_create_context+8)
++#define __NR_pfm_getinfo_evtsets 	(__NR_pfm_create_context+9)
++#define __NR_pfm_delete_evtsets 	(__NR_pfm_create_context+10)
++#define __NR_pfm_unload_context		(__NR_pfm_create_context+11)
+ 
+ /*
+  * Offset of the last N32 flavoured syscall
+  */
+-#define __NR_Linux_syscalls		283
++#define __NR_Linux_syscalls		295
+ 
+ #endif /* _MIPS_SIM == _MIPS_SIM_NABI32 */
+ 
+ #define __NR_N32_Linux			6000
+-#define __NR_N32_Linux_syscalls		283
++#define __NR_N32_Linux_syscalls		295
+ 
+ #ifdef __KERNEL__
+ 
+--- a/include/asm-powerpc/cell-pmu.h
++++ b/include/asm-powerpc/cell-pmu.h
+@@ -61,6 +61,11 @@
+ 
+ /* Macros for the pm_status register. */
+ #define CBE_PM_CTR_OVERFLOW_INTR(ctr)      (1 << (31 - ((ctr) & 7)))
++#define CBE_PM_OVERFLOW_CTRS(pm_status)    (((pm_status) >> 24) & 0xff)
++#define CBE_PM_ALL_OVERFLOW_INTR           0xff000000
++#define CBE_PM_INTERVAL_INTR               0x00800000
++#define CBE_PM_TRACE_BUFFER_FULL_INTR      0x00400000
++#define CBE_PM_TRACE_BUFFER_UNDERFLOW_INTR 0x00200000
+ 
+ enum pm_reg_name {
+ 	group_control,
+--- a/include/asm-powerpc/cell-regs.h
++++ b/include/asm-powerpc/cell-regs.h
+@@ -117,8 +117,9 @@ struct cbe_pmd_regs {
+ 	u8	pad_0x0c1c_0x0c20 [4];				/* 0x0c1c */
+ #define CBE_PMD_FIR_MODE_M8		0x00800
+ 	u64	fir_enable_mask;				/* 0x0c20 */
+-
+-	u8	pad_0x0c28_0x0ca8 [0x0ca8 - 0x0c28];		/* 0x0c28 */
++	u8	pad_0x0c28_0x0c98 [0x0c98 - 0x0c28];		/* 0x0c28 */
++	u64	on_ramp_trace;					/* 0x0c98 */
++	u64	pad_0x0ca0;					/* 0x0ca0 */
+ 	u64	ras_esc_0;					/* 0x0ca8 */
+ 	u8	pad_0x0cb0_0x1000 [0x1000 - 0x0cb0];		/* 0x0cb0 */
+ };
+@@ -218,7 +219,11 @@ extern struct cbe_iic_regs __iomem *cbe_
+ 
+ 
+ struct cbe_mic_tm_regs {
+-	u8	pad_0x0000_0x0040[0x0040 - 0x0000];		/* 0x0000 */
++	u8	pad_0x0000_0x0010[0x0010 - 0x0000];		/* 0x0000 */
++
++	u64	MBL_debug;					/* 0x0010 */
++
++	u8	pad_0x0018_0x0040[0x0040 - 0x0018];		/* 0x0018 */
+ 
+ 	u64	mic_ctl_cnfg2;					/* 0x0040 */
+ #define CBE_MIC_ENABLE_AUX_TRC		0x8000000000000000LL
+@@ -303,6 +308,25 @@ struct cbe_mic_tm_regs {
+ extern struct cbe_mic_tm_regs __iomem *cbe_get_mic_tm_regs(struct device_node *np);
+ extern struct cbe_mic_tm_regs __iomem *cbe_get_cpu_mic_tm_regs(int cpu);
+ 
++/*
++ *
++ *  PPE Privileged MMIO Registers definition. (offset 0x500000 - 0x500fff)
++ *
++ */
++struct cbe_ppe_priv_regs {
++	u8	pad_0x0000_0x0858[0x0858 - 0x0000];		/* 0x0000 */
++
++	u64	L2_debug1;					/* 0x0858 */
++
++	u8	pad_0x0860_0x0958[0x0958 - 0x0860];		/* 0x0860 */
++
++	u64	ciu_dr1;					/* 0x0958 */
++
++	u8	pad_0x0960_0x1000[0x1000 - 0x0960];		/* 0x0960 */
++};
++
++extern struct cbe_ppe_priv_regs __iomem *cbe_get_cpu_ppe_priv_regs(int cpu);
++
+ /* some utility functions to deal with SMT */
+ extern u32 cbe_get_hw_thread_id(int cpu);
+ extern u32 cbe_cpu_to_node(int cpu);
+--- /dev/null
++++ b/include/asm-powerpc/perfmon.h
+@@ -0,0 +1,374 @@
++/*
++ * Copyright (c) 2005 David Gibson, IBM Corporation.
++ *
++ * Based on other versions:
++ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file contains powerpc specific definitions for the perfmon
++ * interface.
++ *
++ * This file MUST never be included directly. Use linux/perfmon.h.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#ifndef _ASM_POWERPC_PERFMON_H_
++#define _ASM_POWERPC_PERFMON_H_
++
++#ifdef __KERNEL__
++
++#include <asm/pmc.h>
++
++enum powerpc_pmu_type {
++	PFM_POWERPC_PMU_NONE,
++	PFM_POWERPC_PMU_604,
++	PFM_POWERPC_PMU_604e,
++	PFM_POWERPC_PMU_750,	/* XXX: Minor event set diffs between IBM and Moto. */
++	PFM_POWERPC_PMU_7400,
++	PFM_POWERPC_PMU_7450,
++	PFM_POWERPC_PMU_POWER4,
++	PFM_POWERPC_PMU_POWER5,
++	PFM_POWERPC_PMU_POWER5p,
++	PFM_POWERPC_PMU_POWER6,
++	PFM_POWERPC_PMU_CELL,
++};
++
++struct pfm_arch_pmu_info {
++	enum powerpc_pmu_type pmu_style;
++
++	void (*write_pmc)(unsigned int cnum, u64 value);
++	void (*write_pmd)(unsigned int cnum, u64 value);
++
++	u64 (*read_pmd)(unsigned int cnum);
++
++	void (*enable_counters)(struct pfm_context *ctx,
++				struct pfm_event_set *set);
++	void (*disable_counters)(struct pfm_context *ctx,
++				 struct pfm_event_set *set);
++
++	void (*irq_handler)(struct pt_regs *regs, struct pfm_context *ctx);
++	void (*get_ovfl_pmds)(struct pfm_context *ctx,
++			      struct pfm_event_set *set);
++
++	/* The following routines are optional. */
++	void (*restore_pmcs)(struct pfm_event_set *set);
++	void (*restore_pmds)(struct pfm_event_set *set);
++
++	int  (*ctxswout_thread)(struct task_struct *task,
++				struct pfm_context *ctx,
++				struct pfm_event_set *set);
++	void (*ctxswin_thread)(struct task_struct *task,
++			       struct pfm_context *ctx,
++			       struct pfm_event_set *set);
++	int  (*load_context)(struct pfm_context *ctx,
++			     struct pfm_event_set *set,
++			     struct task_struct *task);
++	int  (*unload_context)(struct pfm_context *ctx,
++			       struct task_struct *task);
++};
++
++#ifdef CONFIG_PPC32
++#define PFM_ARCH_PMD_STK_ARG	6 /* conservative value */
++#define PFM_ARCH_PMC_STK_ARG	6 /* conservative value */
++#else
++#define PFM_ARCH_PMD_STK_ARG	8 /* conservative value */
++#define PFM_ARCH_PMC_STK_ARG	8 /* conservative value */
++#endif
++
++static inline void pfm_arch_resend_irq(void)
++{}
++
++static inline void pfm_arch_serialize(void)
++{}
++
++static inline void pfm_arch_write_pmc(struct pfm_context *ctx,
++				      unsigned int cnum,
++				      u64 value)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	/*
++	 * we only write to the actual register when monitoring is
++	 * active (pfm_start was issued)
++	 */
++	if (ctx && ctx->flags.started == 0)
++		return;
++
++	BUG_ON(!arch_info->write_pmc);
++
++	arch_info->write_pmc(cnum, value);
++}
++
++static inline void pfm_arch_write_pmd(struct pfm_context *ctx,
++				      unsigned int cnum, u64 value)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	value &= pfm_pmu_conf->ovfl_mask;
++
++	BUG_ON(!arch_info->write_pmd);
++
++	arch_info->write_pmd(cnum, value);
++}
++
++static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	BUG_ON(!arch_info->read_pmd);
++
++	return arch_info->read_pmd(cnum);
++}
++
++/*
++ * For some CPUs, the upper bits of a counter must be set in order for the
++ * overflow interrupt to happen. On overflow, the counter has wrapped around,
++ * and the upper bits are cleared. This function may be used to set them back.
++ */
++static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx,
++					   unsigned int cnum)
++{
++	u64 val = pfm_arch_read_pmd(ctx, cnum);
++
++	/* This masks out overflow bit 31 */
++	pfm_arch_write_pmd(ctx, cnum, val);
++}
++
++/*
++ * At certain points, perfmon needs to know if monitoring has been
++ * explicitely started/stopped by user via pfm_start/pfm_stop. The
++ * information is tracked in flags.started. However on certain
++ * architectures, it may be possible to start/stop directly from
++ * user level with a single assembly instruction bypassing
++ * the kernel. This function must be used to determine by
++ * an arch-specific mean if monitoring is actually started/stopped.
++ */
++static inline int pfm_arch_is_active(struct pfm_context *ctx)
++{
++	return ctx->flags.started;
++}
++
++static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
++					 struct pfm_context *ctx,
++					 struct pfm_event_set *set)
++{}
++
++static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
++					struct pfm_context *ctx,
++					struct pfm_event_set *set)
++{}
++
++void pfm_arch_init_percpu(void);
++int  pfm_arch_is_monitoring_active(struct pfm_context *ctx);
++int  pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx,
++			      struct pfm_event_set *set);
++void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx,
++			     struct pfm_event_set *set);
++void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
++			  struct pfm_event_set *set);
++void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
++			   struct pfm_event_set *set);
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
++int  pfm_arch_get_ovfl_pmds(struct pfm_context *ctx,
++			    struct pfm_event_set *set);
++char *pfm_arch_get_pmu_module_name(void);
++/*
++ * called from __pfm_interrupt_handler(). ctx is not NULL.
++ * ctx is locked. PMU interrupt is masked.
++ *
++ * must stop all monitoring to ensure handler has consistent view.
++ * must collect overflowed PMDs bitmask  into povfls_pmds and
++ * npend_ovfls. If no interrupt detected then npend_ovfls
++ * must be set to zero.
++ */
++static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	pfm_arch_stop(current, ctx, set);
++}
++
++void powerpc_irq_handler(struct pt_regs *regs);
++
++/*
++ * unfreeze PMU from pfm_do_interrupt_handler()
++ * ctx may be NULL for spurious
++ */
++static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
++{
++	struct pfm_arch_pmu_info *arch_info;
++
++	if (!ctx)
++		return;
++
++	PFM_DBG_ovfl("state=%d", ctx->state);
++
++	ctx->flags.started = 1;
++
++	if (ctx->state == PFM_CTX_MASKED)
++		return;
++
++	arch_info = pfm_pmu_conf->arch_info;
++	BUG_ON(!arch_info->enable_counters);
++	arch_info->enable_counters(ctx, ctx->active_set);
++}
++
++/*
++ * PowerPC does not save the PMDs during pfm_arch_intr_freeze_pmu(), thus
++ * this routine needs to do it when switching sets on overflow
++ */
++static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
++					   struct pfm_event_set *set)
++{
++	pfm_save_pmds(ctx, set);
++}
++
++/*
++ * this function is called from the PMU interrupt handler ONLY.
++ * On PPC, the PMU is frozen via arch_stop, masking would be implemented
++ * via arch-stop as well. Given that the PMU is already stopped when
++ * entering the interrupt handler, we do not need to stop it again, so
++ * this function is a nop.
++ */
++static inline void pfm_arch_mask_monitoring(struct pfm_context *ctx,
++					    struct pfm_event_set *set)
++{}
++
++/*
++ * Simply need to start the context in order to unmask.
++ */
++static inline void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
++					      struct pfm_event_set *set)
++{
++	pfm_arch_start(current, ctx, set);
++}
++
++
++static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
++{
++	return 0;
++}
++
++static inline void pfm_arch_pmu_config_remove(void)
++{}
++
++static inline int pfm_arch_context_create(struct pfm_context *ctx,
++					  u32 ctx_flags)
++{
++	return 0;
++}
++
++static inline void pfm_arch_context_free(struct pfm_context *ctx)
++{}
++
++/* not necessary on PowerPC */
++static inline void pfm_cacheflush(void *addr, unsigned int len)
++{}
++
++/*
++ * function called from pfm_setfl_sane(). Context is locked
++ * and interrupts are masked.
++ * The value of flags is the value of ctx_flags as passed by
++ * user.
++ *
++ * function must check arch-specific set flags.
++ * Return:
++ * 	1 when flags are valid
++ *      0 on error
++ */
++static inline int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
++{
++	return 0;
++}
++
++static inline int pfm_arch_init(void)
++{
++	return 0;
++}
++
++static inline int pfm_arch_load_context(struct pfm_context *ctx,
++					struct pfm_event_set *set,
++					struct task_struct *task)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	int rc = 0;
++
++	if (arch_info->load_context)
++		rc = arch_info->load_context(ctx, set, task);
++
++	return rc;
++}
++
++static inline int pfm_arch_unload_context(struct pfm_context *ctx,
++					  struct task_struct *task)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	int rc = 0;
++
++	if (arch_info->unload_context)
++		rc = arch_info->unload_context(ctx, task);
++
++	return rc;
++}
++
++/*
++ * not applicable to powerpc
++ */
++static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
++					       size_t rsize, struct file *filp)
++{
++	return -EINVAL;
++}
++
++static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
++			     char __user *buf,
++			     int non_block,
++			     size_t size)
++{
++	return -EINVAL;
++}
++
++static inline int pfm_arch_pmu_acquire(void)
++{
++	return reserve_pmc_hardware(powerpc_irq_handler);
++}
++
++static inline void pfm_arch_pmu_release(void)
++{
++	release_pmc_hardware();
++}
++
++struct pfm_arch_context {
++	/* Cell: Most recent value of the pm_status
++	 * register read by the interrupt handler.
++	 *
++	 * Interrupt handler sets last_read_updated if it
++	 * just read and updated last_read_pm_status
++	 */
++	u32 last_read_pm_status;
++	u32 last_read_updated;
++	u64 powergs_pmc5, powergs_pmc6;
++	u64 delta_tb, delta_tb_start;
++	u64 delta_purr, delta_purr_start;
++};
++
++#define PFM_ARCH_CTX_SIZE sizeof(struct pfm_arch_context)
++/*
++ * PowerPC does not need extra alignment requirements for the sampling buffer
++ */
++#define PFM_ARCH_SMPL_ALIGN_SIZE	0
++
++
++#endif /* __KERNEL__ */
++#endif /* _ASM_POWERPC_PERFMON_H_ */
+--- /dev/null
++++ b/include/asm-powerpc/perfmon_const.h
+@@ -0,0 +1,30 @@
++/*
++ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file contains powerpc specific definitions for the perfmon
++ * interface.
++ *
++ * This file MUST never be included directly. Use linux/perfmon.h.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#ifndef _ASM_POWERPC_PERFMON_CONST_H_
++#define _ASM_POWERPC_PERFMON_CONST_H_
++
++#define PFM_ARCH_MAX_PMCS	(256+64) /* 256 HW 64 SW */
++#define PFM_ARCH_MAX_PMDS	(256+64) /* 256 HW 64 SW */
++
++#endif /* _ASM_POWERPC_PERFMON_CONST_H_ */
+--- a/include/asm-powerpc/reg.h
++++ b/include/asm-powerpc/reg.h
+@@ -684,6 +684,7 @@
+ #define PV_POWER5	0x003A
+ #define PV_POWER5p	0x003B
+ #define PV_970FX	0x003C
++#define PV_POWER6	0x003E
+ #define PV_630		0x0040
+ #define PV_630p	0x0041
+ #define PV_970MP	0x0044
+--- a/include/asm-powerpc/systbl.h
++++ b/include/asm-powerpc/systbl.h
+@@ -313,3 +313,15 @@ COMPAT_SYS_SPU(timerfd)
+ SYSCALL_SPU(eventfd)
+ COMPAT_SYS_SPU(sync_file_range2)
+ COMPAT_SYS(fallocate)
++SYSCALL(pfm_create_context)
++SYSCALL(pfm_write_pmcs)
++SYSCALL(pfm_write_pmds)
++SYSCALL(pfm_read_pmds)
++SYSCALL(pfm_load_context)
++SYSCALL(pfm_start)
++SYSCALL(pfm_stop)
++SYSCALL(pfm_restart)
++SYSCALL(pfm_create_evtsets)
++SYSCALL(pfm_getinfo_evtsets)
++SYSCALL(pfm_delete_evtsets)
++SYSCALL(pfm_unload_context)
+--- a/include/asm-powerpc/thread_info.h
++++ b/include/asm-powerpc/thread_info.h
+@@ -142,10 +142,12 @@ static inline struct thread_info *curren
+ #define _TIF_FREEZE		(1<<TIF_FREEZE)
+ #define _TIF_RUNLATCH		(1<<TIF_RUNLATCH)
+ #define _TIF_ABI_PENDING	(1<<TIF_ABI_PENDING)
++#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
++#define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
+ #define _TIF_SYSCALL_T_OR_A	(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SECCOMP)
+ 
+ #define _TIF_USER_WORK_MASK	( _TIF_SIGPENDING | \
+-				 _TIF_NEED_RESCHED | _TIF_RESTORE_SIGMASK)
++				 _TIF_NEED_RESCHED | _TIF_RESTORE_SIGMASK| _TIF_PERFMON_WORK)
+ #define _TIF_PERSYSCALL_MASK	(_TIF_RESTOREALL|_TIF_NOERROR)
+ 
+ /* Bits in local_flags */
+--- a/include/asm-powerpc/unistd.h
++++ b/include/asm-powerpc/unistd.h
+@@ -332,10 +332,22 @@
+ #define __NR_eventfd		307
+ #define __NR_sync_file_range2	308
+ #define __NR_fallocate		309
++#define __NR_pfm_create_context	310
++#define __NR_pfm_write_pmcs	(__NR_pfm_create_context+1)
++#define __NR_pfm_write_pmds	(__NR_pfm_create_context+2)
++#define __NR_pfm_read_pmds	(__NR_pfm_create_context+3)
++#define __NR_pfm_load_context	(__NR_pfm_create_context+4)
++#define __NR_pfm_start		(__NR_pfm_create_context+5)
++#define __NR_pfm_stop		(__NR_pfm_create_context+6)
++#define __NR_pfm_restart	(__NR_pfm_create_context+7)
++#define __NR_pfm_create_evtsets	(__NR_pfm_create_context+8)
++#define __NR_pfm_getinfo_evtsets (__NR_pfm_create_context+9)
++#define __NR_pfm_delete_evtsets (__NR_pfm_create_context+10)
++#define __NR_pfm_unload_context	(__NR_pfm_create_context+11)
+ 
+ #ifdef __KERNEL__
+ 
+-#define __NR_syscalls		310
++#define __NR_syscalls		322
+ 
+ #define __NR__exit __NR_exit
+ #define NR_syscalls	__NR_syscalls
+--- a/include/asm-sparc/Kbuild
++++ b/include/asm-sparc/Kbuild
+@@ -11,5 +11,4 @@ header-y += traps.h
+ header-y += vfc_ioctls.h
+ 
+ unifdef-y += fbio.h
+-unifdef-y += perfctr.h
+ unifdef-y += psr.h
+--- a/include/asm-sparc/perfctr.h
++++ /dev/null
+@@ -1,173 +0,0 @@
+-/*----------------------------------------
+-  PERFORMANCE INSTRUMENTATION  
+-  Guillaume Thouvenin           08/10/98
+-  David S. Miller               10/06/98
+-  ---------------------------------------*/
+-#ifndef PERF_COUNTER_API
+-#define PERF_COUNTER_API
+-
+-/* sys_perfctr() interface.  First arg is operation code
+- * from enumeration below.  The meaning of further arguments
+- * are determined by the operation code.
+- *
+- * int sys_perfctr(int opcode, unsigned long arg0,
+- *                 unsigned long arg1, unsigned long arg2)
+- *
+- * Pointers which are passed by the user are pointers to 64-bit
+- * integers.
+- *
+- * Once enabled, performance counter state is retained until the
+- * process either exits or performs an exec.  That is, performance
+- * counters remain enabled for fork/clone children.
+- */
+-enum perfctr_opcode {
+-	/* Enable UltraSparc performance counters, ARG0 is pointer
+-	 * to 64-bit accumulator for D0 counter in PIC, ARG1 is pointer
+-	 * to 64-bit accumulator for D1 counter.  ARG2 is a pointer to
+-	 * the initial PCR register value to use.
+-	 */
+-	PERFCTR_ON,
+-
+-	/* Disable UltraSparc performance counters.  The PCR is written
+-	 * with zero and the user counter accumulator pointers and
+-	 * working PCR register value are forgotten.
+-	 */
+-	PERFCTR_OFF,
+-
+-	/* Add current D0 and D1 PIC values into user pointers given
+-	 * in PERFCTR_ON operation.  The PIC is cleared before returning.
+-	 */
+-	PERFCTR_READ,
+-
+-	/* Clear the PIC register. */
+-	PERFCTR_CLRPIC,
+-
+-	/* Begin using a new PCR value, the pointer to which is passed
+-	 * in ARG0.  The PIC is also cleared after the new PCR value is
+-	 * written.
+-	 */
+-	PERFCTR_SETPCR,
+-
+-	/* Store in pointer given in ARG0 the current PCR register value
+-	 * being used.
+-	 */
+-	PERFCTR_GETPCR
+-};
+-
+-/* I don't want the kernel's namespace to be polluted with this
+- * stuff when this file is included.  --DaveM
+- */
+-#ifndef __KERNEL__
+-
+-#define  PRIV 0x00000001
+-#define  SYS  0x00000002
+-#define  USR  0x00000004
+-
+-/* Pic.S0 Selection Bit Field Encoding, Ultra-I/II  */
+-#define  CYCLE_CNT            0x00000000
+-#define  INSTR_CNT            0x00000010
+-#define  DISPATCH0_IC_MISS    0x00000020
+-#define  DISPATCH0_STOREBUF   0x00000030
+-#define  IC_REF               0x00000080
+-#define  DC_RD                0x00000090
+-#define  DC_WR                0x000000A0
+-#define  LOAD_USE             0x000000B0
+-#define  EC_REF               0x000000C0
+-#define  EC_WRITE_HIT_RDO     0x000000D0
+-#define  EC_SNOOP_INV         0x000000E0
+-#define  EC_RD_HIT            0x000000F0
+-
+-/* Pic.S0 Selection Bit Field Encoding, Ultra-III  */
+-#define  US3_CYCLE_CNT	      	0x00000000
+-#define  US3_INSTR_CNT	      	0x00000010
+-#define  US3_DISPATCH0_IC_MISS	0x00000020
+-#define  US3_DISPATCH0_BR_TGT	0x00000030
+-#define  US3_DISPATCH0_2ND_BR	0x00000040
+-#define  US3_RSTALL_STOREQ	0x00000050
+-#define  US3_RSTALL_IU_USE	0x00000060
+-#define  US3_IC_REF		0x00000080
+-#define  US3_DC_RD		0x00000090
+-#define  US3_DC_WR		0x000000a0
+-#define  US3_EC_REF		0x000000c0
+-#define  US3_EC_WR_HIT_RTO	0x000000d0
+-#define  US3_EC_SNOOP_INV	0x000000e0
+-#define  US3_EC_RD_MISS		0x000000f0
+-#define  US3_PC_PORT0_RD	0x00000100
+-#define  US3_SI_SNOOP		0x00000110
+-#define  US3_SI_CIQ_FLOW	0x00000120
+-#define  US3_SI_OWNED		0x00000130
+-#define  US3_SW_COUNT_0		0x00000140
+-#define  US3_IU_BR_MISS_TAKEN	0x00000150
+-#define  US3_IU_BR_COUNT_TAKEN	0x00000160
+-#define  US3_DISP_RS_MISPRED	0x00000170
+-#define  US3_FA_PIPE_COMPL	0x00000180
+-#define  US3_MC_READS_0		0x00000200
+-#define  US3_MC_READS_1		0x00000210
+-#define  US3_MC_READS_2		0x00000220
+-#define  US3_MC_READS_3		0x00000230
+-#define  US3_MC_STALLS_0	0x00000240
+-#define  US3_MC_STALLS_2	0x00000250
+-
+-/* Pic.S1 Selection Bit Field Encoding, Ultra-I/II  */
+-#define  CYCLE_CNT_D1         0x00000000
+-#define  INSTR_CNT_D1         0x00000800
+-#define  DISPATCH0_IC_MISPRED 0x00001000
+-#define  DISPATCH0_FP_USE     0x00001800
+-#define  IC_HIT               0x00004000
+-#define  DC_RD_HIT            0x00004800
+-#define  DC_WR_HIT            0x00005000
+-#define  LOAD_USE_RAW         0x00005800
+-#define  EC_HIT               0x00006000
+-#define  EC_WB                0x00006800
+-#define  EC_SNOOP_CB          0x00007000
+-#define  EC_IT_HIT            0x00007800
+-
+-/* Pic.S1 Selection Bit Field Encoding, Ultra-III  */
+-#define  US3_CYCLE_CNT_D1	0x00000000
+-#define  US3_INSTR_CNT_D1	0x00000800
+-#define  US3_DISPATCH0_MISPRED	0x00001000
+-#define  US3_IC_MISS_CANCELLED	0x00001800
+-#define  US3_RE_ENDIAN_MISS	0x00002000
+-#define  US3_RE_FPU_BYPASS	0x00002800
+-#define  US3_RE_DC_MISS		0x00003000
+-#define  US3_RE_EC_MISS		0x00003800
+-#define  US3_IC_MISS		0x00004000
+-#define  US3_DC_RD_MISS		0x00004800
+-#define  US3_DC_WR_MISS		0x00005000
+-#define  US3_RSTALL_FP_USE	0x00005800
+-#define  US3_EC_MISSES		0x00006000
+-#define  US3_EC_WB		0x00006800
+-#define  US3_EC_SNOOP_CB	0x00007000
+-#define  US3_EC_IC_MISS		0x00007800
+-#define  US3_RE_PC_MISS		0x00008000
+-#define  US3_ITLB_MISS		0x00008800
+-#define  US3_DTLB_MISS		0x00009000
+-#define  US3_WC_MISS		0x00009800
+-#define  US3_WC_SNOOP_CB	0x0000a000
+-#define  US3_WC_SCRUBBED	0x0000a800
+-#define  US3_WC_WB_WO_READ	0x0000b000
+-#define  US3_PC_SOFT_HIT	0x0000c000
+-#define  US3_PC_SNOOP_INV	0x0000c800
+-#define  US3_PC_HARD_HIT	0x0000d000
+-#define  US3_PC_PORT1_RD	0x0000d800
+-#define  US3_SW_COUNT_1		0x0000e000
+-#define  US3_IU_STAT_BR_MIS_UNTAKEN	0x0000e800
+-#define  US3_IU_STAT_BR_COUNT_UNTAKEN	0x0000f000
+-#define  US3_PC_MS_MISSES	0x0000f800
+-#define  US3_MC_WRITES_0	0x00010800
+-#define  US3_MC_WRITES_1	0x00011000
+-#define  US3_MC_WRITES_2	0x00011800
+-#define  US3_MC_WRITES_3	0x00012000
+-#define  US3_MC_STALLS_1	0x00012800
+-#define  US3_MC_STALLS_3	0x00013000
+-#define  US3_RE_RAW_MISS	0x00013800
+-#define  US3_FM_PIPE_COMPLETION	0x00014000
+-
+-struct vcounter_struct {
+-  unsigned long long vcnt0;
+-  unsigned long long vcnt1;
+-};
+-
+-#endif /* !(__KERNEL__) */
+-
+-#endif /* !(PERF_COUNTER_API) */
+--- a/include/asm-sparc/unistd.h
++++ b/include/asm-sparc/unistd.h
+@@ -330,8 +330,20 @@
+ #define __NR_timerfd		312
+ #define __NR_eventfd		313
+ #define __NR_fallocate		314
++#define __NR_pfm_create_context 315
++#define __NR_pfm_write_pmcs	316
++#define __NR_pfm_write_pmds	317
++#define __NR_pfm_read_pmds	318
++#define __NR_pfm_load_context	319
++#define __NR_pfm_start		320
++#define __NR_pfm_stop		321
++#define __NR_pfm_restart	322
++#define __NR_pfm_create_evtsets	323
++#define __NR_pfm_getinfo_evtsets 324
++#define __NR_pfm_delete_evtsets	325
++#define __NR_pfm_unload_context	326
+ 
+-#define NR_SYSCALLS		315
++#define NR_SYSCALLS		327
+ 
+ /* Sparc 32-bit only has the "setresuid32", "getresuid32" variants,
+  * it never had the plain ones and there is no value to adding those
+--- a/include/asm-sparc64/Kbuild
++++ b/include/asm-sparc64/Kbuild
+@@ -21,4 +21,3 @@ header-y += utrap.h
+ header-y += watchdog.h
+ 
+ unifdef-y += fbio.h
+-unifdef-y += perfctr.h
+--- a/include/asm-sparc64/hypervisor.h
++++ b/include/asm-sparc64/hypervisor.h
+@@ -2713,6 +2713,30 @@ extern unsigned long sun4v_ldc_revoke(un
+  */
+ #define HV_FAST_SET_PERFREG		0x101
+ 
++#define HV_N2_PERF_SPARC_CTL		0x0
++#define HV_N2_PERF_DRAM_CTL0		0x1
++#define HV_N2_PERF_DRAM_CNT0		0x2
++#define HV_N2_PERF_DRAM_CTL1		0x3
++#define HV_N2_PERF_DRAM_CNT1		0x4
++#define HV_N2_PERF_DRAM_CTL2		0x5
++#define HV_N2_PERF_DRAM_CNT2		0x6
++#define HV_N2_PERF_DRAM_CTL3		0x7
++#define HV_N2_PERF_DRAM_CNT3		0x8
++
++#define HV_FAST_N2_GET_PERFREG		0x104
++#define HV_FAST_N2_SET_PERFREG		0x105
++
++#ifndef __ASSEMBLY__
++extern unsigned long sun4v_niagara_getperf(unsigned long reg,
++					   unsigned long *val);
++extern unsigned long sun4v_niagara_setperf(unsigned long reg,
++					   unsigned long val);
++extern unsigned long sun4v_niagara2_getperf(unsigned long reg,
++					    unsigned long *val);
++extern unsigned long sun4v_niagara2_setperf(unsigned long reg,
++					    unsigned long val);
++#endif
++
+ /* MMU statistics services.
+  *
+  * The hypervisor maintains MMU statistics and privileged code provides
+@@ -2922,6 +2946,7 @@ extern unsigned long sun4v_ncs_request(u
+ #define HV_GRP_NCS			0x0103
+ #define HV_GRP_NIAG_PERF		0x0200
+ #define HV_GRP_FIRE_PERF		0x0201
++#define HV_GRP_NIAG2_PERF		0x0202
+ #define HV_GRP_DIAG			0x0300
+ 
+ #ifndef __ASSEMBLY__
+--- a/include/asm-sparc64/irq.h
++++ b/include/asm-sparc64/irq.h
+@@ -66,6 +66,9 @@ extern void virt_irq_free(unsigned int v
+ 
+ extern void fixup_irqs(void);
+ 
++extern int register_perfctr_intr(void (*handler)(struct pt_regs *));
++extern void release_perfctr_intr(void (*handler)(struct pt_regs *));
++
+ static inline void set_softint(unsigned long bits)
+ {
+ 	__asm__ __volatile__("wr	%0, 0x0, %%set_softint"
+--- a/include/asm-sparc64/perfctr.h
++++ /dev/null
+@@ -1,173 +0,0 @@
+-/*----------------------------------------
+-  PERFORMANCE INSTRUMENTATION  
+-  Guillaume Thouvenin           08/10/98
+-  David S. Miller               10/06/98
+-  ---------------------------------------*/
+-#ifndef PERF_COUNTER_API
+-#define PERF_COUNTER_API
+-
+-/* sys_perfctr() interface.  First arg is operation code
+- * from enumeration below.  The meaning of further arguments
+- * are determined by the operation code.
+- *
+- * int sys_perfctr(int opcode, unsigned long arg0,
+- *                 unsigned long arg1, unsigned long arg2)
+- *
+- * Pointers which are passed by the user are pointers to 64-bit
+- * integers.
+- *
+- * Once enabled, performance counter state is retained until the
+- * process either exits or performs an exec.  That is, performance
+- * counters remain enabled for fork/clone children.
+- */
+-enum perfctr_opcode {
+-	/* Enable UltraSparc performance counters, ARG0 is pointer
+-	 * to 64-bit accumulator for D0 counter in PIC, ARG1 is pointer
+-	 * to 64-bit accumulator for D1 counter.  ARG2 is a pointer to
+-	 * the initial PCR register value to use.
+-	 */
+-	PERFCTR_ON,
+-
+-	/* Disable UltraSparc performance counters.  The PCR is written
+-	 * with zero and the user counter accumulator pointers and
+-	 * working PCR register value are forgotten.
+-	 */
+-	PERFCTR_OFF,
+-
+-	/* Add current D0 and D1 PIC values into user pointers given
+-	 * in PERFCTR_ON operation.  The PIC is cleared before returning.
+-	 */
+-	PERFCTR_READ,
+-
+-	/* Clear the PIC register. */
+-	PERFCTR_CLRPIC,
+-
+-	/* Begin using a new PCR value, the pointer to which is passed
+-	 * in ARG0.  The PIC is also cleared after the new PCR value is
+-	 * written.
+-	 */
+-	PERFCTR_SETPCR,
+-
+-	/* Store in pointer given in ARG0 the current PCR register value
+-	 * being used.
+-	 */
+-	PERFCTR_GETPCR
+-};
+-
+-/* I don't want the kernel's namespace to be polluted with this
+- * stuff when this file is included.  --DaveM
+- */
+-#ifndef __KERNEL__
+-
+-#define  PRIV 0x00000001
+-#define  SYS  0x00000002
+-#define  USR  0x00000004
+-
+-/* Pic.S0 Selection Bit Field Encoding, Ultra-I/II  */
+-#define  CYCLE_CNT            0x00000000
+-#define  INSTR_CNT            0x00000010
+-#define  DISPATCH0_IC_MISS    0x00000020
+-#define  DISPATCH0_STOREBUF   0x00000030
+-#define  IC_REF               0x00000080
+-#define  DC_RD                0x00000090
+-#define  DC_WR                0x000000A0
+-#define  LOAD_USE             0x000000B0
+-#define  EC_REF               0x000000C0
+-#define  EC_WRITE_HIT_RDO     0x000000D0
+-#define  EC_SNOOP_INV         0x000000E0
+-#define  EC_RD_HIT            0x000000F0
+-
+-/* Pic.S0 Selection Bit Field Encoding, Ultra-III  */
+-#define  US3_CYCLE_CNT	      	0x00000000
+-#define  US3_INSTR_CNT	      	0x00000010
+-#define  US3_DISPATCH0_IC_MISS	0x00000020
+-#define  US3_DISPATCH0_BR_TGT	0x00000030
+-#define  US3_DISPATCH0_2ND_BR	0x00000040
+-#define  US3_RSTALL_STOREQ	0x00000050
+-#define  US3_RSTALL_IU_USE	0x00000060
+-#define  US3_IC_REF		0x00000080
+-#define  US3_DC_RD		0x00000090
+-#define  US3_DC_WR		0x000000a0
+-#define  US3_EC_REF		0x000000c0
+-#define  US3_EC_WR_HIT_RTO	0x000000d0
+-#define  US3_EC_SNOOP_INV	0x000000e0
+-#define  US3_EC_RD_MISS		0x000000f0
+-#define  US3_PC_PORT0_RD	0x00000100
+-#define  US3_SI_SNOOP		0x00000110
+-#define  US3_SI_CIQ_FLOW	0x00000120
+-#define  US3_SI_OWNED		0x00000130
+-#define  US3_SW_COUNT_0		0x00000140
+-#define  US3_IU_BR_MISS_TAKEN	0x00000150
+-#define  US3_IU_BR_COUNT_TAKEN	0x00000160
+-#define  US3_DISP_RS_MISPRED	0x00000170
+-#define  US3_FA_PIPE_COMPL	0x00000180
+-#define  US3_MC_READS_0		0x00000200
+-#define  US3_MC_READS_1		0x00000210
+-#define  US3_MC_READS_2		0x00000220
+-#define  US3_MC_READS_3		0x00000230
+-#define  US3_MC_STALLS_0	0x00000240
+-#define  US3_MC_STALLS_2	0x00000250
+-
+-/* Pic.S1 Selection Bit Field Encoding, Ultra-I/II  */
+-#define  CYCLE_CNT_D1         0x00000000
+-#define  INSTR_CNT_D1         0x00000800
+-#define  DISPATCH0_IC_MISPRED 0x00001000
+-#define  DISPATCH0_FP_USE     0x00001800
+-#define  IC_HIT               0x00004000
+-#define  DC_RD_HIT            0x00004800
+-#define  DC_WR_HIT            0x00005000
+-#define  LOAD_USE_RAW         0x00005800
+-#define  EC_HIT               0x00006000
+-#define  EC_WB                0x00006800
+-#define  EC_SNOOP_CB          0x00007000
+-#define  EC_IT_HIT            0x00007800
+-
+-/* Pic.S1 Selection Bit Field Encoding, Ultra-III  */
+-#define  US3_CYCLE_CNT_D1	0x00000000
+-#define  US3_INSTR_CNT_D1	0x00000800
+-#define  US3_DISPATCH0_MISPRED	0x00001000
+-#define  US3_IC_MISS_CANCELLED	0x00001800
+-#define  US3_RE_ENDIAN_MISS	0x00002000
+-#define  US3_RE_FPU_BYPASS	0x00002800
+-#define  US3_RE_DC_MISS		0x00003000
+-#define  US3_RE_EC_MISS		0x00003800
+-#define  US3_IC_MISS		0x00004000
+-#define  US3_DC_RD_MISS		0x00004800
+-#define  US3_DC_WR_MISS		0x00005000
+-#define  US3_RSTALL_FP_USE	0x00005800
+-#define  US3_EC_MISSES		0x00006000
+-#define  US3_EC_WB		0x00006800
+-#define  US3_EC_SNOOP_CB	0x00007000
+-#define  US3_EC_IC_MISS		0x00007800
+-#define  US3_RE_PC_MISS		0x00008000
+-#define  US3_ITLB_MISS		0x00008800
+-#define  US3_DTLB_MISS		0x00009000
+-#define  US3_WC_MISS		0x00009800
+-#define  US3_WC_SNOOP_CB	0x0000a000
+-#define  US3_WC_SCRUBBED	0x0000a800
+-#define  US3_WC_WB_WO_READ	0x0000b000
+-#define  US3_PC_SOFT_HIT	0x0000c000
+-#define  US3_PC_SNOOP_INV	0x0000c800
+-#define  US3_PC_HARD_HIT	0x0000d000
+-#define  US3_PC_PORT1_RD	0x0000d800
+-#define  US3_SW_COUNT_1		0x0000e000
+-#define  US3_IU_STAT_BR_MIS_UNTAKEN	0x0000e800
+-#define  US3_IU_STAT_BR_COUNT_UNTAKEN	0x0000f000
+-#define  US3_PC_MS_MISSES	0x0000f800
+-#define  US3_MC_WRITES_0	0x00010800
+-#define  US3_MC_WRITES_1	0x00011000
+-#define  US3_MC_WRITES_2	0x00011800
+-#define  US3_MC_WRITES_3	0x00012000
+-#define  US3_MC_STALLS_1	0x00012800
+-#define  US3_MC_STALLS_3	0x00013000
+-#define  US3_RE_RAW_MISS	0x00013800
+-#define  US3_FM_PIPE_COMPLETION	0x00014000
+-
+-struct vcounter_struct {
+-  unsigned long long vcnt0;
+-  unsigned long long vcnt1;
+-};
+-
+-#endif /* !(__KERNEL__) */
+-
+-#endif /* !(PERF_COUNTER_API) */
+--- /dev/null
++++ b/include/asm-sparc64/perfmon.h
+@@ -0,0 +1,298 @@
++#ifndef _SPARC64_PERFMON_H_
++#define _SPARC64_PERFMON_H_
++
++#ifdef __KERNEL__
++
++#include <linux/irq.h>
++#include <asm-sparc64/system.h>
++
++#define PFM_ARCH_PMD_STK_ARG	2
++#define PFM_ARCH_PMC_STK_ARG	1
++
++struct pfm_arch_pmu_info {
++	u32 pmu_style;
++};
++
++static inline void pfm_arch_resend_irq(void)
++{
++}
++
++static inline void pfm_arch_serialize(void)
++{
++}
++
++static inline void pfm_arch_unfreeze_pmu(void)
++{
++}
++
++/*
++ * SPARC does not save the PMDs during pfm_arch_intr_freeze_pmu(), thus
++ * this routine needs to do it when switching sets on overflow
++ */
++static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
++						struct pfm_event_set *set)
++{
++	pfm_save_pmds(ctx, set);
++}
++
++extern void pfm_arch_write_pmc(struct pfm_context *ctx,
++			       unsigned int cnum, u64 value);
++extern u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum);
++
++static inline void pfm_arch_write_pmd(struct pfm_context *ctx,
++				      unsigned int cnum, u64 value)
++{
++	u64 pic;
++
++	value &= pfm_pmu_conf->ovfl_mask;
++
++	read_pic(pic);
++
++	switch(cnum) {
++	case 0:
++		pic = (pic & 0xffffffff00000000UL) |
++			(value & 0xffffffffUL);
++		break;
++	case 1:
++		pic = (pic & 0xffffffffUL) |
++			(value << 32UL);
++		break;
++	default:
++		BUG();
++	}
++
++	write_pic(pic);
++}
++
++static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx,
++				    unsigned int cnum)
++{
++	u64 pic;
++
++	read_pic(pic);
++
++	switch(cnum) {
++	case 0:
++		return pic & 0xffffffffUL;
++	case 1:
++		return pic >> 32UL;
++	default:
++		BUG();
++		return 0;
++	}
++}
++
++/*
++ * For some CPUs, the upper bits of a counter must be set in order for the
++ * overflow interrupt to happen. On overflow, the counter has wrapped around,
++ * and the upper bits are cleared. This function may be used to set them back.
++ */
++static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx,
++					   unsigned int cnum)
++{
++	u64 val = pfm_arch_read_pmd(ctx, cnum);
++
++	/* This masks out overflow bit 31 */
++	pfm_arch_write_pmd(ctx, cnum, val);
++}
++
++/*
++ * At certain points, perfmon needs to know if monitoring has been
++ * explicitely started/stopped by user via pfm_start/pfm_stop. The
++ * information is tracked in ctx.flags.started. However on certain
++ * architectures, it may be possible to start/stop directly from
++ * user level with a single assembly instruction bypassing
++ * the kernel. This function must be used to determine by
++ * an arch-specific mean if monitoring is actually started/stopped.
++ */
++static inline int pfm_arch_is_active(struct pfm_context *ctx)
++{
++	return ctx->flags.started;
++}
++
++static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
++					 struct pfm_context *ctx,
++					 struct pfm_event_set *set)
++{
++}
++
++static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
++					struct pfm_context *ctx,
++					struct pfm_event_set *set)
++{
++}
++
++static inline void pfm_arch_ctxswin_thread(struct task_struct *task,
++					   struct pfm_context *ctx,
++					   struct pfm_event_set *set)
++{
++}
++
++int  pfm_arch_is_monitoring_active(struct pfm_context *ctx);
++int  pfm_arch_ctxswout_thread(struct task_struct *task,
++			      struct pfm_context *ctx,
++			      struct pfm_event_set *set);
++void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx,
++		   struct pfm_event_set *set);
++void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx,
++		    struct pfm_event_set *set);
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
++char *pfm_arch_get_pmu_module_name(void);
++
++static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
++					    struct pfm_event_set *set)
++{
++	pfm_arch_stop(current, ctx, set);
++	/*
++	 * we mark monitoring as stopped to avoid
++	 * certain side effects especially in
++	 * pfm_switch_sets_from_intr() on
++	 * pfm_arch_restore_pmcs()
++	 */
++	ctx->flags.started = 0;
++}
++
++/*
++ * unfreeze PMU from pfm_do_interrupt_handler()
++ * ctx may be NULL for spurious
++ */
++static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
++{
++	if (!ctx)
++		return;
++
++	PFM_DBG_ovfl("state=%d", ctx->state);
++
++	ctx->flags.started = 1;
++
++	if (ctx->state == PFM_CTX_MASKED)
++		return;
++
++	pfm_arch_restore_pmcs(ctx, ctx->active_set);
++}
++
++static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
++{
++	return 0;
++}
++
++/*
++ * this function is called from the PMU interrupt handler ONLY.
++ * On SPARC, the PMU is frozen via arch_stop, masking would be implemented
++ * via arch-stop as well. Given that the PMU is already stopped when
++ * entering the interrupt handler, we do not need to stop it again, so
++ * this function is a nop.
++ */
++static inline void pfm_arch_mask_monitoring(struct pfm_context *ctx,
++					    struct pfm_event_set *set)
++{
++}
++
++/*
++ * on MIPS masking/unmasking uses the start/stop mechanism, so we simply
++ * need to start here.
++ */
++static inline void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
++					      struct pfm_event_set *set)
++{
++	pfm_arch_start(current, ctx, set);
++}
++
++static inline void pfm_arch_pmu_config_remove(void)
++{
++}
++
++static inline int pfm_arch_context_create(struct pfm_context *ctx,
++					  u32 ctx_flags)
++{
++	return 0;
++}
++
++static inline void pfm_arch_context_free(struct pfm_context *ctx)
++{
++}
++
++/*
++ * function called from pfm_setfl_sane(). Context is locked
++ * and interrupts are masked.
++ * The value of flags is the value of ctx_flags as passed by
++ * user.
++ *
++ * function must check arch-specific set flags.
++ * Return:
++ * 	1 when flags are valid
++ *      0 on error
++ */
++static inline int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
++{
++	return 0;
++}
++
++static inline int pfm_arch_init(void)
++{
++	return 0;
++}
++
++static inline void pfm_arch_init_percpu(void)
++{
++}
++
++static inline int pfm_arch_load_context(struct pfm_context *ctx,
++					struct pfm_event_set *set,
++					struct task_struct *task)
++{
++	return 0;
++}
++
++static inline int pfm_arch_unload_context(struct pfm_context *ctx,
++					  struct task_struct *task)
++{
++	return 0;
++}
++
++extern void perfmon_interrupt(struct pt_regs *);
++
++static inline int pfm_arch_pmu_acquire(void)
++{
++	return register_perfctr_intr(perfmon_interrupt);
++}
++
++static inline void pfm_arch_pmu_release(void)
++{
++	release_perfctr_intr(perfmon_interrupt);
++}
++
++/*
++ * not used for sparc
++ */
++static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
++					       size_t rsize, struct file *filp)
++{
++	return -EINVAL;
++}
++
++static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
++					   char __user *buf,
++					   int non_block,
++					   size_t size)
++{
++	return -EINVAL;
++}
++struct pfm_arch_context {
++	/* empty */
++};
++
++#define PFM_ARCH_CTX_SIZE	sizeof(struct pfm_arch_context)
++/*
++ * SPARC needs extra alignment for the sampling buffer
++ */
++#define PFM_ARCH_SMPL_ALIGN_SIZE	(16 * 1024)
++
++static inline void pfm_cacheflush(void *addr, unsigned int len)
++{
++}
++
++#endif /* __KERNEL__ */
++
++#endif /* _SPARC64_PERFMON_H_ */
+--- /dev/null
++++ b/include/asm-sparc64/perfmon_const.h
+@@ -0,0 +1,7 @@
++#ifndef _SPARC64_PERFMON_CONST_H_
++#define _SPARC64_PERFMON_CONST_H_
++
++#define PFM_ARCH_MAX_PMCS	2
++#define PFM_ARCH_MAX_PMDS	3
++
++#endif /* _SPARC64_PERFMON_CONST_H_ */
+--- a/include/asm-sparc64/system.h
++++ b/include/asm-sparc64/system.h
+@@ -29,6 +29,10 @@ enum sparc_cpu {
+ #define ARCH_SUN4C_SUN4 0
+ #define ARCH_SUN4 0
+ 
++extern char *sparc_cpu_type;
++extern char *sparc_fpu_type;
++extern char *sparc_pmu_type;
++
+ /* These are here in an effort to more fully work around Spitfire Errata
+  * #51.  Essentially, if a memory barrier occurs soon after a mispredicted
+  * branch, the chip can stop executing instructions until a trap occurs.
+@@ -101,15 +105,13 @@ do {	__asm__ __volatile__("ba,pt	%%xcc, 
+ #define write_pcr(__p) __asm__ __volatile__("wr	%0, 0x0, %%pcr" : : "r" (__p))
+ #define read_pic(__p)  __asm__ __volatile__("rd %%pic, %0" : "=r" (__p))
+ 
+-/* Blackbird errata workaround.  See commentary in
+- * arch/sparc64/kernel/smp.c:smp_percpu_timer_interrupt()
+- * for more information.
+- */
+-#define reset_pic()    						\
+-	__asm__ __volatile__("ba,pt	%xcc, 99f\n\t"		\
++/* Blackbird errata workaround.  */
++#define write_pic(val)    					\
++	__asm__ __volatile__("ba,pt	%%xcc, 99f\n\t"		\
+ 			     ".align	64\n"			\
+-			  "99:wr	%g0, 0x0, %pic\n\t"	\
+-			     "rd	%pic, %g0")
++			  "99:wr	%0, 0x0, %%pic\n\t"	\
++			     "rd	%%pic, %%g0" : : "r" (val))
++#define reset_pic()	write_pic(0)
+ 
+ #ifndef __ASSEMBLY__
+ 
+@@ -141,14 +143,9 @@ do {						\
+ 	 * and 2 stores in this critical code path.  -DaveM
+ 	 */
+ #define switch_to(prev, next, last)					\
+-do {	if (test_thread_flag(TIF_PERFCTR)) {				\
+-		unsigned long __tmp;					\
+-		read_pcr(__tmp);					\
+-		current_thread_info()->pcr_reg = __tmp;			\
+-		read_pic(__tmp);					\
+-		current_thread_info()->kernel_cntd0 += (unsigned int)(__tmp);\
+-		current_thread_info()->kernel_cntd1 += ((__tmp) >> 32);	\
+-	}								\
++do {	if (test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW)		\
++	    || test_tsk_thread_flag(next, TIF_PERFMON_CTXSW))		\
++		pfm_ctxsw(prev, next);					\
+ 	flush_tlb_pending();						\
+ 	save_and_clear_fpu();						\
+ 	/* If you are tempted to conditionalize the following */	\
+@@ -192,11 +189,6 @@ do {	if (test_thread_flag(TIF_PERFCTR)) 
+ 	        "l1", "l2", "l3", "l4", "l5", "l6", "l7",		\
+ 	  "i0", "i1", "i2", "i3", "i4", "i5",				\
+ 	  "o0", "o1", "o2", "o3", "o4", "o5",       "o7");		\
+-	/* If you fuck with this, update ret_from_syscall code too. */	\
+-	if (test_thread_flag(TIF_PERFCTR)) {				\
+-		write_pcr(current_thread_info()->pcr_reg);		\
+-		reset_pic();						\
+-	}								\
+ } while(0)
+ 
+ static inline unsigned long xchg32(__volatile__ unsigned int *m, unsigned int val)
+--- a/include/asm-sparc64/thread_info.h
++++ b/include/asm-sparc64/thread_info.h
+@@ -59,11 +59,6 @@ struct thread_info {
+ 	unsigned long		gsr[7];
+ 	unsigned long		xfsr[7];
+ 
+-	__u64			__user *user_cntd0;
+-	__u64			__user *user_cntd1;
+-	__u64			kernel_cntd0, kernel_cntd1;
+-	__u64			pcr_reg;
+-
+ 	struct restart_block	restart_block;
+ 
+ 	struct pt_regs		*kern_una_regs;
+@@ -97,15 +92,10 @@ struct thread_info {
+ #define TI_RWIN_SPTRS	0x000003c8	
+ #define TI_GSR		0x00000400
+ #define TI_XFSR		0x00000438
+-#define TI_USER_CNTD0	0x00000470
+-#define TI_USER_CNTD1	0x00000478
+-#define TI_KERN_CNTD0	0x00000480
+-#define TI_KERN_CNTD1	0x00000488
+-#define TI_PCR		0x00000490
+-#define TI_RESTART_BLOCK 0x00000498
+-#define TI_KUNA_REGS	0x000004c0
+-#define TI_KUNA_INSN	0x000004c8
+-#define TI_FPREGS	0x00000500
++#define TI_RESTART_BLOCK 0x00000470
++#define TI_KUNA_REGS	0x00000498
++#define TI_KUNA_INSN	0x000004a0
++#define TI_FPREGS	0x000004c0
+ 
+ /* We embed this in the uppermost byte of thread_info->flags */
+ #define FAULT_CODE_WRITE	0x01	/* Write access, implies D-TLB	   */
+@@ -221,11 +211,11 @@ register struct thread_info *current_thr
+ #define TIF_RESTORE_SIGMASK	1	/* restore signal mask in do_signal() */
+ #define TIF_SIGPENDING		2	/* signal pending */
+ #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
+-#define TIF_PERFCTR		4	/* performance counters active */
++/* Bit 4 is available */
+ #define TIF_UNALIGNED		5	/* allowed to do unaligned accesses */
+ #define TIF_NEWSIGNALS		6	/* wants new-style signals */
+ #define TIF_32BIT		7	/* 32-bit binary */
+-/* flag bit 8 is available */
++#define TIF_PERFMON_WORK	8	/* work for pfm_handle_work() */
+ #define TIF_SECCOMP		9	/* secure computing */
+ #define TIF_SYSCALL_AUDIT	10	/* syscall auditing active */
+ /* flag bit 11 is available */
+@@ -236,23 +226,25 @@ register struct thread_info *current_thr
+ #define TIF_ABI_PENDING		12
+ #define TIF_MEMDIE		13
+ #define TIF_POLLING_NRFLAG	14
++#define TIF_PERFMON_CTXSW	15	/* perfmon needs ctxsw calls */
+ 
+ #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
+ #define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
+ #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
+-#define _TIF_PERFCTR		(1<<TIF_PERFCTR)
+ #define _TIF_UNALIGNED		(1<<TIF_UNALIGNED)
+ #define _TIF_NEWSIGNALS		(1<<TIF_NEWSIGNALS)
+ #define _TIF_32BIT		(1<<TIF_32BIT)
++#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
+ #define _TIF_SECCOMP		(1<<TIF_SECCOMP)
+ #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
+ #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
+ #define _TIF_ABI_PENDING	(1<<TIF_ABI_PENDING)
+ #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
++#define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
+ 
+ #define _TIF_USER_WORK_MASK	((0xff << TI_FLAG_WSAVED_SHIFT) | \
+ 				 (_TIF_SIGPENDING | _TIF_RESTORE_SIGMASK | \
+-				  _TIF_NEED_RESCHED | _TIF_PERFCTR))
++				  _TIF_NEED_RESCHED))
+ 
+ #endif /* __KERNEL__ */
+ 
+--- a/include/asm-sparc64/unistd.h
++++ b/include/asm-sparc64/unistd.h
+@@ -332,8 +332,20 @@
+ #define __NR_timerfd		312
+ #define __NR_eventfd		313
+ #define __NR_fallocate		314
++#define __NR_pfm_create_context 315
++#define __NR_pfm_write_pmcs	316
++#define __NR_pfm_write_pmds	317
++#define __NR_pfm_read_pmds	318
++#define __NR_pfm_load_context	319
++#define __NR_pfm_start		320
++#define __NR_pfm_stop		321
++#define __NR_pfm_restart	322
++#define __NR_pfm_create_evtsets	323
++#define __NR_pfm_getinfo_evtsets 324
++#define __NR_pfm_delete_evtsets	325
++#define __NR_pfm_unload_context	326
+ 
+-#define NR_SYSCALLS		315
++#define NR_SYSCALLS		327
+ 
+ #ifdef __KERNEL__
+ /* sysconf options, for SunOS compatibility */
+--- a/include/asm-x86/apic.h
++++ b/include/asm-x86/apic.h
+@@ -1,5 +1,138 @@
+-#ifdef CONFIG_X86_32
+-# include "apic_32.h"
++#ifndef _ASM_X86_APIC_H
++#define _ASM_X86_APIC_H
++
++#include <linux/pm.h>
++#include <linux/delay.h>
++#include <asm/fixmap.h>
++#include <asm/apicdef.h>
++#include <asm/processor.h>
++#include <asm/system.h>
++
++#define ARCH_APICTIMER_STOPS_ON_C3	1
++
++#define Dprintk(x...)
++
++/*
++ * Debugging macros
++ */
++#define APIC_QUIET   0
++#define APIC_VERBOSE 1
++#define APIC_DEBUG   2
++
++extern int apic_verbosity;
++extern int timer_over_8254;
++extern int local_apic_timer_c2_ok;
++extern int local_apic_timer_disabled;
++
++extern int apic_runs_main_timer;
++extern int ioapic_force;
++extern int disable_apic_timer;
++extern unsigned boot_cpu_id;
++
++/*
++ * Define the default level of output to be very little
++ * This can be turned up by using apic=verbose for more
++ * information and apic=debug for _lots_ of information.
++ * apic_verbosity is defined in apic.c
++ */
++#define apic_printk(v, s, a...) do {       \
++		if ((v) <= apic_verbosity) \
++			printk(s, ##a);    \
++	} while (0)
++
++
++extern void generic_apic_probe(void);
++
++#ifdef CONFIG_X86_LOCAL_APIC
++
++/*
++ * Basic functions accessing APICs.
++ */
++#ifdef CONFIG_PARAVIRT
++#include <asm/paravirt.h>
+ #else
+-# include "apic_64.h"
++#define apic_write native_apic_write
++#define apic_write_atomic native_apic_write_atomic
++#define apic_read native_apic_read
++#define setup_boot_clock setup_boot_APIC_clock
++#define setup_secondary_clock setup_secondary_APIC_clock
+ #endif
++
++static inline fastcall void native_apic_write(unsigned long reg, u32 v)
++{
++	*((volatile u32 *)(APIC_BASE + reg)) = v;
++}
++
++static inline fastcall void native_apic_write_atomic(unsigned long reg,
++						     unsigned long v)
++{
++	xchg((volatile unsigned long*)(APIC_BASE + reg), v);
++}
++
++static inline fastcall u32 native_apic_read(unsigned long reg)
++{
++	return *((volatile u32 *)(APIC_BASE + reg));
++}
++
++extern void apic_wait_icr_idle(void);
++extern u32 safe_apic_wait_icr_idle(void);
++extern int get_physical_broadcast(void);
++
++#ifdef CONFIG_X86_GOOD_APIC
++# define FORCE_READ_AROUND_WRITE 0
++# define apic_read_around(x)
++# define apic_write_around(x, y) apic_write((x), (y))
++#else
++# define FORCE_READ_AROUND_WRITE 1
++# define apic_read_around(x) apic_read(x)
++# define apic_write_around(x, y) apic_write_atomic((x), (y))
++#endif
++
++static inline void ack_APIC_irq(void)
++{
++	/*
++	 * ack_APIC_irq() actually gets compiled as a single instruction:
++	 * - a single rmw on Pentium/82489DX
++	 * - a single write on P6+ cores (CONFIG_X86_GOOD_APIC)
++	 * ... yummie.
++	 */
++
++	/* Docs say use 0 for future compatibility */
++	apic_write_around(APIC_EOI, 0);
++}
++
++extern int lapic_get_maxlvt(void);
++extern void clear_local_APIC(void);
++extern void connect_bsp_APIC(void);
++extern void disconnect_bsp_APIC(int virt_wire_setup);
++extern void disable_local_APIC(void);
++extern void lapic_shutdown(void);
++extern int verify_local_APIC(void);
++extern void cache_APIC_registers(void);
++extern void sync_Arb_IDs(void);
++extern void init_bsp_APIC(void);
++extern void setup_local_APIC(void);
++extern void init_apic_mappings(void);
++extern void setup_boot_APIC_clock(void);
++extern void setup_secondary_APIC_clock(void);
++extern int APIC_init_uniprocessor(void);
++extern void enable_NMI_through_LVT0(void *dummy);
++
++/*
++ * On 32bit this is mach-xxx local
++ */
++#ifdef CONFIG_X86_64
++extern void setup_apic_routing(void);
++#endif
++
++extern u8 setup_APIC_eilvt_mce(u8 vector, u8 msg_type, u8 mask);
++extern u8 setup_APIC_eilvt_ibs(u8 vector, u8 msg_type, u8 mask);
++
++extern int apic_is_clustered_box(void);
++
++#else /* !CONFIG_X86_LOCAL_APIC */
++static inline void lapic_shutdown(void) { }
++
++#endif /* !CONFIG_X86_LOCAL_APIC */
++
++#endif /* __ASM_APIC_H */
+--- a/include/asm-x86/apic_32.h
++++ /dev/null
+@@ -1,127 +0,0 @@
+-#ifndef __ASM_APIC_H
+-#define __ASM_APIC_H
+-
+-#include <linux/pm.h>
+-#include <linux/delay.h>
+-#include <asm/fixmap.h>
+-#include <asm/apicdef.h>
+-#include <asm/processor.h>
+-#include <asm/system.h>
+-
+-#define Dprintk(x...)
+-
+-/*
+- * Debugging macros
+- */
+-#define APIC_QUIET   0
+-#define APIC_VERBOSE 1
+-#define APIC_DEBUG   2
+-
+-extern int apic_verbosity;
+-
+-/*
+- * Define the default level of output to be very little
+- * This can be turned up by using apic=verbose for more
+- * information and apic=debug for _lots_ of information.
+- * apic_verbosity is defined in apic.c
+- */
+-#define apic_printk(v, s, a...) do {       \
+-		if ((v) <= apic_verbosity) \
+-			printk(s, ##a);    \
+-	} while (0)
+-
+-
+-extern void generic_apic_probe(void);
+-
+-#ifdef CONFIG_X86_LOCAL_APIC
+-
+-/*
+- * Basic functions accessing APICs.
+- */
+-#ifdef CONFIG_PARAVIRT
+-#include <asm/paravirt.h>
+-#else
+-#define apic_write native_apic_write
+-#define apic_write_atomic native_apic_write_atomic
+-#define apic_read native_apic_read
+-#define setup_boot_clock setup_boot_APIC_clock
+-#define setup_secondary_clock setup_secondary_APIC_clock
+-#endif
+-
+-static __inline fastcall void native_apic_write(unsigned long reg,
+-						unsigned long v)
+-{
+-	*((volatile unsigned long *)(APIC_BASE+reg)) = v;
+-}
+-
+-static __inline fastcall void native_apic_write_atomic(unsigned long reg,
+-						       unsigned long v)
+-{
+-	xchg((volatile unsigned long *)(APIC_BASE+reg), v);
+-}
+-
+-static __inline fastcall unsigned long native_apic_read(unsigned long reg)
+-{
+-	return *((volatile unsigned long *)(APIC_BASE+reg));
+-}
+-
+-void apic_wait_icr_idle(void);
+-unsigned long safe_apic_wait_icr_idle(void);
+-int get_physical_broadcast(void);
+-
+-#ifdef CONFIG_X86_GOOD_APIC
+-# define FORCE_READ_AROUND_WRITE 0
+-# define apic_read_around(x)
+-# define apic_write_around(x,y) apic_write((x),(y))
+-#else
+-# define FORCE_READ_AROUND_WRITE 1
+-# define apic_read_around(x) apic_read(x)
+-# define apic_write_around(x,y) apic_write_atomic((x),(y))
+-#endif
+-
+-static inline void ack_APIC_irq(void)
+-{
+-	/*
+-	 * ack_APIC_irq() actually gets compiled as a single instruction:
+-	 * - a single rmw on Pentium/82489DX
+-	 * - a single write on P6+ cores (CONFIG_X86_GOOD_APIC)
+-	 * ... yummie.
+-	 */
+-
+-	/* Docs say use 0 for future compatibility */
+-	apic_write_around(APIC_EOI, 0);
+-}
+-
+-extern int lapic_get_maxlvt(void);
+-extern void clear_local_APIC(void);
+-extern void connect_bsp_APIC (void);
+-extern void disconnect_bsp_APIC (int virt_wire_setup);
+-extern void disable_local_APIC (void);
+-extern void lapic_shutdown (void);
+-extern int verify_local_APIC (void);
+-extern void cache_APIC_registers (void);
+-extern void sync_Arb_IDs (void);
+-extern void init_bsp_APIC (void);
+-extern void setup_local_APIC (void);
+-extern void init_apic_mappings (void);
+-extern void smp_local_timer_interrupt (void);
+-extern void setup_boot_APIC_clock (void);
+-extern void setup_secondary_APIC_clock (void);
+-extern int APIC_init_uniprocessor (void);
+-
+-extern void enable_NMI_through_LVT0 (void * dummy);
+-
+-#define ARCH_APICTIMER_STOPS_ON_C3	1
+-
+-extern int timer_over_8254;
+-extern int local_apic_timer_c2_ok;
+-
+-extern int local_apic_timer_disabled;
+-
+-#else /* !CONFIG_X86_LOCAL_APIC */
+-static inline void lapic_shutdown(void) { }
+-#define local_apic_timer_c2_ok		1
+-
+-#endif /* !CONFIG_X86_LOCAL_APIC */
+-
+-#endif /* __ASM_APIC_H */
+--- a/include/asm-x86/apic_64.h
++++ /dev/null
+@@ -1,102 +0,0 @@
+-#ifndef __ASM_APIC_H
+-#define __ASM_APIC_H
+-
+-#include <linux/pm.h>
+-#include <linux/delay.h>
+-#include <asm/fixmap.h>
+-#include <asm/apicdef.h>
+-#include <asm/system.h>
+-
+-#define Dprintk(x...)
+-
+-/*
+- * Debugging macros
+- */
+-#define APIC_QUIET   0
+-#define APIC_VERBOSE 1
+-#define APIC_DEBUG   2
+-
+-extern int apic_verbosity;
+-extern int apic_runs_main_timer;
+-extern int ioapic_force;
+-extern int disable_apic_timer;
+-
+-/*
+- * Define the default level of output to be very little
+- * This can be turned up by using apic=verbose for more
+- * information and apic=debug for _lots_ of information.
+- * apic_verbosity is defined in apic.c
+- */
+-#define apic_printk(v, s, a...) do {       \
+-		if ((v) <= apic_verbosity) \
+-			printk(s, ##a);    \
+-	} while (0)
+-
+-struct pt_regs;
+-
+-/*
+- * Basic functions accessing APICs.
+- */
+-
+-static __inline void apic_write(unsigned long reg, unsigned int v)
+-{
+-	*((volatile unsigned int *)(APIC_BASE+reg)) = v;
+-}
+-
+-static __inline unsigned int apic_read(unsigned long reg)
+-{
+-	return *((volatile unsigned int *)(APIC_BASE+reg));
+-}
+-
+-extern void apic_wait_icr_idle(void);
+-extern unsigned int safe_apic_wait_icr_idle(void);
+-
+-static inline void ack_APIC_irq(void)
+-{
+-	/*
+-	 * ack_APIC_irq() actually gets compiled as a single instruction:
+-	 * - a single rmw on Pentium/82489DX
+-	 * - a single write on P6+ cores (CONFIG_X86_GOOD_APIC)
+-	 * ... yummie.
+-	 */
+-
+-	/* Docs say use 0 for future compatibility */
+-	apic_write(APIC_EOI, 0);
+-}
+-
+-extern int get_maxlvt (void);
+-extern void clear_local_APIC (void);
+-extern void connect_bsp_APIC (void);
+-extern void disconnect_bsp_APIC (int virt_wire_setup);
+-extern void disable_local_APIC (void);
+-extern void lapic_shutdown (void);
+-extern int verify_local_APIC (void);
+-extern void cache_APIC_registers (void);
+-extern void sync_Arb_IDs (void);
+-extern void init_bsp_APIC (void);
+-extern void setup_local_APIC (void);
+-extern void init_apic_mappings (void);
+-extern void smp_local_timer_interrupt (void);
+-extern void setup_boot_APIC_clock (void);
+-extern void setup_secondary_APIC_clock (void);
+-extern int APIC_init_uniprocessor (void);
+-extern void setup_apic_routing(void);
+-
+-extern void setup_APIC_extended_lvt(unsigned char lvt_off, unsigned char vector,
+-				    unsigned char msg_type, unsigned char mask);
+-
+-extern int apic_is_clustered_box(void);
+-
+-#define K8_APIC_EXT_LVT_BASE    0x500
+-#define K8_APIC_EXT_INT_MSG_FIX 0x0
+-#define K8_APIC_EXT_INT_MSG_SMI 0x2
+-#define K8_APIC_EXT_INT_MSG_NMI 0x4
+-#define K8_APIC_EXT_INT_MSG_EXT 0x7
+-#define K8_APIC_EXT_LVT_ENTRY_THRESHOLD    0
+-
+-#define ARCH_APICTIMER_STOPS_ON_C3	1
+-
+-extern unsigned boot_cpu_id;
+-extern int local_apic_timer_c2_ok;
+-
+-#endif /* __ASM_APIC_H */
+--- a/include/asm-x86/apicdef.h
++++ b/include/asm-x86/apicdef.h
+@@ -1,5 +1,413 @@
++#ifndef _ASM_X86_APICDEF_H
++#define _ASM_X86_APICDEF_H
++
++/*
++ * Constants for various Intel APICs. (local APIC, IOAPIC, etc.)
++ *
++ * Alan Cox <Alan.Cox@linux.org>, 1995.
++ * Ingo Molnar <mingo@redhat.com>, 1999, 2000
++ */
++
++#define	APIC_DEFAULT_PHYS_BASE	0xfee00000
++
++#define	APIC_ID		0x20
++
++#ifdef CONFIG_X86_64
++# define	APIC_ID_MASK		(0xFFu<<24)
++# define	GET_APIC_ID(x)		(((x)>>24)&0xFFu)
++# define	SET_APIC_ID(x)		(((x)<<24))
++#endif
++
++#define	APIC_LVR	0x30
++#define		APIC_LVR_MASK		0xFF00FF
++#define		GET_APIC_VERSION(x)	((x)&0xFFu)
++#define		GET_APIC_MAXLVT(x)	(((x)>>16)&0xFFu)
++#define		APIC_INTEGRATED(x)	((x)&0xF0u)
++#define		APIC_XAPIC(x)		((x) >= 0x14)
++#define	APIC_TASKPRI	0x80
++#define		APIC_TPRI_MASK		0xFFu
++#define	APIC_ARBPRI	0x90
++#define		APIC_ARBPRI_MASK	0xFFu
++#define	APIC_PROCPRI	0xA0
++#define	APIC_EOI	0xB0
++#define		APIC_EIO_ACK		0x0
++#define	APIC_RRR	0xC0
++#define	APIC_LDR	0xD0
++#define		APIC_LDR_MASK		(0xFFu<<24)
++#define		GET_APIC_LOGICAL_ID(x)	(((x)>>24)&0xFFu)
++#define		SET_APIC_LOGICAL_ID(x)	(((x)<<24))
++#define		APIC_ALL_CPUS		0xFFu
++#define	APIC_DFR	0xE0
++#define		APIC_DFR_CLUSTER		0x0FFFFFFFul
++#define		APIC_DFR_FLAT			0xFFFFFFFFul
++#define	APIC_SPIV	0xF0
++#define		APIC_SPIV_FOCUS_DISABLED	(1<<9)
++#define		APIC_SPIV_APIC_ENABLED		(1<<8)
++#define	APIC_ISR	0x100
++#define	APIC_ISR_NR     0x8     /* Number of 32 bit ISR registers. */
++#define	APIC_TMR	0x180
++#define	APIC_IRR	0x200
++#define	APIC_ESR	0x280
++#define		APIC_ESR_SEND_CS	0x00001
++#define		APIC_ESR_RECV_CS	0x00002
++#define		APIC_ESR_SEND_ACC	0x00004
++#define		APIC_ESR_RECV_ACC	0x00008
++#define		APIC_ESR_SENDILL	0x00020
++#define		APIC_ESR_RECVILL	0x00040
++#define		APIC_ESR_ILLREGA	0x00080
++#define	APIC_ICR	0x300
++#define		APIC_DEST_SELF		0x40000
++#define		APIC_DEST_ALLINC	0x80000
++#define		APIC_DEST_ALLBUT	0xC0000
++#define		APIC_ICR_RR_MASK	0x30000
++#define		APIC_ICR_RR_INVALID	0x00000
++#define		APIC_ICR_RR_INPROG	0x10000
++#define		APIC_ICR_RR_VALID	0x20000
++#define		APIC_INT_LEVELTRIG	0x08000
++#define		APIC_INT_ASSERT		0x04000
++#define		APIC_ICR_BUSY		0x01000
++#define		APIC_DEST_LOGICAL	0x00800
++#define		APIC_DEST_PHYSICAL	0x00000
++#define		APIC_DM_FIXED		0x00000
++#define		APIC_DM_LOWEST		0x00100
++#define		APIC_DM_SMI		0x00200
++#define		APIC_DM_REMRD		0x00300
++#define		APIC_DM_NMI		0x00400
++#define		APIC_DM_INIT		0x00500
++#define		APIC_DM_STARTUP		0x00600
++#define		APIC_DM_EXTINT		0x00700
++#define		APIC_VECTOR_MASK	0x000FF
++#define	APIC_ICR2	0x310
++#define		GET_APIC_DEST_FIELD(x)	(((x)>>24)&0xFF)
++#define		SET_APIC_DEST_FIELD(x)	((x)<<24)
++#define	APIC_LVTT	0x320
++#define	APIC_LVTTHMR	0x330
++#define	APIC_LVTPC	0x340
++#define	APIC_LVT0	0x350
++#define		APIC_LVT_TIMER_BASE_MASK	(0x3<<18)
++#define		GET_APIC_TIMER_BASE(x)		(((x)>>18)&0x3)
++#define		SET_APIC_TIMER_BASE(x)		(((x)<<18))
++#define		APIC_TIMER_BASE_CLKIN		0x0
++#define		APIC_TIMER_BASE_TMBASE		0x1
++#define		APIC_TIMER_BASE_DIV		0x2
++#define		APIC_LVT_TIMER_PERIODIC		(1<<17)
++#define		APIC_LVT_MASKED			(1<<16)
++#define		APIC_LVT_LEVEL_TRIGGER		(1<<15)
++#define		APIC_LVT_REMOTE_IRR		(1<<14)
++#define		APIC_INPUT_POLARITY		(1<<13)
++#define		APIC_SEND_PENDING		(1<<12)
++#define		APIC_MODE_MASK			0x700
++#define		GET_APIC_DELIVERY_MODE(x)	(((x)>>8)&0x7)
++#define		SET_APIC_DELIVERY_MODE(x, y)	(((x)&~0x700)|((y)<<8))
++#define			APIC_MODE_FIXED		0x0
++#define			APIC_MODE_NMI		0x4
++#define			APIC_MODE_EXTINT	0x7
++#define	APIC_LVT1	0x360
++#define	APIC_LVTERR	0x370
++#define	APIC_TMICT	0x380
++#define	APIC_TMCCT	0x390
++#define	APIC_TDCR	0x3E0
++#define		APIC_TDR_DIV_TMBASE	(1<<2)
++#define		APIC_TDR_DIV_1		0xB
++#define		APIC_TDR_DIV_2		0x0
++#define		APIC_TDR_DIV_4		0x1
++#define		APIC_TDR_DIV_8		0x2
++#define		APIC_TDR_DIV_16		0x3
++#define		APIC_TDR_DIV_32		0x8
++#define		APIC_TDR_DIV_64		0x9
++#define		APIC_TDR_DIV_128	0xA
++#define	APIC_EILVT0     0x500
++#define		APIC_EILVT_NR_AMD_K8	1	/* Number of extended interrupts */
++#define		APIC_EILVT_NR_AMD_10H	4
++#define		APIC_EILVT_LVTOFF(x)	(((x)>>4)&0xF)
++#define		APIC_EILVT_MSG_FIX	0x0
++#define		APIC_EILVT_MSG_SMI	0x2
++#define		APIC_EILVT_MSG_NMI	0x4
++#define		APIC_EILVT_MSG_EXT	0x7
++#define		APIC_EILVT_MASKED	(1<<16)
++#define	APIC_EILVT1     0x510
++#define	APIC_EILVT2     0x520
++#define	APIC_EILVT3     0x530
++
++#define APIC_BASE (fix_to_virt(FIX_APIC_BASE))
++
+ #ifdef CONFIG_X86_32
+-# include "apicdef_32.h"
++# define MAX_IO_APICS 64
+ #else
+-# include "apicdef_64.h"
++# define MAX_IO_APICS 128
++# define MAX_LOCAL_APIC 256
++#endif
++
++/*
++ * All x86-64 systems are xAPIC compatible.
++ * In the following, "apicid" is a physical APIC ID.
++ */
++#define XAPIC_DEST_CPUS_SHIFT	4
++#define XAPIC_DEST_CPUS_MASK	((1u << XAPIC_DEST_CPUS_SHIFT) - 1)
++#define XAPIC_DEST_CLUSTER_MASK	(XAPIC_DEST_CPUS_MASK << XAPIC_DEST_CPUS_SHIFT)
++#define APIC_CLUSTER(apicid)	((apicid) & XAPIC_DEST_CLUSTER_MASK)
++#define APIC_CLUSTERID(apicid)	(APIC_CLUSTER(apicid) >> XAPIC_DEST_CPUS_SHIFT)
++#define APIC_CPUID(apicid)	((apicid) & XAPIC_DEST_CPUS_MASK)
++#define NUM_APIC_CLUSTERS	((BAD_APICID + 1) >> XAPIC_DEST_CPUS_SHIFT)
++
++/*
++ * the local APIC register structure, memory mapped. Not terribly well
++ * tested, but we might eventually use this one in the future - the
++ * problem why we cannot use it right now is the P5 APIC, it has an
++ * errata which cannot take 8-bit reads and writes, only 32-bit ones ...
++ */
++#define u32 unsigned int
++
++struct local_apic {
++
++/*000*/	struct { u32 __reserved[4]; } __reserved_01;
++
++/*010*/	struct { u32 __reserved[4]; } __reserved_02;
++
++/*020*/	struct { /* APIC ID Register */
++		u32   __reserved_1	: 24,
++			phys_apic_id	:  4,
++			__reserved_2	:  4;
++		u32 __reserved[3];
++	} id;
++
++/*030*/	const
++	struct { /* APIC Version Register */
++		u32   version		:  8,
++			__reserved_1	:  8,
++			max_lvt		:  8,
++			__reserved_2	:  8;
++		u32 __reserved[3];
++	} version;
++
++/*040*/	struct { u32 __reserved[4]; } __reserved_03;
++
++/*050*/	struct { u32 __reserved[4]; } __reserved_04;
++
++/*060*/	struct { u32 __reserved[4]; } __reserved_05;
++
++/*070*/	struct { u32 __reserved[4]; } __reserved_06;
++
++/*080*/	struct { /* Task Priority Register */
++		u32   priority	:  8,
++			__reserved_1	: 24;
++		u32 __reserved_2[3];
++	} tpr;
++
++/*090*/	const
++	struct { /* Arbitration Priority Register */
++		u32   priority	:  8,
++			__reserved_1	: 24;
++		u32 __reserved_2[3];
++	} apr;
++
++/*0A0*/	const
++	struct { /* Processor Priority Register */
++		u32   priority	:  8,
++			__reserved_1	: 24;
++		u32 __reserved_2[3];
++	} ppr;
++
++/*0B0*/	struct { /* End Of Interrupt Register */
++		u32   eoi;
++		u32 __reserved[3];
++	} eoi;
++
++/*0C0*/	struct { u32 __reserved[4]; } __reserved_07;
++
++/*0D0*/	struct { /* Logical Destination Register */
++		u32   __reserved_1	: 24,
++			logical_dest	:  8;
++		u32 __reserved_2[3];
++	} ldr;
++
++/*0E0*/	struct { /* Destination Format Register */
++		u32   __reserved_1	: 28,
++			model		:  4;
++		u32 __reserved_2[3];
++	} dfr;
++
++/*0F0*/	struct { /* Spurious Interrupt Vector Register */
++		u32	spurious_vector	:  8,
++			apic_enabled	:  1,
++			focus_cpu	:  1,
++			__reserved_2	: 22;
++		u32 __reserved_3[3];
++	} svr;
++
++/*100*/	struct { /* In Service Register */
++/*170*/		u32 bitfield;
++		u32 __reserved[3];
++	} isr [8];
++
++/*180*/	struct { /* Trigger Mode Register */
++/*1F0*/		u32 bitfield;
++		u32 __reserved[3];
++	} tmr [8];
++
++/*200*/	struct { /* Interrupt Request Register */
++/*270*/		u32 bitfield;
++		u32 __reserved[3];
++	} irr [8];
++
++/*280*/	union { /* Error Status Register */
++		struct {
++			u32   send_cs_error			:  1,
++				receive_cs_error		:  1,
++				send_accept_error		:  1,
++				receive_accept_error		:  1,
++				__reserved_1			:  1,
++				send_illegal_vector		:  1,
++				receive_illegal_vector		:  1,
++				illegal_register_address	:  1,
++				__reserved_2			: 24;
++			u32 __reserved_3[3];
++		} error_bits;
++		struct {
++			u32 errors;
++			u32 __reserved_3[3];
++		} all_errors;
++	} esr;
++
++/*290*/	struct { u32 __reserved[4]; } __reserved_08;
++
++/*2A0*/	struct { u32 __reserved[4]; } __reserved_09;
++
++/*2B0*/	struct { u32 __reserved[4]; } __reserved_10;
++
++/*2C0*/	struct { u32 __reserved[4]; } __reserved_11;
++
++/*2D0*/	struct { u32 __reserved[4]; } __reserved_12;
++
++/*2E0*/	struct { u32 __reserved[4]; } __reserved_13;
++
++/*2F0*/	struct { u32 __reserved[4]; } __reserved_14;
++
++/*300*/	struct { /* Interrupt Command Register 1 */
++		u32   vector			:  8,
++			delivery_mode		:  3,
++			destination_mode	:  1,
++			delivery_status		:  1,
++			__reserved_1		:  1,
++			level			:  1,
++			trigger			:  1,
++			__reserved_2		:  2,
++			shorthand		:  2,
++			__reserved_3		:  12;
++		u32 __reserved_4[3];
++	} icr1;
++
++/*310*/	struct { /* Interrupt Command Register 2 */
++		union {
++			u32   __reserved_1	: 24,
++				phys_dest	:  4,
++				__reserved_2	:  4;
++			u32   __reserved_3	: 24,
++				logical_dest	:  8;
++		} dest;
++		u32 __reserved_4[3];
++	} icr2;
++
++/*320*/	struct { /* LVT - Timer */
++		u32   vector		:  8,
++			__reserved_1	:  4,
++			delivery_status	:  1,
++			__reserved_2	:  3,
++			mask		:  1,
++			timer_mode	:  1,
++			__reserved_3	: 14;
++		u32 __reserved_4[3];
++	} lvt_timer;
++
++/*330*/	struct { /* LVT - Thermal Sensor */
++		u32  vector		:  8,
++			delivery_mode	:  3,
++			__reserved_1	:  1,
++			delivery_status	:  1,
++			__reserved_2	:  3,
++			mask		:  1,
++			__reserved_3	: 15;
++		u32 __reserved_4[3];
++	} lvt_thermal;
++
++/*340*/	struct { /* LVT - Performance Counter */
++		u32   vector		:  8,
++			delivery_mode	:  3,
++			__reserved_1	:  1,
++			delivery_status	:  1,
++			__reserved_2	:  3,
++			mask		:  1,
++			__reserved_3	: 15;
++		u32 __reserved_4[3];
++	} lvt_pc;
++
++/*350*/	struct { /* LVT - LINT0 */
++		u32   vector		:  8,
++			delivery_mode	:  3,
++			__reserved_1	:  1,
++			delivery_status	:  1,
++			polarity	:  1,
++			remote_irr	:  1,
++			trigger		:  1,
++			mask		:  1,
++			__reserved_2	: 15;
++		u32 __reserved_3[3];
++	} lvt_lint0;
++
++/*360*/	struct { /* LVT - LINT1 */
++		u32   vector		:  8,
++			delivery_mode	:  3,
++			__reserved_1	:  1,
++			delivery_status	:  1,
++			polarity	:  1,
++			remote_irr	:  1,
++			trigger		:  1,
++			mask		:  1,
++			__reserved_2	: 15;
++		u32 __reserved_3[3];
++	} lvt_lint1;
++
++/*370*/	struct { /* LVT - Error */
++		u32   vector		:  8,
++			__reserved_1	:  4,
++			delivery_status	:  1,
++			__reserved_2	:  3,
++			mask		:  1,
++			__reserved_3	: 15;
++		u32 __reserved_4[3];
++	} lvt_error;
++
++/*380*/	struct { /* Timer Initial Count Register */
++		u32   initial_count;
++		u32 __reserved_2[3];
++	} timer_icr;
++
++/*390*/	const
++	struct { /* Timer Current Count Register */
++		u32   curr_count;
++		u32 __reserved_2[3];
++	} timer_ccr;
++
++/*3A0*/	struct { u32 __reserved[4]; } __reserved_16;
++
++/*3B0*/	struct { u32 __reserved[4]; } __reserved_17;
++
++/*3C0*/	struct { u32 __reserved[4]; } __reserved_18;
++
++/*3D0*/	struct { u32 __reserved[4]; } __reserved_19;
++
++/*3E0*/	struct { /* Timer Divide Configuration Register */
++		u32   divisor		:  4,
++			__reserved_1	: 28;
++		u32 __reserved_2[3];
++	} timer_dcr;
++
++/*3F0*/	struct { u32 __reserved[4]; } __reserved_20;
++
++} __attribute__ ((packed));
++
++#undef u32
++
++#define BAD_APICID 0xFFu
++
+ #endif
+--- a/include/asm-x86/apicdef_32.h
++++ /dev/null
+@@ -1,375 +0,0 @@
+-#ifndef __ASM_APICDEF_H
+-#define __ASM_APICDEF_H
+-
+-/*
+- * Constants for various Intel APICs. (local APIC, IOAPIC, etc.)
+- *
+- * Alan Cox <Alan.Cox@linux.org>, 1995.
+- * Ingo Molnar <mingo@redhat.com>, 1999, 2000
+- */
+-
+-#define		APIC_DEFAULT_PHYS_BASE	0xfee00000
+- 
+-#define		APIC_ID		0x20
+-#define		APIC_LVR	0x30
+-#define			APIC_LVR_MASK		0xFF00FF
+-#define			GET_APIC_VERSION(x)	((x)&0xFF)
+-#define			GET_APIC_MAXLVT(x)	(((x)>>16)&0xFF)
+-#define			APIC_INTEGRATED(x)	((x)&0xF0)
+-#define			APIC_XAPIC(x)		((x) >= 0x14)
+-#define		APIC_TASKPRI	0x80
+-#define			APIC_TPRI_MASK		0xFF
+-#define		APIC_ARBPRI	0x90
+-#define			APIC_ARBPRI_MASK	0xFF
+-#define		APIC_PROCPRI	0xA0
+-#define		APIC_EOI	0xB0
+-#define			APIC_EIO_ACK		0x0		/* Write this to the EOI register */
+-#define		APIC_RRR	0xC0
+-#define		APIC_LDR	0xD0
+-#define			APIC_LDR_MASK		(0xFF<<24)
+-#define			GET_APIC_LOGICAL_ID(x)	(((x)>>24)&0xFF)
+-#define			SET_APIC_LOGICAL_ID(x)	(((x)<<24))
+-#define			APIC_ALL_CPUS		0xFF
+-#define		APIC_DFR	0xE0
+-#define			APIC_DFR_CLUSTER		0x0FFFFFFFul
+-#define			APIC_DFR_FLAT			0xFFFFFFFFul
+-#define		APIC_SPIV	0xF0
+-#define			APIC_SPIV_FOCUS_DISABLED	(1<<9)
+-#define			APIC_SPIV_APIC_ENABLED		(1<<8)
+-#define		APIC_ISR	0x100
+-#define         APIC_ISR_NR     0x8     /* Number of 32 bit ISR registers. */
+-#define		APIC_TMR	0x180
+-#define 	APIC_IRR	0x200
+-#define 	APIC_ESR	0x280
+-#define			APIC_ESR_SEND_CS	0x00001
+-#define			APIC_ESR_RECV_CS	0x00002
+-#define			APIC_ESR_SEND_ACC	0x00004
+-#define			APIC_ESR_RECV_ACC	0x00008
+-#define			APIC_ESR_SENDILL	0x00020
+-#define			APIC_ESR_RECVILL	0x00040
+-#define			APIC_ESR_ILLREGA	0x00080
+-#define		APIC_ICR	0x300
+-#define			APIC_DEST_SELF		0x40000
+-#define			APIC_DEST_ALLINC	0x80000
+-#define			APIC_DEST_ALLBUT	0xC0000
+-#define			APIC_ICR_RR_MASK	0x30000
+-#define			APIC_ICR_RR_INVALID	0x00000
+-#define			APIC_ICR_RR_INPROG	0x10000
+-#define			APIC_ICR_RR_VALID	0x20000
+-#define			APIC_INT_LEVELTRIG	0x08000
+-#define			APIC_INT_ASSERT		0x04000
+-#define			APIC_ICR_BUSY		0x01000
+-#define			APIC_DEST_LOGICAL	0x00800
+-#define			APIC_DM_FIXED		0x00000
+-#define			APIC_DM_LOWEST		0x00100
+-#define			APIC_DM_SMI		0x00200
+-#define			APIC_DM_REMRD		0x00300
+-#define			APIC_DM_NMI		0x00400
+-#define			APIC_DM_INIT		0x00500
+-#define			APIC_DM_STARTUP		0x00600
+-#define			APIC_DM_EXTINT		0x00700
+-#define			APIC_VECTOR_MASK	0x000FF
+-#define		APIC_ICR2	0x310
+-#define			GET_APIC_DEST_FIELD(x)	(((x)>>24)&0xFF)
+-#define			SET_APIC_DEST_FIELD(x)	((x)<<24)
+-#define		APIC_LVTT	0x320
+-#define		APIC_LVTTHMR	0x330
+-#define		APIC_LVTPC	0x340
+-#define		APIC_LVT0	0x350
+-#define			APIC_LVT_TIMER_BASE_MASK	(0x3<<18)
+-#define			GET_APIC_TIMER_BASE(x)		(((x)>>18)&0x3)
+-#define			SET_APIC_TIMER_BASE(x)		(((x)<<18))
+-#define			APIC_TIMER_BASE_CLKIN		0x0
+-#define			APIC_TIMER_BASE_TMBASE		0x1
+-#define			APIC_TIMER_BASE_DIV		0x2
+-#define			APIC_LVT_TIMER_PERIODIC		(1<<17)
+-#define			APIC_LVT_MASKED			(1<<16)
+-#define			APIC_LVT_LEVEL_TRIGGER		(1<<15)
+-#define			APIC_LVT_REMOTE_IRR		(1<<14)
+-#define			APIC_INPUT_POLARITY		(1<<13)
+-#define			APIC_SEND_PENDING		(1<<12)
+-#define			APIC_MODE_MASK			0x700
+-#define			GET_APIC_DELIVERY_MODE(x)	(((x)>>8)&0x7)
+-#define			SET_APIC_DELIVERY_MODE(x,y)	(((x)&~0x700)|((y)<<8))
+-#define				APIC_MODE_FIXED		0x0
+-#define				APIC_MODE_NMI		0x4
+-#define				APIC_MODE_EXTINT	0x7
+-#define 	APIC_LVT1	0x360
+-#define		APIC_LVTERR	0x370
+-#define		APIC_TMICT	0x380
+-#define		APIC_TMCCT	0x390
+-#define		APIC_TDCR	0x3E0
+-#define			APIC_TDR_DIV_TMBASE	(1<<2)
+-#define			APIC_TDR_DIV_1		0xB
+-#define			APIC_TDR_DIV_2		0x0
+-#define			APIC_TDR_DIV_4		0x1
+-#define			APIC_TDR_DIV_8		0x2
+-#define			APIC_TDR_DIV_16		0x3
+-#define			APIC_TDR_DIV_32		0x8
+-#define			APIC_TDR_DIV_64		0x9
+-#define			APIC_TDR_DIV_128	0xA
+-
+-#define APIC_BASE (fix_to_virt(FIX_APIC_BASE))
+-
+-#define MAX_IO_APICS 64
+-
+-/*
+- * the local APIC register structure, memory mapped. Not terribly well
+- * tested, but we might eventually use this one in the future - the
+- * problem why we cannot use it right now is the P5 APIC, it has an
+- * errata which cannot take 8-bit reads and writes, only 32-bit ones ...
+- */
+-#define u32 unsigned int
+-
+-
+-struct local_apic {
+-
+-/*000*/	struct { u32 __reserved[4]; } __reserved_01;
+-
+-/*010*/	struct { u32 __reserved[4]; } __reserved_02;
+-
+-/*020*/	struct { /* APIC ID Register */
+-		u32   __reserved_1	: 24,
+-			phys_apic_id	:  4,
+-			__reserved_2	:  4;
+-		u32 __reserved[3];
+-	} id;
+-
+-/*030*/	const
+-	struct { /* APIC Version Register */
+-		u32   version		:  8,
+-			__reserved_1	:  8,
+-			max_lvt		:  8,
+-			__reserved_2	:  8;
+-		u32 __reserved[3];
+-	} version;
+-
+-/*040*/	struct { u32 __reserved[4]; } __reserved_03;
+-
+-/*050*/	struct { u32 __reserved[4]; } __reserved_04;
+-
+-/*060*/	struct { u32 __reserved[4]; } __reserved_05;
+-
+-/*070*/	struct { u32 __reserved[4]; } __reserved_06;
+-
+-/*080*/	struct { /* Task Priority Register */
+-		u32   priority	:  8,
+-			__reserved_1	: 24;
+-		u32 __reserved_2[3];
+-	} tpr;
+-
+-/*090*/	const
+-	struct { /* Arbitration Priority Register */
+-		u32   priority	:  8,
+-			__reserved_1	: 24;
+-		u32 __reserved_2[3];
+-	} apr;
+-
+-/*0A0*/	const
+-	struct { /* Processor Priority Register */
+-		u32   priority	:  8,
+-			__reserved_1	: 24;
+-		u32 __reserved_2[3];
+-	} ppr;
+-
+-/*0B0*/	struct { /* End Of Interrupt Register */
+-		u32   eoi;
+-		u32 __reserved[3];
+-	} eoi;
+-
+-/*0C0*/	struct { u32 __reserved[4]; } __reserved_07;
+-
+-/*0D0*/	struct { /* Logical Destination Register */
+-		u32   __reserved_1	: 24,
+-			logical_dest	:  8;
+-		u32 __reserved_2[3];
+-	} ldr;
+-
+-/*0E0*/	struct { /* Destination Format Register */
+-		u32   __reserved_1	: 28,
+-			model		:  4;
+-		u32 __reserved_2[3];
+-	} dfr;
+-
+-/*0F0*/	struct { /* Spurious Interrupt Vector Register */
+-		u32	spurious_vector	:  8,
+-			apic_enabled	:  1,
+-			focus_cpu	:  1,
+-			__reserved_2	: 22;
+-		u32 __reserved_3[3];
+-	} svr;
+-
+-/*100*/	struct { /* In Service Register */
+-/*170*/		u32 bitfield;
+-		u32 __reserved[3];
+-	} isr [8];
+-
+-/*180*/	struct { /* Trigger Mode Register */
+-/*1F0*/		u32 bitfield;
+-		u32 __reserved[3];
+-	} tmr [8];
+-
+-/*200*/	struct { /* Interrupt Request Register */
+-/*270*/		u32 bitfield;
+-		u32 __reserved[3];
+-	} irr [8];
+-
+-/*280*/	union { /* Error Status Register */
+-		struct {
+-			u32   send_cs_error			:  1,
+-				receive_cs_error		:  1,
+-				send_accept_error		:  1,
+-				receive_accept_error		:  1,
+-				__reserved_1			:  1,
+-				send_illegal_vector		:  1,
+-				receive_illegal_vector		:  1,
+-				illegal_register_address	:  1,
+-				__reserved_2			: 24;
+-			u32 __reserved_3[3];
+-		} error_bits;
+-		struct {
+-			u32 errors;
+-			u32 __reserved_3[3];
+-		} all_errors;
+-	} esr;
+-
+-/*290*/	struct { u32 __reserved[4]; } __reserved_08;
+-
+-/*2A0*/	struct { u32 __reserved[4]; } __reserved_09;
+-
+-/*2B0*/	struct { u32 __reserved[4]; } __reserved_10;
+-
+-/*2C0*/	struct { u32 __reserved[4]; } __reserved_11;
+-
+-/*2D0*/	struct { u32 __reserved[4]; } __reserved_12;
+-
+-/*2E0*/	struct { u32 __reserved[4]; } __reserved_13;
+-
+-/*2F0*/	struct { u32 __reserved[4]; } __reserved_14;
+-
+-/*300*/	struct { /* Interrupt Command Register 1 */
+-		u32   vector			:  8,
+-			delivery_mode		:  3,
+-			destination_mode	:  1,
+-			delivery_status		:  1,
+-			__reserved_1		:  1,
+-			level			:  1,
+-			trigger			:  1,
+-			__reserved_2		:  2,
+-			shorthand		:  2,
+-			__reserved_3		:  12;
+-		u32 __reserved_4[3];
+-	} icr1;
+-
+-/*310*/	struct { /* Interrupt Command Register 2 */
+-		union {
+-			u32   __reserved_1	: 24,
+-				phys_dest	:  4,
+-				__reserved_2	:  4;
+-			u32   __reserved_3	: 24,
+-				logical_dest	:  8;
+-		} dest;
+-		u32 __reserved_4[3];
+-	} icr2;
+-
+-/*320*/	struct { /* LVT - Timer */
+-		u32   vector		:  8,
+-			__reserved_1	:  4,
+-			delivery_status	:  1,
+-			__reserved_2	:  3,
+-			mask		:  1,
+-			timer_mode	:  1,
+-			__reserved_3	: 14;
+-		u32 __reserved_4[3];
+-	} lvt_timer;
+-
+-/*330*/	struct { /* LVT - Thermal Sensor */
+-		u32  vector		:  8,
+-			delivery_mode	:  3,
+-			__reserved_1	:  1,
+-			delivery_status	:  1,
+-			__reserved_2	:  3,
+-			mask		:  1,
+-			__reserved_3	: 15;
+-		u32 __reserved_4[3];
+-	} lvt_thermal;
+-
+-/*340*/	struct { /* LVT - Performance Counter */
+-		u32   vector		:  8,
+-			delivery_mode	:  3,
+-			__reserved_1	:  1,
+-			delivery_status	:  1,
+-			__reserved_2	:  3,
+-			mask		:  1,
+-			__reserved_3	: 15;
+-		u32 __reserved_4[3];
+-	} lvt_pc;
+-
+-/*350*/	struct { /* LVT - LINT0 */
+-		u32   vector		:  8,
+-			delivery_mode	:  3,
+-			__reserved_1	:  1,
+-			delivery_status	:  1,
+-			polarity	:  1,
+-			remote_irr	:  1,
+-			trigger		:  1,
+-			mask		:  1,
+-			__reserved_2	: 15;
+-		u32 __reserved_3[3];
+-	} lvt_lint0;
+-
+-/*360*/	struct { /* LVT - LINT1 */
+-		u32   vector		:  8,
+-			delivery_mode	:  3,
+-			__reserved_1	:  1,
+-			delivery_status	:  1,
+-			polarity	:  1,
+-			remote_irr	:  1,
+-			trigger		:  1,
+-			mask		:  1,
+-			__reserved_2	: 15;
+-		u32 __reserved_3[3];
+-	} lvt_lint1;
+-
+-/*370*/	struct { /* LVT - Error */
+-		u32   vector		:  8,
+-			__reserved_1	:  4,
+-			delivery_status	:  1,
+-			__reserved_2	:  3,
+-			mask		:  1,
+-			__reserved_3	: 15;
+-		u32 __reserved_4[3];
+-	} lvt_error;
+-
+-/*380*/	struct { /* Timer Initial Count Register */
+-		u32   initial_count;
+-		u32 __reserved_2[3];
+-	} timer_icr;
+-
+-/*390*/	const
+-	struct { /* Timer Current Count Register */
+-		u32   curr_count;
+-		u32 __reserved_2[3];
+-	} timer_ccr;
+-
+-/*3A0*/	struct { u32 __reserved[4]; } __reserved_16;
+-
+-/*3B0*/	struct { u32 __reserved[4]; } __reserved_17;
+-
+-/*3C0*/	struct { u32 __reserved[4]; } __reserved_18;
+-
+-/*3D0*/	struct { u32 __reserved[4]; } __reserved_19;
+-
+-/*3E0*/	struct { /* Timer Divide Configuration Register */
+-		u32   divisor		:  4,
+-			__reserved_1	: 28;
+-		u32 __reserved_2[3];
+-	} timer_dcr;
+-
+-/*3F0*/	struct { u32 __reserved[4]; } __reserved_20;
+-
+-} __attribute__ ((packed));
+-
+-#undef u32
+-
+-#endif
+--- a/include/asm-x86/apicdef_64.h
++++ /dev/null
+@@ -1,392 +0,0 @@
+-#ifndef __ASM_APICDEF_H
+-#define __ASM_APICDEF_H
+-
+-/*
+- * Constants for various Intel APICs. (local APIC, IOAPIC, etc.)
+- *
+- * Alan Cox <Alan.Cox@linux.org>, 1995.
+- * Ingo Molnar <mingo@redhat.com>, 1999, 2000
+- */
+-
+-#define		APIC_DEFAULT_PHYS_BASE	0xfee00000
+- 
+-#define		APIC_ID		0x20
+-#define			APIC_ID_MASK		(0xFFu<<24)
+-#define			GET_APIC_ID(x)		(((x)>>24)&0xFFu)
+-#define			SET_APIC_ID(x)		(((x)<<24))
+-#define		APIC_LVR	0x30
+-#define			APIC_LVR_MASK		0xFF00FF
+-#define			GET_APIC_VERSION(x)	((x)&0xFFu)
+-#define			GET_APIC_MAXLVT(x)	(((x)>>16)&0xFFu)
+-#define			APIC_INTEGRATED(x)	((x)&0xF0u)
+-#define		APIC_TASKPRI	0x80
+-#define			APIC_TPRI_MASK		0xFFu
+-#define		APIC_ARBPRI	0x90
+-#define			APIC_ARBPRI_MASK	0xFFu
+-#define		APIC_PROCPRI	0xA0
+-#define		APIC_EOI	0xB0
+-#define			APIC_EIO_ACK		0x0		/* Write this to the EOI register */
+-#define		APIC_RRR	0xC0
+-#define		APIC_LDR	0xD0
+-#define			APIC_LDR_MASK		(0xFFu<<24)
+-#define			GET_APIC_LOGICAL_ID(x)	(((x)>>24)&0xFFu)
+-#define			SET_APIC_LOGICAL_ID(x)	(((x)<<24))
+-#define			APIC_ALL_CPUS		0xFFu
+-#define		APIC_DFR	0xE0
+-#define			APIC_DFR_CLUSTER		0x0FFFFFFFul
+-#define			APIC_DFR_FLAT			0xFFFFFFFFul
+-#define		APIC_SPIV	0xF0
+-#define			APIC_SPIV_FOCUS_DISABLED	(1<<9)
+-#define			APIC_SPIV_APIC_ENABLED		(1<<8)
+-#define		APIC_ISR	0x100
+-#define		APIC_ISR_NR	0x8	/* Number of 32 bit ISR registers. */
+-#define		APIC_TMR	0x180
+-#define 	APIC_IRR	0x200
+-#define 	APIC_ESR	0x280
+-#define			APIC_ESR_SEND_CS	0x00001
+-#define			APIC_ESR_RECV_CS	0x00002
+-#define			APIC_ESR_SEND_ACC	0x00004
+-#define			APIC_ESR_RECV_ACC	0x00008
+-#define			APIC_ESR_SENDILL	0x00020
+-#define			APIC_ESR_RECVILL	0x00040
+-#define			APIC_ESR_ILLREGA	0x00080
+-#define		APIC_ICR	0x300
+-#define			APIC_DEST_SELF		0x40000
+-#define			APIC_DEST_ALLINC	0x80000
+-#define			APIC_DEST_ALLBUT	0xC0000
+-#define			APIC_ICR_RR_MASK	0x30000
+-#define			APIC_ICR_RR_INVALID	0x00000
+-#define			APIC_ICR_RR_INPROG	0x10000
+-#define			APIC_ICR_RR_VALID	0x20000
+-#define			APIC_INT_LEVELTRIG	0x08000
+-#define			APIC_INT_ASSERT		0x04000
+-#define			APIC_ICR_BUSY		0x01000
+-#define			APIC_DEST_LOGICAL	0x00800
+-#define			APIC_DEST_PHYSICAL	0x00000
+-#define			APIC_DM_FIXED		0x00000
+-#define			APIC_DM_LOWEST		0x00100
+-#define			APIC_DM_SMI		0x00200
+-#define			APIC_DM_REMRD		0x00300
+-#define			APIC_DM_NMI		0x00400
+-#define			APIC_DM_INIT		0x00500
+-#define			APIC_DM_STARTUP		0x00600
+-#define			APIC_DM_EXTINT		0x00700
+-#define			APIC_VECTOR_MASK	0x000FF
+-#define		APIC_ICR2	0x310
+-#define			GET_APIC_DEST_FIELD(x)	(((x)>>24)&0xFF)
+-#define			SET_APIC_DEST_FIELD(x)	((x)<<24)
+-#define		APIC_LVTT	0x320
+-#define		APIC_LVTTHMR	0x330
+-#define		APIC_LVTPC	0x340
+-#define		APIC_LVT0	0x350
+-#define			APIC_LVT_TIMER_BASE_MASK	(0x3<<18)
+-#define			GET_APIC_TIMER_BASE(x)		(((x)>>18)&0x3)
+-#define			SET_APIC_TIMER_BASE(x)		(((x)<<18))
+-#define			APIC_TIMER_BASE_CLKIN		0x0
+-#define			APIC_TIMER_BASE_TMBASE		0x1
+-#define			APIC_TIMER_BASE_DIV		0x2
+-#define			APIC_LVT_TIMER_PERIODIC		(1<<17)
+-#define			APIC_LVT_MASKED			(1<<16)
+-#define			APIC_LVT_LEVEL_TRIGGER		(1<<15)
+-#define			APIC_LVT_REMOTE_IRR		(1<<14)
+-#define			APIC_INPUT_POLARITY		(1<<13)
+-#define			APIC_SEND_PENDING		(1<<12)
+-#define			APIC_MODE_MASK			0x700
+-#define			GET_APIC_DELIVERY_MODE(x)	(((x)>>8)&0x7)
+-#define			SET_APIC_DELIVERY_MODE(x,y)	(((x)&~0x700)|((y)<<8))
+-#define				APIC_MODE_FIXED		0x0
+-#define				APIC_MODE_NMI		0x4
+-#define				APIC_MODE_EXTINT	0x7
+-#define 	APIC_LVT1	0x360
+-#define		APIC_LVTERR	0x370
+-#define		APIC_TMICT	0x380
+-#define		APIC_TMCCT	0x390
+-#define		APIC_TDCR	0x3E0
+-#define			APIC_TDR_DIV_TMBASE	(1<<2)
+-#define			APIC_TDR_DIV_1		0xB
+-#define			APIC_TDR_DIV_2		0x0
+-#define			APIC_TDR_DIV_4		0x1
+-#define			APIC_TDR_DIV_8		0x2
+-#define			APIC_TDR_DIV_16		0x3
+-#define			APIC_TDR_DIV_32		0x8
+-#define			APIC_TDR_DIV_64		0x9
+-#define			APIC_TDR_DIV_128	0xA
+-
+-#define APIC_BASE (fix_to_virt(FIX_APIC_BASE))
+-
+-#define MAX_IO_APICS 128
+-#define MAX_LOCAL_APIC 256
+-
+-/*
+- * All x86-64 systems are xAPIC compatible.
+- * In the following, "apicid" is a physical APIC ID.
+- */
+-#define XAPIC_DEST_CPUS_SHIFT	4
+-#define XAPIC_DEST_CPUS_MASK	((1u << XAPIC_DEST_CPUS_SHIFT) - 1)
+-#define XAPIC_DEST_CLUSTER_MASK	(XAPIC_DEST_CPUS_MASK << XAPIC_DEST_CPUS_SHIFT)
+-#define APIC_CLUSTER(apicid)	((apicid) & XAPIC_DEST_CLUSTER_MASK)
+-#define APIC_CLUSTERID(apicid)	(APIC_CLUSTER(apicid) >> XAPIC_DEST_CPUS_SHIFT)
+-#define APIC_CPUID(apicid)	((apicid) & XAPIC_DEST_CPUS_MASK)
+-#define NUM_APIC_CLUSTERS	((BAD_APICID + 1) >> XAPIC_DEST_CPUS_SHIFT)
+-
+-/*
+- * the local APIC register structure, memory mapped. Not terribly well
+- * tested, but we might eventually use this one in the future - the
+- * problem why we cannot use it right now is the P5 APIC, it has an
+- * errata which cannot take 8-bit reads and writes, only 32-bit ones ...
+- */
+-#define u32 unsigned int
+-
+-struct local_apic {
+-
+-/*000*/	struct { u32 __reserved[4]; } __reserved_01;
+-
+-/*010*/	struct { u32 __reserved[4]; } __reserved_02;
+-
+-/*020*/	struct { /* APIC ID Register */
+-		u32   __reserved_1	: 24,
+-			phys_apic_id	:  4,
+-			__reserved_2	:  4;
+-		u32 __reserved[3];
+-	} id;
+-
+-/*030*/	const
+-	struct { /* APIC Version Register */
+-		u32   version		:  8,
+-			__reserved_1	:  8,
+-			max_lvt		:  8,
+-			__reserved_2	:  8;
+-		u32 __reserved[3];
+-	} version;
+-
+-/*040*/	struct { u32 __reserved[4]; } __reserved_03;
+-
+-/*050*/	struct { u32 __reserved[4]; } __reserved_04;
+-
+-/*060*/	struct { u32 __reserved[4]; } __reserved_05;
+-
+-/*070*/	struct { u32 __reserved[4]; } __reserved_06;
+-
+-/*080*/	struct { /* Task Priority Register */
+-		u32   priority	:  8,
+-			__reserved_1	: 24;
+-		u32 __reserved_2[3];
+-	} tpr;
+-
+-/*090*/	const
+-	struct { /* Arbitration Priority Register */
+-		u32   priority	:  8,
+-			__reserved_1	: 24;
+-		u32 __reserved_2[3];
+-	} apr;
+-
+-/*0A0*/	const
+-	struct { /* Processor Priority Register */
+-		u32   priority	:  8,
+-			__reserved_1	: 24;
+-		u32 __reserved_2[3];
+-	} ppr;
+-
+-/*0B0*/	struct { /* End Of Interrupt Register */
+-		u32   eoi;
+-		u32 __reserved[3];
+-	} eoi;
+-
+-/*0C0*/	struct { u32 __reserved[4]; } __reserved_07;
+-
+-/*0D0*/	struct { /* Logical Destination Register */
+-		u32   __reserved_1	: 24,
+-			logical_dest	:  8;
+-		u32 __reserved_2[3];
+-	} ldr;
+-
+-/*0E0*/	struct { /* Destination Format Register */
+-		u32   __reserved_1	: 28,
+-			model		:  4;
+-		u32 __reserved_2[3];
+-	} dfr;
+-
+-/*0F0*/	struct { /* Spurious Interrupt Vector Register */
+-		u32	spurious_vector	:  8,
+-			apic_enabled	:  1,
+-			focus_cpu	:  1,
+-			__reserved_2	: 22;
+-		u32 __reserved_3[3];
+-	} svr;
+-
+-/*100*/	struct { /* In Service Register */
+-/*170*/		u32 bitfield;
+-		u32 __reserved[3];
+-	} isr [8];
+-
+-/*180*/	struct { /* Trigger Mode Register */
+-/*1F0*/		u32 bitfield;
+-		u32 __reserved[3];
+-	} tmr [8];
+-
+-/*200*/	struct { /* Interrupt Request Register */
+-/*270*/		u32 bitfield;
+-		u32 __reserved[3];
+-	} irr [8];
+-
+-/*280*/	union { /* Error Status Register */
+-		struct {
+-			u32   send_cs_error			:  1,
+-				receive_cs_error		:  1,
+-				send_accept_error		:  1,
+-				receive_accept_error		:  1,
+-				__reserved_1			:  1,
+-				send_illegal_vector		:  1,
+-				receive_illegal_vector		:  1,
+-				illegal_register_address	:  1,
+-				__reserved_2			: 24;
+-			u32 __reserved_3[3];
+-		} error_bits;
+-		struct {
+-			u32 errors;
+-			u32 __reserved_3[3];
+-		} all_errors;
+-	} esr;
+-
+-/*290*/	struct { u32 __reserved[4]; } __reserved_08;
+-
+-/*2A0*/	struct { u32 __reserved[4]; } __reserved_09;
+-
+-/*2B0*/	struct { u32 __reserved[4]; } __reserved_10;
+-
+-/*2C0*/	struct { u32 __reserved[4]; } __reserved_11;
+-
+-/*2D0*/	struct { u32 __reserved[4]; } __reserved_12;
+-
+-/*2E0*/	struct { u32 __reserved[4]; } __reserved_13;
+-
+-/*2F0*/	struct { u32 __reserved[4]; } __reserved_14;
+-
+-/*300*/	struct { /* Interrupt Command Register 1 */
+-		u32   vector			:  8,
+-			delivery_mode		:  3,
+-			destination_mode	:  1,
+-			delivery_status		:  1,
+-			__reserved_1		:  1,
+-			level			:  1,
+-			trigger			:  1,
+-			__reserved_2		:  2,
+-			shorthand		:  2,
+-			__reserved_3		:  12;
+-		u32 __reserved_4[3];
+-	} icr1;
+-
+-/*310*/	struct { /* Interrupt Command Register 2 */
+-		union {
+-			u32   __reserved_1	: 24,
+-				phys_dest	:  4,
+-				__reserved_2	:  4;
+-			u32   __reserved_3	: 24,
+-				logical_dest	:  8;
+-		} dest;
+-		u32 __reserved_4[3];
+-	} icr2;
+-
+-/*320*/	struct { /* LVT - Timer */
+-		u32   vector		:  8,
+-			__reserved_1	:  4,
+-			delivery_status	:  1,
+-			__reserved_2	:  3,
+-			mask		:  1,
+-			timer_mode	:  1,
+-			__reserved_3	: 14;
+-		u32 __reserved_4[3];
+-	} lvt_timer;
+-
+-/*330*/	struct { /* LVT - Thermal Sensor */
+-		u32  vector		:  8,
+-			delivery_mode	:  3,
+-			__reserved_1	:  1,
+-			delivery_status	:  1,
+-			__reserved_2	:  3,
+-			mask		:  1,
+-			__reserved_3	: 15;
+-		u32 __reserved_4[3];
+-	} lvt_thermal;
+-
+-/*340*/	struct { /* LVT - Performance Counter */
+-		u32   vector		:  8,
+-			delivery_mode	:  3,
+-			__reserved_1	:  1,
+-			delivery_status	:  1,
+-			__reserved_2	:  3,
+-			mask		:  1,
+-			__reserved_3	: 15;
+-		u32 __reserved_4[3];
+-	} lvt_pc;
+-
+-/*350*/	struct { /* LVT - LINT0 */
+-		u32   vector		:  8,
+-			delivery_mode	:  3,
+-			__reserved_1	:  1,
+-			delivery_status	:  1,
+-			polarity	:  1,
+-			remote_irr	:  1,
+-			trigger		:  1,
+-			mask		:  1,
+-			__reserved_2	: 15;
+-		u32 __reserved_3[3];
+-	} lvt_lint0;
+-
+-/*360*/	struct { /* LVT - LINT1 */
+-		u32   vector		:  8,
+-			delivery_mode	:  3,
+-			__reserved_1	:  1,
+-			delivery_status	:  1,
+-			polarity	:  1,
+-			remote_irr	:  1,
+-			trigger		:  1,
+-			mask		:  1,
+-			__reserved_2	: 15;
+-		u32 __reserved_3[3];
+-	} lvt_lint1;
+-
+-/*370*/	struct { /* LVT - Error */
+-		u32   vector		:  8,
+-			__reserved_1	:  4,
+-			delivery_status	:  1,
+-			__reserved_2	:  3,
+-			mask		:  1,
+-			__reserved_3	: 15;
+-		u32 __reserved_4[3];
+-	} lvt_error;
+-
+-/*380*/	struct { /* Timer Initial Count Register */
+-		u32   initial_count;
+-		u32 __reserved_2[3];
+-	} timer_icr;
+-
+-/*390*/	const
+-	struct { /* Timer Current Count Register */
+-		u32   curr_count;
+-		u32 __reserved_2[3];
+-	} timer_ccr;
+-
+-/*3A0*/	struct { u32 __reserved[4]; } __reserved_16;
+-
+-/*3B0*/	struct { u32 __reserved[4]; } __reserved_17;
+-
+-/*3C0*/	struct { u32 __reserved[4]; } __reserved_18;
+-
+-/*3D0*/	struct { u32 __reserved[4]; } __reserved_19;
+-
+-/*3E0*/	struct { /* Timer Divide Configuration Register */
+-		u32   divisor		:  4,
+-			__reserved_1	: 28;
+-		u32 __reserved_2[3];
+-	} timer_dcr;
+-
+-/*3F0*/	struct { u32 __reserved[4]; } __reserved_20;
+-
+-} __attribute__ ((packed));
+-
+-#undef u32
+-
+-#define BAD_APICID 0xFFu
+-
+-#endif
+--- a/include/asm-x86/cpufeature_32.h
++++ b/include/asm-x86/cpufeature_32.h
+@@ -165,6 +165,7 @@
+ #define cpu_has_pebs 		boot_cpu_has(X86_FEATURE_PEBS)
+ #define cpu_has_clflush		boot_cpu_has(X86_FEATURE_CLFLSH)
+ #define cpu_has_bts 		boot_cpu_has(X86_FEATURE_BTS)
++#define cpu_has_arch_perfmon 	boot_cpu_has(X86_FEATURE_ARCH_PERFMON)
+ 
+ #endif /* __ASM_I386_CPUFEATURE_H */
+ 
+--- a/include/asm-x86/hw_irq_64.h
++++ b/include/asm-x86/hw_irq_64.h
+@@ -84,6 +84,7 @@
+  * sources per level' errata.
+  */
+ #define LOCAL_TIMER_VECTOR	0xef
++#define LOCAL_PERFMON_VECTOR	0xee
+ 
+ /*
+  * First APIC vector available to drivers: (vectors 0x30-0xee)
+@@ -91,7 +92,7 @@
+  * levels. (0x80 is the syscall vector)
+  */
+ #define FIRST_DEVICE_VECTOR	(IRQ15_VECTOR + 2)
+-#define FIRST_SYSTEM_VECTOR	0xef   /* duplicated in irq.h */
++#define FIRST_SYSTEM_VECTOR	0xee   /* duplicated in irq.h */
+ 
+ 
+ #ifndef __ASSEMBLY__
+--- a/include/asm-x86/irq_64.h
++++ b/include/asm-x86/irq_64.h
+@@ -29,7 +29,7 @@
+  */
+ #define NR_VECTORS 256
+ 
+-#define FIRST_SYSTEM_VECTOR	0xef   /* duplicated in hw_irq.h */
++#define FIRST_SYSTEM_VECTOR	0xee   /* duplicated in hw_irq.h */
+ 
+ #define NR_IRQS (NR_VECTORS + (32 *NR_CPUS))
+ #define NR_IRQ_VECTORS NR_IRQS
+--- a/include/asm-x86/mach-default/entry_arch.h
++++ b/include/asm-x86/mach-default/entry_arch.h
+@@ -31,4 +31,8 @@ BUILD_INTERRUPT(spurious_interrupt,SPURI
+ BUILD_INTERRUPT(thermal_interrupt,THERMAL_APIC_VECTOR)
+ #endif
+ 
++#ifdef CONFIG_PERFMON
++BUILD_INTERRUPT(pmu_interrupt,LOCAL_PERFMON_VECTOR)
++#endif
++
+ #endif
+--- a/include/asm-x86/mach-default/irq_vectors.h
++++ b/include/asm-x86/mach-default/irq_vectors.h
+@@ -56,6 +56,7 @@
+  * sources per level' errata.
+  */
+ #define LOCAL_TIMER_VECTOR	0xef
++#define LOCAL_PERFMON_VECTOR	0xee
+ 
+ /*
+  * First APIC vector available to drivers: (vectors 0x30-0xee)
+@@ -63,7 +64,7 @@
+  * levels. (0x80 is the syscall vector)
+  */
+ #define FIRST_DEVICE_VECTOR	0x31
+-#define FIRST_SYSTEM_VECTOR	0xef
++#define FIRST_SYSTEM_VECTOR	0xee
+ 
+ #define TIMER_IRQ 0
+ 
+--- a/include/asm-x86/msr-index.h
++++ b/include/asm-x86/msr-index.h
+@@ -76,6 +76,7 @@
+ /* AMD64 MSRs. Not complete. See the architecture manual for a more
+    complete list. */
+ 
++#define MSR_AMD64_NB_CFG		0xc001001f
+ #define MSR_AMD64_IBSFETCHCTL		0xc0011030
+ #define MSR_AMD64_IBSFETCHLINAD		0xc0011031
+ #define MSR_AMD64_IBSFETCHPHYSAD	0xc0011032
+--- a/include/asm-x86/paravirt.h
++++ b/include/asm-x86/paravirt.h
+@@ -150,9 +150,9 @@ struct pv_apic_ops {
+ 	 * Direct APIC operations, principally for VMI.  Ideally
+ 	 * these shouldn't be in this interface.
+ 	 */
+-	void (*apic_write)(unsigned long reg, unsigned long v);
++	void (*apic_write)(unsigned long reg, u32 v);
+ 	void (*apic_write_atomic)(unsigned long reg, unsigned long v);
+-	unsigned long (*apic_read)(unsigned long reg);
++	u32 (*apic_read)(unsigned long reg);
+ 	void (*setup_boot_clock)(void);
+ 	void (*setup_secondary_clock)(void);
+ 
+--- /dev/null
++++ b/include/asm-x86/perfmon.h
+@@ -0,0 +1,520 @@
++/*
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * Copyright (c) 2007 Advanced Micro Devices, Inc.
++ * Contributed by Robert Richter <robert.richter@amd.com>
++ *
++ * This file contains X86 Processor Family specific definitions
++ * for the perfmon interface. This covers P6, Pentium M, P4/Xeon
++ * (32-bit and 64-bit, i.e., EM64T) and AMD X86-64.
++ *
++ * This file MUST never be included directly. Use linux/perfmon.h.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#ifndef _ASM_X86_PERFMON_H_
++#define _ASM_X86_PERFMON_H_
++
++#ifdef __KERNEL__
++
++#ifdef CONFIG_4KSTACKS
++#define PFM_ARCH_PMD_STK_ARG	2
++#define PFM_ARCH_PMC_STK_ARG	2
++#else
++#define PFM_ARCH_PMD_STK_ARG	4 /* about 700 bytes of stack space */
++#define PFM_ARCH_PMC_STK_ARG	4 /* about 200 bytes of stack space */
++#endif
++
++/*
++ * For P4:
++ * - bits 31 - 63 reserved
++ * - T1_OS and T1_USR bits are reserved - set depending on logical proc
++ *      user mode application should use T0_OS and T0_USR to indicate
++ * RSVD: reserved bits must be 1
++ */
++#define PFM_ESCR_RSVD  ~0x000000007ffffffcULL
++
++/*
++ * bitmask for reg_type
++ */
++#define PFM_REGT_NA		0x0000	/* not available */
++#define PFM_REGT_EN		0x0001	/* has enable bit (cleared on ctxsw) */
++#define PFM_REGT_ESCR		0x0002	/* P4: ESCR */
++#define PFM_REGT_CCCR		0x0004	/* P4: CCCR */
++#define PFM_REGT_PEBS		0x0010	/* PEBS related */
++#define PFM_REGT_NOHT		0x0020	/* unavailable with HT */
++#define PFM_REGT_CTR		0x0040	/* counter */
++#define PFM_REGT_OTH		0x0080	/* other type of register */
++#define PFM_REGT_IBS		0x0100	/* IBS register set */
++#define PFM_REGT_IBS_EXT	0x0200	/* IBS extended register set */
++
++/*
++ * This design and the partitioning of resources for SMT (hyper threads)
++ * is very static and limited due to limitations in the number of ESCRs
++ * and CCCRs per group.
++ */
++#define MAX_SMT_ID 1
++
++/*
++ * For extended register information in addition to address that is used
++ * at runtime to figure out the mapping of reg addresses to logical procs
++ * and association of registers to hardware specific features
++ */
++struct pfm_arch_ext_reg {
++	/*
++	 * one each for the logical CPUs.  Index 0 corresponds to T0 and
++	 * index 1 corresponds to T1.  Index 1 can be zero if no T1
++	 * complement reg exists.
++	 */
++	unsigned long addrs[MAX_SMT_ID+1];
++	unsigned int ctr;	/* for CCCR/PERFEVTSEL, associated counter */
++	unsigned int reg_type;
++};
++
++typedef int (*pfm_check_session_t)(struct pfm_context *ctx);
++
++struct pfm_arch_pmu_info {
++	struct pfm_arch_ext_reg pmc_addrs[PFM_MAX_PMCS];
++	struct pfm_arch_ext_reg pmd_addrs[PFM_MAX_PMDS];
++	u64 enable_mask[PFM_PMC_BV]; /* PMC registers with enable bit */
++
++	u16 max_ena;		/* highest enable bit + 1 */
++	u16 flags;		/* PMU feature flags */
++	u16 pebs_ctr_idx;	/* index of PEBS counter for overflow */
++	u16 reserved;		/* for future use */
++
++	/*
++	 * optional callbacks invoked by pfm_arch_*load_context()
++	 */
++	int (*load_context)(struct pfm_context *ctx);
++	int (*unload_context)(struct pfm_context *ctx);
++
++	u16 ibsfetchctl_pmc;	/* AMD: index of IBSFETCHCTL PMC register */
++	u16 ibsfetchctl_pmd;	/* AMD: index of IBSFETCHCTL PMD register */
++	u16 ibsopctl_pmc;	/* AMD: index of IBSOPCTL PMC register */
++	u16 ibsopctl_pmd;	/* AMD: index of IBSOPCTL PMD register */
++	u8  ibs_eilvt_off;	/* AMD: extended interrupt LVT offset */
++	u8  pmu_style;		/* type of PMU: P4, P6, CORE, AMD64 */
++};
++
++/*
++ * X86 PMU style
++ */
++#define PFM_X86_PMU_P4		1 /* Intel P4/Xeon/EM64T processor PMU */
++#define PFM_X86_PMU_P6		2 /* Intel P6/Pentium M */
++#define PFM_X86_PMU_CORE	3 /* Intel Core PMU */
++#define PFM_X86_PMU_AMD64	4 /* AMD64 PMU (K8, family 10h) */
++
++/*
++ * PMU feature flags
++ */
++#define PFM_X86_FL_PMU_DS	0x01	/* Intel: support for Data Save Area (DS) */
++#define PFM_X86_FL_PMU_PEBS	0x02	/* Intel: support PEBS (implies DS) */
++#define PFM_X86_FL_USE_NMI	0x04	/* user asking for NMI */
++#define PFM_X86_FL_NO_SHARING	0x08	/* no sharing with other subsystems */
++#define PFM_X86_FL_SHARING	0x10	/* PMU is being shared */
++#define PFM_X86_FL_IBS		0x20	/* AMD: PMU has IBS support */
++#define PFM_X86_FL_IBS_EXT	0x40	/* AMD: PMU has IBSext support */
++#define PFM_X86_FL_USE_EI	0x80	/* AMD: PMU uses extended interrupts */
++
++/*
++ * architecture specific context extension.
++ * located at: (struct pfm_arch_context *)(ctx+1)
++ */
++
++struct pfm_arch_p4_context {
++	u32	npend_ovfls;	/* P4 NMI #pending ovfls */
++	u32	reserved;
++	u64	povfl_pmds[PFM_PMD_BV]; /* P4 NMI overflowed counters */
++	u64	saved_cccrs[PFM_MAX_PMCS];
++};
++
++struct pfm_x86_context_flags {
++	unsigned int insecure:1;  /* insecure monitoring for non-self session */
++	unsigned int use_pebs:1;  /* PEBS used */
++	unsigned int use_ds:1;    /* DS used */
++	unsigned int reserved:29; /* for future use */
++};
++
++struct pfm_arch_context {
++	u64				saved_real_iip;	/* instr pointer of last NMI intr (ctxsw) */
++	struct pfm_x86_context_flags	flags;		/* arch-specific flags */
++	void				*ds_area;	/* address of DS management area */
++	struct pfm_arch_p4_context	*p4;		/* P4 specific state */
++};
++
++void __pfm_read_reg_p4(const struct pfm_arch_ext_reg *xreg, u64 *val);
++void __pfm_write_reg_p4(const struct pfm_arch_ext_reg *xreg, u64 val);
++
++
++extern int  pfm_arch_init(void);
++extern void pfm_arch_resend_irq(void);
++
++static inline void pfm_arch_serialize(void)
++{}
++
++/*
++ * on x86, the PMDs are already saved by pfm_arch_freeze_pmu()
++ * when entering the PMU interrupt handler, thus, we do not need
++ * to save them again in pfm_switch_sets_from_intr()
++ */
++static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
++						struct pfm_event_set *set)
++{}
++
++/*
++ * in certain situations, ctx may be NULL
++ */
++static inline void pfm_arch_write_pmc(struct pfm_context *ctx, unsigned int cnum, u64 value)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	/*
++	 * we only write to the actual register when monitoring is
++	 * active (pfm_start was issued)
++	 */
++	if (ctx && ctx->flags.started == 0)
++		return;
++
++	PFM_DBG_ovfl("pfm_arch_write_pmc(0x%lx, 0x%Lx)",
++		     pfm_pmu_conf->pmc_desc[cnum].hw_addr,
++		     (unsigned long long) value);
++
++	if (arch_info->pmu_style == PFM_X86_PMU_P4)
++		__pfm_write_reg_p4(&arch_info->pmc_addrs[cnum], value);
++	else
++		wrmsrl(pfm_pmu_conf->pmc_desc[cnum].hw_addr, value);
++}
++
++static inline void pfm_arch_write_pmd(struct pfm_context *ctx, unsigned int cnum, u64 value)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	/*
++	 * to make sure the counter overflows, we set the bits from
++	 * bit 31 till the width of the counters.
++	 * We clear any other unimplemented bits.
++	 */
++	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_C64)
++		value = (value | ~pfm_pmu_conf->ovfl_mask)
++		      & ~pfm_pmu_conf->pmd_desc[cnum].rsvd_msk;
++
++	PFM_DBG_ovfl("pfm_arch_write_pmd(0x%lx, 0x%Lx)",
++		     pfm_pmu_conf->pmd_desc[cnum].hw_addr,
++		     (unsigned long long) value);
++
++	if (arch_info->pmu_style == PFM_X86_PMU_P4)
++		__pfm_write_reg_p4(&arch_info->pmd_addrs[cnum], value);
++	else
++		wrmsrl(pfm_pmu_conf->pmd_desc[cnum].hw_addr, value);
++}
++
++static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	u64 tmp;
++	if (arch_info->pmu_style == PFM_X86_PMU_P4)
++		__pfm_read_reg_p4(&arch_info->pmd_addrs[cnum], &tmp);
++	else
++		rdmsrl(pfm_pmu_conf->pmd_desc[cnum].hw_addr, tmp);
++
++	PFM_DBG_ovfl("pfm_arch_read_pmd(0x%lx) = 0x%Lx",
++		     pfm_pmu_conf->pmd_desc[cnum].hw_addr,
++		     (unsigned long long) tmp);
++	return tmp;
++}
++
++static inline u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	u64 tmp;
++	if (arch_info->pmu_style == PFM_X86_PMU_P4)
++		__pfm_read_reg_p4(&arch_info->pmc_addrs[cnum], &tmp);
++	else
++		rdmsrl(pfm_pmu_conf->pmc_desc[cnum].hw_addr, tmp);
++
++	PFM_DBG_ovfl("pfm_arch_read_pmc(0x%lx) = 0x%016Lx",
++		     pfm_pmu_conf->pmc_desc[cnum].hw_addr,
++		     (unsigned long long) tmp);
++	return tmp;
++}
++
++/*
++ * At certain points, perfmon needs to know if monitoring has been
++ * explicitely started/stopped by user via pfm_start/pfm_stop. The
++ * information is tracked in flags.started. However on certain
++ * architectures, it may be possible to start/stop directly from
++ * user level with a single assembly instruction bypassing
++ * the kernel. This function is used to determine by
++ * an arch-specific mean if monitoring is actually started/stopped.
++ */
++static inline int pfm_arch_is_active(struct pfm_context *ctx)
++{
++	return ctx->flags.started;
++}
++
++static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
++					 struct pfm_context *ctx,
++					 struct pfm_event_set *set)
++{}
++
++static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
++					struct pfm_context *ctx,
++					struct pfm_event_set *set)
++{}
++
++static inline void pfm_arch_init_percpu(void)
++{}
++
++/* not necessary on IA-64 */
++static inline void pfm_cacheflush(void *addr, unsigned int len)
++{}
++
++int  pfm_arch_ctxswout_thread(struct task_struct *task,
++			      struct pfm_context *ctx,
++			      struct pfm_event_set *set);
++
++void pfm_arch_ctxswin_thread(struct task_struct *task,
++			     struct pfm_context *ctx,
++			     struct pfm_event_set *set);
++
++void pfm_arch_stop(struct task_struct *task,
++		   struct pfm_context *ctx, struct pfm_event_set *set);
++void pfm_arch_start(struct task_struct *task,
++		    struct pfm_context *ctx, struct pfm_event_set *set);
++
++void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
++void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
++int  pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg);
++void pfm_arch_pmu_config_remove(void);
++char *pfm_arch_get_pmu_module_name(void);
++
++static inline int pfm_arch_unload_context(struct pfm_context *ctx,
++					  struct task_struct *task)
++{
++	struct pfm_arch_pmu_info *arch_info;
++	struct pfm_arch_context *ctx_arch;
++	int ret = 0;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	arch_info = pfm_pmu_conf->arch_info;
++	if (arch_info->unload_context) {
++		ret = arch_info->unload_context(ctx);
++	}
++
++	if (ctx_arch->flags.insecure) {
++		PFM_DBG("clear cr4.pce");
++		clear_in_cr4(X86_CR4_PCE);
++	}
++
++	return ret;
++}
++
++static inline int pfm_arch_load_context(struct pfm_context *ctx,
++					struct pfm_event_set *set,
++					struct task_struct *task)
++{
++	struct pfm_arch_pmu_info *arch_info;
++	struct pfm_arch_context *ctx_arch;
++	int ret = 0;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * RDPMC is automatically authorized in system-wide and
++	 * also in self-monitoring per-thread context.
++	 * It may be authorized in other situations if the
++	 * PFM_X86_FL_INSECURE flags was set
++	 */
++	if (ctx->flags.system || task == current) {
++		PFM_DBG("set cr4.pce");
++		set_in_cr4(X86_CR4_PCE);
++		ctx_arch->flags.insecure = 1;
++	}
++
++	arch_info = pfm_pmu_conf->arch_info;
++	if (arch_info->load_context) {
++		ret = arch_info->load_context(ctx);
++	}
++	return ret;
++}
++
++/*
++ * this function is called from the PMU interrupt handler ONLY.
++ * On x86, the PMU is frozen via arch_stop, masking would be implemented
++ * via arch-stop as well. Given that the PMU is already stopped when
++ * entering the interrupt handler, we do not need to stop it again, so
++ * this function is a nop.
++ */
++static inline void pfm_arch_mask_monitoring(struct pfm_context *ctx,
++					    struct pfm_event_set *set)
++{}
++
++/*
++ * on x86 masking/unmasking uses the start/stop mechanism, so we simply
++ * need to start here.
++ */
++static inline void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
++					      struct pfm_event_set *set)
++{
++	pfm_arch_start(current, ctx, set);
++}
++
++/*
++ * called from __pfm_interrupt_handler(). ctx is not NULL.
++ * ctx is locked. interrupts are masked
++ *
++ * The following actions must take place:
++ *  - stop all monitoring to ensure handler has consistent view.
++ *  - collect overflowed PMDs bitmask into povfls_pmds and
++ *    npend_ovfls. If no interrupt detected then npend_ovfls
++ *    must be set to zero.
++ */
++static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
++					    struct pfm_event_set *set)
++{
++	/*
++	 * on X86, freezing is equivalent to stopping
++	 */
++	pfm_arch_stop(current, ctx, set);
++
++	/*
++	 * we mark monitoring as stopped to avoid
++	 * certain side effects especially in
++	 * pfm_switch_sets_from_intr() and
++	 * pfm_arch_restore_pmcs()
++	 */
++	ctx->flags.started = 0;
++}
++
++/*
++ * unfreeze PMU from pfm_do_interrupt_handler().
++ * ctx may be NULL for spurious interrupts.
++ * interrupts are masked.
++ */
++static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
++{
++	if (ctx == NULL)
++		return;
++
++	PFM_DBG_ovfl("state=%d", ctx->state);
++
++	/*
++	 * restore flags.started which is cleared in
++	 * pfm_arch_intr_freeze_pmu()
++	 */
++	ctx->flags.started = 1;
++
++	if (ctx->state == PFM_CTX_MASKED)
++		return;
++
++	pfm_arch_restore_pmcs(ctx, ctx->active_set);
++}
++
++
++/*
++ * function called from pfm_setfl_sane(). Context is locked
++ * and interrupts are masked.
++ * The value of flags is the value of ctx_flags as passed by
++ * user.
++ *
++ * function must check arch-specific set flags.
++ * Return:
++ *      1 when flags are valid
++ *      0 on error
++ */
++static inline int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
++{
++	return 0;
++}
++
++int pfm_arch_pmu_acquire(void);
++void pfm_arch_pmu_release(void);
++
++/*
++ * For some CPUs, the upper bits of a counter must be set in order for the
++ * overflow interrupt to happen. On overflow, the counter has wrapped around,
++ * and the upper bits are cleared. This function may be used to set them back.
++ *
++ * x86: The current version loses whatever is remaining in the counter,
++ * which is usually has a small count. In order not to loose this count,
++ * we do a read-modify-write to set the upper bits while preserving the
++ * low-order bits. This is slow but works.
++ */
++static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx, unsigned int cnum)
++{
++	u64 val;
++	val = pfm_arch_read_pmd(ctx, cnum);
++	pfm_arch_write_pmd(ctx, cnum, val);
++}
++
++/*
++ * not used for i386/x86_64
++ */
++static inline int pfm_smpl_buffer_alloc_compat(struct pfm_context *ctx,
++					       size_t rsize, struct file *filp)
++{
++	return -EINVAL;
++}
++static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
++			     char __user *buf,
++			     int non_block,
++			     size_t size)
++{
++	return -EINVAL;
++}
++
++static inline int pfm_arch_context_create(struct pfm_context *ctx, u32 ctx_flags)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	struct pfm_arch_context *ctx_arch;
++
++	if (arch_info->pmu_style != PFM_X86_PMU_P4)
++		return 0;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	ctx_arch->p4 = kzalloc(sizeof(*(ctx_arch->p4)), GFP_KERNEL);
++	if (!ctx_arch->p4)
++		return -ENOMEM;
++
++	return 0;
++}
++
++static inline void pfm_arch_context_free(struct pfm_context *ctx)
++{
++	struct pfm_arch_context *ctx_arch;
++
++	ctx_arch = pfm_ctx_arch(ctx);
++
++	/*
++	 * we do not check if P4, because it would be NULL and
++	 * kfree can deal with NULL
++	 */
++	kfree(ctx_arch->p4);
++}
++
++#define PFM_ARCH_CTX_SIZE	(sizeof(struct pfm_arch_context))
++/*
++ * i386/x86_64 do not need extra alignment requirements for the sampling buffer
++ */
++#define PFM_ARCH_SMPL_ALIGN_SIZE	0
++
++asmlinkage void  pmu_interrupt(void);
++
++#endif /* __KERNEL__ */
++
++#endif /* _ASM_X86_PERFMON_H_ */
+--- /dev/null
++++ b/include/asm-x86/perfmon_const.h
+@@ -0,0 +1,30 @@
++/*
++ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file contains i386/x86_64 specific definitions for the perfmon
++ * interface.
++ *
++ * This file MUST never be included directly. Use linux/perfmon.h.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++  */
++#ifndef _ASM_X86_PERFMON_CONST_H_
++#define _ASM_X86_PERFMON_CONST_H_
++
++#define PFM_ARCH_MAX_PMCS	(256+64) /* 256 HW 64 SW */
++#define PFM_ARCH_MAX_PMDS	(256+64) /* 256 HW 64 SW */
++
++#endif /* _ASM_X86_PERFMON_CONST_H_ */
+--- /dev/null
++++ b/include/asm-x86/perfmon_pebs_core_smpl.h
+@@ -0,0 +1,164 @@
++/*
++ * Copyright (c) 2005-2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ *
++ * This file implements the sampling format to support Intel
++ * Precise Event Based Sampling (PEBS) feature of Intel Core
++ * processors, such as Intel Core 2.
++ *
++ * What is PEBS?
++ * ------------
++ *  This is a hardware feature to enhance sampling by providing
++ *  better precision as to where a sample is taken. This avoids the
++ *  typical skew in the instruction one can observe with any
++ *  interrupt-based sampling technique.
++ *
++ *  PEBS also lowers sampling overhead significantly by having the
++ *  processor store samples instead of the OS. PMU interrupt are only
++ *  generated after multiple samples are written.
++ *
++ *  Another benefit of PEBS is that samples can be captured inside
++ *  critical sections where interrupts are masked.
++ *
++ * How does it work?
++ *  PEBS effectively implements a Hw buffer. The Os must pass a region
++ *  of memory where samples are to be stored. The region can have any
++ *  size. The OS must also specify the sampling period to reload. The PMU
++ *  will interrupt when it reaches the end of the buffer or a specified
++ *  threshold location inside the memory region.
++ *
++ *  The description of the buffer is stored in the Data Save Area (DS).
++ *  The samples are stored sequentially in the buffer. The format of the
++ *  buffer is fixed and specified in the PEBS documentation.  The sample
++ *  format does not change between 32-bit and 64-bit modes unlike on the
++ *  Pentium 4 version of PEBS.
++ *
++ *  PEBS does not work when HyperThreading is enabled due to certain MSR
++ *  being shared being to two threads.
++ *
++ *  What does the format do?
++ *   It provides access to the PEBS feature for both 32-bit and 64-bit
++ *   processors that support it.
++ *
++ *   The same code and data structures are used for both 32-bit and 64-bi
++ *   modes. A single format name is used for both modes. In 32-bit mode,
++ *   some of the extended registers are written to zero in each sample.
++ *
++ *   It is important to realize that the format provides a zero-copy
++ *   environment for the samples, i.e,, the OS never touches the
++ *   samples. Whatever the processor write is directly accessible to
++ *   the user.
++ *
++ *   Parameters to the buffer can be passed via pfm_create_context() in
++ *   the pfm_pebs_smpl_arg structure.
++ */
++#ifndef __PERFMON_PEBS_CORE_SMPL_H__
++#define __PERFMON_PEBS_CORE_SMPL_H__ 1
++
++/*
++ * The 32-bit and 64-bit formats are identical, thus we use only
++ * one name for the format.
++ */
++#define PFM_PEBS_CORE_SMPL_NAME	"pebs_core"
++
++/*
++ * format specific parameters (passed at context creation)
++ *
++ * intr_thres: index from start of buffer of entry where the
++ * PMU interrupt must be triggered. It must be several samples
++ * short of the end of the buffer.
++ */
++struct pfm_pebs_core_smpl_arg {
++	u64 cnt_reset;	  /* counter reset value */
++	size_t buf_size;  /* size of the PEBS buffer in bytes */
++	size_t intr_thres;/* index of PEBS interrupt threshold entry */
++	u64 reserved[6];  /* for future use */
++};
++
++/*
++ * Data Save Area (32 and 64-bit mode)
++ *
++ * The DS area is exposed to the user. To determine the number
++ * of samples available in PEBS, it is necessary to substract
++ * pebs_index from pebs_base.
++ *
++ * Layout of the structure is mandated by hardware and specified
++ * in the Intel documentation.
++ */
++struct pfm_ds_area_core {
++	u64 bts_buf_base;
++	u64 bts_index;
++	u64 bts_abs_max;
++	u64 bts_intr_thres;
++	u64 pebs_buf_base;
++	u64 pebs_index;
++	u64 pebs_abs_max;
++	u64 pebs_intr_thres;
++	u64 pebs_cnt_reset;
++};
++
++/*
++ * This header is at the beginning of the sampling buffer returned to the user.
++ *
++ * Because of PEBS alignement constraints, the actual PEBS buffer area does
++ * not necessarily begin right after the header. The hdr_start_offs must be
++ * used to compute the first byte of the buffer. The offset is defined as
++ * the number of bytes between the end of the header and the beginning of
++ * the buffer. As such the formula is:
++ * 	actual_buffer = (unsigned long)(hdr+1)+hdr->hdr_start_offs
++ */
++struct pfm_pebs_core_smpl_hdr {
++	u64 overflows;			/* #overflows for buffer */
++	size_t buf_size;		/* bytes in the buffer */
++	size_t start_offs; 		/* actual buffer start offset */
++	u32 version;			/* smpl format version */
++	u32 reserved1;			/* for future use */
++	u64 reserved2[5];		/* for future use */
++	struct pfm_ds_area_core ds;	/* data save area */
++};
++
++/*
++ * Sample format as mandated by Intel documentation.
++ * The same format is used in both 32 and 64 bit modes.
++ */
++struct pfm_pebs_core_smpl_entry {
++	u64	eflags;
++	u64	ip;
++	u64	eax;
++	u64	ebx;
++	u64	ecx;
++	u64	edx;
++	u64	esi;
++	u64	edi;
++	u64	ebp;
++	u64	esp;
++	u64	r8;	/* 0 in 32-bit mode */
++	u64	r9;	/* 0 in 32-bit mode */
++	u64	r10;	/* 0 in 32-bit mode */
++	u64	r11;	/* 0 in 32-bit mode */
++	u64	r12;	/* 0 in 32-bit mode */
++	u64	r13;	/* 0 in 32-bit mode */
++	u64	r14;	/* 0 in 32-bit mode */
++	u64	r15;	/* 0 in 32-bit mode */
++};
++
++#define PFM_PEBS_CORE_SMPL_VERSION_MAJ 1U
++#define PFM_PEBS_CORE_SMPL_VERSION_MIN 0U
++#define PFM_PEBS_CORE_SMPL_VERSION (((PFM_PEBS_CORE_SMPL_VERSION_MAJ&0xffff)<<16)|\
++				   (PFM_PEBS_CORE_SMPL_VERSION_MIN & 0xffff))
++
++#endif /* __PERFMON_PEBS_CORE_SMPL_H__ */
+--- /dev/null
++++ b/include/asm-x86/perfmon_pebs_p4_smpl.h
+@@ -0,0 +1,193 @@
++/*
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ *
++ * This file implements the sampling format to support Intel
++ * Precise Event Based Sampling (PEBS) feature of Pentium 4
++ * and other Netburst-based processors. Not to be used for
++ * Intel Core-based processors.
++ *
++ * What is PEBS?
++ * ------------
++ *  This is a hardware feature to enhance sampling by providing
++ *  better precision as to where a sample is taken. This avoids the
++ *  typical skew in the instruction one can observe with any
++ *  interrupt-based sampling technique.
++ *
++ *  PEBS also lowers sampling overhead significantly by having the
++ *  processor store samples instead of the OS. PMU interrupt are only
++ *  generated after multiple samples are written.
++ *
++ *  Another benefit of PEBS is that samples can be captured inside
++ *  critical sections where interrupts are masked.
++ *
++ * How does it work?
++ *  PEBS effectively implements a Hw buffer. The Os must pass a region
++ *  of memory where samples are to be stored. The region can have any
++ *  size. The OS must also specify the sampling period to reload. The PMU
++ *  will interrupt when it reaches the end of the buffer or a specified
++ *  threshold location inside the memory region.
++ *
++ *  The description of the buffer is stored in the Data Save Area (DS).
++ *  The samples are stored sequentially in the buffer. The format of the
++ *  buffer is fixed and specified in the PEBS documentation.  The sample
++ *  format changes between 32-bit and 64-bit modes due to extended register
++ *  file.
++ *
++ *  PEBS does not work when HyperThreading is enabled due to certain MSR
++ *  being shared being to two threads.
++ *
++ *  What does the format do?
++ *   It provides access to the PEBS feature for both 32-bit and 64-bit
++ *   processors that support it.
++ *
++ *   The same code is used for both 32-bit and 64-bit modes, but different
++ *   format names are used because the two modes are not compatible due to
++ *   data model and register file differences. Similarly the public data
++ *   structures describing the samples are different.
++ *
++ *   It is important to realize that the format provides a zero-copy environment
++ *   for the samples, i.e,, the OS never touches the samples. Whatever the
++ *   processor write is directly accessible to the user.
++ *
++ *   Parameters to the buffer can be passed via pfm_create_context() in
++ *   the pfm_pebs_smpl_arg structure.
++ *
++ *   It is not possible to mix a 32-bit PEBS application on top of a 64-bit
++ *   host kernel.
++ */
++#ifndef __PERFMON_PEBS_P4_SMPL_H__
++#define __PERFMON_PEBS_P4_SMPL_H__ 1
++
++#ifdef __i386__
++/*
++ * The 32-bit and 64-bit formats are not compatible, thus we have
++ * two different identifications so that 32-bit programs running on
++ * 64-bit OS will fail to use the 64-bit PEBS support.
++ */
++#define PFM_PEBS_P4_SMPL_NAME	"pebs32_p4"
++#else
++#define PFM_PEBS_P4_SMPL_NAME	"pebs64_p4"
++#endif
++
++/*
++ * format specific parameters (passed at context creation)
++ *
++ * intr_thres: index from start of buffer of entry where the
++ * PMU interrupt must be triggered. It must be several samples
++ * short of the end of the buffer.
++ */
++struct pfm_pebs_p4_smpl_arg {
++	u64 cnt_reset;	  /* counter reset value */
++	size_t buf_size;  /* size of the PEBS buffer in bytes */
++	size_t intr_thres;/* index of PEBS interrupt threshold entry */
++	u64 reserved[6];  /* for future use */
++};
++
++/*
++ * Data Save Area (32 and 64-bit mode)
++ *
++ * The DS area must be exposed to the user because this is the only
++ * way to report on the number of valid entries recorded by the CPU.
++ * This is required when the buffer is not full, i..e, there was not
++ * PMU interrupt.
++ *
++ * Layout of the structure is mandated by hardware and specified in
++ * the Intel documentation.
++ */
++struct pfm_ds_area_p4 {
++	unsigned long	bts_buf_base;
++	unsigned long	bts_index;
++	unsigned long	bts_abs_max;
++	unsigned long	bts_intr_thres;
++	unsigned long	pebs_buf_base;
++	unsigned long	pebs_index;
++	unsigned long	pebs_abs_max;
++	unsigned long	pebs_intr_thres;
++	u64     	pebs_cnt_reset;
++};
++
++/*
++ * This header is at the beginning of the sampling buffer returned to the user.
++ *
++ * Because of PEBS alignement constraints, the actual PEBS buffer area does
++ * not necessarily begin right after the header. The hdr_start_offs must be
++ * used to compute the first byte of the buffer. The offset is defined as
++ * the number of bytes between the end of the header and the beginning of
++ * the buffer. As such the formula is:
++ * 	actual_buffer = (unsigned long)(hdr+1)+hdr->hdr_start_offs
++ */
++struct pfm_pebs_p4_smpl_hdr {
++	u64 overflows;			/* #overflows for buffer */
++	size_t buf_size;		/* bytes in the buffer */
++	size_t start_offs; 		/* actual buffer start offset */
++	u32 version;			/* smpl format version */
++	u32 reserved1;			/* for future use */
++	u64 reserved2[5];		/* for future use */
++	struct pfm_ds_area_p4 ds;	/* data save area */
++};
++
++/*
++ * 64-bit PEBS record format is described in
++ * http://www.intel.com/technology/64bitextensions/30083502.pdf
++ *
++ * The format does not peek at samples. The sample structure is only
++ * used to ensure that the buffer is large enough to accomodate one
++ * sample.
++ */
++#ifdef __i386__
++struct pfm_pebs_p4_smpl_entry {
++	u32	eflags;
++	u32	ip;
++	u32	eax;
++	u32	ebx;
++	u32	ecx;
++	u32	edx;
++	u32	esi;
++	u32	edi;
++	u32	ebp;
++	u32	esp;
++};
++#else
++struct pfm_pebs_p4_smpl_entry {
++	u64	eflags;
++	u64	ip;
++	u64	eax;
++	u64	ebx;
++	u64	ecx;
++	u64	edx;
++	u64	esi;
++	u64	edi;
++	u64	ebp;
++	u64	esp;
++	u64	r8;
++	u64	r9;
++	u64	r10;
++	u64	r11;
++	u64	r12;
++	u64	r13;
++	u64	r14;
++	u64	r15;
++};
++#endif
++
++#define PFM_PEBS_P4_SMPL_VERSION_MAJ 1U
++#define PFM_PEBS_P4_SMPL_VERSION_MIN 0U
++#define PFM_PEBS_P4_SMPL_VERSION (((PFM_PEBS_P4_SMPL_VERSION_MAJ&0xffff)<<16)|\
++				   (PFM_PEBS_P4_SMPL_VERSION_MIN & 0xffff))
++
++#endif /* __PERFMON_PEBS_P4_SMPL_H__ */
+--- a/include/asm-x86/thread_info_32.h
++++ b/include/asm-x86/thread_info_32.h
+@@ -132,11 +132,13 @@ static inline struct thread_info *curren
+ #define TIF_SYSCALL_AUDIT	6	/* syscall auditing active */
+ #define TIF_SECCOMP		7	/* secure computing */
+ #define TIF_RESTORE_SIGMASK	8	/* restore signal mask in do_signal() */
++#define TIF_PERFMON_WORK	9	/* work for pfm_handle_work() */
+ #define TIF_MEMDIE		16
+ #define TIF_DEBUG		17	/* uses debug registers */
+ #define TIF_IO_BITMAP		18	/* uses I/O bitmap */
+ #define TIF_FREEZE		19	/* is freezing for suspend */
+ #define TIF_NOTSC		20	/* TSC is not accessible in userland */
++#define TIF_PERFMON_CTXSW	21	/* perfmon needs ctxsw calls */
+ 
+ #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
+ #define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
+@@ -151,6 +153,8 @@ static inline struct thread_info *curren
+ #define _TIF_IO_BITMAP		(1<<TIF_IO_BITMAP)
+ #define _TIF_FREEZE		(1<<TIF_FREEZE)
+ #define _TIF_NOTSC		(1<<TIF_NOTSC)
++#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
++#define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
+ 
+ /* work to do on interrupt/exception return */
+ #define _TIF_WORK_MASK \
+@@ -160,8 +164,8 @@ static inline struct thread_info *curren
+ #define _TIF_ALLWORK_MASK	(0x0000FFFF & ~_TIF_SECCOMP)
+ 
+ /* flags to check in __switch_to() */
+-#define _TIF_WORK_CTXSW_NEXT (_TIF_IO_BITMAP | _TIF_NOTSC | _TIF_DEBUG)
+-#define _TIF_WORK_CTXSW_PREV (_TIF_IO_BITMAP | _TIF_NOTSC)
++#define _TIF_WORK_CTXSW_NEXT (_TIF_IO_BITMAP | _TIF_NOTSC | _TIF_DEBUG | _TIF_PERFMON_CTXSW)
++#define _TIF_WORK_CTXSW_PREV (_TIF_IO_BITMAP | _TIF_NOTSC | _TIF_PERFMON_CTXSW)
+ 
+ /*
+  * Thread-synchronous status.
+--- a/include/asm-x86/thread_info_64.h
++++ b/include/asm-x86/thread_info_64.h
+@@ -107,6 +107,7 @@ static inline struct thread_info *stack_
+  * Warning: layout of LSW is hardcoded in entry.S
+  */
+ #define TIF_SYSCALL_TRACE	0	/* syscall trace active */
++#define TIF_PERFMON_WORK	1	/* work for pfm_handle_work() */
+ #define TIF_SIGPENDING		2	/* signal pending */
+ #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
+ #define TIF_SINGLESTEP		4	/* reenable singlestep on user return*/
+@@ -123,6 +124,7 @@ static inline struct thread_info *stack_
+ #define TIF_DEBUG		21	/* uses debug registers */
+ #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
+ #define TIF_FREEZE		23	/* is freezing for suspend */
++#define TIF_PERFMON_CTXSW	24	/* perfmon needs ctxsw calls */
+ 
+ #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
+ #define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
+@@ -139,6 +141,8 @@ static inline struct thread_info *stack_
+ #define _TIF_DEBUG		(1<<TIF_DEBUG)
+ #define _TIF_IO_BITMAP		(1<<TIF_IO_BITMAP)
+ #define _TIF_FREEZE		(1<<TIF_FREEZE)
++#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
++#define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
+ 
+ /* work to do on interrupt/exception return */
+ #define _TIF_WORK_MASK \
+@@ -147,7 +151,7 @@ static inline struct thread_info *stack_
+ #define _TIF_ALLWORK_MASK (0x0000FFFF & ~_TIF_SECCOMP)
+ 
+ /* flags to check in __switch_to() */
+-#define _TIF_WORK_CTXSW (_TIF_DEBUG|_TIF_IO_BITMAP)
++#define _TIF_WORK_CTXSW (_TIF_DEBUG|_TIF_IO_BITMAP|_TIF_PERFMON_CTXSW)
+ 
+ #define PREEMPT_ACTIVE     0x10000000
+ 
+--- a/include/asm-x86/unistd_32.h
++++ b/include/asm-x86/unistd_32.h
+@@ -330,10 +330,22 @@
+ #define __NR_timerfd		322
+ #define __NR_eventfd		323
+ #define __NR_fallocate		324
++#define __NR_pfm_create_context	325
++#define __NR_pfm_write_pmcs	(__NR_pfm_create_context+1)
++#define __NR_pfm_write_pmds	(__NR_pfm_create_context+2)
++#define __NR_pfm_read_pmds	(__NR_pfm_create_context+3)
++#define __NR_pfm_load_context	(__NR_pfm_create_context+4)
++#define __NR_pfm_start		(__NR_pfm_create_context+5)
++#define __NR_pfm_stop		(__NR_pfm_create_context+6)
++#define __NR_pfm_restart	(__NR_pfm_create_context+7)
++#define __NR_pfm_create_evtsets	(__NR_pfm_create_context+8)
++#define __NR_pfm_getinfo_evtsets (__NR_pfm_create_context+9)
++#define __NR_pfm_delete_evtsets (__NR_pfm_create_context+10)
++#define __NR_pfm_unload_context	(__NR_pfm_create_context+11)
+ 
+ #ifdef __KERNEL__
+ 
+-#define NR_syscalls 325
++#define NR_syscalls 337
+ 
+ #define __ARCH_WANT_IPC_PARSE_VERSION
+ #define __ARCH_WANT_OLD_READDIR
+--- a/include/asm-x86/unistd_64.h
++++ b/include/asm-x86/unistd_64.h
+@@ -635,6 +635,30 @@ __SYSCALL(__NR_timerfd, sys_timerfd)
+ __SYSCALL(__NR_eventfd, sys_eventfd)
+ #define __NR_fallocate				285
+ __SYSCALL(__NR_fallocate, sys_fallocate)
++#define __NR_pfm_create_context	286
++__SYSCALL(__NR_pfm_create_context, sys_pfm_create_context)
++#define __NR_pfm_write_pmcs	(__NR_pfm_create_context+1)
++__SYSCALL(__NR_pfm_write_pmcs, sys_pfm_write_pmcs)
++#define __NR_pfm_write_pmds	(__NR_pfm_create_context+2)
++__SYSCALL(__NR_pfm_write_pmds, sys_pfm_write_pmds)
++#define __NR_pfm_read_pmds	(__NR_pfm_create_context+3)
++ __SYSCALL(__NR_pfm_read_pmds, sys_pfm_read_pmds)
++#define __NR_pfm_load_context	(__NR_pfm_create_context+4)
++__SYSCALL(__NR_pfm_load_context, sys_pfm_load_context)
++#define __NR_pfm_start		(__NR_pfm_create_context+5)
++__SYSCALL(__NR_pfm_start, sys_pfm_start)
++#define __NR_pfm_stop		(__NR_pfm_create_context+6)
++__SYSCALL(__NR_pfm_stop, sys_pfm_stop)
++#define __NR_pfm_restart	(__NR_pfm_create_context+7)
++__SYSCALL(__NR_pfm_restart, sys_pfm_restart)
++#define __NR_pfm_create_evtsets	(__NR_pfm_create_context+8)
++__SYSCALL(__NR_pfm_create_evtsets, sys_pfm_create_evtsets)
++#define __NR_pfm_getinfo_evtsets (__NR_pfm_create_context+9)
++__SYSCALL(__NR_pfm_getinfo_evtsets, sys_pfm_getinfo_evtsets)
++#define __NR_pfm_delete_evtsets (__NR_pfm_create_context+10)
++__SYSCALL(__NR_pfm_delete_evtsets, sys_pfm_delete_evtsets)
++#define __NR_pfm_unload_context	(__NR_pfm_create_context+11)
++__SYSCALL(__NR_pfm_unload_context, sys_pfm_unload_context)
+ 
+ #ifndef __NO_STUBS
+ #define __ARCH_WANT_OLD_READDIR
+--- a/include/linux/pci_ids.h
++++ b/include/linux/pci_ids.h
+@@ -497,6 +497,8 @@
+ #define PCI_DEVICE_ID_AMD_K8_NB_ADDRMAP	0x1101
+ #define PCI_DEVICE_ID_AMD_K8_NB_MEMCTL	0x1102
+ #define PCI_DEVICE_ID_AMD_K8_NB_MISC	0x1103
++#define PCI_DEVICE_ID_AMD_10H_NB	0x1200
++#define PCI_DEVICE_ID_AMD_10H_NB_MISC	0x1203
+ #define PCI_DEVICE_ID_AMD_LANCE		0x2000
+ #define PCI_DEVICE_ID_AMD_LANCE_HOME	0x2001
+ #define PCI_DEVICE_ID_AMD_SCSI		0x2020
+--- /dev/null
++++ b/include/linux/perfmon.h
+@@ -0,0 +1,748 @@
++/*
++ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++
++#ifndef __LINUX_PERFMON_H__
++#define __LINUX_PERFMON_H__
++
++#ifdef CONFIG_PERFMON
++
++/*
++ * include arch-specific constants
++ *
++ * constants are split for arch-specific perfmon.h
++ * to avoid cyclic dependency with the structures defined
++ * in this file.
++ */
++#include <asm/perfmon_const.h>
++
++#define PFM_MAX_PMCS	PFM_ARCH_MAX_PMCS
++#define PFM_MAX_PMDS	PFM_ARCH_MAX_PMDS
++
++/*
++ * number of elements for each type of bitvector
++ * all bitvectors use u64 fixed size type on all architectures.
++ */
++#define PFM_BVSIZE(x)	(((x)+(sizeof(u64)<<3)-1) / (sizeof(u64)<<3))
++#define PFM_PMD_BV	PFM_BVSIZE(PFM_MAX_PMDS)
++#define PFM_PMC_BV	PFM_BVSIZE(PFM_MAX_PMCS)
++
++/*
++ * PMC/PMD flags to use with pfm_write_pmds() or pfm_write_pmcs()
++ *
++ * event means:
++ * 	- for counters: when the 64-bit counter overflows
++ * 	- for others  : when the PMD generates an interrupt
++ *
++ * PFM_REGFL_NO_EMUL64: must be set of the PMC controlling the counting PMD
++ *
++ * reg_flags layout:
++ * bit 00-15 : generic flags
++ * bit 16-23 : arch-specific flags
++ * bit 24-31 : error codes
++ */
++#define PFM_REGFL_OVFL_NOTIFY	0x1	/* PMD: send notification on event */
++#define PFM_REGFL_RANDOM	0x2	/* PMD: randomize value after event */
++#define PFM_REGFL_NO_EMUL64	0x4	/* PMC: no 64-bit emulation for counter */
++
++/*
++ * event set flags layout:
++ * bits[00-15] : generic flags
++ * bits[16-31] : arch-specific flags (see asm/perfmon.h)
++ */
++#define PFM_SETFL_OVFL_SWITCH	0x01 /* enable switch on overflow */
++#define PFM_SETFL_TIME_SWITCH	0x02 /* enable switch on timeout */
++
++/*
++ * argument to pfm_create_context() system call
++ * structure shared with user level
++ */
++struct pfarg_ctx {
++	__u32		ctx_flags;	  /* noblock/block/syswide */
++	__u32		ctx_reserved1;	  /* ret arg: fd for context */
++	__u64		ctx_reserved2[7]; /* for future use */
++};
++
++/*
++ * context flags (ctx_flags)
++ *
++ * bits[00-15]: generic flags
++ * bits[16-31]: arch-specific flags (see perfmon_const.h)
++ */
++#define PFM_FL_NOTIFY_BLOCK    	 0x01	/* block task on user notifications */
++#define PFM_FL_SYSTEM_WIDE	 0x02	/* create a system wide context */
++#define PFM_FL_OVFL_NO_MSG	 0x80   /* no overflow msgs */
++
++/*
++ * argument to pfm_write_pmcs() system call.
++ * structure shared with user level
++ */
++struct pfarg_pmc {
++	__u16 reg_num;		/* which register */
++	__u16 reg_set;		/* event set for this register */
++	__u32 reg_flags;	/* REGFL flags */
++	__u64 reg_value;	/* pmc value */
++	__u64 reg_reserved2[4];	/* for future use */
++};
++
++/*
++ * argument to pfm_write_pmds() and pfm_read_pmds() system calls.
++ * structure shared with user level
++ */
++struct pfarg_pmd {
++	__u16 reg_num;	   	/* which register */
++	__u16 reg_set;	   	/* event set for this register */
++	__u32 reg_flags; 	/* REGFL flags */
++	__u64 reg_value;	/* initial pmc/pmd value */
++	__u64 reg_long_reset;	/* value to reload after notification */
++	__u64 reg_short_reset;  /* reset after counter overflow */
++	__u64 reg_last_reset_val;	/* return: PMD last reset value */
++	__u64 reg_ovfl_switch_cnt;	/* #overflows before switch */
++	__u64 reg_reset_pmds[PFM_PMD_BV]; /* reset on overflow */
++	__u64 reg_smpl_pmds[PFM_PMD_BV];  /* record in sample */
++	__u64 reg_smpl_eventid; /* opaque event identifier */
++	__u64 reg_random_mask; 	/* bitmask used to limit random value */
++	__u32 reg_random_seed;  /* seed for randomization (OBSOLETE) */
++	__u32 reg_reserved2[7];	/* for future use */
++};
++
++/*
++ * optional argument to pfm_start() system call. Pass NULL if not needed.
++ * structure shared with user level
++ */
++struct pfarg_start {
++	__u16 start_set;	/* event set to start with */
++	__u16 start_reserved1;	/* for future use */
++	__u32 start_reserved2;	/* for future use */
++	__u64 reserved3[3];	/* for future use */
++};
++
++/*
++ * argument to pfm_load_context() system call.
++ * structure shared with user level
++ */
++struct pfarg_load {
++	__u32	load_pid;	   /* thread or CPU to attach to */
++	__u16	load_set;	   /* set to load first */
++	__u16	load_reserved1;	   /* for future use */
++	__u64	load_reserved2[3]; /* for future use */
++};
++
++/*
++ * argument to pfm_create_evtsets() and pfm_delete_evtsets() system calls.
++ * structure shared with user level.
++ */
++struct pfarg_setdesc {
++	__u16	set_id;		  /* which set */
++	__u16	set_reserved1;	  /* for future use */
++	__u32	set_flags; 	  /* SETFL flags  */
++	__u64	set_timeout;	  /* req/eff switch timeout in nsecs */
++	__u64	reserved[6];	  /* for future use */
++};
++
++/*
++ * argument to pfm_getinfo_evtsets() system call.
++ * structure shared with user level
++ */
++struct pfarg_setinfo {
++	__u16	set_id;			/* which set */
++	__u16	set_reserved1;		/* for future use */
++	__u32	set_flags;		/* out: SETFL flags */
++	__u64 	set_ovfl_pmds[PFM_PMD_BV]; /* out: last ovfl PMDs */
++	__u64	set_runs;		/* out: #times the set was active */
++	__u64	set_timeout;		/* out: effective/leftover switch timeout in nsecs */
++	__u64	set_act_duration;	/* out: time set was active in nsecs */
++	__u64	set_avail_pmcs[PFM_PMC_BV];/* out: available PMCs */
++	__u64	set_avail_pmds[PFM_PMD_BV];/* out: available PMDs */
++	__u64	set_reserved3[6];	/* for future use */
++};
++
++/*
++ * default value for the user and group security parameters in
++ * /proc/sys/kernel/perfmon/sys_group
++ * /proc/sys/kernel/perfmon/task_group
++ */
++#define PFM_GROUP_PERM_ANY	-1	/* any user/group */
++
++/*
++ * overflow notification message.
++ * structure shared with user level
++ */
++struct pfarg_ovfl_msg {
++	__u32 		msg_type;	/* message type: PFM_MSG_OVFL */
++	__u32		msg_ovfl_pid;	/* process id */
++	__u16 		msg_active_set;	/* active set at overflow */
++	__u16 		msg_ovfl_cpu;	/* cpu of PMU interrupt */
++	__u32		msg_ovfl_tid;	/* thread id */
++	__u64		msg_ovfl_ip;    /* IP on PMU intr */
++	__u64		msg_ovfl_pmds[PFM_PMD_BV];/* overflowed PMDs */
++};
++
++#define PFM_MSG_OVFL	1	/* an overflow happened */
++#define PFM_MSG_END	2	/* task to which context was attached ended */
++
++/*
++ * generic notification message (union).
++ * union shared with user level
++ */
++union pfarg_msg {
++	__u32	type;
++	struct pfarg_ovfl_msg pfm_ovfl_msg;
++};
++
++/*
++ * perfmon version number
++ */
++#define PFM_VERSION_MAJ		 2U
++#define PFM_VERSION_MIN		 7U
++#define PFM_VERSION		 (((PFM_VERSION_MAJ&0xffff)<<16)|\
++				  (PFM_VERSION_MIN & 0xffff))
++#define PFM_VERSION_MAJOR(x)	 (((x)>>16) & 0xffff)
++#define PFM_VERSION_MINOR(x)	 ((x) & 0xffff)
++
++/*
++ * This part of the header file is meant for kernel level code only including
++ * kernel modules
++ */
++#ifdef __KERNEL__
++
++#include <linux/file.h>
++#include <linux/seq_file.h>
++#include <linux/interrupt.h>
++#include <linux/kobject.h>
++
++/*
++ * perfmon context state
++ */
++#define PFM_CTX_UNLOADED	1 /* context is not loaded onto any task */
++#define PFM_CTX_LOADED		2 /* context is loaded onto a task */
++#define PFM_CTX_MASKED		3 /* context is loaded, monitoring is masked */
++#define PFM_CTX_ZOMBIE		4 /* context lost owner but is still attached */
++
++/*
++ * depth of message queue
++ *
++ * Depth cannot be bigger than 255 (see reset_count)
++ */
++#define PFM_MSGS_ORDER		3 /* log2(number of messages) */
++#define PFM_MSGS_COUNT		(1<<PFM_MSGS_ORDER) /* number of messages */
++#define PFM_MSGQ_MASK		(PFM_MSGS_COUNT-1)
++
++/*
++ * type of PMD reset for pfm_reset_pmds() or pfm_switch_sets*()
++ */
++#define PFM_PMD_RESET_SHORT	1	/* use short reset value */
++#define PFM_PMD_RESET_LONG	2	/* use long reset value  */
++
++/*
++ * describe the content of the pfm_syst_info field
++ * layout:
++ * 	bits[00-15] : generic
++ *	bits[16-31] : arch-specific flags (see asm/perfmon.h)
++ */
++#define PFM_CPUINFO_TIME_SWITCH	0x1 /* current set is time-switched */
++
++struct pfm_controls {
++	int	debug;		/* debugging via syslog */
++	int	debug_ovfl;	/* overflow handling debugging */
++	gid_t	sys_group;	/* gid to create a syswide context */
++	gid_t	task_group;	/* gid to create a per-task context */
++	size_t	arg_mem_max;	/* maximum vector argument size */
++	size_t	smpl_buffer_mem_max; /* max buf mem, -1 for infinity */
++	int pmd_read;
++};
++DECLARE_PER_CPU(u32, pfm_syst_info);
++DECLARE_PER_CPU(struct task_struct *, pmu_owner);
++DECLARE_PER_CPU(struct pfm_context *, pmu_ctx);
++DECLARE_PER_CPU(u64, pmu_activation_number);
++DECLARE_PER_CPU(struct pfm_stats, pfm_stats);
++DECLARE_PER_CPU(struct hrtimer, pfm_hrtimer);
++
++/*
++ * logging
++ */
++#define PFM_ERR(f, x...)  printk(KERN_ERR     "perfmon: " f "\n", ## x)
++#define PFM_WARN(f, x...) printk(KERN_WARNING "perfmon: " f "\n", ## x)
++#define PFM_LOG(f, x...)  printk(KERN_NOTICE  "perfmon: " f "\n", ## x)
++#define PFM_INFO(f, x...) printk(KERN_INFO    "perfmon: " f "\n", ## x)
++
++/*
++ * debugging
++ *
++ * Printk rate limiting is enforced to avoid getting flooded with too many
++ * error messages on the console (which could render the machine unresponsive).
++ * To get full debug output (turn off ratelimit):
++ * 	$ echo 0 >/proc/sys/kernel/printk_ratelimit
++ */
++#ifdef CONFIG_PERFMON_DEBUG
++#define PFM_DBG(f, x...) \
++	do { \
++		if (unlikely(pfm_controls.debug >0 && printk_ratelimit())) { \
++			printk("perfmon: %s.%d: CPU%d [%d]: " f "\n", \
++			       __FUNCTION__, __LINE__, \
++			       smp_processor_id(), current->pid , ## x); \
++		} \
++	} while (0)
++
++#define PFM_DBG_ovfl(f, x...) \
++	do { \
++		if (unlikely(pfm_controls.debug_ovfl >0 && printk_ratelimit())) { \
++			printk("perfmon: %s.%d: CPU%d [%d]: " f "\n", \
++			       __FUNCTION__, __LINE__, \
++			       smp_processor_id(), current->pid , ## x); \
++		} \
++	} while (0)
++#else
++#define PFM_DBG(f, x...)	do {} while(0)
++#define PFM_DBG_ovfl(f, x...)	do {} while(0)
++#endif
++
++/*
++ * PMD information
++ */
++struct pfm_pmd {
++	u64 value;		/* currnet 64-bit value */
++	u64 lval;		/* last reset value */
++	u64 ovflsw_thres;	 /* #overflows left before switching */
++	u64 long_reset;		/* reset value on sampling overflow */
++	u64 short_reset;    	/* reset value on overflow */
++	u64 reset_pmds[PFM_PMD_BV];  /* pmds to reset on overflow */
++	u64 smpl_pmds[PFM_PMD_BV];   /* pmds to record on overflow */
++	u64 mask;		 /* mask for generator */
++	u64 ovflsw_ref_thres;	 /* #overflows before switching to next set */
++	u64 eventid;	 	 /* overflow event identifier */
++	u32 flags;		 /* notify/do not notify */
++};
++
++/*
++ * perfmon context: encapsulates all the state of a monitoring session
++ */
++struct pfm_event_set {
++	u16 id;
++	u16 id_next;			/* which set to go to from this one */
++	u32 flags;			/* public set flags */
++	u64 runs;			/* number of activations */
++	struct list_head list;		/* next in the ordered list */
++	u32 priv_flags;			/* private flags */
++	u32 npend_ovfls;		/* number of pending PMD overflow */
++
++	u64 used_pmds[PFM_PMD_BV];    /* used PMDs */
++	u64 povfl_pmds[PFM_PMD_BV];   /* pending overflowed PMDs */
++	u64 ovfl_pmds[PFM_PMD_BV];    /* last overflowed PMDs */
++	u64 reset_pmds[PFM_PMD_BV];   /* union of PMDs to reset */
++	u64 ovfl_notify[PFM_PMD_BV];  /* notify on overflow */
++	u64 pmcs[PFM_MAX_PMCS];	      /* PMC values */
++
++	u16 nused_pmds;			    /* max number of used PMDs */
++	u16 nused_pmcs;			    /* max number of used PMCs */
++
++	struct pfm_pmd pmds[PFM_MAX_PMDS];  /* 64-bit SW PMDs */
++
++	ktime_t hrtimer_exp;		/* switch timeout reference */
++	ktime_t hrtimer_rem;		/* per-thread remainder timeout */
++
++	u64 duration_start;		    /* start ns */
++	u64 duration;			    /* total active ns */
++	u64 used_pmcs[PFM_PMC_BV];    /* used PMCs (keep for arbitration) */
++};
++
++/*
++ * common private event set flags (priv_flags)
++ *
++ * upper 16 bits: for arch-specific use
++ * lower 16 bits: for common use
++ */
++#define PFM_SETFL_PRIV_MOD_PMDS 0x1 /* PMD register(s) modified */
++#define PFM_SETFL_PRIV_MOD_PMCS 0x2 /* PMC register(s) modified */
++#define PFM_SETFL_PRIV_SWITCH	0x4 /* must switch set on restart */
++#define PFM_SETFL_PRIV_MOD_BOTH	(PFM_SETFL_PRIV_MOD_PMDS | PFM_SETFL_PRIV_MOD_PMCS)
++
++/*
++ * context flags
++ */
++struct pfm_context_flags {
++	unsigned int block:1;		/* task blocks on user notifications */
++	unsigned int system:1;		/* do system wide monitoring */
++	unsigned int no_msg:1;		/* no message sent on overflow */
++	unsigned int switch_ovfl:1;	/* switch set on counter ovfl */
++	unsigned int switch_time:1;	/* switch set on timeout */
++	unsigned int started:1;		/* pfm_start() issued */
++	unsigned int work_type:2;	/* type of work for pfm_handle_work */
++	unsigned int mmap_nlock:1;	/* no lock in pfm_release_buf_space */
++	unsigned int ia64_v20_compat:1;	/* context is IA-64 v2.0 mode */
++	unsigned int can_restart:8;	/* allowed to issue a PFM_RESTART */
++	unsigned int reset_count:8;	/* number of pending resets */
++	unsigned int reserved:6;	/* for future use */
++};
++
++/*
++ * values for work_type (TIF_PERFMON_WORK must be set)
++ */
++#define PFM_WORK_NONE	0	/* nothing to do */
++#define PFM_WORK_RESET	1	/* reset overflowed counters */
++#define PFM_WORK_BLOCK	2	/* block current thread */
++#define PFM_WORK_ZOMBIE	3	/* cleanup zombie context */
++
++/*
++ * check_mask bitmask values for pfm_check_task_state()
++ */
++#define PFM_CMD_STOPPED		0x01	/* command needs thread stopped */
++#define PFM_CMD_UNLOADED	0x02	/* command needs ctx unloaded */
++#define PFM_CMD_UNLOAD		0x04	/* command is unload */
++
++#include <linux/perfmon_pmu.h>
++#include <linux/perfmon_fmt.h>
++
++/*
++ * context: encapsulates all the state of a monitoring session
++ */
++struct pfm_context {
++	spinlock_t		lock;	/* context protection */
++
++	struct pfm_context_flags flags;	/*  flags */
++	u32			state;	/* state */
++	struct task_struct 	*task;	/* attached task */
++
++	struct completion       restart_complete;/* block on notification */
++	u64 			last_act;	/* last activation */
++	u32			last_cpu;   	/* last CPU used (SMP only) */
++	u32			cpu;		/* cpu bound to context */
++
++	struct pfm_smpl_fmt	*smpl_fmt;	/* buffer format callbacks */
++	void			*smpl_addr;	/* user smpl buffer base */
++	size_t			smpl_size;	/* user smpl buffer size */
++	void			*real_smpl_addr;/* actual smpl buffer base */
++	size_t			real_smpl_size; /* actual smpl buffer size */
++
++	wait_queue_head_t 	msgq_wait;	/* pfm_read() wait queue */
++
++	union pfarg_msg		msgq[PFM_MSGS_COUNT];
++	int			msgq_head;
++	int			msgq_tail;
++
++	struct fasync_struct	*async_queue;
++
++	struct pfm_event_set	*active_set;  /* active set */
++	struct list_head	set_list;    /* ordered list of sets */
++
++	/*
++	 * save stack space by allocating temporary variables for
++	 * pfm_overflow_handler() in pfm_context
++	 */
++	struct pfm_ovfl_arg 	ovfl_arg;
++	u64			ovfl_ovfl_notify[PFM_PMD_BV];
++};
++
++static inline struct pfm_arch_context *pfm_ctx_arch(struct pfm_context *c)
++{
++	return (struct pfm_arch_context *)(c+1);
++}
++
++static inline void pfm_set_pmu_owner(struct task_struct *task,
++				     struct pfm_context *ctx)
++{
++	BUG_ON(task && task->pid == 0);
++	__get_cpu_var(pmu_owner) = task;
++	__get_cpu_var(pmu_ctx) = ctx;
++}
++
++static inline void pfm_inc_activation(void)
++{
++	__get_cpu_var(pmu_activation_number)++;
++}
++
++static inline void pfm_set_activation(struct pfm_context *ctx)
++{
++	ctx->last_act = __get_cpu_var(pmu_activation_number);
++}
++
++static inline void pfm_set_last_cpu(struct pfm_context *ctx, int cpu)
++{
++	ctx->last_cpu = cpu;
++}
++
++extern struct pfm_pmu_config  *pfm_pmu_conf;
++extern struct pfm_controls pfm_controls;
++extern int perfmon_disabled;
++
++int  pfm_get_args(void __user *ureq, size_t sz, size_t lsz, void *laddr,
++		  void **req, void **to_free);
++
++int pfm_get_task(struct pfm_context *ctx, pid_t pid, struct task_struct **task);
++int pfm_get_smpl_arg(char __user *fmt_uname, void __user *uaddr, size_t usize, void **arg,
++		     struct pfm_smpl_fmt **fmt);
++
++int pfm_alloc_fd(struct file **cfile);
++
++int __pfm_write_pmcs(struct pfm_context *ctx, struct pfarg_pmc *req, int count);
++int __pfm_write_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count,
++		     int compat);
++int __pfm_read_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count);
++int __pfm_load_context(struct pfm_context *ctx, struct pfarg_load *req,
++		       struct task_struct *task);
++int __pfm_unload_context(struct pfm_context *ctx, int *can_release);
++int __pfm_stop(struct pfm_context *ctx, int *release_info);
++int  __pfm_restart(struct pfm_context *ctx, int *unblock);
++int __pfm_start(struct pfm_context *ctx, struct pfarg_start *start);
++int __pfm_delete_evtsets(struct pfm_context *ctx, void *arg, int count);
++int __pfm_getinfo_evtsets(struct pfm_context *ctx, struct pfarg_setinfo *req,
++			  int count);
++int __pfm_create_evtsets(struct pfm_context *ctx, struct pfarg_setdesc *req,
++			int count);
++
++int __pfm_create_context(struct pfarg_ctx *req,
++			 struct pfm_smpl_fmt *fmt,
++			 void *fmt_arg,
++			 int mode,
++			 struct pfm_context **new_ctx);
++
++int pfm_check_task_state(struct pfm_context *ctx, int check_mask,
++			 unsigned long *flags);
++
++struct pfm_event_set *pfm_find_set(struct pfm_context *ctx, u16 set_id,
++				   int alloc);
++
++struct pfm_context *pfm_get_ctx(int fd);
++
++void pfm_context_free(struct pfm_context *ctx);
++struct pfm_context *pfm_context_alloc(void);
++int pfm_pmu_conf_get(int autoload);
++void pfm_pmu_conf_put(void);
++
++int pfm_pmu_acquire(void);
++void pfm_pmu_release(void);
++
++int pfm_reserve_session(int is_system, u32 cpu);
++int pfm_release_session(int is_system, u32 cpu);
++
++int pfm_reserve_allcpus(void);
++int pfm_release_allcpus(void);
++
++int pfm_smpl_buffer_alloc(struct pfm_context *ctx, size_t rsize);
++int pfm_reserve_buf_space(size_t size);
++void pfm_release_buf_space(struct pfm_context *ctx, size_t size);
++
++struct pfm_smpl_fmt *pfm_smpl_fmt_get(char *name);
++void pfm_smpl_fmt_put(struct pfm_smpl_fmt *fmt);
++
++int  pfm_init_sysfs(void);
++int  pfm_init_debugfs(void);
++ssize_t pfm_sysfs_session_show(char *buf, size_t sz, int what);
++int pfm_sysfs_remove_pmu(struct pfm_pmu_config *pmu);
++int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu);
++
++int pfm_sysfs_add_fmt(struct pfm_smpl_fmt *fmt);
++int pfm_sysfs_remove_fmt(struct pfm_smpl_fmt *fmt);
++
++int pfm_debugfs_add_cpu(int mycpu);
++void pfm_debugfs_del_cpu(int mycpu);
++
++void pfm_interrupt_handler(unsigned long iip, struct pt_regs *regs);
++void pfm_save_prev_context(struct pfm_context *ctxp);
++
++void pfm_reset_pmds(struct pfm_context *ctx, struct pfm_event_set *set,
++		    int num_pmds,
++		    int reset_mode);
++
++int pfm_prepare_sets(struct pfm_context *ctx, struct pfm_event_set *act_set);
++int pfm_sets_init(void);
++
++int pfm_mmap_set(struct pfm_context *ctx, struct vm_area_struct *vma,
++		 size_t size);
++
++void pfm_free_sets(struct pfm_context *ctx);
++void pfm_init_evtset(struct pfm_event_set *set);
++void pfm_switch_sets_from_intr(struct pfm_context *ctx);
++enum hrtimer_restart pfm_handle_switch_timeout(struct hrtimer *t);
++
++enum hrtimer_restart pfm_switch_sets(struct pfm_context *ctx,
++		    struct pfm_event_set *new_set,
++		    int reset_mode,
++		    int no_restart);
++
++void pfm_save_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
++int pfm_ovfl_notify_user(struct pfm_context *ctx,
++			struct pfm_event_set *set,
++			unsigned long ip);
++
++int pfm_init_fs(void);
++
++/*
++ * When adding new stats, make sure you also
++ * update the name table in perfmon_debugfs.c
++ */
++enum pfm_stats_names {
++	PFM_ST_ovfl_intr_all_count = 0,
++	PFM_ST_ovfl_intr_ns,
++	PFM_ST_ovfl_intr_p1_ns,
++	PFM_ST_ovfl_intr_p2_ns,
++	PFM_ST_ovfl_intr_p3_ns,
++	PFM_ST_ovfl_intr_spurious_count,
++	PFM_ST_ovfl_intr_replay_count,
++	PFM_ST_ovfl_intr_regular_count,
++	PFM_ST_handle_work_count,
++	PFM_ST_ovfl_notify_count,
++	PFM_ST_reset_pmds_count,
++	PFM_ST_pfm_restart_count,
++	PFM_ST_fmt_handler_calls,
++	PFM_ST_fmt_handler_ns,
++	PFM_ST_set_switch_count,
++	PFM_ST_set_switch_ns,
++	PFM_ST_ctxsw_count,
++	PFM_ST_ctxsw_ns,
++	PFM_ST_handle_timeout_count,
++	PFM_ST_ovfl_intr_nmi_count,
++	PFM_ST_LAST	/* last entry marked */
++};
++#define PFM_NUM_STATS PFM_ST_LAST
++
++struct pfm_stats {
++	u64 v[PFM_NUM_STATS];
++	struct dentry *dirs[PFM_NUM_STATS];
++	struct dentry *cpu_dir;
++	char cpu_name[8];
++};
++
++#define pfm_stats_get(x)  __get_cpu_var(pfm_stats).v[PFM_ST_##x]
++#define pfm_stats_inc(x)  __get_cpu_var(pfm_stats).v[PFM_ST_##x]++
++#define pfm_stats_add(x,y)  __get_cpu_var(pfm_stats).v[PFM_ST_##x] += (y)
++
++/*
++ * include arch-specific kernel level only definitions
++ * (split with perfmon_api.h is necessary to avoid circular
++ *  dependencies on certain data structures definitions)
++ */
++#include <asm/perfmon.h>
++
++extern const struct file_operations pfm_file_ops;
++/*
++ * max vector argument elements for local storage (no kmalloc/kfree)
++ * The PFM_ARCH_PM*_ARG should be defined in the arch specific perfmon.h
++ * file. If not, default (conservative) values are used
++ */
++
++#ifndef PFM_ARCH_PMC_STK_ARG
++#define PFM_ARCH_PMC_STK_ARG	1
++#endif
++
++#ifndef PFM_ARCH_PMD_STK_ARG
++#define PFM_ARCH_PMD_STK_ARG	1
++#endif
++
++#define PFM_PMC_STK_ARG	PFM_ARCH_PMC_STK_ARG
++#define PFM_PMD_STK_ARG	PFM_ARCH_PMD_STK_ARG
++
++#define PFM_BPL		64
++#define PFM_LBPL	6	/* log2(BPL) */
++
++/*
++ * upper limit for count in calls that take vector arguments. This is used
++ * to prevent for multiplication overflow when we compute actual storage size
++ */
++#define PFM_MAX_ARG_COUNT(m) (INT_MAX/sizeof(*(m)))
++
++/*
++ * read a single PMD register. PMD register mapping is provided by PMU
++ * description module. Virtual PMD registers have special handler.
++ */
++static inline u64 pfm_read_pmd(struct pfm_context *ctx, unsigned int cnum)
++{
++	if (unlikely(pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_V))
++		return pfm_pmu_conf->pmd_sread(ctx, cnum);
++
++	return pfm_arch_read_pmd(ctx, cnum);
++}
++
++static inline void pfm_write_pmd(struct pfm_context *ctx, unsigned int cnum, u64 value)
++{
++	/*
++	 * PMD writes are ignored for read-only registers
++	 */
++	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_RO)
++		return;
++
++	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_V) {
++		pfm_pmu_conf->pmd_swrite(ctx, cnum, value);
++		return;
++	}
++	/*
++	 * clear unimplemented bits
++	 */
++	value &= ~pfm_pmu_conf->pmd_desc[cnum].rsvd_msk;
++
++	pfm_arch_write_pmd(ctx, cnum, value);
++}
++
++#define cast_ulp(_x) ((unsigned long *)_x)
++
++#define PFM_NORMAL      0
++#define PFM_COMPAT      1
++
++void __pfm_exit_thread(struct task_struct *task);
++void __pfm_copy_thread(struct task_struct *task);
++void pfm_ctxsw(struct task_struct *prev, struct task_struct *next);
++void pfm_handle_work(struct pt_regs *regs);
++void __pfm_init_percpu (void *dummy);
++void pfm_cpu_disable(void);
++
++static inline void pfm_exit_thread(struct task_struct *task)
++{
++	if (task->pfm_context)
++		__pfm_exit_thread(task);
++}
++
++static inline void pfm_copy_thread(struct task_struct *task)
++{
++	/*
++	 * context or perfmon TIF state  is NEVER inherited
++	 * in child task. Holds for per-thread and system-wide
++	 */
++	task->pfm_context = NULL;
++	clear_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
++	clear_tsk_thread_flag(task, TIF_PERFMON_WORK);
++}
++
++static inline void pfm_init_percpu(void)
++{
++	__pfm_init_percpu(NULL);
++}
++
++#endif /* __KERNEL__ */
++
++#else /* !CONFIG_PERFMON */
++#ifdef __KERNEL__
++
++#define tsks_have_perfmon(p, n)	(0)
++#define pfm_cpu_disable()		do { } while (0)
++#define pfm_init_percpu()		do { } while (0)
++#define pfm_exit_thread(_t)  		do { } while (0)
++#define pfm_handle_work(_t)    		do { } while (0)
++#define pfm_copy_thread(_t)		do { } while (0)
++#define pfm_ctxsw(_p, _t)     		do { } while (0)
++#define	pfm_release_allcpus()		do { } while (0)
++#define	pfm_reserve_allcpus()		(0)
++#ifdef __ia64__
++#define pfm_release_dbregs(_t) 		do { } while (0)
++#define pfm_use_dbregs(_t)     		(0)
++#endif
++
++#endif /* __KERNEL__ */
++
++#endif /* CONFIG_PERFMON */
++
++#endif /* __LINUX_PERFMON_H__ */
+--- /dev/null
++++ b/include/linux/perfmon_dfl_smpl.h
+@@ -0,0 +1,78 @@
++/*
++ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++ *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file implements the new dfl sampling buffer format
++ * for perfmon2 subsystem.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#ifndef __PERFMON_DFL_SMPL_H__
++#define __PERFMON_DFL_SMPL_H__ 1
++
++/*
++ * format specific parameters (passed at context creation)
++ */
++struct pfm_dfl_smpl_arg {
++	__u64 buf_size;		/* size of the buffer in bytes */
++	__u32 buf_flags;	/* buffer specific flags */
++	__u32 reserved1;	/* for future use */
++	__u64 reserved[6];	/* for future use */
++};
++
++/*
++ * This header is at the beginning of the sampling buffer returned to the user.
++ * It is directly followed by the first record.
++ */
++struct pfm_dfl_smpl_hdr {
++	__u64 hdr_count;	/* how many valid entries */
++	__u64 hdr_cur_offs;	/* current offset from top of buffer */
++	__u64 hdr_overflows;	/* #overflows for buffer */
++	__u64 hdr_buf_size;	/* bytes in the buffer */
++	__u64 hdr_min_buf_space;/* minimal buffer size (internal use) */
++	__u32 hdr_version;	/* smpl format version */
++	__u32 hdr_buf_flags;	/* copy of buf_flags */
++	__u64 hdr_reserved[10];	/* for future use */
++};
++
++/*
++ * Entry header in the sampling buffer.  The header is directly followed
++ * with the values of the PMD registers of interest saved in increasing
++ * index order: PMD4, PMD5, and so on. How many PMDs are present depends
++ * on how the session was programmed.
++ *
++ * In the case where multiple counters overflow at the same time, multiple
++ * entries are written consecutively.
++ *
++ * last_reset_value member indicates the initial value of the overflowed PMD.
++ */
++struct pfm_dfl_smpl_entry {
++	__u32	pid;		/* thread id (for NPTL, this is gettid()) */
++	__u16	ovfl_pmd;	/* index of overflowed PMD for this sample */
++	__u16	reserved;	/* for future use */
++	__u64	last_reset_val;	/* initial value of overflowed PMD */
++	__u64	ip;		/* where did the overflow interrupt happened  */
++	__u64	tstamp;		/* overflow timetamp */
++	__u16	cpu;		/* cpu on which the overfow occurred */
++	__u16	set;		/* event set active when overflow ocurred   */
++	__u32	tgid;		/* thread group id (for NPTL, this is getpid())*/
++};
++
++#define PFM_DFL_SMPL_VERSION_MAJ 1U
++#define PFM_DFL_SMPL_VERSION_MIN 0U
++#define PFM_DFL_SMPL_VERSION (((PFM_DFL_SMPL_VERSION_MAJ&0xffff)<<16)|\
++				(PFM_DFL_SMPL_VERSION_MIN & 0xffff))
++
++#endif /* __PERFMON_DFL_SMPL_H__ */
+--- /dev/null
++++ b/include/linux/perfmon_fmt.h
+@@ -0,0 +1,88 @@
++/*
++ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * Interface for custom sampling buffer format modules
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#ifndef __PERFMON_FMT_H__
++#define __PERFMON_FMT_H__ 1
++
++#include <linux/kobject.h>
++
++struct pfm_ovfl_arg {
++	u16 ovfl_pmd;	/* index of overflowed PMD  */
++	u16 active_set;	/* set active at the time of the overflow */
++	u32 ovfl_ctrl;	/* control flags */
++	u64 pmd_last_reset;	/* last reset value of overflowed PMD */
++	u64 smpl_pmds_values[PFM_MAX_PMDS]; 	/* values of other PMDs */
++	u64 pmd_eventid;	/* eventid associated with PMD */
++	u16 num_smpl_pmds;	/* number of PMDS in smpl_pmd_values */
++};
++
++/*
++ * ovfl_ctrl bitmask of flags
++ */
++#define PFM_OVFL_CTRL_NOTIFY	0x1	/* notify user */
++#define PFM_OVFL_CTRL_RESET	0x2	/* reset overflowed pmds */
++#define PFM_OVFL_CTRL_MASK	0x4	/* mask monitoring */
++
++
++typedef int (*fmt_validate_t )(u32 flags, u16 npmds, void *arg);
++typedef	int (*fmt_getsize_t)(u32 flags, void *arg, size_t *size);
++typedef int (*fmt_init_t)(struct pfm_context *ctx, void *buf, u32 flags, u16 nmpds, void *arg);
++typedef int (*fmt_restart_t)(int is_active, u32 *ovfl_ctrl, void *buf);
++typedef int (*fmt_exit_t)(void *buf);
++typedef int (*fmt_handler_t)(void *buf, struct pfm_ovfl_arg *arg,
++			     unsigned long ip, u64 stamp, void *data);
++
++struct pfm_smpl_fmt {
++	char		*fmt_name;	/* name of the format (required) */
++	size_t		fmt_arg_size;	/* size of fmt args for ctx create */
++	u32		fmt_flags;	/* format specific flags */
++	u32		fmt_version;	/* format version number */
++
++	fmt_validate_t	fmt_validate;	/* validate context flags */
++	fmt_getsize_t	fmt_getsize;	/* get size for sampling buffer */
++	fmt_init_t	fmt_init;	/* initialize buffer area */
++	fmt_handler_t	fmt_handler;	/* overflow handler (required) */
++	fmt_restart_t	fmt_restart;	/* restart after notification  */
++	fmt_exit_t	fmt_exit;	/* context termination */
++
++	struct list_head fmt_list;	/* internal use only */
++
++	struct kobject	kobj;		/* sysfs internal use only */
++	struct module	*owner;		/* pointer to module owner */
++	u32		fmt_qdepth;	/* Max notify queue depth (required) */
++};
++#define to_smpl_fmt(n) container_of(n, struct pfm_smpl_fmt, kobj)
++
++#define PFM_FMTFL_IS_BUILTIN	0x1	/* fmt is compiled in */
++/*
++ * we need to know whether the format is builtin or compiled
++ * as a module
++ */
++#ifdef MODULE
++#define PFM_FMT_BUILTIN_FLAG	0	/* not built as a module */
++#else
++#define PFM_FMT_BUILTIN_FLAG	PFM_PMUFL_IS_BUILTIN /* built as a module */
++#endif
++
++int pfm_fmt_register(struct pfm_smpl_fmt *fmt);
++int pfm_fmt_unregister(struct pfm_smpl_fmt *fmt);
++void pfm_sysfs_builtin_fmt_add(void);
++
++#endif /* __PERFMON_FMT_H__ */
+--- /dev/null
++++ b/include/linux/perfmon_pmu.h
+@@ -0,0 +1,178 @@
++/*
++ * Copyright (c) 2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * Interface for PMU description modules
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#ifndef __PERFMON_PMU_H__
++#define __PERFMON_PMU_H__ 1
++
++/*
++ * generic information about a PMC or PMD register
++ */
++struct pfm_regmap_desc {
++	u16  type;		/* role of the register */
++	u16  reserved1;		/* for future use */
++	u32  reserved2;		/* for future use */
++	u64  dfl_val;		/* power-on default value (quiescent) */
++	u64  rsvd_msk;		/* reserved bits: 1 means reserved */
++	u64  no_emul64_msk;	/* bits to clear for PFM_REGFL_NO_EMUL64 */
++	unsigned long hw_addr;	/* HW register address or index */
++	struct kobject	kobj;	/* for internal use only */
++	char *desc;		/* HW register description string */
++};
++#define to_reg(n) container_of(n, struct pfm_regmap_desc, kobj)
++
++/*
++ * pfm_reg_desc helper macros
++ */
++#define PMC_D(t,d,v,r,n, h)   \
++	{ .type = t,          \
++	  .desc = d,          \
++	  .dfl_val = v,       \
++	  .rsvd_msk = r,      \
++	  .no_emul64_msk = n, \
++	  .hw_addr = h	      \
++	}
++
++#define PMD_D(t,d, h)         \
++	{ .type = t,          \
++	  .desc = d,          \
++	  .rsvd_msk = 0,      \
++	  .no_emul64_msk = 0, \
++	  .hw_addr = h	      \
++	}
++
++#define PMX_NA \
++	{ .type = PFM_REG_NA }
++
++/*
++ * type of a PMU register (16-bit bitmask) for use with pfm_reg_desc.type
++ */
++#define PFM_REG_NA	0x00  /* not avail. (not impl.,no access) must be 0 */
++#define PFM_REG_I	0x01  /* PMC/PMD: implemented */
++#define PFM_REG_WC	0x02  /* PMC: has write_checker */
++#define PFM_REG_C64	0x04  /* PMD: 64-bit virtualization */
++#define PFM_REG_RO	0x08  /* PMD: read-only (writes ignored) */
++#define PFM_REG_V	0x10  /* PMD: virtual reg (provided by PMU description) */
++#define PFM_REG_INTR	0x20  /* PMD: register can generate interrupt */
++#define PFM_REG_NO64	0x100 /* PMC: supports PFM_REGFL_NO_EMUL64 */
++
++/*
++ * define some shortcuts for common types
++ */
++#define PFM_REG_W	(PFM_REG_WC|PFM_REG_I)
++#define PFM_REG_W64	(PFM_REG_WC|PFM_REG_NO64|PFM_REG_I)
++#define PFM_REG_C	(PFM_REG_C64|PFM_REG_INTR|PFM_REG_I)
++#define PFM_REG_I64	(PFM_REG_NO64|PFM_REG_I)
++#define PFM_REG_IRO	(PFM_REG_I|PFM_REG_RO)
++
++typedef int (*pfm_pmc_check_t)(struct pfm_context *ctx,
++			       struct pfm_event_set *set,
++			       struct pfarg_pmc *req);
++
++typedef int (*pfm_pmd_check_t)(struct pfm_context *ctx,
++			       struct pfm_event_set *set,
++			       struct pfarg_pmd *req);
++
++
++typedef u64 (*pfm_pmd_sread_t)(struct pfm_context *ctx, unsigned int cnum);
++typedef void (*pfm_pmd_swrite_t)(struct pfm_context *ctx, unsigned int cnum, u64 val);
++
++/*
++ * registers description
++ */
++struct pfm_regdesc {
++	u64 pmcs[PFM_PMC_BV];		/* available PMC */
++	u64 pmds[PFM_PMD_BV];		/* available PMD */
++	u64 rw_pmds[PFM_PMD_BV];	/* available RW PMD */
++	u64 intr_pmds[PFM_PMD_BV];	/* PMD generating intr */
++	u64 cnt_pmds[PFM_PMD_BV];	/* PMD counters */
++	u16 max_pmc;			/* highest+1 avail PMC */
++	u16 max_pmd;			/* highest+1 avail PMD */
++	u16 max_rw_pmd;			/* highest+1 avail RW PMD */
++	u16 first_intr_pmd;		/* first intr PMD */
++	u16 max_intr_pmd;		/* highest+1 intr PMD */
++	u16 num_rw_pmd;			/* number of avail RW PMD */
++	u16 num_pmcs;			/* number of logical PMCS */
++	u16 num_pmds;			/* number of logical PMDS */
++	u16 num_counters;		/* number of counting PMD */
++};
++
++/*
++ * structure used by pmu description modules
++ *
++ * probe_pmu() routine return value:
++ * 	- 1 means recognized PMU
++ * 	- 0 means not recognized PMU
++ */
++struct pfm_pmu_config {
++	char *pmu_name;				/* PMU family name */
++	char *version;				/* config module version number */
++
++	int counter_width;			/* width of hardware counter */
++
++	struct pfm_regmap_desc	*pmc_desc;	/* PMC register descriptions */
++	struct pfm_regmap_desc	*pmd_desc;	/* PMD register descriptions */
++
++	pfm_pmc_check_t		pmc_write_check;/* PMC write checker callback (optional) */
++	pfm_pmd_check_t		pmd_write_check;/* PMD write checker callback (optional) */
++	pfm_pmd_check_t		pmd_read_check;	/* PMD read checker callback  (optional) */
++
++	pfm_pmd_sread_t		pmd_sread;	/* PMD model specific read (optional) */
++	pfm_pmd_swrite_t	pmd_swrite;	/* PMD model specific write (optional) */
++
++	int             	(*probe_pmu)(void);/* probe PMU routine */
++
++	u16			num_pmc_entries;/* number of entries in pmc_desc */
++	u16			num_pmd_entries;/* number of entries in pmd_desc */
++
++	void			*arch_info;	/* arch-specific information */
++	u32			flags;		/* set of flags */
++
++	struct module		*owner;		/* pointer to module struct */
++
++	/*
++	 * fields computed internally, do not set in module
++	 */
++	struct pfm_regdesc	regs;		/* registers currently available */
++	struct pfm_regdesc	full_regs;	/* registers presented by module */
++
++	u64			ovfl_mask;	/* overflow mask */
++	struct kobject		kobj;		/* for internal use only */
++};
++#define to_pmu(n) container_of(n, struct pfm_pmu_config, kobj)
++
++/*
++ * pfm_pmu_config flags
++ */
++#define PFM_PMUFL_IS_BUILTIN	0x1	/* pmu config is compiled in */
++
++/*
++ * we need to know whether the PMU description is builtin or compiled
++ * as a module
++ */
++#ifdef MODULE
++#define PFM_PMU_BUILTIN_FLAG	0	/* not built as a module */
++#else
++#define PFM_PMU_BUILTIN_FLAG	PFM_PMUFL_IS_BUILTIN /* built as a module */
++#endif
++
++int pfm_pmu_register(struct pfm_pmu_config *cfg);
++void pfm_pmu_unregister(struct pfm_pmu_config *cfg);
++
++#endif /* __PERFMON_PMU_H__ */
+--- a/include/linux/sched.h
++++ b/include/linux/sched.h
+@@ -94,6 +94,7 @@ struct sched_param {
+ struct exec_domain;
+ struct futex_pi_state;
+ struct bio;
++struct pfm_context;
+ 
+ /*
+  * List of flags we want to share for kernel threads,
+@@ -1178,6 +1179,9 @@ struct task_struct {
+ 	int make_it_fail;
+ #endif
+ 	struct prop_local_single dirties;
++#ifdef CONFIG_PERFMON
++	struct pfm_context *pfm_context;
++#endif
+ };
+ 
+ /*
+--- a/include/linux/syscalls.h
++++ b/include/linux/syscalls.h
+@@ -29,6 +29,13 @@ struct msqid_ds;
+ struct new_utsname;
+ struct nfsctl_arg;
+ struct __old_kernel_stat;
++struct pfarg_ctx;
++struct pfarg_pmc;
++struct pfarg_pmd;
++struct pfarg_start;
++struct pfarg_load;
++struct pfarg_setinfo;
++struct pfarg_setdesc;
+ struct pollfd;
+ struct rlimit;
+ struct rusage;
+@@ -614,4 +621,27 @@ asmlinkage long sys_fallocate(int fd, in
+ 
+ int kernel_execve(const char *filename, char *const argv[], char *const envp[]);
+ 
++asmlinkage long sys_pfm_create_context(struct pfarg_ctx __user *ureq,
++				       void __user *uarg, size_t smpl_size);
++asmlinkage long sys_pfm_write_pmcs(int fd, struct pfarg_pmc __user *ureq,
++				   int count);
++asmlinkage long sys_pfm_write_pmds(int fd, struct pfarg_pmd __user *ureq,
++				   int count);
++asmlinkage long sys_pfm_read_pmds(int fd, struct pfarg_pmd __user *ureq,
++				  int count);
++asmlinkage long sys_pfm_restart(int fd);
++asmlinkage long sys_pfm_stop(int fd);
++asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *ureq);
++asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ureq);
++asmlinkage long sys_pfm_unload_context(int fd);
++asmlinkage long sys_pfm_delete_evtsets(int fd,
++				       struct pfarg_setinfo __user *ureq,
++				       int count);
++asmlinkage long sys_pfm_create_evtsets(int fd,
++				       struct pfarg_setdesc __user *ureq,
++				       int count);
++asmlinkage long sys_pfm_getinfo_evtsets(int fd,
++					struct pfarg_setinfo __user *ureq,
++					int count);
++
+ #endif
+--- a/kernel/sched.c
++++ b/kernel/sched.c
+@@ -63,6 +63,7 @@
+ #include <linux/reciprocal_div.h>
+ #include <linux/unistd.h>
+ #include <linux/pagemap.h>
++#include <linux/perfmon.h>
+ 
+ #include <asm/tlb.h>
+ #include <asm/irq_regs.h>
+--- a/kernel/sys_ni.c
++++ b/kernel/sys_ni.c
+@@ -122,6 +122,19 @@ cond_syscall(sys_vm86);
+ cond_syscall(compat_sys_ipc);
+ cond_syscall(compat_sys_sysctl);
+ 
++cond_syscall(sys_pfm_create_context);
++cond_syscall(sys_pfm_write_pmcs);
++cond_syscall(sys_pfm_write_pmds);
++cond_syscall(sys_pfm_read_pmds);
++cond_syscall(sys_pfm_restart);
++cond_syscall(sys_pfm_start);
++cond_syscall(sys_pfm_stop);
++cond_syscall(sys_pfm_load_context);
++cond_syscall(sys_pfm_unload_context);
++cond_syscall(sys_pfm_create_evtsets);
++cond_syscall(sys_pfm_delete_evtsets);
++cond_syscall(sys_pfm_getinfo_evtsets);
++
+ /* arch-specific weak syscall entries */
+ cond_syscall(sys_pciconfig_read);
+ cond_syscall(sys_pciconfig_write);
+--- /dev/null
++++ b/perfmon/Makefile
+@@ -0,0 +1,8 @@
++#
++# Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
++# Contributed by Stephane Eranian <eranian@hpl.hp.com>
++#
++obj-$(CONFIG_PERFMON) = perfmon.o perfmon_rw.o perfmon_res.o perfmon_fmt.o \
++			perfmon_pmu.o perfmon_sysfs.o perfmon_syscalls.o   \
++			perfmon_file.o perfmon_ctxsw.o perfmon_intr.o	   \
++			perfmon_dfl_smpl.o perfmon_sets.o perfmon_debugfs.o
+--- /dev/null
++++ b/perfmon/perfmon.c
+@@ -0,0 +1,1801 @@
++/*
++ * perfmon.c: perfmon2 core functions
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://www.hpl.hp.com/research/linux/perfmon
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/kernel.h>
++#include <linux/vmalloc.h>
++#include <linux/poll.h>
++#include <linux/ptrace.h>
++#include <linux/perfmon.h>
++#include <linux/cpu.h>
++#include <linux/random.h>
++
++/*
++ * internal variables
++ */
++static struct kmem_cache *pfm_ctx_cachep;
++
++/*
++ * external variables
++ */
++DEFINE_PER_CPU(struct task_struct *, pmu_owner);
++DEFINE_PER_CPU(struct pfm_context  *, pmu_ctx);
++DEFINE_PER_CPU(u64, pmu_activation_number);
++DEFINE_PER_CPU(struct pfm_stats, pfm_stats);
++DEFINE_PER_CPU(struct hrtimer, pfm_hrtimer);
++
++#define PFM_INVALID_ACTIVATION	((u64)~0)
++
++int perfmon_disabled;	/* >0 if perfmon is disabled */
++
++/*
++ * Reset PMD register flags
++ */
++#define PFM_PMD_RESET_NONE	0	/* do not reset (pfm_switch_set) */
++#define PFM_PMD_RESET_SHORT	1	/* use short reset value */
++#define PFM_PMD_RESET_LONG	2	/* use long reset value  */
++
++/*
++ * get a new message slot from the queue. If the queue is full NULL
++ * is returned and monitoring stops.
++ */
++static union pfarg_msg *pfm_get_new_msg(struct pfm_context *ctx)
++{
++	int next;
++
++	next = ctx->msgq_head & PFM_MSGQ_MASK;
++
++	if ((ctx->msgq_head - ctx->msgq_tail) == PFM_MSGS_COUNT)
++		return NULL;
++
++	/*
++	 * move to next possible slot
++	 */
++	ctx->msgq_head++;
++
++	PFM_DBG_ovfl("head=%d tail=%d msg=%d",
++		ctx->msgq_head & PFM_MSGQ_MASK,
++		ctx->msgq_tail & PFM_MSGQ_MASK,
++		next);
++
++	return ctx->msgq+next;
++}
++
++void pfm_context_free(struct pfm_context *ctx)
++{
++	struct pfm_smpl_fmt *fmt;
++
++	pfm_arch_context_free(ctx);
++
++	fmt = ctx->smpl_fmt;
++
++	pfm_free_sets(ctx);
++
++	if (ctx->smpl_addr) {
++		PFM_DBG("freeing sampling buffer @%p size=%zu",
++			ctx->real_smpl_addr,
++			ctx->real_smpl_size);
++
++		pfm_release_buf_space(ctx, ctx->real_smpl_size);
++
++		if (fmt->fmt_exit)
++			(*fmt->fmt_exit)(ctx->smpl_addr);
++
++		vfree(ctx->real_smpl_addr);
++	}
++
++	PFM_DBG("free ctx @%p", ctx);
++	kmem_cache_free(pfm_ctx_cachep, ctx);
++	/*
++	 * decrease refcount on:
++	 * 	- PMU description table
++	 * 	- sampling format
++	 */
++	pfm_pmu_conf_put();
++	pfm_smpl_fmt_put(fmt);
++	pfm_pmu_release();
++}
++
++/*
++ * only called in for the current task
++ */
++static int pfm_setup_smpl_fmt(struct pfm_smpl_fmt *fmt, void *fmt_arg,
++				struct pfm_context *ctx, u32 ctx_flags,
++				int mode, struct file *filp)
++{
++	size_t size = 0;
++	int ret = 0;
++
++	/*
++	 * validate parameters
++	 */
++	if (fmt->fmt_validate) {
++		ret = (*fmt->fmt_validate)(ctx_flags, pfm_pmu_conf->regs.num_pmds,
++					   fmt_arg);
++		PFM_DBG("validate(0x%x,%p)=%d", ctx_flags, fmt_arg, ret);
++		if (ret)
++			goto error;
++	}
++
++	/*
++	 * check if buffer format wants to use perfmon
++	 * buffer allocation/mapping service
++	 */
++	size = 0;
++	if (fmt->fmt_getsize) {
++		ret = (*fmt->fmt_getsize)(ctx_flags, fmt_arg, &size);
++		if (ret) {
++			PFM_DBG("cannot get size ret=%d", ret);
++			goto error;
++		}
++	}
++
++	if (size) {
++		if (mode == PFM_COMPAT)
++			ret = pfm_smpl_buffer_alloc_compat(ctx, size, filp);
++		else
++			ret = pfm_smpl_buffer_alloc(ctx, size);
++		if (ret)
++			goto error;
++
++	}
++
++	if (fmt->fmt_init) {
++		ret = (*fmt->fmt_init)(ctx, ctx->smpl_addr, ctx_flags,
++				       pfm_pmu_conf->regs.num_pmds,
++				       fmt_arg);
++		if (ret)
++			goto error_buffer;
++	}
++	return 0;
++
++error_buffer:
++	pfm_release_buf_space(ctx, ctx->real_smpl_size);
++	/*
++	 * we do not call fmt_exit, if init has failed
++	 */
++	vfree(ctx->real_smpl_addr);
++error:
++	return ret;
++}
++
++/*
++ * interrupts are masked when entering this function.
++ * context must be in MASKED state when calling.
++ */
++static void pfm_unmask_monitoring(struct pfm_context *ctx,
++				  struct pfm_event_set *set)
++{
++	if (ctx->state != PFM_CTX_MASKED)
++		return;
++
++	PFM_DBG_ovfl("unmasking monitoring");
++
++	/*
++	 * must be done before calling
++	 * pfm_arch_unmask_monitoring()
++	 */
++	ctx->state = PFM_CTX_LOADED;
++
++	/*
++	 * we need to restore the PMDs because they
++	 * may have been modified by user while MASKED in which
++	 * case the actual registers were not updated
++	 *
++	 * XXX: could be avoided in system-wide mode
++	 */
++	pfm_arch_restore_pmds(ctx, set);
++
++	pfm_arch_unmask_monitoring(ctx, set);
++
++	set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
++
++	/*
++	 * reset set duration timer
++	 */
++	set->duration_start = sched_clock();
++}
++
++/*
++ * called from pfm_smpl_buffer_alloc_old() (IA64-COMPAT)
++ * and pfm_setup_smpl_fmt()
++ *
++ * interrupts are enabled, context is not locked.
++ */
++int pfm_smpl_buffer_alloc(struct pfm_context *ctx, size_t rsize)
++{
++#if PFM_ARCH_SMPL_ALIGN_SIZE > 0
++#define PFM_ALIGN_SMPL(a, f) (void *)((((unsigned long)(a))+(f-1)) & ~(f-1))
++#else
++#define PFM_ALIGN_SMPL(a, f) (a)
++#endif
++	void *addr, *real_addr;
++	size_t size, real_size;
++	int ret;
++
++	might_sleep();
++
++	/*
++	 * align page boundary
++	 */
++	size = PAGE_ALIGN(rsize);
++
++	/*
++	 * On some arch, it may be necessary to get an alignment greater
++	 * than page size to avoid certain cache effects (e.g., MIPS).
++	 * This is the reason for PFM_ARCH_SMPL_ALIGN_SIZE.
++	 */
++	real_size = size + PFM_ARCH_SMPL_ALIGN_SIZE;
++
++	PFM_DBG("buffer req_size=%zu actual_size=%zu before", rsize, size);
++
++	ret = pfm_reserve_buf_space(real_size);
++	if (ret)
++		return ret;
++
++	PFM_DBG("buffer req_size=%zu size=%zu real_size=%zu",
++		rsize,
++		size,
++		real_size);
++
++	/*
++	 * vmalloc can sleep. we do not hold
++	 * any spinlock and interrupts are enabled
++	 */
++	real_addr = addr = vmalloc(real_size);
++	if (!real_addr) {
++		PFM_DBG("cannot allocate sampling buffer");
++		goto unres;
++	}
++
++	/*
++	 * align the useable sampling buffer address to the arch requirement
++	 * This is a nop on most architectures
++	 */
++	addr = PFM_ALIGN_SMPL(real_addr, PFM_ARCH_SMPL_ALIGN_SIZE);
++
++	memset(addr, 0, real_size);
++
++	/*
++	 * due to cache aliasing, it may be necessary to flush the pages
++	 * on certain architectures (e.g., MIPS)
++	 */
++	pfm_cacheflush(addr, real_size);
++
++	/*
++	 * what needs to be freed
++	 */
++	ctx->real_smpl_addr = real_addr;
++	ctx->real_smpl_size = real_size;
++
++	/*
++	 * what is actually available to user
++	 */
++	ctx->smpl_addr = addr;
++	ctx->smpl_size = size;
++
++	PFM_DBG("kernel smpl buffer @ used=%p real=%p", addr, real_addr);
++
++	return 0;
++unres:
++	PFM_DBG("buffer req_size=%zu actual_size=%zu error", rsize, size);
++	pfm_release_buf_space(ctx, real_size);
++	return -ENOMEM;
++}
++
++void pfm_reset_pmds(struct pfm_context *ctx,
++		    struct pfm_event_set *set,
++		    int num_pmds,
++		    int reset_mode)
++{
++	u64 val, mask, new_seed;
++	struct pfm_pmd *reg;
++	unsigned int i, not_masked;
++
++	not_masked = ctx->state != PFM_CTX_MASKED;
++
++	PFM_DBG_ovfl("%s r_pmds=0x%llx not_masked=%d",
++		reset_mode == PFM_PMD_RESET_LONG ? "long" : "short",
++		(unsigned long long)set->reset_pmds[0],
++		not_masked);
++
++	pfm_stats_inc(reset_pmds_count);
++
++	for (i = 0; num_pmds; i++) {
++		if (test_bit(i, cast_ulp(set->reset_pmds))) {
++			num_pmds--;
++
++			reg = set->pmds + i;
++
++			val = reset_mode == PFM_PMD_RESET_LONG ? reg->long_reset : reg->short_reset;
++
++			if (reg->flags & PFM_REGFL_RANDOM) {
++				mask = reg->mask;
++				new_seed = random32();
++
++				/* construct a full 64-bit random value: */
++				if ((unlikely(mask >> 32) != 0))
++					new_seed |= (u64)random32() << 32;
++
++				/* counter values are negative numbers! */
++				val -= (new_seed & mask);
++			}
++
++			set->pmds[i].value = val;
++			reg->lval = val;
++
++			/*
++			 * not all PMD to reset are necessarily
++			 * counters
++			 */
++			if (not_masked)
++				pfm_write_pmd(ctx, i, val);
++
++			PFM_DBG_ovfl("set%u pmd%u sval=0x%llx",
++					set->id,
++					i,
++					(unsigned long long)val);
++		}
++	}
++
++	/*
++	 * done with reset
++	 */
++	bitmap_zero(cast_ulp(set->reset_pmds), i);
++
++	/*
++	 * make changes visible
++	 */
++	if (not_masked)
++		pfm_arch_serialize();
++}
++
++/*
++ * called from pfm_handle_work() and __pfm_restart()
++ * for system-wide and per-thread context to resume
++ * monitoring after a user level notification.
++ *
++ * In both cases, the context is locked and interrupts
++ * are disabled.
++ */
++static void pfm_resume_after_ovfl(struct pfm_context *ctx)
++{
++	struct pfm_smpl_fmt *fmt;
++	u32 rst_ctrl;
++	struct pfm_event_set *set;
++	u64 *reset_pmds;
++	void *hdr;
++	int state, ret;
++
++	hdr = ctx->smpl_addr;
++	fmt = ctx->smpl_fmt;
++	state = ctx->state;
++	set = ctx->active_set;
++	ret = 0;
++
++	if (hdr) {
++		rst_ctrl = 0;
++		prefetch(hdr);
++	} else
++		rst_ctrl= PFM_OVFL_CTRL_RESET;
++
++	/*
++	 * if using a sampling buffer format and it has a restart callback,
++	 * then invoke it. hdr may be NULL, if the format does not use a
++	 * perfmon buffer
++	 */
++	if (fmt && fmt->fmt_restart)
++		ret = (*fmt->fmt_restart)(state == PFM_CTX_LOADED, &rst_ctrl, hdr);
++
++	reset_pmds = set->reset_pmds;
++
++	PFM_DBG("fmt_restart=%d reset_count=%d set=%u r_pmds=0x%llx switch=%d ctx_state=%d",
++		ret,
++		ctx->flags.reset_count,
++		set->id,
++		(unsigned long long)reset_pmds[0],
++		(set->priv_flags & PFM_SETFL_PRIV_SWITCH),
++		state);
++
++	if (!ret) {
++		/*
++		 * switch set if needed
++		 */
++		if (set->priv_flags & PFM_SETFL_PRIV_SWITCH) {
++			set->priv_flags &= ~PFM_SETFL_PRIV_SWITCH;
++			pfm_switch_sets(ctx, NULL, PFM_PMD_RESET_LONG, 0);
++			set = ctx->active_set;
++		} else if (rst_ctrl & PFM_OVFL_CTRL_RESET) {
++			int nn;
++			nn = bitmap_weight(cast_ulp(set->reset_pmds),
++					   pfm_pmu_conf->regs.max_pmd);
++			if (nn)
++				pfm_reset_pmds(ctx, set, nn, PFM_PMD_RESET_LONG);
++		}
++
++		if (!(rst_ctrl & PFM_OVFL_CTRL_MASK))
++			pfm_unmask_monitoring(ctx, set);
++		else
++			PFM_DBG("stopping monitoring?");
++		ctx->state = PFM_CTX_LOADED;
++	}
++}
++
++/*
++ * This function is always called after pfm_stop has been issued
++ */
++static void pfm_flush_pmds(struct task_struct *task, struct pfm_context *ctx)
++{
++	struct pfm_event_set *set;
++	u64 *cnt_pmds;
++	u64 ovfl_mask;
++	u16 num_ovfls, i, first;
++
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	first = pfm_pmu_conf->regs.first_intr_pmd;
++	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
++
++	/*
++	 * save active set
++	 * UP:
++	 * 	if not current task and due to lazy, state may
++	 * 	still be live
++	 * for system-wide, guaranteed to run on correct CPU
++	 */
++	if (__get_cpu_var(pmu_ctx) == ctx) {
++		/*
++		 * pending overflows have been saved by pfm_stop()
++		 */
++		pfm_save_pmds(ctx, ctx->active_set);
++		pfm_set_pmu_owner(NULL, NULL);
++		PFM_DBG("released ownership");
++	}
++
++	/*
++	 * look for pending interrupts
++	 */
++	list_for_each_entry(set, &ctx->set_list, list) {
++
++		if (!set->npend_ovfls)
++			continue;
++
++		num_ovfls = set->npend_ovfls;
++		PFM_DBG("set%u nintrs=%u", set->id, num_ovfls);
++
++		for (i = first; num_ovfls; i++) {
++			if (test_bit(i, cast_ulp(set->povfl_pmds))) {
++				/* only correct value for counters */
++				if(test_bit(i, cast_ulp(cnt_pmds))) {
++					set->pmds[i].value += 1 + ovfl_mask;
++				}
++				num_ovfls--;
++			}
++			PFM_DBG("pmd%u set=%u val=0x%llx",
++				i,
++				set->id,
++				(unsigned long long)set->pmds[i].value);
++		}
++		/*
++		 * we need to clear to prevent a pfm_getinfo_evtsets() from
++		 * returning stale data even after the context is unloaded
++		 */
++		set->npend_ovfls = 0;
++		bitmap_zero(cast_ulp(set->povfl_pmds),
++			    pfm_pmu_conf->regs.max_intr_pmd);
++	}
++}
++
++/*
++ * This function is called when we need to perform asynchronous
++ * work on a context. This function is called ONLY when about to
++ * return to user mode (very much like with signals handling).
++ *
++ * There are several reasons why we come here:
++ *
++ *  - per-thread mode, not self-monitoring, to reset the counters
++ *    after a pfm_restart() by the thread controlling the context
++ *
++ *  - because we are zombie and we need to cleanup our state
++ *
++ *  - because we need to block after an overflow notification
++ *    on a context with the PFM_OVFL_NOTIFY_BLOCK flag
++ *
++ * This function is never called for a system-wide context.
++ *
++ * pfm_handle_work() can be called with interrupts enabled
++ * (TIF_NEED_RESCHED) or disabled. The down_interruptible
++ * call may sleep, therefore we must re-enable interrupts
++ * to avoid deadlocks. It is safe to do so because this function
++ * is called ONLY when returning to user level, in which case
++ * there is no risk of kernel stack overflow due to deep
++ * interrupt nesting.
++ */
++void pfm_handle_work(struct pt_regs *regs)
++{
++	struct pfm_context *ctx;
++	unsigned long flags, dummy_flags;
++	int type, ret, release_info;
++
++#ifdef CONFIG_PPC
++	/*
++	 * This is just a temporary fix. Obviously we'd like to fix the powerpc
++	 * code to make that check before calling __pfm_handle_work() to
++	 * prevent the function call overhead, but the call is made from assembly
++	 * code, so it will take a little while to figure out how to perform the
++	 * check correctly.
++	 */
++	if (!test_thread_flag(TIF_PERFMON_WORK))
++		return;
++#endif
++
++	if (!user_mode(regs))
++		return;
++
++	clear_thread_flag(TIF_PERFMON_WORK);
++
++	pfm_stats_inc(handle_work_count);
++
++	ctx = current->pfm_context;
++	if (ctx == NULL) {
++		PFM_DBG("[%d] has no ctx", current->pid);
++		return;
++	}
++
++	BUG_ON(ctx->flags.system);
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	type = ctx->flags.work_type;
++	ctx->flags.work_type = PFM_WORK_NONE;
++
++	PFM_DBG("work_type=%d reset_count=%d",
++		type,
++		ctx->flags.reset_count);
++
++	switch(type) {
++		case PFM_WORK_ZOMBIE:
++			goto do_zombie;
++		case PFM_WORK_RESET:
++			/* simply reset, no blocking */
++			goto skip_blocking;
++		case PFM_WORK_NONE:
++			PFM_DBG("unexpected PFM_WORK_NONE");
++			goto nothing_todo;
++		case PFM_WORK_BLOCK:
++			break;
++		default:
++			PFM_DBG("unkown type=%d", type);
++			goto nothing_todo;
++	}
++
++	/*
++	 * restore interrupt mask to what it was on entry.
++	 * Could be enabled/disabled.
++	 */
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	/*
++	 * force interrupt enable because of down_interruptible()
++	 */
++	local_irq_enable();
++
++	PFM_DBG("before block sleeping");
++
++	/*
++	 * may go through without blocking on SMP systems
++	 * if restart has been received already by the time we call down()
++	 */
++	ret = wait_for_completion_interruptible(&ctx->restart_complete);
++
++	PFM_DBG("after block sleeping ret=%d", ret);
++
++	/*
++	 * lock context and mask interrupts again
++	 * We save flags into a dummy because we may have
++	 * altered interrupts mask compared to entry in this
++	 * function.
++	 */
++	spin_lock_irqsave(&ctx->lock, dummy_flags);
++
++	if (ctx->state == PFM_CTX_ZOMBIE)
++		goto do_zombie;
++
++	/*
++	 * in case of interruption of down() we don't restart anything
++	 */
++	if (ret < 0)
++		goto nothing_todo;
++
++skip_blocking:
++	/*
++	 * iterate over the number of pending resets
++	 * There are certain situations where there may be
++	 * multiple notifications sent before a pfm_restart().
++	 * As such, it may be that multiple pfm_restart() are
++	 * issued before the monitored thread gets to
++	 * pfm_handle_work(). To avoid losing restarts, pfm_restart()
++	 * increments a counter (reset_counts). Here, we take this
++	 * into account by potentially calling pfm_resume_after_ovfl()
++	 * multiple times. It is up to the sampling format to take the
++	 * appropriate actions.
++	 */
++	while(ctx->flags.reset_count) {
++		pfm_resume_after_ovfl(ctx);
++		ctx->flags.reset_count--;
++	}
++
++nothing_todo:
++	/*
++	 * restore flags as they were upon entry
++	 */
++	spin_unlock_irqrestore(&ctx->lock, flags);
++	return;
++
++do_zombie:
++	PFM_DBG("context is zombie, bailing out");
++
++	__pfm_unload_context(ctx, &release_info);
++
++	/*
++	 * keep the spinlock check happy
++	 */
++	spin_unlock(&ctx->lock);
++
++	/*
++	 * enable interrupt for vfree()
++	 */
++	local_irq_enable();
++
++	/*
++	 * cancel timer now that context is unlocked
++	 */
++	if (release_info & 0x2) {
++		ret = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
++		PFM_DBG("timeout cancel=%d", ret);
++	}
++
++	/*
++	 * actual context free
++	 */
++	pfm_context_free(ctx);
++
++	/*
++	 * restore interrupts as they were upon entry
++	 */
++	local_irq_restore(flags);
++
++	/* always true */
++	if (release_info & 0x1)
++		pfm_release_session(0, 0);
++}
++
++static int pfm_notify_user(struct pfm_context *ctx)
++{
++	if (ctx->state == PFM_CTX_ZOMBIE) {
++		PFM_DBG("ignoring overflow notification, owner is zombie");
++		return 0;
++	}
++	PFM_DBG_ovfl("waking up somebody");
++
++	wake_up_interruptible(&ctx->msgq_wait);
++
++	/*
++	 * it is safe to call kill_fasync() from an interrupt
++	 * handler. kill_fasync()  grabs two RW locks (fasync_lock,
++	 * tasklist_lock) in read mode. There is conflict only in
++	 * case the PMU interrupt occurs during a write mode critical
++	 * section. This cannot happen because for both locks, the
++	 * write mode is always using interrupt masking (write_lock_irq).
++	 */
++	kill_fasync (&ctx->async_queue, SIGIO, POLL_IN);
++
++	return 0;
++}
++
++/*
++ * send a counter overflow notification message to
++ * user. First appends the message to the queue, then
++ * wake up ay waiter on the file descriptor
++ *
++ * context is locked and interrupts are disabled (no preemption).
++ */
++int pfm_ovfl_notify_user(struct pfm_context *ctx,
++			struct pfm_event_set *set,
++			unsigned long ip)
++{
++	union pfarg_msg *msg = NULL;
++	u64 *ovfl_pmds;
++
++	if (!ctx->flags.no_msg) {
++		msg = pfm_get_new_msg(ctx);
++		if (msg == NULL) {
++			/*
++			 * when message queue fills up it is because the user
++			 * did not extract the message, yet issued
++			 * pfm_restart(). At this point, we stop sending
++			 * notification, thus the user will not be able to get
++			 * new samples when using the default format.
++			 */
++			PFM_DBG_ovfl("no more notification msgs");
++			return -1;
++		}
++
++		msg->pfm_ovfl_msg.msg_type = PFM_MSG_OVFL;
++		msg->pfm_ovfl_msg.msg_ovfl_pid = current->pid;
++		msg->pfm_ovfl_msg.msg_active_set = set->id;
++
++		ovfl_pmds = msg->pfm_ovfl_msg.msg_ovfl_pmds;
++
++		/*
++		 * copy bitmask of all pmd that interrupted last
++		 */
++		bitmap_copy(cast_ulp(ovfl_pmds), cast_ulp(set->ovfl_pmds),
++			    pfm_pmu_conf->regs.max_intr_pmd);
++
++		msg->pfm_ovfl_msg.msg_ovfl_cpu = smp_processor_id();
++		msg->pfm_ovfl_msg.msg_ovfl_tid = current->tgid;
++		msg->pfm_ovfl_msg.msg_ovfl_ip = ip;
++
++		pfm_stats_inc(ovfl_notify_count);
++	}
++
++	PFM_DBG_ovfl("ip=0x%lx o_pmds=0x%llx",
++		     ip,
++		     (unsigned long long)set->ovfl_pmds[0]);
++
++	return pfm_notify_user(ctx);
++}
++
++/*
++ * In per-thread mode, when not self-monitoring, perfmon
++ * sends a 'end' notification message when the monitored
++ * thread where the context is attached is exiting.
++ *
++ * This helper message alleviate the need to track the activity
++ * of the thread/process when it is not directly related, i.e.,
++ * was attached vs was forked/execd. In other words, no need
++ * to keep the thread ptraced.
++ *
++ * the context must be locked and interrupts disabled.
++ */
++static int pfm_end_notify_user(struct pfm_context *ctx)
++{
++	union pfarg_msg *msg;
++
++	msg = pfm_get_new_msg(ctx);
++	if (msg == NULL) {
++		PFM_ERR("%s no more msgs", __FUNCTION__);
++		return -1;
++	}
++	/* no leak */
++	memset(msg, 0, sizeof(*msg));
++
++	msg->type = PFM_MSG_END;
++
++	PFM_DBG("end msg: msg=%p no_msg=%d",
++		msg,
++		ctx->flags.no_msg);
++
++	return pfm_notify_user(ctx);
++}
++
++/*
++ * called only from exit_thread(): task == current
++ * we come here only if current has a context
++ * attached (loaded or masked or zombie)
++ */
++void __pfm_exit_thread(struct task_struct *task)
++{
++	struct pfm_context *ctx;
++	unsigned long flags;
++	int free_ok = 0, release_info = 0;
++	int ret;
++
++	ctx  = task->pfm_context;
++
++	BUG_ON(ctx->flags.system);
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	PFM_DBG("state=%d", ctx->state);
++
++	/*
++	 * __pfm_unload_context() cannot fail
++	 * in the context states we are interested in
++	 */
++	switch (ctx->state) {
++	case PFM_CTX_LOADED:
++	case PFM_CTX_MASKED:
++		__pfm_unload_context(ctx, &release_info);
++		pfm_end_notify_user(ctx);
++		break;
++	case PFM_CTX_ZOMBIE:
++		__pfm_unload_context(ctx, &release_info);
++		free_ok = 1;
++		break;
++	default:
++		BUG_ON(ctx->state != PFM_CTX_LOADED);
++		break;
++	}
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	/*
++	 * cancel timer now that context is unlocked
++	 */
++	if (release_info & 0x2) {
++		ret = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
++		PFM_DBG("timeout cancel=%d", ret);
++	}
++
++	if (release_info & 0x1)
++		pfm_release_session(0, 0);
++
++	/*
++	 * All memory free operations (especially for vmalloc'ed memory)
++	 * MUST be done with interrupts ENABLED.
++	 */
++	if (free_ok)
++		pfm_context_free(ctx);
++}
++
++/*
++ * CPU hotplug event nofication callback
++ *
++ * We use the callback to do manage the sysfs interface.
++ * Note that the actual shutdown of monitoring on the CPU
++ * is done in pfm_cpu_disable(), see comments there for more
++ * information.
++ */
++static int pfm_cpu_notify(struct notifier_block *nfb,
++			  unsigned long action, void *hcpu)
++{
++	unsigned int cpu = (unsigned long)hcpu;
++	int ret = NOTIFY_OK;
++
++	pfm_pmu_conf_get(0);
++
++	switch (action) {
++	case CPU_ONLINE:
++		pfm_debugfs_add_cpu(cpu);
++		PFM_INFO("CPU%d is online", cpu);
++		break;
++	case CPU_UP_PREPARE:
++		PFM_INFO("CPU%d prepare online", cpu);
++		break;
++	case CPU_UP_CANCELED:
++		pfm_debugfs_del_cpu(cpu);
++		PFM_INFO("CPU%d is up canceled", cpu);
++		break;
++	case CPU_DOWN_PREPARE:
++		PFM_INFO("CPU%d prepare offline", cpu);
++		break;
++	case CPU_DOWN_FAILED:
++		PFM_INFO("CPU%d is down failed", cpu);
++		break;
++	case CPU_DEAD:
++		pfm_debugfs_del_cpu(cpu);
++		PFM_INFO("CPU%d is offline", cpu);
++		break;
++	}
++	pfm_pmu_conf_put();
++	return ret;
++}
++
++static struct notifier_block pfm_cpu_notifier ={
++	.notifier_call = pfm_cpu_notify
++};
++
++/*
++ * called from cpu_init() and pfm_pmu_register()
++ */
++void __pfm_init_percpu(void *dummy)
++{
++	struct hrtimer *h;
++
++	h = &__get_cpu_var(pfm_hrtimer);
++
++	pfm_arch_init_percpu();
++
++	/*
++	 * initialize per-cpu high res timer
++	 */
++	hrtimer_init(h, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
++	h->function = pfm_handle_switch_timeout;
++}
++
++/*
++ * global initialization routine, executed only once
++ */
++int __init pfm_init(void)
++{
++	int ret;
++
++	PFM_LOG("version %u.%u, compiled: " __DATE__ ", " __TIME__,
++		PFM_VERSION_MAJ, PFM_VERSION_MIN);
++
++	pfm_ctx_cachep = kmem_cache_create("pfm_context",
++				   sizeof(struct pfm_context)+PFM_ARCH_CTX_SIZE,
++				   SLAB_HWCACHE_ALIGN, 0, NULL);
++	if (pfm_ctx_cachep == NULL) {
++		PFM_ERR("cannot initialize context slab");
++		goto error_disable;
++	}
++
++	ret = pfm_sets_init();
++	if (ret)
++		goto error_disable;
++
++	if (pfm_init_fs())
++		goto error_disable;
++
++	if (pfm_init_sysfs())
++		goto error_disable;
++
++	/* not critical, so no error checking */
++	pfm_init_debugfs();
++
++	/*
++	 * one time, arch-specific global initialization
++	 */
++	if (pfm_arch_init())
++		goto error_disable;
++
++	/*
++	 * register CPU hotplug event notifier
++	 */
++	ret = register_cpu_notifier(&pfm_cpu_notifier);
++	if (!ret)
++		return 0;
++
++error_disable:
++	PFM_INFO("perfmon is disabled due to initialization error");
++	perfmon_disabled = 1;
++	return -1;
++}
++
++/*
++ * must use subsys_initcall() to ensure that the perfmon2 core
++ * is initialized before any PMU description module when they are
++ * compiled in.
++ */
++subsys_initcall(pfm_init);
++
++/*
++ * function used to start monitoring. When operating in per-thread
++ * mode and when not self-monitoring, the monitored thread must be
++ * stopped.
++ *
++ * The pfarg_start argument is optional and may be used to designate
++ * the initial event set to activate. Wehn missing, the last active
++ * set is used. For the first activation, set0 is used.
++ *
++ * On some architectures, e.g., IA-64, it may be possible to start monitoring
++ * without calling this function under certain conditions (per-thread and self
++ * monitoring).
++ *
++ * the context is locked and interrupts are disabled.
++ */
++int __pfm_start(struct pfm_context *ctx, struct pfarg_start *start)
++{
++	struct task_struct *task, *owner_task;
++	struct pfm_event_set *new_set, *old_set;
++	int is_self;
++
++	task = ctx->task;
++
++	/*
++	 * UNLOADED: error
++	 * LOADED  : normal start, nop if started unless set is different
++	 * MASKED  : nop or change set when unmasking
++	 * ZOMBIE  : cannot happen
++	 */
++	if (ctx->state == PFM_CTX_UNLOADED)
++		return -EINVAL;
++
++	old_set = new_set = ctx->active_set;
++
++	/*
++	 * always the case for system-wide
++	 */
++	if (task == NULL)
++		task = current;
++
++	is_self = task == current;
++
++	/*
++	 * argument is provided?
++	 */
++	if (start) {
++		/*
++		 * find the set to load first
++		 */
++		new_set = pfm_find_set(ctx, start->start_set, 0);
++		if (new_set == NULL) {
++			PFM_DBG("event set%u does not exist",
++				start->start_set);
++			return -EINVAL;
++		}
++	}
++
++	PFM_DBG("cur_set=%u req_set=%u",
++		old_set->id,
++		new_set->id);
++
++	/*
++	 * if we need to change the active set we need
++	 * to check if we can access the PMU
++	 */
++	if (new_set != old_set) {
++
++		owner_task = __get_cpu_var(pmu_owner);
++		/*
++		 * system-wide: must run on the right CPU
++		 * per-thread : must be the owner of the PMU context
++		 *
++		 * pfm_switch_sets() returns with monitoring stopped
++		 */
++		if (is_self) {
++			pfm_switch_sets(ctx, new_set, PFM_PMD_RESET_LONG, 1);
++		} else {
++			/*
++			 * In a UP kernel, the PMU may contain the state
++			 * of the task we want to operate on, yet the task
++			 * may be switched out (lazy save). We need to save
++			 * current state (old_set), switch active_set and
++			 * mark it for reload.
++			 */
++			if (owner_task == task)
++				pfm_save_pmds(ctx, old_set);
++			ctx->active_set = new_set;
++			new_set->priv_flags |= PFM_SETFL_PRIV_MOD_BOTH;
++		}
++	}
++	/*
++	 * mark as started
++	 * must be done before calling pfm_arch_start()
++	 */
++	ctx->flags.started = 1;
++
++
++	pfm_arch_start(task, ctx, new_set);
++
++	/*
++	 * we check whether we had a pending ovfl before restarting.
++	 * If so we need to regenerate the interrupt to make sure we
++	 * keep recorded samples. For non-self monitoring this check
++	 * is done in the pfm_ctxswin_thread() routine.
++	 *
++	 * we check new_set/old_set because pfm_switch_sets() already
++	 * takes care of replaying the pending interrupts
++	 */
++	if (is_self && new_set != old_set && new_set->npend_ovfls) {
++		pfm_arch_resend_irq();
++		pfm_stats_inc(ovfl_intr_replay_count);
++	}
++
++	/*
++	 * activate timeout for system-wide, self-montoring
++	 */
++	if (is_self && new_set->flags & PFM_SETFL_TIME_SWITCH) {
++		hrtimer_start(&__get_cpu_var(pfm_hrtimer), new_set->hrtimer_exp, HRTIMER_MODE_REL);
++		PFM_DBG("armed timeout for set%u", new_set->id);
++	}
++
++	/*
++	 * we restart total duration even if context was
++	 * already started. In that case, counts are simply
++	 * reset.
++	 *
++	 * For per-thread, if not self-monitoring, the statement
++	 * below will have no effect because thread is stopped.
++	 * The field is reset of ctxsw in.
++	 */
++	new_set->duration_start = sched_clock();
++
++	return 0;
++}
++
++/*
++ * function used to stop monitoring. When operating in per-thread
++ * mode and when not self-monitoring, the monitored thread must be
++ * stopped.
++ *
++ * the context is locked and interrupts are disabled.
++ */
++int __pfm_stop(struct pfm_context *ctx, int *release_info)
++{
++	struct pfm_event_set *set;
++	struct task_struct *task;
++	u64 now;
++	int state;
++
++	*release_info = 0;
++
++	now = sched_clock();
++	state = ctx->state;
++	set = ctx->active_set;
++
++	/*
++	 * context must be attached (zombie cannot happen)
++	 */
++	if (state == PFM_CTX_UNLOADED)
++		return -EINVAL;
++
++	task = ctx->task;
++
++	PFM_DBG("ctx_task=[%d] ctx_state=%d is_system=%d",
++		task ? task->pid : -1,
++		state,
++		ctx->flags.system);
++
++	/*
++	 * this happens for system-wide context
++	 */
++	if (task == NULL)
++		task = current;
++
++	/*
++	 * compute elapsed time
++	 *
++	 * unless masked, compute elapsed duration, stop timeout
++	 */
++	if (task == current && state == PFM_CTX_LOADED) {
++		/*
++		 * timeout cancel must be deferred until context is unlocked
++		 * to avoid race with pfm_handle_switch_timeout()
++		 */
++		if (set->flags & PFM_SETFL_TIME_SWITCH)
++			*release_info |= 0x2;
++
++		set->duration += now - set->duration_start;
++	}
++
++	pfm_arch_stop(task, ctx, set);
++
++	ctx->flags.started = 0;
++	/*
++	 * starting now, in-flight PMU interrupt for this context
++	 * are treated as spurious
++	 */
++	return 0;
++}
++
++/*
++ * function called from sys_pfm_restart(). It is used when overflow
++ * notification is requested. For each notification received, the user
++ * must call pfm_restart() to indicate to the kernel that it is done
++ * processing the notification.
++ *
++ * When the caller is doing user level sampling, this function resets
++ * the overflowed counters and resumes monitoring which is normally stopped
++ * during notification (always the consequence of a counter overflow).
++ *
++ * When using a sampling format, the format restart() callback is invoked,
++ * overflowed PMDS may be reset based upon decision from sampling format.
++ *
++ * When operating in per-thread mode, and when not self-monitoring, the
++ * monitored thread DOES NOT need to be stopped, unlike for many other calls.
++ *
++ * This means that the effect of the restart may not necessarily be observed
++ * right when returning from the call. For instance, counters may not already
++ * be reset in the other thread.
++ *
++ * When operating in system-wide, the caller must be running on the monitored
++ * CPU.
++ *
++ * The context is locked and interrupts are disabled.
++ *
++ */
++int __pfm_restart(struct pfm_context *ctx, int *unblock)
++{
++	int state;
++
++	state = ctx->state;
++
++	PFM_DBG("state=%d can_restart=%d reset_count=%d",
++		state,
++		ctx->flags.can_restart,
++		ctx->flags.reset_count);
++
++	*unblock = 0;
++
++	switch (state) {
++	case PFM_CTX_MASKED:
++		break;
++	case PFM_CTX_LOADED:
++		if (ctx->smpl_addr && ctx->smpl_fmt->fmt_restart)
++			break;
++	default:
++		PFM_DBG("invalid state=%d", state);
++		return -EBUSY;
++	}
++
++	/*
++	 * first check if allowed to restart, i.e., notifications received
++	 */
++	if (!ctx->flags.can_restart) {
++		PFM_DBG("no restart can_restart=0");
++		return -EBUSY;
++	}
++
++	pfm_stats_inc(pfm_restart_count);
++
++	/*
++	 * at this point, the context is either LOADED or MASKED
++	 */
++	ctx->flags.can_restart--;
++
++	/*
++	 * handle self-monitoring case and system-wide
++	 */
++	if (ctx->task == current || ctx->flags.system) {
++		pfm_resume_after_ovfl(ctx);
++		return 0;
++	}
++
++	/*
++	 * restart another task
++	 */
++
++	/*
++	 * if blocking, then post the semaphore if PFM_CTX_MASKED, i.e.
++	 * the task is blocked or on its way to block. That's the normal
++	 * restart path. If the monitoring is not masked, then the task
++	 * can be actively monitoring and we cannot directly intervene.
++	 * Therefore we use the trap mechanism to catch the task and
++	 * force it to reset the buffer/reset PMDs.
++	 *
++	 * if non-blocking, then we ensure that the task will go into
++	 * pfm_handle_work() before returning to user mode.
++	 *
++	 * We cannot explicitly reset another task, it MUST always
++	 * be done by the task itself. This works for system wide because
++	 * the tool that is controlling the session is logically doing
++	 * "self-monitoring".
++	 */
++	if (ctx->flags.block && state == PFM_CTX_MASKED) {
++		PFM_DBG("unblocking [%d]", ctx->task->pid);
++		/*
++		 * It is not possible to call complete() with the context locked
++		 * otherwise we have a potential deadlock with the PMU context
++		 * switch code due to a lock inversion between task_rq_lock()
++		 * and the context lock.
++		 * Instead we mark whether or not we need to issue the complete
++		 * and we invoke the function once the context lock is released
++		 * in sys_pfm_restart()
++		 */
++		*unblock = 1;
++	} else {
++		PFM_DBG("[%d] armed exit trap", ctx->task->pid);
++		ctx->flags.work_type = PFM_WORK_RESET;
++		set_tsk_thread_flag(ctx->task, TIF_PERFMON_WORK);
++	}
++	ctx->flags.reset_count++;
++	return 0;
++}
++
++/*
++ * function used to attach a context to either a CPU or a thread.
++ * In per-thread mode, and when not self-monitoring, the thread must be
++ * stopped. In system-wide mode, the cpu specified in the pfarg_load.load_tgt
++ * argument must be the current CPU.
++ *
++ * The function must be called with the context locked and interrupts disabled.
++ */
++int __pfm_load_context(struct pfm_context *ctx, struct pfarg_load *req,
++		       struct task_struct *task)
++{
++	struct pfm_event_set *set;
++	struct pfm_context *old;
++	int mycpu;
++	int ret;
++
++	mycpu = smp_processor_id();
++
++	/*
++	 * system-wide: check we are running on the desired CPU
++	 */
++	if (ctx->flags.system && req->load_pid != mycpu) {
++		PFM_DBG("running on wrong CPU: %u vs. %u",
++			mycpu, req->load_pid);
++		return -EINVAL;
++	}
++
++	/*
++	 * locate first set to activate
++	 */
++	set = pfm_find_set(ctx, req->load_set, 0);
++	if (set == NULL) {
++		PFM_DBG("event set%u does not exist", req->load_set);
++		return -EINVAL;
++	}
++
++	/*
++	 * assess sanity of event sets, initialize set state
++	 */
++	ret = pfm_prepare_sets(ctx, set);
++	if (ret) {
++		PFM_DBG("invalid next field pointers in the sets");
++		return -EINVAL;
++	}
++
++	PFM_DBG("load_pid=%d set=%u set_flags=0x%x",
++		req->load_pid,
++		set->id,
++		set->flags);
++
++	/*
++	 * per-thread:
++	 *   - task to attach to is checked in sys_pfm_load_context() to avoid
++	 *     locking issues. if found, and not self,  task refcount was incremented.
++	 */
++	if (ctx->flags.system) {
++		ctx->cpu = mycpu;
++		ctx->task = NULL;
++		task = current;
++	} else {
++		old = cmpxchg(&task->pfm_context, NULL, ctx);
++		if (old != NULL) {
++			PFM_DBG("load_pid=%d has a context "
++				"old=%p new=%p cur=%p",
++				req->load_pid,
++				old,
++				ctx,
++				task->pfm_context);
++			return -EEXIST;
++		}
++		ctx->task = task;
++		ctx->cpu = -1;
++	}
++
++	/*
++	 * perform any architecture specific actions
++	 */
++	ret = pfm_arch_load_context(ctx, set, ctx->task);
++	if (ret)
++		goto error_noload;
++
++	/*
++	 * now reserve the session, before we can proceed with
++	 * actually accessing the PMU hardware
++	 */
++	ret = pfm_reserve_session(ctx->flags.system, ctx->cpu);
++	if (ret)
++		goto error;
++
++	/*
++	 * commit active set
++	 */
++	ctx->active_set = set;
++
++	set->runs++;
++
++	/*
++	 * self-monitoring (incl. system-wide)
++	 */
++	if (task == current) {
++#ifndef CONFIG_SMP
++		/*
++		 * in UP mode, because of lazy save/restore
++		 * there may already be valid state on the PMU.
++		 * We need to push it out before we can load the
++		 * next state
++		 */
++		struct pfm_context *ctxp;
++		ctxp = __get_cpu_var(pmu_ctx);
++		if (ctxp)
++			pfm_save_prev_context(ctxp);
++#endif
++		pfm_set_last_cpu(ctx, mycpu);
++		pfm_inc_activation();
++		pfm_set_activation(ctx);
++
++		/*
++		 * load PMD from set
++		 * load PMC from set
++		 */
++		pfm_arch_restore_pmds(ctx, set);
++		pfm_arch_restore_pmcs(ctx, set);
++
++		/*
++		 * set new ownership
++		 */
++		pfm_set_pmu_owner(ctx->task, ctx);
++	} else {
++		/* force a full reload */
++		ctx->last_act = PFM_INVALID_ACTIVATION;
++		pfm_set_last_cpu(ctx, -1);
++		set->priv_flags |= PFM_SETFL_PRIV_MOD_BOTH;
++		PFM_DBG("context loaded next ctxswin for [%d]", task->pid);
++	}
++
++	if (!ctx->flags.system) {
++		set_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
++		PFM_DBG("[%d] set TIF", task->pid);
++	}
++
++	ctx->flags.work_type = PFM_WORK_NONE;
++	ctx->flags.reset_count = 0;
++
++	/*
++	 * reset message queue
++	 */
++	ctx->msgq_head = ctx->msgq_tail = 0;
++
++	ctx->state = PFM_CTX_LOADED;
++
++	return 0;
++
++error:
++	pfm_arch_unload_context(ctx, task);
++error_noload:
++	/*
++	 * detach context
++	 */
++	if (!ctx->flags.system)
++		task->pfm_context = NULL;
++
++	return ret;
++}
++
++/*
++ * Function used to detach a context from either a CPU or a thread.
++ * In the per-thread case and when not self-monitoring, the thread must be
++ * stopped. After the call, the context is detached and monitoring is stopped.
++ *
++ * The function must be called with the context locked and interrupts disabled.
++ *
++ * release_info value upon return:
++ * 	- 0x0 : cannot free context, no timer to cancel
++ * 	- 0x1 : must free context
++ * 	- 0x2 : cannot free context, must cancel timer
++ * 	- 0x3 : must free context, must cancel timer
++ */
++int __pfm_unload_context(struct pfm_context *ctx, int *release_info)
++{
++	struct task_struct *task;
++	struct pfm_event_set *set;
++	int ret, is_self;
++
++	PFM_DBG("ctx_state=%d task [%d]", ctx->state, ctx->task ? ctx->task->pid : -1);
++
++	*release_info = 0;
++
++	/*
++	 * unload only when necessary
++	 */
++	if (ctx->state == PFM_CTX_UNLOADED)
++		return 0;
++
++	task = ctx->task;
++	set = ctx->active_set;
++	is_self = ctx->flags.system || task == current;
++
++	/*
++	 * stop monitoring
++	 */
++	ret = __pfm_stop(ctx, release_info);
++	if (ret)
++		return ret;
++
++	ctx->state = PFM_CTX_UNLOADED;
++	ctx->flags.can_restart = 0;
++
++	/*
++	 * save PMD registers
++	 * release ownership
++	 */
++	pfm_flush_pmds(task, ctx);
++
++	/*
++	 * arch-specific unload operations
++	 */
++	pfm_arch_unload_context(ctx, task);
++
++	/*
++	 * per-thread: disconnect from monitored task
++	 */
++	if (task) {
++		task->pfm_context = NULL;
++		ctx->task = NULL;
++		clear_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
++		clear_tsk_thread_flag(task, TIF_PERFMON_WORK);
++	}
++
++	*release_info |= 0x1;
++
++	return 0;
++}
++
++static inline int pfm_ctx_flags_sane(u32 ctx_flags)
++{
++	if (ctx_flags & PFM_FL_SYSTEM_WIDE) {
++		if (ctx_flags & PFM_FL_NOTIFY_BLOCK) {
++			PFM_DBG("cannot use blocking mode in syswide mode");
++			return -EINVAL;
++		}
++	}
++	return 0;
++}
++
++/*
++ * check for permissions to create a context.
++ *
++ * A sysadmin may decide to restrict creation of per-thread
++ * and/or system-wide context to a group of users using the
++ * group id via /sys/kernel/perfmon/task_group  and
++ * /sys/kernel/perfmon/sys_group.
++ *
++ * Once we identify a user level package which can be used
++ * to grant/revoke Linux capabilites at login via PAM, we will
++ * be able to use capabilities. We would also need to increase
++ * the size of cap_t to support more than 32 capabilities (it
++ * is currently defined as u32 and 32 capabilities are alrady
++ * defined).
++ */
++static inline int pfm_ctx_permissions(u32 ctx_flags)
++{
++	if (  (ctx_flags & PFM_FL_SYSTEM_WIDE)
++	   && pfm_controls.sys_group != PFM_GROUP_PERM_ANY
++	   && !in_group_p(pfm_controls.sys_group)) {
++		PFM_DBG("user group not allowed to create a syswide ctx");
++		return -EPERM;
++	} else if (pfm_controls.task_group != PFM_GROUP_PERM_ANY
++		   && !in_group_p(pfm_controls.task_group)) {
++		PFM_DBG("user group not allowed to create a task context");
++		return -EPERM;
++	}
++	return 0;
++}
++
++/*
++ * function used to allocate a new context. A context is allocated along
++ * with the default event set. If a sampling format is used, the buffer
++ * may be allocated and initialized.
++ *
++ * The file descriptor identifying the context is allocated and returned
++ * to caller.
++ *
++ * This function operates with no locks and interrupts are enabled.
++ * return:
++ * 	>=0: the file descriptor to identify the context
++ * 	<0 : the error code
++ */
++int __pfm_create_context(struct pfarg_ctx *req,
++			 struct pfm_smpl_fmt *fmt,
++			 void *fmt_arg,
++			 int mode,
++			 struct pfm_context **new_ctx)
++{
++	struct pfm_context *ctx;
++	struct pfm_event_set *set;
++	struct file *filp = NULL;
++	u32 ctx_flags;
++	int fd = 0, ret;
++
++	ctx_flags = req->ctx_flags;
++
++	/* Increase refcount on PMU description */
++	ret = pfm_pmu_conf_get(1);
++	if (ret < 0)
++		goto error_conf;
++
++	ret = pfm_ctx_flags_sane(ctx_flags);
++	if (ret < 0)
++		goto error_alloc;
++
++	ret = pfm_ctx_permissions(ctx_flags);
++	if (ret < 0)
++		goto error_alloc;
++
++	/*
++	 * we can use GFP_KERNEL and potentially sleep because we do
++	 * not hold any lock at this point.
++	 */
++	might_sleep();
++	ret = -ENOMEM;
++	ctx = kmem_cache_zalloc(pfm_ctx_cachep, GFP_KERNEL);
++	if (!ctx)
++		goto error_alloc;
++
++	ret = pfm_pmu_acquire();
++	if (ret)
++		goto error_file;
++	/*
++	 * check if PMU is usable
++	 */
++	if (!(pfm_pmu_conf->regs.num_pmcs && pfm_pmu_conf->regs.num_pmcs)) {
++		PFM_DBG("no usable PMU registers");
++		ret = -EBUSY;
++		goto error_file;
++	}
++
++	/*
++	 * link to format, must be done first for correct
++	 * error handling in pfm_context_free()
++	 */
++	ctx->smpl_fmt = fmt;
++
++	ret = -ENFILE;
++	fd = pfm_alloc_fd(&filp);
++	if (fd < 0)
++		goto error_file;
++
++	/*
++	 * context is unloaded
++	 */
++	ctx->state = PFM_CTX_UNLOADED;
++
++	INIT_LIST_HEAD(&ctx->set_list);
++
++	/*
++	 * initialization of context's flags
++	 * must be done before pfm_find_set()
++	 */
++	ctx->flags.block = (ctx_flags & PFM_FL_NOTIFY_BLOCK) ? 1 : 0;
++	ctx->flags.system = (ctx_flags & PFM_FL_SYSTEM_WIDE) ? 1: 0;
++	ctx->flags.no_msg = (ctx_flags & PFM_FL_OVFL_NO_MSG) ? 1: 0;
++	ctx->flags.ia64_v20_compat = mode == PFM_COMPAT ? 1 : 0;
++
++	/*
++	 * initialize arch-specific section
++	 * must be done before fmt_init()
++	 *
++	 * XXX: fix dependency with fmt_init()
++	 */
++	ret = pfm_arch_context_create(ctx, ctx_flags);
++	if (ret)
++		goto error_set;
++
++	ret = -ENOMEM;
++	/*
++	 * create initial set
++	 */
++	if (pfm_find_set(ctx, 0, 1) == NULL)
++		goto error_set;
++
++	set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
++
++	pfm_init_evtset(set);
++
++	/*
++	 * does the user want to sample?
++	 */
++	if (fmt) {
++		ret = pfm_setup_smpl_fmt(fmt, fmt_arg, ctx, ctx_flags,
++					 mode, filp);
++		if (ret)
++			goto error_set;
++	}
++
++	filp->private_data = ctx;
++
++	spin_lock_init(&ctx->lock);
++	init_completion(&ctx->restart_complete);
++
++	ctx->last_act = PFM_INVALID_ACTIVATION;
++	pfm_set_last_cpu(ctx, -1);
++
++	/*
++	 * initialize notification message queue
++	 */
++	ctx->msgq_head = ctx->msgq_tail = 0;
++	init_waitqueue_head(&ctx->msgq_wait);
++
++	PFM_DBG("ctx=%p flags=0x%x system=%d notify_block=%d no_msg=%d"
++		" use_fmt=%d ctx_fd=%d mode=%d",
++		ctx,
++		ctx_flags,
++		ctx->flags.system,
++		ctx->flags.block,
++		ctx->flags.no_msg,
++		fmt != NULL,
++		fd, mode);
++
++	*new_ctx = ctx;
++
++	/*
++	 * we defer the fd_install until we are certain the call succeeded
++	 * to ensure we do not have to undo its effect. Neither put_filp()
++	 * nor put_unused_fd() undoes the effect of fd_install().
++	 */
++	fd_install(fd, filp);
++
++	return fd;
++
++error_set:
++	put_filp(filp);
++	put_unused_fd(fd);
++error_file:
++	/*
++	 * calls the right *_put() functions
++	 * calls pfm_release_pmu()
++	 */
++	pfm_context_free(ctx);
++	return ret;
++error_alloc:
++	pfm_pmu_conf_put();
++error_conf:
++	pfm_smpl_fmt_put(fmt);
++	return ret;
++}
++
++/*
++ * called from cpu_disable() to detach the perfmon context
++ * from the CPU going down.
++ *
++ * We cannot use the cpu hotplug notifier because we MUST run
++ * on the CPU that is going down to save the PMU state
++ */
++void pfm_cpu_disable(void)
++{
++	struct pfm_context *ctx;
++	unsigned long flags;
++	int is_system, release_info = 0;
++	u32 cpu;
++	int r;
++
++	ctx = __get_cpu_var(pmu_ctx);
++	if (ctx == NULL)
++		return;
++
++	is_system = ctx->flags.system;
++	cpu = ctx->cpu;
++
++	/*
++	 * context is LOADED or MASKED
++	 *
++	 * we unload from CPU. That stops monitoring and does
++	 * all the bookeeping of saving values and updating duration
++	 */
++	spin_lock_irqsave(&ctx->lock, flags);
++	if (is_system)
++		__pfm_unload_context(ctx, &release_info);
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	/*
++	 * cancel timer now that context is unlocked
++	 */
++	if (release_info & 0x2) {
++		r = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
++		PFM_DBG("timeout cancel=%d", r);
++	}
++
++	if (release_info & 0x1)
++		pfm_release_session(is_system, cpu);
++}
+--- /dev/null
++++ b/perfmon/perfmon_ctxsw.c
+@@ -0,0 +1,371 @@
++/*
++ * perfmon_cxtsw.c: perfmon2 context switch code
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/kernel.h>
++#include <linux/perfmon.h>
++
++/*
++ * used only in UP mode
++ */
++void pfm_save_prev_context(struct pfm_context *ctxp)
++{
++	struct pfm_event_set *set;
++
++	/*
++	 * in UP per-thread, due to lazy save
++	 * there could be a context from another
++	 * task. We need to push it first before
++	 * installing our new state
++	 */
++	set = ctxp->active_set;
++	pfm_save_pmds(ctxp, set);
++	/*
++	 * do not clear ownership because we rewrite
++	 * right away
++	 */
++}
++
++void pfm_save_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
++{
++	u64 val, ovfl_mask;
++	u64 *used_pmds, *cnt_pmds;
++	u16 i, num;
++
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	num = set->nused_pmds;
++	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
++	used_pmds = set->used_pmds;
++
++	/*
++	 * save HW PMD, for counters, reconstruct 64-bit value
++	 */
++	for (i = 0; num; i++) {
++		if (test_bit(i, cast_ulp(used_pmds))) {
++			val = pfm_read_pmd(ctx, i);
++			if (likely(test_bit(i, cast_ulp(cnt_pmds))))
++				val = (set->pmds[i].value & ~ovfl_mask) |
++					(val & ovfl_mask);
++			set->pmds[i].value = val;
++			num--;
++		}
++	}
++}
++
++/*
++ * interrupts are  disabled (no preemption)
++ */
++static void __pfm_ctxswin_thread(struct task_struct *task,
++				 struct pfm_context *ctx, u64 now)
++{
++	u64 cur_act;
++	struct pfm_event_set *set;
++	int reload_pmcs, reload_pmds;
++	int mycpu, is_active;
++
++	mycpu = smp_processor_id();
++
++	cur_act = __get_cpu_var(pmu_activation_number);
++	/*
++	 * we need to lock context because it could be accessed
++	 * from another CPU
++	 */
++	spin_lock(&ctx->lock);
++
++	is_active = pfm_arch_is_active(ctx);
++
++	set = ctx->active_set;
++
++	/*
++	 * in case fo zombie, we do not complete ctswin of the
++	 * PMU, and we force a call to pfm_handle_work() to finish
++	 * cleanup, i.e., free context + smpl_buff. The reason for
++	 * deferring to pfm_handle_work() is that it is not possible
++	 * to vfree() with interrupts disabled.
++	 */
++	if (unlikely(ctx->state == PFM_CTX_ZOMBIE)) {
++		ctx->flags.work_type = PFM_WORK_ZOMBIE;
++		set_tsk_thread_flag(task, TIF_PERFMON_WORK);
++		spin_unlock(&ctx->lock);
++		return;
++	}
++
++	/*
++	 * if we were the last user of the PMU on that CPU,
++	 * then nothing to do except restore psr
++	 */
++	if (ctx->last_cpu == mycpu && ctx->last_act == cur_act) {
++		/*
++		 * check for forced reload conditions
++		 */
++		reload_pmcs = set->priv_flags & PFM_SETFL_PRIV_MOD_PMCS;
++		reload_pmds = set->priv_flags & PFM_SETFL_PRIV_MOD_PMDS;
++	} else {
++#ifndef CONFIG_SMP
++		struct pfm_context *ctxp;
++		ctxp = __get_cpu_var(pmu_ctx);
++		if (ctxp)
++			pfm_save_prev_context(ctxp);
++#endif
++		reload_pmcs = 1;
++		reload_pmds = 1;
++	}
++	/* consumed */
++	set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
++
++	if (reload_pmds)
++		pfm_arch_restore_pmds(ctx, set);
++
++	/*
++	 * need to check if had in-flight interrupt in
++	 * pfm_ctxswout_thread(). If at least one bit set, then we must replay
++	 * the interrupt to avoid losing some important performance data.
++	 *
++	 * npend_ovfls is cleared in interrupt handler
++	 */
++	if (set->npend_ovfls) {
++		pfm_arch_resend_irq();
++		pfm_stats_inc(ovfl_intr_replay_count);
++	}
++
++	if (reload_pmcs)
++		pfm_arch_restore_pmcs(ctx, set);
++
++	/*
++	 * record current activation for this context
++	 */
++	pfm_inc_activation();
++	pfm_set_last_cpu(ctx, mycpu);
++	pfm_set_activation(ctx);
++
++	/*
++	 * establish new ownership.
++	 */
++	pfm_set_pmu_owner(task, ctx);
++
++	pfm_arch_ctxswin_thread(task, ctx, set);
++	/*
++	 * set->duration does not count when context in MASKED state.
++	 * set->duration_start is reset in unmask_monitoring()
++	 */
++	set->duration_start = now;
++
++	/*
++	 * re-arm switch timeout, if necessary
++	 * Timeout is active only if monitoring is active, i.e., LOADED + started
++	 *
++	 * We reload the remainder timeout or the full timeout. Remainder is recorded
++	 * on context switch out.
++	 */
++	if (ctx->state == PFM_CTX_LOADED
++	    && (set->flags & PFM_SETFL_TIME_SWITCH) && is_active) {
++		struct hrtimer *h;
++		h = &__get_cpu_var(pfm_hrtimer);
++		if (set->hrtimer_rem.tv64) {
++			hrtimer_start(h, set->hrtimer_rem, HRTIMER_MODE_REL);
++			PFM_DBG_ovfl("armed rem %lld for [%d]",
++					(long long)set->hrtimer_rem.tv64,
++					task->pid);
++			set->hrtimer_rem.tv64 = 0;
++		} else {
++			hrtimer_start(h, set->hrtimer_exp, HRTIMER_MODE_REL);
++			PFM_DBG_ovfl("armed exp for [%d]", task->pid);
++		}
++	}
++	spin_unlock(&ctx->lock);
++}
++
++/*
++ * interrupts are masked, runqueue lock is held.
++ *
++ * In UP. we simply stop monitoring and leave the state
++ * in place, i.e., lazy save
++ */
++static void __pfm_ctxswout_thread(struct task_struct *task,
++				  struct pfm_context *ctx, u64 now)
++{
++	struct pfm_event_set *set;
++	struct hrtimer *h = NULL;
++	int need_save_pmds, is_active;
++
++	/*
++	 * we need to lock context because it could be accessed
++	 * from another CPU
++	 */
++	spin_lock(&ctx->lock);
++
++	is_active = pfm_arch_is_active(ctx);
++	set = ctx->active_set;
++
++	/*
++	 * stop monitoring and
++	 * collect pending overflow information
++	 * needed on ctxswin. We cannot afford to lose
++	 * a PMU interrupt.
++	 */
++	need_save_pmds = pfm_arch_ctxswout_thread(task, ctx, set);
++
++	/*
++	 * accumulate only when set is actively monitoring,
++	 */
++	if (ctx->state == PFM_CTX_LOADED) {
++		set->duration += now - set->duration_start;
++		/*
++		 * timeout only runs while LOADED + started
++		 * record remaining timeout. Used on context switch in
++		 */
++		if (is_active && (set->flags & PFM_SETFL_TIME_SWITCH)) {
++			h = &__get_cpu_var(pfm_hrtimer);
++			set->hrtimer_rem = hrtimer_get_remaining(h);
++		}
++	}
++
++#ifdef CONFIG_SMP
++	/*
++	 * in SMP, release ownership of this PMU.
++	 * PMU interrupts are masked, so nothing
++	 * can happen.
++	 */
++	pfm_set_pmu_owner(NULL, NULL);
++
++	/*
++	 * On some architectures, it is necessary to read the
++	 * PMD registers to check for pending overflow in
++	 * pfm_arch_ctxswout_thread(). In that case, saving of
++	 * the PMDs  may be  done there and not here.
++	 */
++	if (need_save_pmds)
++		pfm_save_pmds(ctx, set);
++#endif
++	spin_unlock(&ctx->lock);
++
++	/*
++	 * cancel timer if necessary
++	 * need to have context unlocked to avoid
++	 * a race condition with pfm_handle_switch_timeout()
++	 *
++	 * hrtimer_cancel() loops until timer is actually cancelled
++	 */
++	if (h)
++		hrtimer_cancel(h);
++}
++
++/*
++ * no need to lock the context. To operate on a system-wide
++ * context, the task has to run on the monitored CPU. In the
++ * case of close issued on another CPU, an IPI is sent but
++ * this routine runs with interrupts masked, so we are
++ * protected
++ *
++ * On some architectures, such as IA-64, it may be necessary
++ * to intervene during the context even in system-wide mode
++ * to modify some machine state.
++ */
++static void __pfm_ctxsw_sys(struct task_struct *prev,
++			    struct task_struct *next)
++{
++	struct pfm_context *ctx;
++	struct pfm_event_set *set;
++
++	ctx = __get_cpu_var(pmu_ctx);
++	if (!ctx) {
++		pr_info("prev=%d tif=%d ctx=%p next=%d tif=%d ctx=%p\n",
++			prev->pid,
++			test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW),
++			prev->pfm_context,
++			next->pid,
++			test_tsk_thread_flag(next, TIF_PERFMON_CTXSW),
++			next->pfm_context);
++		BUG_ON(!ctx);
++	}
++
++	set = ctx->active_set;
++
++	/*
++	 * propagate TIF_PERFMON_CTXSW to ensure that:
++	 * - previous task has TIF_PERFMON_CTXSW cleared, in case it is
++	 *   scheduled onto another CPU where there is syswide monitoring
++	 * - next task has TIF_PERFMON_CTXSW set to ensure it will come back
++	 *   here when context switched out
++	 */
++	clear_tsk_thread_flag(prev, TIF_PERFMON_CTXSW);
++	set_tsk_thread_flag(next, TIF_PERFMON_CTXSW);
++
++	/*
++	 * nothing to do until actually started
++	 * XXX: assumes no mean to start from user level
++	 */
++	if (!ctx->flags.started)
++		return;
++
++	pfm_arch_ctxswout_sys(prev, ctx, set);
++	pfm_arch_ctxswin_sys(next, ctx, set);
++}
++
++/*
++ * come here when either prev or next has TIF_PERFMON_CTXSW flag set
++ * Note that this is not because a task has TIF_PERFMON_CTXSW set that
++ * it has a context attached, e.g., in system-wide on certain arch.
++ */
++void pfm_ctxsw(struct task_struct *prev, struct task_struct *next)
++{
++	struct pfm_context *ctxp, *ctxn;
++	u64 now;
++
++	now = sched_clock();
++
++	ctxp = prev->pfm_context;
++	ctxn = next->pfm_context;
++
++	if (ctxp)
++		__pfm_ctxswout_thread(prev, ctxp, now);
++
++	if (ctxn)
++		__pfm_ctxswin_thread(next, ctxn, now);
++
++	/*
++	 * given that prev and next can never be the same, this
++	 * test is checking that ctxp == ctxn == NULL which is
++	 * an indication we have an active system-wide session on
++	 * this CPU that needs ctxsw intervention. Not all processors
++	 * needs this, IA64 is one.
++	 */
++	if (ctxp == ctxn)
++		__pfm_ctxsw_sys(prev, next);
++
++	pfm_stats_inc(ctxsw_count);
++	pfm_stats_add(ctxsw_ns, sched_clock() - now);
++}
+--- /dev/null
++++ b/perfmon/perfmon_debugfs.c
+@@ -0,0 +1,186 @@
++/*
++ * perfmon_debugfs.c: perfmon2 statistics interface to debugfs
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/kernel.h>
++#include <linux/perfmon.h>
++#include <linux/debugfs.h>
++
++/*
++ * to make the statistics visible to user space:
++ * $ mount -t debugfs none /mnt
++ * $ cd /mnt/perfmon
++ * then choose a CPU subdir
++ */
++DECLARE_PER_CPU(struct pfm_stats, pfm_stats);
++
++static struct dentry *pfm_debugfs_dir;
++
++void pfm_reset_stats(int cpu)
++{
++	struct pfm_stats *st;
++	unsigned long flags;
++
++	st = &per_cpu(pfm_stats, cpu);
++
++	local_irq_save(flags);
++	memset(st->v, 0, sizeof(st->v));
++	local_irq_restore(flags);
++}
++
++static const char *pfm_stats_strs[] = {
++	"ovfl_intr_all_count",
++	"ovfl_intr_ns",
++	"ovfl_intr_p1_ns",
++	"ovfl_intr_p2_ns",
++	"ovfl_intr_p3_ns",
++	"ovfl_intr_spurious_count",
++	"ovfl_intr_replay_count",
++	"ovfl_intr_regular_count",
++	"handle_work_count",
++	"ovfl_notify_count",
++	"reset_pmds_count",
++	"pfm_restart_count",
++	"fmt_handler_calls",
++	"fmt_handler_ns",
++	"set_switch_count",
++	"set_switch_ns",
++	"ctxsw_count",
++	"ctxsw_ns",
++	"handle_timeout_count",
++	"ovfl_intr_nmi_count",
++};
++#define PFM_NUM_STRS ARRAY_SIZE(pfm_stats_strs)
++
++void pfm_debugfs_del_cpu(int cpu)
++{
++	struct pfm_stats *st;
++	int i;
++
++	st = &per_cpu(pfm_stats, cpu);
++
++	for(i=0; i < PFM_NUM_STATS; i++) {
++		if (st->dirs[i])
++			debugfs_remove(st->dirs[i]);
++		st->dirs[i] = NULL;
++	}
++	if (st->cpu_dir)
++		debugfs_remove(st->cpu_dir);
++	st->cpu_dir = NULL;
++}
++
++int pfm_debugfs_add_cpu(int cpu)
++{
++	struct pfm_stats *st;
++	int i;
++
++	/*
++	 * sanity check between stats names and the number
++	 * of entries in the pfm_stats value array.
++	 */
++	if (PFM_NUM_STRS != PFM_NUM_STATS) {
++		PFM_ERR("PFM_NUM_STRS != PFM_NUM_STATS error");
++		return -1;
++	}
++
++	st = &per_cpu(pfm_stats, cpu);
++	sprintf(st->cpu_name, "cpu%d", cpu);
++
++	st->cpu_dir = debugfs_create_dir(st->cpu_name, pfm_debugfs_dir);
++	if (!st->cpu_dir)
++		return -1;
++
++	for (i=0; i < PFM_NUM_STATS; i++) {
++		st->dirs[i] = debugfs_create_u64(pfm_stats_strs[i],
++						 S_IRUGO,
++						 st->cpu_dir,
++						 &st->v[i]);
++		if (!st->dirs[i])
++			goto error;
++	}
++	pfm_reset_stats(cpu);
++	return 0;
++error:
++	while(i >= 0) {
++		debugfs_remove(st->dirs[i]);
++		i--;
++	}
++	debugfs_remove(st->cpu_dir);
++	return -1;
++}
++
++/*
++ * called once from pfm_init()
++ */
++int __init pfm_init_debugfs(void)
++{
++	int cpu1, cpu2, ret;
++
++	pfm_debugfs_dir = debugfs_create_dir("perfmon", NULL);
++	if (!pfm_debugfs_dir)
++		return -1;
++
++	for_each_online_cpu(cpu1) {
++		ret = pfm_debugfs_add_cpu(cpu1);
++		if (ret)
++			goto error;
++	}
++	return 0;
++error:
++	for_each_online_cpu(cpu2) {
++		if (cpu2 == cpu1)
++			break;
++		pfm_debugfs_del_cpu(cpu2);
++	}
++	return -1;
++}
++
++#if 0
++static ssize_t ovfl_intr_spurious_count_show(void *info, char *buf)
++{
++	struct pfm_stats *st = info;
++	return snprintf(buf, PAGE_SIZE, "%llu\n",
++			(unsigned long long)(st->ovfl_intr_all_count
++					     - st->ovfl_intr_regular_count));
++}
++
++static ssize_t ovfl_intr_regular_count_show(void *info, char *buf)
++{
++	struct pfm_stats *st = info;
++	return snprintf(buf, PAGE_SIZE, "%llu\n",
++			(unsigned long long)(st->ovfl_intr_regular_count
++					     - st->ovfl_intr_replay_count));
++}
++#endif
+--- /dev/null
++++ b/perfmon/perfmon_dfl_smpl.c
+@@ -0,0 +1,294 @@
++/*
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *
++ * This file implements the new default sampling buffer format
++ * for the perfmon2 subsystem.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/kernel.h>
++#include <linux/types.h>
++#include <linux/module.h>
++#include <linux/init.h>
++#include <linux/smp.h>
++
++#include <linux/perfmon.h>
++#include <linux/perfmon_dfl_smpl.h>
++
++MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
++MODULE_DESCRIPTION("new perfmon default sampling format");
++MODULE_LICENSE("GPL");
++
++static int pfm_dfl_fmt_validate(u32 ctx_flags, u16 npmds, void *data)
++{
++	struct pfm_dfl_smpl_arg *arg = data;
++	u64 min_buf_size;
++
++	if (data == NULL) {
++		PFM_DBG("no argument passed");
++		return -EINVAL;
++	}
++
++	/*
++	 * sanity check in case size_t is smaller then u64
++	 */
++#if BITS_PER_LONG == 4
++#define MAX_SIZE_T	(1ULL<<(sizeof(size_t)<<3))
++	if (sizeof(size_t) < sizeof(arg->buf_size)) {
++		if (arg->buf_size >= MAX_SIZE_T)
++			return -ETOOBIG;
++	}
++#endif
++
++	/*
++	 * compute min buf size. npmds is the maximum number
++	 * of implemented PMD registers.
++	 */
++	min_buf_size = sizeof(struct pfm_dfl_smpl_hdr)
++	             + (sizeof(struct pfm_dfl_smpl_entry) + (npmds*sizeof(u64)));
++
++	PFM_DBG("validate ctx_flags=0x%x flags=0x%x npmds=%u "
++		   "min_buf_size=%llu buf_size=%llu\n",
++		   ctx_flags,
++		   arg->buf_flags,
++		   npmds,
++		   (unsigned long long)min_buf_size,
++		   (unsigned long long)arg->buf_size);
++
++	/*
++	 * must hold at least the buffer header + one minimally sized entry
++	 */
++	if (arg->buf_size < min_buf_size)
++		return -EINVAL;
++
++	return 0;
++}
++
++static int pfm_dfl_fmt_get_size(u32 flags, void *data, size_t *size)
++{
++	struct pfm_dfl_smpl_arg *arg = data;
++
++	/*
++	 * size has been validated in default_validate
++	 * we can never loose bits from buf_size.
++	 */
++	*size = (size_t)arg->buf_size;
++
++	return 0;
++}
++
++static int pfm_dfl_fmt_init(struct pfm_context *ctx, void *buf, u32 ctx_flags,
++			    u16 npmds, void *data)
++{
++	struct pfm_dfl_smpl_hdr *hdr;
++	struct pfm_dfl_smpl_arg *arg = data;
++
++	hdr = buf;
++
++	hdr->hdr_version = PFM_DFL_SMPL_VERSION;
++	hdr->hdr_buf_size = arg->buf_size;
++	hdr->hdr_buf_flags = arg->buf_flags;
++	hdr->hdr_cur_offs = sizeof(*hdr);
++	hdr->hdr_overflows = 0;
++	hdr->hdr_count = 0;
++	hdr->hdr_min_buf_space = sizeof(struct pfm_dfl_smpl_entry) + (npmds*sizeof(u64));
++	/*
++	 * due to cache aliasing, it may be necessary to flush the cache
++	 * on certain architectures (e.g., MIPS)
++	 */
++	pfm_cacheflush(hdr, sizeof(*hdr));
++
++	PFM_DBG("buffer=%p buf_size=%llu hdr_size=%zu hdr_version=%u.%u "
++		  "min_space=%llu npmds=%u",
++		  buf,
++		  (unsigned long long)hdr->hdr_buf_size,
++		  sizeof(*hdr),
++		  PFM_VERSION_MAJOR(hdr->hdr_version),
++		  PFM_VERSION_MINOR(hdr->hdr_version),
++		  (unsigned long long)hdr->hdr_min_buf_space,
++		  npmds);
++
++	return 0;
++}
++
++/*
++ * called from pfm_overflow_handler() to record a new sample
++ *
++ * context is locked, interrupts are disabled (no preemption)
++ */
++static int pfm_dfl_fmt_handler(void *buf, struct pfm_ovfl_arg *arg,
++			       unsigned long ip, u64 tstamp, void *data)
++{
++	struct pfm_dfl_smpl_hdr *hdr;
++	struct pfm_dfl_smpl_entry *ent;
++	void *cur, *last;
++	u64 *e;
++	size_t entry_size, min_size;
++	u16 npmds, i;
++	u16 ovfl_pmd;
++
++	hdr = buf;
++	cur = buf+hdr->hdr_cur_offs;
++	last = buf+hdr->hdr_buf_size;
++	ovfl_pmd = arg->ovfl_pmd;
++	min_size = hdr->hdr_min_buf_space;
++
++	/*
++	 * precheck for sanity
++	 */
++	if ((last - cur) < min_size)
++		goto full;
++
++	npmds = arg->num_smpl_pmds;
++
++	ent = (struct pfm_dfl_smpl_entry *)cur;
++
++	entry_size = sizeof(*ent) + (npmds << 3);
++
++	/* position for first pmd */
++	e = (u64 *)(ent+1);
++
++	hdr->hdr_count++;
++
++	PFM_DBG_ovfl("count=%llu cur=%p last=%p free_bytes=%zu ovfl_pmd=%d "
++		       "npmds=%u",
++		       (unsigned long long)hdr->hdr_count,
++		       cur, last,
++		       (last-cur),
++		       ovfl_pmd,
++		       npmds);
++
++	/*
++	 * current = task running at the time of the overflow.
++	 *
++	 * per-task mode:
++	 * 	- this is usually the task being monitored.
++	 * 	  Under certain conditions, it might be a different task
++	 *
++	 * system-wide:
++	 * 	- this is not necessarily the task controlling the session
++	 */
++	ent->pid = current->pid;
++	ent->ovfl_pmd = ovfl_pmd;
++	ent->last_reset_val = arg->pmd_last_reset;
++
++	/*
++	 * where did the fault happen (includes slot number)
++	 */
++	ent->ip = ip;
++
++	ent->tstamp = tstamp;
++	ent->cpu = smp_processor_id();
++	ent->set = arg->active_set;
++	ent->tgid = current->tgid;
++
++	/*
++	 * selectively store PMDs in increasing index number
++	 */
++	if (npmds) {
++		u64 *val = arg->smpl_pmds_values;
++		for(i=0; i < npmds; i++) {
++			*e++ = *val++;
++		}
++	}
++
++	/*
++	 * update position for next entry
++	 */
++	hdr->hdr_cur_offs += entry_size;
++	cur += entry_size;
++
++	pfm_cacheflush(hdr, sizeof(*hdr));
++	pfm_cacheflush(ent, entry_size);
++
++	/*
++	 * post check to avoid losing the last sample
++	 */
++	if ((last - cur) < min_size)
++		goto full;
++
++	/* reset before returning from interrupt handler */
++	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
++
++	return 0;
++full:
++	PFM_DBG_ovfl("sampling buffer full free=%zu, count=%llu",
++		     last-cur,
++		     (unsigned long long)hdr->hdr_count);
++
++	/*
++	 * increment number of buffer overflows.
++	 * important to detect duplicate set of samples.
++	 */
++	hdr->hdr_overflows++;
++
++	/*
++	 * request notification and masking of monitoring.
++	 * Notification is still subject to the overflowed
++	 * register having the FL_NOTIFY flag set.
++	 */
++	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY| PFM_OVFL_CTRL_MASK;
++
++	return -ENOBUFS; /* we are full, sorry */
++}
++
++static int pfm_dfl_fmt_restart(int is_active, u32 *ovfl_ctrl, void *buf)
++{
++	struct pfm_dfl_smpl_hdr *hdr;
++
++	hdr = buf;
++
++	hdr->hdr_count = 0;
++	hdr->hdr_cur_offs = sizeof(*hdr);
++
++	pfm_cacheflush(hdr, sizeof(*hdr));
++
++	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
++
++	return 0;
++}
++
++static int pfm_dfl_fmt_exit(void *buf)
++{
++	return 0;
++}
++
++static struct pfm_smpl_fmt dfl_fmt={
++	.fmt_name = "default",
++	.fmt_version = 0x10000,
++	.fmt_arg_size = sizeof(struct pfm_dfl_smpl_arg),
++	.fmt_validate = pfm_dfl_fmt_validate,
++	.fmt_getsize = pfm_dfl_fmt_get_size,
++	.fmt_init = pfm_dfl_fmt_init,
++	.fmt_handler = pfm_dfl_fmt_handler,
++	.fmt_restart = pfm_dfl_fmt_restart,
++	.fmt_exit = pfm_dfl_fmt_exit,
++	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
++	.owner = THIS_MODULE
++};
++
++static int pfm_dfl_fmt_init_module(void)
++{
++	return pfm_fmt_register(&dfl_fmt);
++}
++
++static void pfm_dfl_fmt_cleanup_module(void)
++{
++	pfm_fmt_unregister(&dfl_fmt);
++}
++
++module_init(pfm_dfl_fmt_init_module);
++module_exit(pfm_dfl_fmt_cleanup_module);
+--- /dev/null
++++ b/perfmon/perfmon_file.c
+@@ -0,0 +1,796 @@
++/*
++ * perfmon_file.c: perfmon2 file input/output functions
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/kernel.h>
++#include <linux/module.h>
++#include <linux/file.h>
++#include <linux/poll.h>
++#include <linux/vfs.h>
++#include <linux/pagemap.h>
++#include <linux/mount.h>
++#include <linux/perfmon.h>
++
++#define PFMFS_MAGIC 0xa0b4d889	/* perfmon filesystem magic number */
++
++static inline int pfm_msgq_is_empty(struct pfm_context *ctx)
++{
++	return ctx->msgq_head == ctx->msgq_tail;
++}
++
++static int pfmfs_delete_dentry(struct dentry *dentry)
++{
++	return 1;
++}
++
++static struct dentry_operations pfmfs_dentry_operations = {
++	.d_delete = pfmfs_delete_dentry,
++};
++
++static union pfarg_msg *pfm_get_next_msg(struct pfm_context *ctx)
++{
++	union pfarg_msg *msg;
++
++	PFM_DBG_ovfl("in head=%d tail=%d",
++		ctx->msgq_head & PFM_MSGQ_MASK,
++		ctx->msgq_tail & PFM_MSGQ_MASK);
++
++	if (pfm_msgq_is_empty(ctx))
++		return NULL;
++
++	/*
++	 * get oldest message
++	 */
++	msg = ctx->msgq + (ctx->msgq_tail & PFM_MSGQ_MASK);
++
++	/*
++	 * move tail forward
++	 */
++	ctx->msgq_tail++;
++
++	PFM_DBG_ovfl("out head=%d tail=%d type=%d",
++		ctx->msgq_head & PFM_MSGQ_MASK,
++		ctx->msgq_tail & PFM_MSGQ_MASK,
++		msg->type);
++
++	return msg;
++}
++
++static struct page *pfm_buf_map_pagefault(struct vm_area_struct *vma,
++					  unsigned long address, int *type)
++{
++	void *kaddr;
++	struct pfm_context *ctx;
++	struct page *page;
++	size_t size;
++
++	ctx = vma->vm_private_data;
++	if (ctx == NULL) {
++		PFM_DBG("no ctx");
++		return NOPAGE_SIGBUS;
++	}
++	/*
++	 * size available to user (maybe different from real_smpl_size
++	 */
++	size = ctx->smpl_size;
++
++	if ( (address < (unsigned long) vma->vm_start) ||
++	     (address >= (unsigned long) (vma->vm_start + size)) )
++		return NOPAGE_SIGBUS;
++
++	kaddr = ctx->smpl_addr + (address - vma->vm_start);
++
++	if (type)
++		*type = VM_FAULT_MINOR;
++
++	page = vmalloc_to_page(kaddr);
++	get_page(page);
++
++	PFM_DBG("[%d] start=%p ref_count=%d",
++		current->pid,
++		kaddr, page_count(page));
++
++	return page;
++}
++/*
++ * we need to determine whther or not we are closing the last reference
++ * to the file and thus are going to end up in pfm_close() which eventually
++ * calls pfm_release_buf_space(). In that function, we update the accouting
++ * for locked_vm given that we are actually freeing the sampling buffer. The
++ * issue is that there are multiple paths leading to pfm_release_buf_space(),
++ * from exit(), munmap(), close(). The path coming from munmap() is problematic
++ * becuse do_munmap() grabs mmap_sem in write-mode which is also what
++ * pfm_release_buf_space does. To avoid deadlock, we need to determine where
++ * we are calling from and skip the locking. The vm_ops->close() callback
++ * is invoked for each remove_vma() independently of the number of references
++ * left on the file descriptor, therefore simple reference counter does not
++ * work. We need to determine if this is the last call, and then set a flag
++ * to skip the locking.
++ */
++static void pfm_buf_map_close(struct vm_area_struct *vma)
++{
++	struct file *file;
++	struct pfm_context *ctx;
++
++	file = vma->vm_file;
++	ctx = vma->vm_private_data;
++
++	/*
++	 * if file is going to close, then pfm_close() will
++	 * be called, do not lock in pfm_release_buf
++	 */
++	if (atomic_read(&file->f_count) == 1)
++		ctx->flags.mmap_nlock = 1;
++}
++
++/*
++ * we do not have a close callback because, the locked
++ * memory accounting must be done when the actual buffer
++ * is freed. Munmap does not free the page backing the vma
++ * because they may still be in use by the PMU interrupt handler.
++ */
++struct vm_operations_struct pfm_buf_map_vm_ops = {
++	.nopage	= pfm_buf_map_pagefault,
++	.close = pfm_buf_map_close
++};
++
++static int pfm_mmap_buffer(struct pfm_context *ctx, struct vm_area_struct *vma,
++			   size_t size)
++{
++	if (ctx->smpl_addr == NULL) {
++		PFM_DBG("no sampling buffer to map");
++		return -EINVAL;
++	}
++
++	if (size > ctx->smpl_size) {
++		PFM_DBG("mmap size=%zu >= actual buf size=%zu",
++			size,
++			ctx->smpl_size);
++		return -EINVAL;
++	}
++
++	vma->vm_ops = &pfm_buf_map_vm_ops;
++	vma->vm_private_data = ctx;
++
++	return 0;
++}
++
++static int pfm_mmap(struct file *file, struct vm_area_struct *vma)
++{
++	size_t size;
++	struct pfm_context *ctx;
++	unsigned long flags;
++	int ret;
++
++	PFM_DBG("pfm_file_ops");
++
++	ctx  = file->private_data;
++	size = (vma->vm_end - vma->vm_start);
++
++	if (ctx == NULL)
++		return -EINVAL;
++
++	ret = -EINVAL;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	if (vma->vm_flags & VM_WRITE) {
++		PFM_DBG("cannot map buffer for writing");
++		goto done;
++	}
++
++	PFM_DBG("vm_pgoff=%lu size=%zu vm_start=0x%lx",
++		vma->vm_pgoff,
++		size,
++		vma->vm_start);
++
++	ret = pfm_mmap_buffer(ctx, vma, size);
++	if (ret == 0)
++		vma->vm_flags |= VM_RESERVED;
++
++	PFM_DBG("ret=%d vma_flags=0x%lx vma_start=0x%lx vma_size=%lu",
++		ret,
++		vma->vm_flags,
++		vma->vm_start,
++		vma->vm_end-vma->vm_start);
++done:
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	return ret;
++}
++
++/*
++ * Extract one message from queue.
++ *
++ * return:
++ * 	-EAGAIN:  when non-blocking and nothing is* in the queue.
++ * 	-ERESTARTSYS: when blocking and signal is pending
++ * 	Otherwise returns size of message (sizeof(pfarg_msg))
++ */
++ssize_t __pfm_read(struct pfm_context *ctx, union pfarg_msg *msg_buf, int non_block)
++{
++	union pfarg_msg *msg;
++	ssize_t ret = 0;
++	unsigned long flags;
++	DECLARE_WAITQUEUE(wait, current);
++
++	/*
++	 * we must masks interrupts to avoid a race condition
++	 * with the PMU interrupt handler.
++	 */
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	while (pfm_msgq_is_empty(ctx)) {
++
++		/*
++		 * handle non-blocking reads
++		 * return -EAGAIN
++		 */
++		ret = -EAGAIN;
++		if (non_block)
++			break;
++
++		add_wait_queue(&ctx->msgq_wait, &wait);
++		set_current_state(TASK_INTERRUPTIBLE);
++
++		spin_unlock_irqrestore(&ctx->lock, flags);
++
++		schedule();
++
++		/*
++		 * during this window, another thread may call
++		 * pfm_read() and steal our message
++		 */
++
++		spin_lock_irqsave(&ctx->lock, flags);
++
++		remove_wait_queue(&ctx->msgq_wait, &wait);
++		set_current_state(TASK_RUNNING);
++
++		/*
++		 * check for pending signals
++		 * return -ERESTARTSYS
++		 */
++		ret = -ERESTARTSYS;
++		if(signal_pending(current))
++			break;
++
++		/*
++		 * we may have a message
++		 */
++		ret = 0;
++	}
++
++	/*
++	 * extract message
++	 */
++	if (ret == 0) {
++		msg = pfm_get_next_msg(ctx);
++		BUG_ON(msg == NULL);
++
++		/*
++		 * we must make a local copy before we unlock
++		 * to ensure that the message queue cannot fill
++		 * (overwriting our message) up before
++		 * we do copy_to_user() which cannot be done
++		 * with interrupts masked.
++		 */
++		*msg_buf = *msg;
++
++		ret = sizeof(*msg);
++
++		PFM_DBG("extracted type=%d", msg->type);
++	}
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	PFM_DBG("blocking=%d ret=%zd", non_block, ret);
++
++	return ret;
++}
++
++static ssize_t pfm_read(struct file *filp, char __user *buf, size_t size,
++			loff_t *ppos)
++{
++	struct pfm_context *ctx;
++	union pfarg_msg msg_buf;
++	int non_block, ret;
++
++	PFM_DBG_ovfl("buf=%p size=%zu", buf, size);
++
++	ctx = filp->private_data;
++	if (ctx == NULL) {
++		PFM_ERR("no ctx for pfm_read");
++		return -EINVAL;
++	}
++
++	non_block = filp->f_flags & O_NONBLOCK;
++
++	/*
++	 * detect IA-64 v2.0 context read (message size is different)
++	 * nops on all other architectures
++	 */
++	if (unlikely(ctx->flags.ia64_v20_compat))
++		return pfm_arch_compat_read(ctx,  buf, non_block, size);
++
++	/*
++	 * cannot extract partial messages.
++	 * check even when there is no message
++	 *
++	 * cannot extract more than one message per call. Bytes
++	 * above sizeof(msg) are ignored.
++	 */
++	if (size < sizeof(msg_buf)) {
++		PFM_DBG("message is too small size=%zu must be >=%zu)",
++			size,
++			sizeof(msg_buf));
++		return -EINVAL;
++	}
++
++	ret =  __pfm_read(ctx, &msg_buf, non_block);
++	if (ret > 0) {
++		if (copy_to_user(buf, &msg_buf, sizeof(msg_buf)))
++			ret = -EFAULT;
++	}
++	PFM_DBG_ovfl("ret=%d", ret);
++	return ret;
++}
++
++static ssize_t pfm_write(struct file *file, const char __user *ubuf,
++			  size_t size, loff_t *ppos)
++{
++	PFM_DBG("pfm_write called");
++	return -EINVAL;
++}
++
++static unsigned int pfm_poll(struct file *filp, poll_table *wait)
++{
++	struct pfm_context *ctx;
++	unsigned long flags;
++	unsigned int mask = 0;
++
++	PFM_DBG("pfm_file_ops");
++
++	if (filp->f_op != &pfm_file_ops) {
++		PFM_ERR("pfm_poll bad magic");
++		return 0;
++	}
++
++	ctx = filp->private_data;
++	if (ctx == NULL) {
++		PFM_ERR("pfm_poll no ctx");
++		return 0;
++	}
++
++	PFM_DBG("before poll_wait");
++
++	poll_wait(filp, &ctx->msgq_wait, wait);
++
++	/*
++	 * pfm_msgq_is_empty() is non-atomic
++	 *
++	 * filp is protected by fget() at upper level
++	 * context cannot be closed by another thread.
++	 *
++	 * There may be a race with a PMU interrupt adding
++	 * messages to the queue. But we are interested in
++	 * queue not empty, so adding more messages should
++	 * not really be a problem.
++	 *
++	 * There may be a race with another thread issuing
++	 * a read() and stealing messages from the queue thus
++	 * may return the wrong answer. This could potentially
++	 * lead to a blocking read, because nothing is
++	 * available in the queue
++	 */
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	if (!pfm_msgq_is_empty(ctx))
++		mask =  POLLIN | POLLRDNORM;
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	PFM_DBG("after poll_wait mask=0x%x", mask);
++
++	return mask;
++}
++
++static int pfm_ioctl(struct inode *inode, struct file *file, unsigned int cmd,
++		     unsigned long arg)
++{
++	PFM_DBG("pfm_ioctl called");
++	return -EINVAL;
++}
++
++/*
++ * interrupt cannot be masked when entering this function
++ */
++static inline int __pfm_fasync(int fd, struct file *filp,
++			       struct pfm_context *ctx, int on)
++{
++	int ret;
++
++	ret = fasync_helper (fd, filp, on, &ctx->async_queue);
++
++	PFM_DBG("fd=%d on=%d async_q=%p ret=%d",
++		fd,
++		on,
++		ctx->async_queue, ret);
++
++	return ret;
++}
++
++static int pfm_fasync(int fd, struct file *filp, int on)
++{
++	struct pfm_context *ctx;
++	int ret;
++
++	PFM_DBG("pfm_file_ops");
++
++	ctx = filp->private_data;
++	if (ctx == NULL) {
++		PFM_ERR("pfm_fasync no ctx");
++		return -EBADF;
++	}
++
++	/*
++	 * we cannot mask interrupts during this call because this may
++	 * may go to sleep if memory is not readily avalaible.
++	 *
++	 * We are protected from the context disappearing by the
++	 * get_fd()/put_fd() done in caller. Serialization of this function
++	 * is ensured by caller.
++	 */
++	ret = __pfm_fasync(fd, filp, ctx, on);
++
++	PFM_DBG("pfm_fasync called on fd=%d on=%d async_queue=%p ret=%d",
++		fd,
++		on,
++		ctx->async_queue, ret);
++
++	return ret;
++}
++
++#ifdef CONFIG_SMP
++static void __pfm_close_remote_cpu(void *info)
++{
++	struct pfm_context *ctx = info;
++	int can_release;
++
++	BUG_ON(ctx != __get_cpu_var(pmu_ctx));
++
++	/*
++	 * we are in IPI interrupt handler which has always higher
++	 * priority than PMU interrupt, therefore we do not need to
++	 * mask interrupts. context locking is not needed because we
++	 * are in close(), no more user references.
++	 *
++	 * can_release is ignored, release done on calling CPU
++	 */
++	__pfm_unload_context(ctx, &can_release);
++
++	/*
++	 * we cannot free context here because we are in_interrupt().
++	 * we free on the calling CPU
++	 */
++}
++
++static int pfm_close_remote_cpu(u32 cpu, struct pfm_context *ctx)
++{
++	BUG_ON(irqs_disabled());
++	return smp_call_function_single(cpu, __pfm_close_remote_cpu, ctx, 0, 1);
++}
++#endif /* CONFIG_SMP */
++
++/*
++ * called either on explicit close() or from exit_files().
++ * Only the LAST user of the file gets to this point, i.e., it is
++ * called only ONCE.
++ *
++ * IMPORTANT: we get called ONLY when the refcnt on the file gets to zero
++ * (fput()),i.e, last task to access the file. Nobody else can access the
++ * file at this point.
++ *
++ * When called from exit_files(), the VMA has been freed because exit_mm()
++ * is executed before exit_files().
++ *
++ * When called from exit_files(), the current task is not yet ZOMBIE but we
++ * flush the PMU state to the context.
++ */
++int __pfm_close(struct pfm_context *ctx, struct file *filp)
++{
++	unsigned long flags;
++	int state;
++	int can_free = 1, can_unload = 1;
++	int is_system, can_release = 0;
++	u32 cpu;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	state = ctx->state;
++	is_system = ctx->flags.system;
++	cpu = ctx->cpu;
++
++	PFM_DBG("state=%d", state);
++
++	/*
++	 * check if unload is needed
++	 */
++	if (state == PFM_CTX_UNLOADED)
++		goto doit;
++
++#ifdef CONFIG_SMP
++	/*
++	 * we need to release the resource on the ORIGINAL cpu.
++	 * we need to release the context lock to avoid deadlocks
++	 * on the original CPU, especially in the context switch
++	 * routines. It is safe to unlock because we are in close(),
++	 * in other words, there is no more access from user level.
++	 * we can also unmask interrupts on this CPU because the
++	 * context is running on the original CPU. Context will be
++	 * unloaded and the session will be released on the original
++	 * CPU. Upon return, the caller is guaranteed that the context
++	 * is gone from original CPU.
++	 */
++	if (is_system && cpu != smp_processor_id()) {
++		spin_unlock_irqrestore(&ctx->lock, flags);
++		pfm_close_remote_cpu(cpu, ctx);
++		can_release = 1;
++		goto free_it;
++	}
++
++	if (!is_system && ctx->task != current) {
++		/*
++		 * switch context to zombie state
++		 */
++		ctx->state = PFM_CTX_ZOMBIE;
++
++		PFM_DBG("zombie ctx for [%d]", ctx->task->pid);
++		/*
++		 * must check if other thread is using block overflow
++		 * notification mode. If so make sure it will not block
++		 * because there will not be any pfm_restart() issued.
++		 * When the thread notices the ZOMBIE state, it will clean
++		 * up what is left of the context
++		 */
++		if (state == PFM_CTX_MASKED && ctx->flags.block) {
++			/*
++			 * force task to wake up from MASKED state
++			 */
++			PFM_DBG("waking up [%d]", ctx->task->pid);
++
++			complete(&ctx->restart_complete);
++		}
++		/*
++		 * PMU session will be release by monitored task when it notices
++		 * ZOMBIE state as part of pfm_unload_context()
++		 */
++		can_unload = can_free = 0;
++	}
++#endif
++	if (can_unload)
++		__pfm_unload_context(ctx, &can_release);
++doit:
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++#ifdef CONFIG_SMP
++free_it:
++#endif
++	if (can_release)
++		pfm_release_session(is_system, cpu);
++
++	if (can_free)
++		pfm_context_free(ctx);
++
++	return 0;
++}
++
++static int pfm_close(struct inode *inode, struct file *filp)
++{
++	struct pfm_context *ctx;
++
++	PFM_DBG("called filp=%p", filp);
++
++	ctx = filp->private_data;
++	if (ctx == NULL) {
++		PFM_ERR("no ctx");
++		return -EBADF;
++	}
++	return __pfm_close(ctx, filp);
++}
++
++static int pfm_no_open(struct inode *irrelevant, struct file *dontcare)
++{
++	PFM_DBG("pfm_file_ops");
++
++	return -ENXIO;
++}
++
++/*
++ * pfm_flush() is called from filp_close() on every call to
++ * close(). pfm_close() is only invoked when the last user
++ * calls close(). pfm_close() is never invoked without
++ * pfm_flush() being invoked first.
++ *
++ * Partially free resources:
++ * 	- remove from fasync queue
++ */
++static int pfm_flush(struct file *filp, fl_owner_t id)
++{
++	struct pfm_context *ctx;
++
++	PFM_DBG("called filp=%p", filp);
++
++	ctx = filp->private_data;
++	if (ctx == NULL) {
++		PFM_ERR("pfm_flush no ctx");
++		return -EBADF;
++	}
++
++	/*
++	 * remove our file from the async queue, if we use this mode.
++	 * This can be done without the context being protected. We come
++	 * here when the context has become unreacheable by other tasks.
++	 *
++	 * We may still have active monitoring at this point and we may
++	 * end up in pfm_overflow_handler(). However, fasync_helper()
++	 * operates with interrupts disabled and it cleans up the
++	 * queue. If the PMU handler is called prior to entering
++	 * fasync_helper() then it will send a signal. If it is
++	 * invoked after, it will find an empty queue and no
++	 * signal will be sent. In both case, we are safe
++	 */
++	if (filp->f_flags & FASYNC) {
++		PFM_DBG("cleaning up async_queue=%p", ctx->async_queue);
++		__pfm_fasync (-1, filp, ctx, 0);
++	}
++	return 0;
++}
++
++const struct file_operations pfm_file_ops = {
++	.llseek = no_llseek,
++	.read = pfm_read,
++	.write = pfm_write,
++	.poll = pfm_poll,
++	.ioctl = pfm_ioctl,
++	.open = pfm_no_open, /* special open to disallow open via /proc */
++	.fasync = pfm_fasync,
++	.release = pfm_close,
++	.flush= pfm_flush,
++	.mmap = pfm_mmap
++};
++
++static int pfmfs_get_sb(struct file_system_type *fs_type,
++			int flags, const char *dev_name,
++			void *data, struct vfsmount *mnt)
++{
++	return get_sb_pseudo(fs_type, "pfm:", NULL, PFMFS_MAGIC, mnt);
++}
++
++static struct file_system_type pfm_fs_type = {
++	.name     = "pfmfs",
++	.get_sb   = pfmfs_get_sb,
++	.kill_sb  = kill_anon_super,
++};
++
++/*
++ * pfmfs should _never_ be mounted by userland - too much of security hassle,
++ * no real gain from having the whole whorehouse mounted. So we don't need
++ * any operations on the root directory. However, we need a non-trivial
++ * d_name - pfm: will go nicely and kill the special-casing in procfs.
++ */
++static struct vfsmount *pfmfs_mnt;
++
++int __init pfm_init_fs(void)
++{
++	int err = register_filesystem(&pfm_fs_type);
++	if (!err) {
++		pfmfs_mnt = kern_mount(&pfm_fs_type);
++		err = PTR_ERR(pfmfs_mnt);
++		if (IS_ERR(pfmfs_mnt))
++			unregister_filesystem(&pfm_fs_type);
++		else
++			err = 0;
++	}
++	return err;
++}
++
++int pfm_alloc_fd(struct file **cfile)
++{
++	int fd, ret = 0;
++	struct file *file = NULL;
++	struct inode * inode;
++	char name[32];
++	struct qstr this;
++
++	fd = get_unused_fd();
++	if (fd < 0)
++		return -ENFILE;
++
++	ret = -ENFILE;
++
++	file = get_empty_filp();
++	if (!file)
++		goto out;
++
++	/*
++	 * allocate a new inode
++	 */
++	inode = new_inode(pfmfs_mnt->mnt_sb);
++	if (!inode)
++		goto out;
++
++	PFM_DBG("new inode ino=%ld @%p", inode->i_ino, inode);
++
++	inode->i_sb = pfmfs_mnt->mnt_sb;
++	inode->i_mode = S_IFCHR|S_IRUGO;
++	inode->i_uid = current->fsuid;
++	inode->i_gid = current->fsgid;
++
++	sprintf(name, "[%lu]", inode->i_ino);
++	this.name = name;
++	this.hash = inode->i_ino;
++	this.len = strlen(name);
++
++	ret = -ENOMEM;
++
++	/*
++	 * allocate a new dcache entry
++	 */
++	file->f_dentry = d_alloc(pfmfs_mnt->mnt_sb->s_root, &this);
++	if (!file->f_dentry)
++		goto out;
++
++	file->f_dentry->d_op = &pfmfs_dentry_operations;
++
++	d_add(file->f_dentry, inode);
++	file->f_vfsmnt = mntget(pfmfs_mnt);
++	file->f_mapping = inode->i_mapping;
++
++	file->f_op = &pfm_file_ops;
++	file->f_mode = FMODE_READ;
++	file->f_flags = O_RDONLY;
++	file->f_pos  = 0;
++
++	*cfile = file;
++
++	return fd;
++out:
++	if (file)
++		put_filp(file);
++	put_unused_fd(fd);
++	return ret;
++}
+--- /dev/null
++++ b/perfmon/perfmon_fmt.c
+@@ -0,0 +1,218 @@
++/*
++ * perfmon_fmt.c: perfmon2 sampling buffer format management
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_smpl_fmt_lock);
++static LIST_HEAD(pfm_smpl_fmt_list);
++
++static inline int fmt_is_mod(struct pfm_smpl_fmt *f)
++{
++	return !(f->fmt_flags & PFM_FMTFL_IS_BUILTIN);
++}
++
++static struct pfm_smpl_fmt *pfm_find_fmt(char *name)
++{
++	struct pfm_smpl_fmt * entry;
++
++	list_for_each_entry(entry, &pfm_smpl_fmt_list, fmt_list) {
++		if (!strcmp(entry->fmt_name, name))
++			return entry;
++	}
++	return NULL;
++}
++/*
++ * find a buffer format based on its name
++ */
++struct pfm_smpl_fmt *pfm_smpl_fmt_get(char *name)
++{
++	struct pfm_smpl_fmt * fmt;
++
++	spin_lock(&pfm_smpl_fmt_lock);
++
++	fmt = pfm_find_fmt(name);
++
++	/*
++	 * increase module refcount
++	 */
++	if (fmt && fmt_is_mod(fmt) && !try_module_get(fmt->owner))
++		fmt = NULL;
++
++	spin_unlock(&pfm_smpl_fmt_lock);
++
++	return fmt;
++}
++
++void pfm_smpl_fmt_put(struct pfm_smpl_fmt *fmt)
++{
++	if (fmt == NULL || !fmt_is_mod(fmt))
++		return;
++	BUG_ON(fmt->owner == NULL);
++
++	spin_lock(&pfm_smpl_fmt_lock);
++	module_put(fmt->owner);
++	spin_unlock(&pfm_smpl_fmt_lock);
++}
++
++int pfm_fmt_register(struct pfm_smpl_fmt *fmt)
++{
++	int ret = 0;
++
++	if (perfmon_disabled) {
++		PFM_INFO("perfmon disabled, cannot add sampling format");
++		return -ENOSYS;
++	}
++
++	/* some sanity checks */
++	if (fmt == NULL) {
++		PFM_INFO("perfmon: NULL format for register");
++		return -EINVAL;
++	}
++
++	if (fmt->fmt_name == NULL) {
++		PFM_INFO("perfmon: format has no name");
++		return -EINVAL;
++	}
++
++	if (fmt->fmt_qdepth > PFM_MSGS_COUNT) {
++		PFM_INFO("perfmon: format %s requires %u msg queue depth (max %d)",
++		       fmt->fmt_name,
++		       fmt->fmt_qdepth,
++		       PFM_MSGS_COUNT);
++		return -EINVAL;
++	}
++
++	/*
++	 * fmt is missing the initialization of .owner = THIS_MODULE
++	 * this is only valid when format is compiled as a module
++	 */
++	if (fmt->owner == NULL && fmt_is_mod(fmt)) {
++		PFM_INFO("format %s has no module owner", fmt->fmt_name);
++		return -EINVAL;
++	}
++	/*
++	 * we need at least a handler
++	 */
++	if (fmt->fmt_handler == NULL) {
++		PFM_INFO("format %s has no handler", fmt->fmt_name);
++		return -EINVAL;
++	}
++
++	/*
++	 * format argument size cannot be bigger than PAGE_SIZE
++	 */
++	if (fmt->fmt_arg_size > PAGE_SIZE) {
++		PFM_INFO("format %s arguments too big", fmt->fmt_name);
++		return -EINVAL;
++	}
++
++	spin_lock(&pfm_smpl_fmt_lock);
++
++	/*
++	 * because of sysfs, we cannot have two formats with the same name
++	 */
++	if (pfm_find_fmt(fmt->fmt_name)) {
++		PFM_INFO("format %s already registered", fmt->fmt_name);
++		ret = -EBUSY;
++		goto out;
++	}
++
++	ret = pfm_sysfs_add_fmt(fmt);
++	if (ret) {
++		PFM_INFO("sysfs cannot add format entry for %s", fmt->fmt_name);
++		goto out;
++	}
++
++	list_add(&fmt->fmt_list, &pfm_smpl_fmt_list);
++
++	PFM_INFO("added sampling format %s", fmt->fmt_name);
++out:
++	spin_unlock(&pfm_smpl_fmt_lock);
++
++	return ret;
++}
++EXPORT_SYMBOL(pfm_fmt_register);
++
++int pfm_fmt_unregister(struct pfm_smpl_fmt *fmt)
++{
++	struct pfm_smpl_fmt *fmt2;
++	int ret = 0;
++
++	if (!fmt || !fmt->fmt_name) {
++		PFM_DBG("invalid fmt");
++		return -EINVAL;
++	}
++
++	spin_lock(&pfm_smpl_fmt_lock);
++
++	fmt2 = pfm_find_fmt(fmt->fmt_name);
++	if (!fmt) {
++		PFM_INFO("unregister failed, format not registered");
++		ret = -EINVAL;
++		goto out;
++	}
++	list_del_init(&fmt->fmt_list);
++
++	pfm_sysfs_remove_fmt(fmt);
++
++	PFM_INFO("removed sampling format: %s", fmt->fmt_name);
++
++out:
++	spin_unlock(&pfm_smpl_fmt_lock);
++	return ret;
++
++}
++EXPORT_SYMBOL(pfm_fmt_unregister);
++
++/*
++ * we defer adding the builtin formats to /sys/kernel/perfmon/formats
++ * until after the pfm sysfs subsystem is initialized. This function
++ * is called from pfm_sysfs_init()
++ */
++void pfm_sysfs_builtin_fmt_add(void)
++{
++	struct pfm_smpl_fmt * entry;
++
++	/*
++	 * locking not needed, kernel not fully booted
++	 * when called
++	 */
++	list_for_each_entry(entry, &pfm_smpl_fmt_list, fmt_list) {
++		pfm_sysfs_add_fmt(entry);
++	}
++}
+--- /dev/null
++++ b/perfmon/perfmon_intr.c
+@@ -0,0 +1,577 @@
++/*
++ * perfmon_intr.c: perfmon2 interrupt handling
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/kernel.h>
++#include <linux/perfmon.h>
++#include <linux/module.h>
++
++static inline void pfm_mask_monitoring(struct pfm_context *ctx,
++				       struct pfm_event_set *set)
++{
++	u64 now;
++
++
++	now = sched_clock();
++
++	/*
++	 * we save the PMD values such that we can read them while
++	 * MASKED without having the thread stopped
++	 * because monitoring is stopped
++	 *
++	 * XXX: could be avoided in system-wide
++	 */
++	pfm_save_pmds(ctx, set);
++	pfm_arch_mask_monitoring(ctx, set);
++	/*
++	 * accumulate the set duration up to this point
++	 */
++	set->duration += now - set->duration_start;
++
++	ctx->state = PFM_CTX_MASKED;
++
++	PFM_DBG_ovfl("can_restart=%u", ctx->flags.can_restart);
++}
++
++/*
++ * main overflow processing routine.
++ *
++ * set->num_ovfl_pmds is 0 when returning from this function even though
++ * set->ovfl_pmds[] may have bits set. When leaving set->num_ovfl_pmds
++ * must never be used to determine if there was a pending overflow.
++ */
++static void pfm_overflow_handler(struct pfm_context *ctx, struct pfm_event_set *set,
++				 unsigned long ip,
++				 struct pt_regs *regs)
++{
++	struct pfm_ovfl_arg *ovfl_arg;
++	struct pfm_event_set *set_orig;
++	void *hdr;
++	u64 old_val, ovfl_mask, new_val, ovfl_thres;
++	u64 *ovfl_notify, *ovfl_pmds, *pend_ovfls;
++	u64 *smpl_pmds, *reset_pmds, *cnt_pmds;
++	u64 now, t0, t1;
++	u32 ovfl_ctrl, num_ovfl, num_ovfl_orig;
++	u16 i, max_pmd, max_intr, first_intr;
++	u8 must_switch, has_64b_ovfl;
++	u8 ctx_block, has_notify, has_ovfl_sw;
++
++	now = t0 = sched_clock();
++
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++
++	max_pmd = pfm_pmu_conf->regs.max_pmd;
++	first_intr = pfm_pmu_conf->regs.first_intr_pmd;
++	max_intr = pfm_pmu_conf->regs.max_intr_pmd;
++	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
++
++	ovfl_pmds = set->ovfl_pmds;
++	num_ovfl = num_ovfl_orig = set->npend_ovfls;
++	pend_ovfls = set->povfl_pmds;
++	has_ovfl_sw = set->flags & PFM_SETFL_OVFL_SWITCH;
++	set_orig = set;
++
++	if (unlikely(ctx->state == PFM_CTX_ZOMBIE))
++		goto stop_monitoring;
++
++	must_switch = has_64b_ovfl = 0;
++
++	hdr = ctx->smpl_addr;
++
++	PFM_DBG_ovfl("intr_pmds=0x%llx npend=%u ip=%p, blocking=%d "
++		     "u_pmds=0x%llx use_fmt=%u",
++		     (unsigned long long)pend_ovfls[0],
++		     num_ovfl,
++		     (void *)ip,
++		     ctx->flags.block,
++		     (unsigned long long)set->used_pmds[0],
++		     ctx->smpl_fmt != NULL);
++
++	/*
++	 * initialize temporary bitvectors
++	 * we allocate bitvectors in the context
++	 * rather than on the stack to minimize stack
++	 * space consumption. PMU interrupt is very high
++	 * which implies possible deep nesting of interrupt
++	 * hence limited kernel stack space.
++	 *
++	 * This is safe because a context can only be in the
++	 * overflow handler once at a time
++	 */
++	reset_pmds = set->reset_pmds;
++	ovfl_notify = ctx->ovfl_ovfl_notify;
++
++	bitmap_zero(cast_ulp(reset_pmds), max_pmd);
++
++	/*
++	 * first we update the virtual counters
++	 *
++	 * we leverage num_ovfl to minimize number of
++	 * iterations of the loop.
++	 *
++	 * The i < max_intr is just a sanity check
++	 */
++	for (i = first_intr; num_ovfl && i < max_intr ; i++) {
++		/*
++		 * skip pmd which did not overflow
++		 */
++		if (!test_bit(i, cast_ulp(pend_ovfls)))
++			continue;
++
++		num_ovfl--;
++
++		/*
++		 * Update software value for counters ONLY
++		 *
++		 * Note that the pmd is not necessarily 0 at this point as
++		 * qualified events may have happened before the PMU was
++		 * frozen. The residual count is not taken into consideration
++		 * here but will be with any read of the pmd
++		 */
++		ovfl_thres = set->pmds[i].ovflsw_thres;
++
++		if (likely(test_bit(i, cast_ulp(cnt_pmds)))) {
++			old_val = new_val = set->pmds[i].value;
++			new_val += 1 + ovfl_mask;
++			set->pmds[i].value = new_val;
++		}  else {
++			/* for non counter which interrupt, we consider
++			 * this equivalent to a 64-bit counter overflow.
++			 */
++			old_val = 1; new_val = 0;
++		}
++
++		/*
++		 * check for overflow condition
++		 */
++		if (likely(old_val > new_val)) {
++			has_64b_ovfl = 1;
++			if (has_ovfl_sw && ovfl_thres > 0) {
++				if (ovfl_thres == 1)
++					must_switch = 1;
++				set->pmds[i].ovflsw_thres = ovfl_thres - 1;
++			}
++
++			/*
++			 * what to reset because of this overflow
++			 */
++			__set_bit(i, cast_ulp(reset_pmds));
++
++			bitmap_or(cast_ulp(reset_pmds),
++				  cast_ulp(reset_pmds),
++				  cast_ulp(set->pmds[i].reset_pmds),
++				  max_pmd);
++
++		} else {
++			/*
++			 * only keep track of 64-bit overflows or
++			 * assimilated
++			 */
++			__clear_bit(i, cast_ulp(pend_ovfls));
++
++			/*
++			 * on some PMU, it may be necessary to re-arm the PMD
++			 */
++			pfm_arch_ovfl_reset_pmd(ctx, i);
++		}
++
++		PFM_DBG_ovfl("ovfl=%s pmd%u new=0x%llx old=0x%llx "
++			     "hw_pmd=0x%llx o_pmds=0x%llx must_switch=%u "
++			     "o_thres=%llu o_thres_ref=%llu",
++			     old_val > new_val ? "64-bit" : "HW",
++			     i,
++			     (unsigned long long)new_val,
++			     (unsigned long long)old_val,
++			     (unsigned long long)pfm_read_pmd(ctx, i),
++			     (unsigned long long)ovfl_pmds[0],
++			     must_switch,
++			     (unsigned long long)set->pmds[i].ovflsw_thres,
++			     (unsigned long long)set->pmds[i].ovflsw_ref_thres);
++	}
++
++	/*
++	 * mark the overflows as consumed
++	 */
++	set->npend_ovfls = 0;
++
++	ctx_block = ctx->flags.block;
++
++	t1 = sched_clock();
++	pfm_stats_add(ovfl_intr_p1_ns, t1 - t0);
++	t0 = t1;
++
++	/*
++	 * there was no 64-bit overflow, nothing else to do
++	 */
++	if (!has_64b_ovfl)
++		return;
++
++	/*
++	 * copy pending_ovfls to ovfl_pmd. It is used in
++	 * the notification message or getinfo_evtsets().
++	 *
++	 * pend_ovfls modified to reflect only 64-bit overflows
++	 */
++	bitmap_copy(cast_ulp(ovfl_pmds),
++		    cast_ulp(pend_ovfls),
++		    max_intr);
++
++	/*
++	 * build ovfl_notify bitmask from ovfl_pmds
++	 */
++	bitmap_and(cast_ulp(ovfl_notify),
++		   cast_ulp(pend_ovfls),
++		   cast_ulp(set->ovfl_notify),
++		   max_intr);
++
++	has_notify = !bitmap_empty(cast_ulp(ovfl_notify), max_intr);
++	/*
++	 * must reset for next set of overflows
++	 */
++	bitmap_zero(cast_ulp(pend_ovfls), max_intr);
++
++	/*
++	 * check for format
++	 */
++	if (likely(ctx->smpl_fmt)) {
++		u64 start_cycles, end_cycles;
++		u64 *cnt_pmds;
++		int j, k, ret = 0;
++
++		ovfl_ctrl = 0;
++		num_ovfl = num_ovfl_orig;
++		ovfl_arg = &ctx->ovfl_arg;
++		cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
++
++		ovfl_arg->active_set = set->id;
++
++		for (i = first_intr; num_ovfl && !ret; i++) {
++
++			if (!test_bit(i, cast_ulp(ovfl_pmds)))
++				continue;
++
++			num_ovfl--;
++
++			ovfl_arg->ovfl_pmd = i;
++			ovfl_arg->ovfl_ctrl = 0;
++
++			ovfl_arg->pmd_last_reset = set->pmds[i].lval;
++			ovfl_arg->pmd_eventid = set->pmds[i].eventid;
++
++			/*
++			 * copy values of pmds of interest.
++			 * Sampling format may use them
++			 * We do not initialize the unused smpl_pmds_values
++			 */
++			k = 0;
++			smpl_pmds = set->pmds[i].smpl_pmds;
++			if (!bitmap_empty(cast_ulp(smpl_pmds), max_pmd)) {
++
++				for (j = 0; j < max_pmd; j++) {
++
++					if (!test_bit(j, cast_ulp(smpl_pmds)))
++						continue;
++
++					new_val = pfm_read_pmd(ctx, j);
++
++					/* for counters, build 64-bit value */
++					if (test_bit(j, cast_ulp(cnt_pmds))) {
++						new_val = (set->pmds[j].value & ~ovfl_mask)
++							| (new_val & ovfl_mask);
++					}
++					ovfl_arg->smpl_pmds_values[k++] = new_val;
++
++					PFM_DBG_ovfl("s_pmd_val[%u]="
++						     "pmd%u=0x%llx",
++						     k, j,
++						     (unsigned long long)new_val);
++				}
++			}
++			ovfl_arg->num_smpl_pmds = k;
++
++			pfm_stats_inc(fmt_handler_calls);
++
++			start_cycles = sched_clock();
++
++			/*
++			 * call custom buffer format record (handler) routine
++			 */
++			ret = (*ctx->smpl_fmt->fmt_handler)(hdr,
++							    ovfl_arg,
++							    ip,
++							    now,
++							    regs);
++
++			end_cycles = sched_clock();
++
++			/*
++			 * for PFM_OVFL_CTRL_MASK and PFM_OVFL_CTRL_NOTIFY
++			 * we take the union
++			 *
++			 * The reset_pmds mask is constructed automatically
++			 * on overflow. When the actual reset takes place
++			 * depends on the masking, switch and notification
++			 * status. It may be deferred until pfm_restart().
++			 */
++			ovfl_ctrl |= ovfl_arg->ovfl_ctrl;
++
++			pfm_stats_add(fmt_handler_ns, end_cycles - start_cycles);
++		}
++		/*
++		 * when the format cannot handle the rest of the overflow,
++		 * we abort right here
++		 */
++		if (ret) {
++			PFM_DBG_ovfl("handler aborted at PMD%u ret=%d",
++				     i, ret);
++		}
++	} else {
++		/*
++		 * When no sampling format is used, the default
++		 * is:
++		 * 	- mask monitoring
++		 * 	- notify user if requested
++		 *
++		 * If notification is not requested, monitoring is masked
++		 * and overflowed counters are not reset (saturation).
++		 * This mimics the behavior of the default sampling format.
++		 */
++		ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY;
++
++		if (!must_switch || has_notify)
++			ovfl_ctrl |= PFM_OVFL_CTRL_MASK;
++	}
++	t1 = sched_clock();
++	pfm_stats_add(ovfl_intr_p2_ns, t1 - t0);
++	t0 = t1;
++
++	PFM_DBG_ovfl("set%u o_notify=0x%llx o_pmds=0x%llx "
++		     "r_pmds=0x%llx ovfl_ctrl=0x%x",
++		     set->id,
++		     (unsigned long long)ovfl_notify[0],
++		     (unsigned long long)ovfl_pmds[0],
++		     (unsigned long long)reset_pmds[0],
++		     ovfl_ctrl);
++
++	/*
++	 * we only reset (short reset) when we are not masking. Otherwise
++	 * the reset is postponed until restart.
++	 */
++	if (likely(!(ovfl_ctrl & PFM_OVFL_CTRL_MASK))) {
++		if (must_switch) {
++			/*
++			 * pfm_switch_sets() takes care
++			 * of resetting new set if needed
++			 */
++			pfm_switch_sets_from_intr(ctx);
++
++			/*
++			 * update our view of the active set
++			 */
++			set = ctx->active_set;
++
++			must_switch = 0;
++		} else if (ovfl_ctrl & PFM_OVFL_CTRL_RESET) {
++			u16 nn;
++			t0 = sched_clock();
++			nn = bitmap_weight(cast_ulp(reset_pmds), max_pmd);
++			if (nn)
++				pfm_reset_pmds(ctx, set, nn, PFM_PMD_RESET_SHORT);
++		}
++		/*
++		 * do not block if not masked
++		 */
++		ctx_block = 0;
++	} else {
++		pfm_mask_monitoring(ctx, set);
++	}
++	/*
++	 * if we have not switched here, then remember for the
++	 * time monitoring is restarted
++	 */
++	if (must_switch)
++		set->priv_flags |= PFM_SETFL_PRIV_SWITCH;
++
++	/*
++	 * block only if CTRL_NOTIFY+CTRL_MASK and requested by user
++	 *
++	 * Defer notification until last operation in the handler
++	 * to avoid spinlock contention
++	 */
++	if (has_notify && (ovfl_ctrl & PFM_OVFL_CTRL_NOTIFY)) {
++		int ret;
++		if (ctx_block) {
++			ctx->flags.work_type = PFM_WORK_BLOCK;
++			set_thread_flag(TIF_PERFMON_WORK);
++		}
++		/*
++		 * Sanity check on the queue.
++		 * Should never happen because queue must be sized
++		 * appropriatly for format
++		 */
++		ret = pfm_ovfl_notify_user(ctx, set_orig, ip);
++		if (unlikely(ret)) {
++			if (ctx->state == PFM_CTX_LOADED)
++				pfm_mask_monitoring(ctx, set);
++		} else {
++			ctx->flags.can_restart++;
++			PFM_DBG_ovfl("can_restart=%u", ctx->flags.can_restart);
++		}
++	}
++
++	t1 = sched_clock();
++	pfm_stats_add(ovfl_intr_p3_ns, t1 - t0);
++
++	return;
++
++stop_monitoring:
++	/*
++	 * Does not happen for a system-wide context nor for a
++	 * self-monitored context. We cannot attach to kernel-only
++	 * thread, thus it is safe to set TIF bits, i.e., the thread
++	 * will eventually leave the kernel or die and either we will
++	 * catch the context and clean it up in pfm_handler_work() or
++	 * pfm_exit_thread().
++	 *
++	 * Mask until we get to pfm_handle_work()
++	 */
++	pfm_mask_monitoring(ctx, set);
++
++	PFM_DBG_ovfl("ctx is zombie, converted to spurious");
++	ctx->flags.work_type = PFM_WORK_ZOMBIE;
++	set_thread_flag(TIF_PERFMON_WORK);
++}
++
++/*
++ * interrupts are masked
++ *
++ * Context locking necessary to avoid concurrent accesses from other CPUs
++ * 	- For per-thread, we must prevent pfm_restart() which works when
++ * 	  context is LOADED or MASKED
++ */
++static void __pfm_interrupt_handler(unsigned long iip, struct pt_regs *regs)
++{
++	struct task_struct *task;
++	struct pfm_context *ctx;
++	struct pfm_event_set *set;
++
++	pfm_stats_inc(ovfl_intr_all_count);
++
++	task = __get_cpu_var(pmu_owner);
++	ctx = __get_cpu_var(pmu_ctx);
++
++	if (unlikely(ctx == NULL)) {
++		PFM_DBG_ovfl("no ctx");
++		goto spurious;
++	}
++
++	spin_lock(&ctx->lock);
++
++	set = ctx->active_set;
++
++	/*
++	 * For SMP per-thread, it is not possible to have
++	 * owner != NULL && task != current.
++	 *
++	 * For UP per-thread, because of lazy save, it
++	 * is possible to receive an interrupt in another task
++	 * which is not using the PMU. This means
++	 * that the interrupt was in-flight at the
++	 * time of pfm_ctxswout_thread(). In that
++	 * case it will be replayed when the task
++	 * is scheduled again. Hence we convert to spurious.
++	 *
++	 * The basic rule is that an overflow is always
++	 * processed in the context of the task that
++	 * generated it for all per-thread contexts.
++	 *
++	 * for system-wide, task is always NULL
++	 */
++#ifndef CONFIG_SMP
++	if (unlikely((task && current->pfm_context != ctx))) {
++		PFM_DBG_ovfl("spurious: not owned by current task");
++		goto spurious;
++	}
++#endif
++	if (unlikely(!pfm_arch_is_active(ctx))) {
++		PFM_DBG_ovfl("spurious: monitoring non active");
++		goto spurious;
++	}
++
++	/*
++	 * freeze PMU and collect overflowed PMD registers
++	 * into set->povfl_pmds. Number of overflowed PMDs reported
++	 * in set->npend_ovfls
++	 */
++	pfm_arch_intr_freeze_pmu(ctx, set);
++	if (unlikely(!set->npend_ovfls)) {
++		PFM_DBG_ovfl("no npend_ovfls");
++		goto spurious;
++	}
++
++	pfm_stats_inc(ovfl_intr_regular_count);
++
++	pfm_overflow_handler(ctx, set, iip, regs);
++
++	pfm_arch_intr_unfreeze_pmu(ctx);
++
++	spin_unlock(&ctx->lock);
++
++	return;
++
++spurious:
++	/* ctx may be NULL */
++	pfm_arch_intr_unfreeze_pmu(ctx);
++	if (ctx)
++		spin_unlock(&ctx->lock);
++
++	pfm_stats_inc(ovfl_intr_spurious_count);
++}
++
++void pfm_interrupt_handler(unsigned long iip, struct pt_regs *regs)
++{
++	u64 start;
++
++	BUG_ON(!irqs_disabled());
++
++	start = sched_clock();
++
++	__pfm_interrupt_handler(iip, regs);
++
++	pfm_stats_add(ovfl_intr_ns, sched_clock() - start);
++}
++EXPORT_SYMBOL(pfm_interrupt_handler);
++
+--- /dev/null
++++ b/perfmon/perfmon_pmu.c
+@@ -0,0 +1,599 @@
++/*
++ * perfmon_pmu.c: perfmon2 PMU configuration management
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/perfmon.h>
++
++#ifndef CONFIG_MODULE_UNLOAD
++#define module_refcount(n)	1
++#endif
++
++static __cacheline_aligned_in_smp int request_mod_in_progress;
++static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_pmu_conf_lock);
++
++static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_pmu_acq_lock);
++static u32 pfm_pmu_acquired;
++
++/*
++ * perfmon core must acces PMU information ONLY through pfm_pmu_conf
++ * if pfm_pmu_conf is NULL, then no description is registered
++ */
++struct pfm_pmu_config	*pfm_pmu_conf;
++EXPORT_SYMBOL(pfm_pmu_conf);
++
++static inline int pmu_is_module(struct pfm_pmu_config *c)
++{
++	return !(c->flags & PFM_PMUFL_IS_BUILTIN);
++}
++
++/*
++ * compute the following:
++ * 	- max_pmc, num_pmcs max_pmd, num_pmds
++ * 	- first_intr_pmd, max_rw_pmd
++ * based on existing regdesc->pmds and regdesc->pmcs
++ */
++static void pfm_pmu_regdesc_calc_limits(struct pfm_regdesc *d)
++{
++	u16 n, n2, n_counters, i;
++	int max1, max2, max3, first_intr, first_i;
++
++	n = 0;
++	max1 = max2 = -1;
++	for (i = 0; i < pfm_pmu_conf->num_pmc_entries;  i++) {
++		if (!test_bit(i, cast_ulp(d->pmcs)))
++			continue;
++		max1 = i;
++		n++;
++	}
++	d->max_pmc = max1 + 1;
++	d->num_pmcs = n;
++
++	n = n_counters = n2 = 0;
++	max1 = max2 = max3 = first_intr = first_i = -1;
++	for (i = 0; i < pfm_pmu_conf->num_pmd_entries;  i++) {
++		if (!test_bit(i, cast_ulp(d->pmds)))
++			continue;
++
++		if (first_i == -1)
++			first_i = i;
++
++		max1 = i;
++		n++;
++
++		/*
++		 * read-write registers
++		 */
++		if (!(pfm_pmu_conf->pmd_desc[i].type & PFM_REG_RO)) {
++			max3 = i;
++			n2++;
++		}
++
++		/*
++		 * counters registers
++		 */
++		if (pfm_pmu_conf->pmd_desc[i].type & PFM_REG_C64)
++			n_counters++;
++
++		if (pfm_pmu_conf->pmd_desc[i].type & PFM_REG_INTR) {
++			max2 = i;
++			if (first_intr == -1)
++				first_intr = i;
++		}
++	}
++	d->max_pmd = max1 + 1;
++	d->first_intr_pmd = first_intr == -1 ?  first_i : first_intr;
++
++	d->max_intr_pmd  = max2 + 1;
++
++	d->num_counters = n_counters;
++	d->num_pmds = n;
++	d->max_rw_pmd = max3 + 1;
++	d->num_rw_pmd = n2;
++}
++
++static int pfm_regdesc_init(struct pfm_regdesc *d, struct pfm_pmu_config *cfg)
++{
++	u16 n, n2, n_counters, i;
++	int max1, max2, max3, first_intr, first_i;
++
++	memset(d, 0 , sizeof(*d));
++	/*
++	 * compute the number of implemented PMC from the
++	 * description table
++	 */
++	n = 0;
++	max1 = max2 = -1;
++	for (i = 0; i < cfg->num_pmc_entries;  i++) {
++		if (!(cfg->pmc_desc[i].type & PFM_REG_I))
++			continue;
++
++		__set_bit(i, cast_ulp(d->pmcs));
++
++		max1 = i;
++		n++;
++	}
++
++	if (!n) {
++		PFM_INFO("%s PMU description has no PMC registers",
++			 cfg->pmu_name);
++		return -EINVAL;
++	}
++
++	d->max_pmc = max1 + 1;
++	d->num_pmcs = n;
++
++	n = n_counters = n2 = 0;
++	max1 = max2 = max3 = first_intr = first_i = -1;
++	for (i = 0; i < cfg->num_pmd_entries;  i++) {
++		if (!(cfg->pmd_desc[i].type & PFM_REG_I))
++			continue;
++
++		if (first_i == -1)
++			first_i = i;
++
++		__set_bit(i, cast_ulp(d->pmds));
++		max1 = i;
++		n++;
++
++		/*
++		 * read-write registers
++		 */
++		if (!(cfg->pmd_desc[i].type & PFM_REG_RO)) {
++			__set_bit(i, cast_ulp(d->rw_pmds));
++			max3 = i;
++			n2++;
++		}
++
++		/*
++		 * counters registers
++		 */
++		if (cfg->pmd_desc[i].type & PFM_REG_C64) {
++			__set_bit(i, cast_ulp(d->cnt_pmds));
++			n_counters++;
++		}
++
++		/*
++		 * PMD with intr capabilities
++		 */
++		if (cfg->pmd_desc[i].type & PFM_REG_INTR) {
++			__set_bit(i, cast_ulp(d->intr_pmds));
++			max2 = i;
++			if (first_intr == -1)
++				first_intr = i;
++		}
++	}
++
++	if (!n) {
++		PFM_INFO("%s PMU description has no PMD registers",
++			 cfg->pmu_name);
++		return -EINVAL;
++	}
++
++	d->max_pmd = max1 + 1;
++	d->first_intr_pmd = first_intr == -1 ?  first_i : first_intr;
++
++	d->max_intr_pmd  = max2 + 1;
++
++	d->num_counters = n_counters;
++	d->num_pmds = n;
++	d->max_rw_pmd = max3 + 1;
++	d->num_rw_pmd = n2;
++
++	return 0;
++}
++
++/*
++ * initialize PMU configuration from PMU config descriptor
++ */
++static int pfm_pmu_config_init(struct pfm_pmu_config *cfg)
++{
++	int ret;
++
++	/*
++	 * we build the register description using the full mapping
++	 * table as defined by the module. On first use, we update
++	 * the current description (regs) based on local constraints.
++	 */
++	ret = pfm_regdesc_init(&cfg->full_regs, cfg);
++	if (ret)
++		return ret;
++
++	if (!cfg->version)
++		cfg->version = "0.0";
++
++	pfm_pmu_conf = cfg;
++	pfm_pmu_conf->ovfl_mask = (1ULL << cfg->counter_width) -1;
++
++	PFM_INFO("%s PMU detected, %u PMCs, %u PMDs, %u counters (%u bits)",
++		 pfm_pmu_conf->pmu_name,
++		 pfm_pmu_conf->full_regs.num_pmcs,
++		 pfm_pmu_conf->full_regs.num_pmds,
++		 pfm_pmu_conf->full_regs.num_counters,
++		 pfm_pmu_conf->counter_width);
++
++	return 0;
++}
++
++int pfm_pmu_register(struct pfm_pmu_config *cfg)
++{
++	u16 i, nspec, nspec_ro, num_pmcs, num_pmds, num_wc = 0;
++	int type, ret = -EBUSY;
++
++	if (perfmon_disabled) {
++		PFM_INFO("perfmon disabled, cannot add PMU description");
++		return -ENOSYS;
++	}
++
++	nspec = nspec_ro = num_pmds = num_pmcs = 0;
++
++	/* some sanity checks */
++	if (cfg == NULL || cfg->pmu_name == NULL) {
++		PFM_INFO("PMU config descriptor is invalid");
++		return -EINVAL;
++	}
++
++	/* must have a probe */
++	if (cfg->probe_pmu == NULL) {
++		PFM_INFO("PMU config has no probe routine");
++		return -EINVAL;
++	}
++
++	/*
++	 * execute probe routine before anything else as it
++	 * may update configuration tables
++	 */
++	if ((*cfg->probe_pmu)() == -1) {
++		PFM_INFO("%s PMU detection failed", cfg->pmu_name);
++		return -EINVAL;
++	}
++
++	if (!(cfg->flags & PFM_PMUFL_IS_BUILTIN) && cfg->owner == NULL) {
++		PFM_INFO("PMU config %s is missing owner", cfg->pmu_name);
++		return -EINVAL;
++	}
++
++	if (!cfg->num_pmd_entries) {
++		PFM_INFO("%s needs to define num_pmd_entries", cfg->pmu_name);
++		return -EINVAL;
++	}
++
++	if (!cfg->num_pmc_entries) {
++		PFM_INFO("%s needs to define num_pmc_entries", cfg->pmu_name);
++		return -EINVAL;
++	}
++
++	if (!cfg->counter_width) {
++		PFM_INFO("PMU config %s, zero width counters", cfg->pmu_name);
++		return -EINVAL;
++	}
++
++	/*
++	 * REG_RO, REG_V not supported on PMC registers
++	 */
++	for (i = 0; i < cfg->num_pmc_entries;  i++) {
++
++		type = cfg->pmc_desc[i].type;
++
++		if (type & PFM_REG_I)
++			num_pmcs++;
++
++		if (type & PFM_REG_WC)
++			num_wc++;
++
++		if (type & PFM_REG_V) {
++			PFM_INFO("PFM_REG_V is not supported on "
++				 "PMCs (PMC%d)", i);
++			return -EINVAL;
++		}
++		if (type & PFM_REG_RO) {
++			PFM_INFO("PFM_REG_RO meaningless on "
++				 "PMCs (PMC%u)", i);
++			return -EINVAL;
++		}
++	}
++
++	if (num_wc && cfg->pmc_write_check == NULL) {
++		PFM_INFO("some PMCs have write-checker but no callback provided\n");
++		return -EINVAL;
++	}
++
++	/*
++	 * check virtual PMD registers
++	 */
++	num_wc= 0;
++	for (i = 0; i < cfg->num_pmd_entries;  i++) {
++
++		type = cfg->pmd_desc[i].type;
++
++		if (type & PFM_REG_I)
++			num_pmds++;
++
++		if (type & PFM_REG_V) {
++			nspec++;
++			if (type & PFM_REG_RO)
++				nspec_ro++;
++		}
++
++		if (type & PFM_REG_WC)
++			num_wc++;
++	}
++
++	if (num_wc && cfg->pmd_write_check == NULL) {
++		PFM_INFO("PMD have write-checker but no callback provided\n");
++		return -EINVAL;
++	}
++
++	if (nspec && cfg->pmd_sread == NULL) {
++		PFM_INFO("PMU config is missing pmd_sread()");
++		return -EINVAL;
++	}
++
++	nspec = nspec - nspec_ro;
++	if (nspec && cfg->pmd_swrite == NULL) {
++		PFM_INFO("PMU config is missing pmd_swrite()");
++		return -EINVAL;
++	}
++
++	if (num_pmcs >= PFM_MAX_PMCS) {
++		PFM_INFO("%s PMCS registers exceed name space [0-%u]",
++			 cfg->pmu_name,
++			 PFM_MAX_PMCS);
++		return -EINVAL;
++	}
++	if (num_pmds >= PFM_MAX_PMDS) {
++		PFM_INFO("%s PMDS registers exceed name space [0-%u]",
++			 cfg->pmu_name,
++			 PFM_MAX_PMDS);
++		return -EINVAL;
++	}
++	spin_lock(&pfm_pmu_conf_lock);
++
++	if (pfm_pmu_conf)
++		goto unlock;
++
++	ret = pfm_pmu_config_init(cfg);
++	if (ret)
++		goto unlock;
++
++	ret = pfm_arch_pmu_config_init(pfm_pmu_conf);
++	if (ret)
++		goto unlock;
++
++	ret = pfm_sysfs_add_pmu(pfm_pmu_conf);
++	if (ret) {
++		pfm_arch_pmu_config_remove();
++		pfm_pmu_conf = NULL;
++	}
++
++unlock:
++	spin_unlock(&pfm_pmu_conf_lock);
++
++	if (ret) {
++		PFM_INFO("register %s PMU error %d", cfg->pmu_name, ret);
++	} else {
++		PFM_INFO("%s PMU installed", cfg->pmu_name);
++		/*
++		 * (re)initialize PMU on each PMU now that we have a description
++		 */
++		on_each_cpu(__pfm_init_percpu, cfg, 0, 0);
++	}
++	return ret;
++}
++EXPORT_SYMBOL(pfm_pmu_register);
++
++/*
++ * remove PMU description. Caller must pass address of current
++ * configuration. This is mostly for sanity checking as only
++ * one config can exist at any time.
++ *
++ * We are using the module refcount mechanism to protect against
++ * removal while the configuration is being used. As long as there is
++ * one context, a PMU configuration cannot be removed. The protection is
++ * managed in module logic.
++ */
++void pfm_pmu_unregister(struct pfm_pmu_config *cfg)
++{
++	if (!(cfg ||pfm_pmu_conf))
++		return;
++
++	spin_lock(&pfm_pmu_conf_lock);
++
++	BUG_ON(module_refcount(pfm_pmu_conf->owner));
++
++	if (cfg->owner == pfm_pmu_conf->owner) {
++		pfm_arch_pmu_config_remove();
++		pfm_sysfs_remove_pmu(pfm_pmu_conf);
++		pfm_pmu_conf = NULL;
++	}
++
++	spin_unlock(&pfm_pmu_conf_lock);
++}
++EXPORT_SYMBOL(pfm_pmu_unregister);
++
++static int pfm_pmu_request_module(void)
++{
++	char *mod_name;
++	int ret;
++
++	mod_name = pfm_arch_get_pmu_module_name();
++	if (mod_name == NULL)
++		return -ENOSYS;
++
++	ret = request_module(mod_name);
++
++	PFM_DBG("mod=%s ret=%d\n", mod_name, ret);
++	return ret;
++}
++
++/*
++ * autoload:
++ * 	0     : do not try to autoload the PMU description module
++ * 	not 0 : try to autoload the PMU description module
++ */
++int pfm_pmu_conf_get(int autoload)
++{
++	int ret;
++
++	spin_lock(&pfm_pmu_conf_lock);
++
++	if (request_mod_in_progress) {
++		ret = -ENOSYS;
++		goto skip;
++	}
++
++	if (autoload && pfm_pmu_conf == NULL) {
++
++		request_mod_in_progress = 1;
++
++		spin_unlock(&pfm_pmu_conf_lock);
++
++		pfm_pmu_request_module();
++
++		spin_lock(&pfm_pmu_conf_lock);
++
++		request_mod_in_progress = 0;
++
++		/*
++		 * request_module() may succeed but the module
++		 * may not have registered properly so we need
++		 * to check
++		 */
++	}
++
++	ret = pfm_pmu_conf == NULL ? -ENOSYS : 0;
++	if (!ret && pmu_is_module(pfm_pmu_conf)
++	    && !try_module_get(pfm_pmu_conf->owner))
++		ret = -ENOSYS;
++skip:
++	spin_unlock(&pfm_pmu_conf_lock);
++
++	return ret;
++}
++
++void pfm_pmu_conf_put(void)
++{
++	if (pfm_pmu_conf == NULL || !pmu_is_module(pfm_pmu_conf))
++		return;
++
++	spin_lock(&pfm_pmu_conf_lock);
++	module_put(pfm_pmu_conf->owner);
++	spin_unlock(&pfm_pmu_conf_lock);
++}
++
++
++/*
++ * acquire PMU resource from lower-level PMU register allocator
++ * (currently perfctr-watchdog.c)
++ *
++ * acquisition is done when the first context is created (and not
++ * when it is loaded). We grab all that is defined in the description
++ * module and then we make adjustments at the arch-specific level.
++ *
++ * The PMU resource is released when the last perfmon context is
++ * destroyed.
++ *
++ * interrupts are not masked
++ */
++int pfm_pmu_acquire(void)
++{
++	int ret = 0;
++
++	spin_lock(&pfm_pmu_acq_lock);
++
++	PFM_DBG("pmu_acquired=%d", pfm_pmu_acquired);
++
++	pfm_pmu_acquired++;
++
++	if (pfm_pmu_acquired == 1) {
++		/*
++		 * copy full description and then check if arch-specific
++		 * layer needs some adjustments
++		 */
++		pfm_pmu_conf->regs = pfm_pmu_conf->full_regs;
++
++		ret = pfm_arch_pmu_acquire();
++		if (ret) {
++			pfm_pmu_acquired--;
++		} else {
++			/*
++			 * calculate new limits (num, max)
++			 */
++			pfm_pmu_regdesc_calc_limits(&pfm_pmu_conf->regs);
++
++			/* available PMU ressources */
++			PFM_DBG("PMU acquired: %u PMCs, %u PMDs, %u counters",
++				pfm_pmu_conf->regs.num_pmcs,
++				pfm_pmu_conf->regs.num_pmds,
++				pfm_pmu_conf->regs.num_counters);
++		}
++	}
++	spin_unlock(&pfm_pmu_acq_lock);
++
++	return ret;
++}
++
++/*
++ * release the PMU resource
++ *
++ * actual release happens when last context is destroyed
++ *
++ * interrupts are not masked
++ */
++void pfm_pmu_release(void)
++{
++	BUG_ON(irqs_disabled());
++
++	/*
++	 * we need to use a spinlock because release takes some time
++	 * and we may have a race with pfm_pmu_acquire()
++	 */
++	spin_lock(&pfm_pmu_acq_lock);
++
++	PFM_DBG("pmu_acquired=%d", pfm_pmu_acquired);
++
++	/*
++	 * we decouple test and decrement because if we had errors
++	 * in pfm_pmu_acquire(), we still come here on pfm_context_free()
++	 * but with pfm_pmu_acquire=0
++	 */
++	if (pfm_pmu_acquired > 0 && --pfm_pmu_acquired == 0) {
++		pfm_arch_pmu_release();
++		memset(&pfm_pmu_conf->regs, 0, sizeof(pfm_pmu_conf->regs));
++		PFM_DBG("PMU released");
++	}
++	spin_unlock(&pfm_pmu_acq_lock);
++}
+--- /dev/null
++++ b/perfmon/perfmon_res.c
+@@ -0,0 +1,419 @@
++/*
++ * perfmon_res.c:  perfmon2 resource allocations
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/spinlock.h>
++#include <linux/perfmon.h>
++#include <linux/module.h>
++
++/*
++ * global information about all sessions
++ * mostly used to synchronize between system wide and per-process
++ */
++struct pfm_sessions {
++	u32		pfs_task_sessions;/* #num loaded per-thread sessions */
++	size_t		pfs_smpl_buffer_mem_cur; /* current smpl buf mem usage */
++	cpumask_t	pfs_sys_cpumask;  /* bitmask of used cpus (system-wide) */
++};
++
++static struct pfm_sessions pfm_sessions;
++
++static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_sessions_lock);
++
++/*
++ * sampling buffer allocated by perfmon must be
++ * checked against max usage thresholds for security
++ * reasons.
++ *
++ * The first level check is against the system wide limit
++ * as indicated by the system administrator in /proc/sys/kernel/perfmon
++ *
++ * The second level check is on a per-process basis using
++ * RLIMIT_MEMLOCK limit.
++ *
++ * Operating on the current task only.
++ */
++int pfm_reserve_buf_space(size_t size)
++{
++	struct mm_struct *mm;
++	unsigned long locked;
++	unsigned long buf_mem, buf_mem_max;
++	unsigned long flags;
++
++	spin_lock_irqsave(&pfm_sessions_lock, flags);
++
++	/*
++	 * check against global buffer limit
++	 */
++	buf_mem_max = pfm_controls.smpl_buffer_mem_max;
++	buf_mem = pfm_sessions.pfs_smpl_buffer_mem_cur + size;
++
++	if (buf_mem <= buf_mem_max) {
++		pfm_sessions.pfs_smpl_buffer_mem_cur = buf_mem;
++
++		PFM_DBG("buf_mem_max=%lu current_buf_mem=%lu",
++			buf_mem_max,
++			buf_mem);
++	}
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++
++	if (buf_mem > buf_mem_max) {
++		PFM_DBG("smpl buffer memory threshold reached");
++		return -ENOMEM;
++	}
++
++	/*
++	 * check against RLIMIT_MEMLOCK
++	 */
++	mm = get_task_mm(current);
++
++	down_write(&mm->mmap_sem);
++
++	locked  = mm->locked_vm << PAGE_SHIFT;
++	locked += size;
++
++	if (locked > current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur) {
++
++		PFM_DBG("RLIMIT_MEMLOCK reached ask_locked=%lu rlim_cur=%lu",
++			locked,
++			current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
++
++		up_write(&mm->mmap_sem);
++		mmput(mm);
++		goto unres;
++	}
++
++	mm->locked_vm = locked >> PAGE_SHIFT;
++
++	up_write(&mm->mmap_sem);
++
++	mmput(mm);
++
++	return 0;
++
++unres:
++	/*
++	 * remove global buffer memory allocation
++	 */
++	spin_lock_irqsave(&pfm_sessions_lock, flags);
++
++	pfm_sessions.pfs_smpl_buffer_mem_cur -= size;
++
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++
++	return -ENOMEM;
++}
++/*
++ *There exist multiple paths leading to this function. We need to
++ * be very careful withlokcing on the mmap_sem as it may already be
++ * held by the time we come here.
++ * The following paths exist:
++ *
++ * exit path:
++ * sys_exit_group
++ *    do_group_exit
++ *     do_exit
++ *      exit_mm
++ *       mmput
++ *        exit_mmap
++ *         remove_vma
++ *          fput
++ *           __fput
++ *            pfm_close
++ *             __pfm_close
++ *              pfm_context_free
++ * 	         pfm_release_buf_space
++ * munmap path:
++ * sys_munmap
++ *  do_munmap
++ *   remove_vma
++ *    fput
++ *     __fput
++ *      pfm_close
++ *       __pfm_close
++ *        pfm_context_free
++ *         pfm_release_buf_space
++ *
++ * close path:
++ * sys_close
++ *  filp_close
++ *   fput
++ *    __fput
++ *     pfm_close
++ *      __pfm_close
++ *       pfm_context_free
++ *        pfm_release_buf_space
++ *
++ * The issue is that on the munmap() path, the mmap_sem is already held
++ * in write-mode by the time we come here. To avoid the deadlock, we need
++ * to know where we are coming from and skip down_write(). If is fairly
++ * difficult to know this because of the lack of good hooks and
++ * the fact that, there may not have been any mmap() of the sampling buffer
++ * (i.e. create_context() followed by close() or exit()).
++ *
++ * We use a set flag ctx->flags.mmap_nlock which is toggle in the vm_ops
++ * callback in remove_vma() which is called systematically for the call, so
++ * on all but the pure close() path. The exit path does not already hold
++ * the lock but this is exit so there is no task->mm by the time we come here.
++ *
++ * The mmap_nlock is set only when unmapping and this is the LAST reference
++ * to the file (i.e., close() followed by munmap()).
++ */
++void pfm_release_buf_space(struct pfm_context *ctx, size_t size)
++{
++	unsigned long flags;
++	struct mm_struct *mm;
++
++	mm = get_task_mm(current);
++	if (mm) {
++		if (ctx->flags.mmap_nlock == 0) {
++			PFM_DBG("doing down_write");
++			down_write(&mm->mmap_sem);
++		}
++
++		mm->locked_vm -= size >> PAGE_SHIFT;
++
++		PFM_DBG("locked_vm=%lu size=%zu", mm->locked_vm, size);
++
++		if (ctx->flags.mmap_nlock == 0)
++			up_write(&mm->mmap_sem);
++
++		mmput(mm);
++	}
++
++	spin_lock_irqsave(&pfm_sessions_lock, flags);
++
++	pfm_sessions.pfs_smpl_buffer_mem_cur -= size;
++
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++}
++
++int pfm_reserve_session(int is_system, u32 cpu)
++{
++	unsigned long flags;
++	u32 nsys_cpus;
++	int ret = 0;
++
++	/*
++	 * validy checks on cpu_mask have been done upstream
++	 */
++	spin_lock_irqsave(&pfm_sessions_lock, flags);
++
++	nsys_cpus = cpus_weight(pfm_sessions.pfs_sys_cpumask);
++
++	PFM_DBG("in  sys=%u task=%u is_sys=%d cpu=%u",
++		nsys_cpus,
++		pfm_sessions.pfs_task_sessions,
++		is_system,
++		cpu);
++
++	if (is_system) {
++		/*
++		 * cannot mix system wide and per-task sessions
++		 */
++		if (pfm_sessions.pfs_task_sessions > 0) {
++			PFM_DBG("%u conflicting task_sessions",
++				pfm_sessions.pfs_task_sessions);
++			ret = -EBUSY;
++			goto abort;
++		}
++
++		if (cpu_isset(cpu, pfm_sessions.pfs_sys_cpumask)) {
++			PFM_DBG("conflicting session on CPU%u", cpu);
++			ret = -EBUSY;
++			goto abort;
++		}
++
++		PFM_DBG("reserved session on CPU%u", cpu);
++
++		cpu_set(cpu, pfm_sessions.pfs_sys_cpumask);
++		nsys_cpus++;
++	} else {
++		if (nsys_cpus) {
++			ret = -EBUSY;
++			goto abort;
++		}
++		pfm_sessions.pfs_task_sessions++;
++	}
++
++	PFM_DBG("out sys=%u task=%u is_sys=%d cpu=%u",
++		nsys_cpus,
++		pfm_sessions.pfs_task_sessions,
++		is_system,
++		cpu);
++
++abort:
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++
++	return ret;
++}
++
++/*
++ * called from __pfm_unload_context()
++ */
++int pfm_release_session(int is_system, u32 cpu)
++{
++	unsigned long flags;
++
++	spin_lock_irqsave(&pfm_sessions_lock, flags);
++
++	PFM_DBG("in sys_sessions=%u task_sessions=%u syswide=%d cpu=%u",
++		cpus_weight(pfm_sessions.pfs_sys_cpumask),
++		pfm_sessions.pfs_task_sessions,
++		is_system, cpu);
++
++	if (is_system)
++		cpu_clear(cpu, pfm_sessions.pfs_sys_cpumask);
++	else
++		pfm_sessions.pfs_task_sessions--;
++
++	PFM_DBG("out sys_sessions=%u task_sessions=%u syswide=%d cpu=%u",
++		cpus_weight(pfm_sessions.pfs_sys_cpumask),
++		pfm_sessions.pfs_task_sessions,
++		is_system, cpu);
++
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++	return 0;
++}
++
++int pfm_reserve_allcpus(void)
++{
++	unsigned long flags;
++	u32 nsys_cpus, cpu;
++
++	spin_lock_irqsave(&pfm_sessions_lock, flags);
++
++	nsys_cpus = cpus_weight(pfm_sessions.pfs_sys_cpumask);
++
++	PFM_DBG("in  sys=%u task=%u",
++		nsys_cpus,
++		pfm_sessions.pfs_task_sessions);
++
++	if (nsys_cpus) {
++		PFM_DBG("already some system-wide sessions");
++		goto abort;
++	}
++
++	/*
++	 * cannot mix system wide and per-task sessions
++	 */
++	if (pfm_sessions.pfs_task_sessions) {
++		PFM_DBG("%u conflicting task_sessions",
++		  	pfm_sessions.pfs_task_sessions);
++		goto abort;
++	}
++
++	for_each_online_cpu(cpu) {
++		cpu_set(cpu, pfm_sessions.pfs_sys_cpumask);
++		nsys_cpus++;
++	}
++
++	PFM_DBG("out sys=%u task=%u",
++		nsys_cpus,
++		pfm_sessions.pfs_task_sessions);
++
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++
++	return 0;
++
++abort:
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++
++	return -EBUSY;
++}
++EXPORT_SYMBOL(pfm_reserve_allcpus);
++
++int pfm_release_allcpus(void)
++{
++	unsigned long flags;
++	u32 nsys_cpus, cpu;
++
++	spin_lock_irqsave(&pfm_sessions_lock, flags);
++
++	nsys_cpus = cpus_weight(pfm_sessions.pfs_sys_cpumask);
++
++	PFM_DBG("in  sys=%u task=%u",
++		nsys_cpus,
++		pfm_sessions.pfs_task_sessions);
++
++	/*
++	 * XXX: could use __cpus_clear() with nbits
++	 */
++	for_each_online_cpu(cpu) {
++		cpu_clear(cpu, pfm_sessions.pfs_sys_cpumask);
++		nsys_cpus--;
++	}
++
++	PFM_DBG("out sys=%u task=%u",
++		nsys_cpus,
++		pfm_sessions.pfs_task_sessions);
++
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++
++	return 0;
++}
++EXPORT_SYMBOL(pfm_release_allcpus);
++
++/*
++ * called from perfmon_sysfs.c:
++ *  what=0 : pfs_task_sessions
++ *  what=1 : cpus_weight(pfs_sys_cpumask)
++ *  what=2 : smpl_buffer_mem_cur
++ *  what=3 : pmu model name
++ *
++ * return number of bytes written into buf (up to sz)
++ */
++ssize_t pfm_sysfs_session_show(char *buf, size_t sz, int what)
++{
++	unsigned long flags;
++
++	spin_lock_irqsave(&pfm_sessions_lock, flags);
++
++	switch (what) {
++	case 0: snprintf(buf, sz, "%u\n", pfm_sessions.pfs_task_sessions);
++		break;
++	case 1: snprintf(buf, sz, "%d\n", cpus_weight(pfm_sessions.pfs_sys_cpumask));
++		break;
++	case 2: snprintf(buf, sz, "%zu\n", pfm_sessions.pfs_smpl_buffer_mem_cur);
++		break;
++	case 3:
++		snprintf(buf, sz, "%s\n",
++			pfm_pmu_conf ?	pfm_pmu_conf->pmu_name
++				     :	"unknown\n");
++	}
++	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
++	return strlen(buf);
++}
+--- /dev/null
++++ b/perfmon/perfmon_rw.c
+@@ -0,0 +1,591 @@
++/*
++ * perfmon.c: perfmon2 PMC/PMD read/write system calls
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net/
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/module.h>
++#include <linux/kernel.h>
++#include <linux/perfmon.h>
++
++#define PFM_REGFL_PMC_ALL	(PFM_REGFL_NO_EMUL64)
++#define PFM_REGFL_PMD_ALL	(PFM_REGFL_RANDOM|PFM_REGFL_OVFL_NOTIFY)
++
++/*
++ * function called from sys_pfm_write_pmds() to write the
++ * requested PMD registers. The function succeeds whether the context is
++ * attached or not. When attached to another thread, that thread must be
++ * stopped.
++ *
++ * compat: is used only on IA-64 to maintain backward compatibility with v2.0
++ *
++ * The context is locked and interrupts are disabled.
++ */
++int __pfm_write_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count,
++		     int compat)
++{
++	struct pfm_event_set *set, *active_set;
++	u64 value, ovfl_mask;
++	u64 *smpl_pmds, *reset_pmds, *impl_pmds, *impl_rw_pmds;
++	u32 req_flags, flags;
++	u16 cnum, pmd_type, max_pmd, max_pmc;
++	u16 set_id, prev_set_id;
++	int i, can_access_pmu;
++	int is_counter;
++	int ret;
++	pfm_pmd_check_t	wr_func;
++
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	active_set = ctx->active_set;
++	max_pmd	= pfm_pmu_conf->regs.max_pmd;
++	max_pmc	= pfm_pmu_conf->regs.max_pmc;
++	impl_pmds = pfm_pmu_conf->regs.pmds;
++	impl_rw_pmds = pfm_pmu_conf->regs.rw_pmds;
++	wr_func = pfm_pmu_conf->pmd_write_check;
++	set = NULL;
++
++	prev_set_id = 0;
++	can_access_pmu = 0;
++
++	/*
++	 * we cannot access the actual PMD registers when monitoring is masked
++	 */
++	if (unlikely(ctx->state == PFM_CTX_LOADED))
++		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
++			       || ctx->flags.system;
++
++	ret = -EINVAL;
++
++	for (i = 0; i < count; i++, req++) {
++
++		cnum = req->reg_num;
++		set_id = req->reg_set;
++		req_flags = req->reg_flags;
++		smpl_pmds = req->reg_smpl_pmds;
++		reset_pmds = req->reg_reset_pmds;
++		flags = 0;
++
++		/*
++		 * cannot write to unexisting
++		 * writes to read-only register are ignored
++		 */
++		if (unlikely(cnum >= max_pmd
++		    || !test_bit(cnum, cast_ulp(impl_pmds)))) {
++			PFM_DBG("pmd%u is not available", cnum);
++			goto error;
++		}
++
++		pmd_type = pfm_pmu_conf->pmd_desc[cnum].type;
++		is_counter = pmd_type & PFM_REG_C64;
++
++		if (likely(!compat)) {
++			/*
++			 * ensure only valid flags are set
++			 */
++			if (req_flags & ~(PFM_REGFL_PMD_ALL)) {
++				PFM_DBG("pmd%u: invalid flags=0x%x",
++					cnum, req_flags);
++				goto error;
++			}
++			/*
++			 * OVFL_NOTIFY is valid for all types of PMD.
++			 * non counting PMD may trigger PMU interrupt
++			 * and thus may trigger recording of a sample.
++			 * This is the case with AMD family 16 and the
++			 * IBS feature, for instance.
++			 */
++			if (req_flags & PFM_REGFL_OVFL_NOTIFY)
++				flags |= PFM_REGFL_OVFL_NOTIFY;
++
++			/*
++			 * We allow randomization to non counting
++			 * PMD as well.
++			 */
++			if (req_flags & PFM_REGFL_RANDOM)
++				flags |= PFM_REGFL_RANDOM;
++
++			/*
++			 * verify validity of smpl_pmds
++			 */
++			if (unlikely(!bitmap_subset(cast_ulp(smpl_pmds),
++						    cast_ulp(impl_pmds),
++						    max_pmd))) {
++				PFM_DBG("invalid smpl_pmds=0x%llx "
++					"for pmd%u",
++					(unsigned long long)smpl_pmds[0],
++					cnum);
++				goto error;
++			}
++
++			/*
++			 * verify validity of reset_pmds
++			 * check against impl_rw_pmds because it is not
++			 * possible to reset read-only PMDs
++			 */
++			if (unlikely(!bitmap_subset(cast_ulp(reset_pmds),
++						    cast_ulp(impl_rw_pmds),
++						    max_pmd))) {
++				PFM_DBG("invalid reset_pmds=0x%llx "
++					"for pmd%u",
++					(unsigned long long)reset_pmds[0],
++					cnum);
++				goto error;
++			}
++		}
++
++		/*
++		 * locate event set
++		 */
++		if (i == 0 || set_id != prev_set_id) {
++			set = pfm_find_set(ctx, set_id, 0);
++			if (set == NULL) {
++				PFM_DBG("event set%u does not exist",
++					set_id);
++				goto error;
++			}
++		}
++
++		/*
++		 * execute write checker, if any
++		 */
++		if (unlikely(wr_func && (pmd_type & PFM_REG_WC))) {
++			ret = (*wr_func)(ctx, set, req);
++			if (ret)
++				goto error;
++
++		}
++
++		value = req->reg_value;
++
++		/*
++		 * now commit changes to software state
++		 */
++
++		if (likely(!compat)) {
++			set->pmds[cnum].flags = flags;
++
++			/*
++			 * copy reset and sampling bitvectors
++			 */
++			bitmap_copy(cast_ulp(set->pmds[cnum].reset_pmds),
++					cast_ulp(reset_pmds),
++					max_pmd);
++
++			bitmap_copy(cast_ulp(set->pmds[cnum].smpl_pmds),
++					cast_ulp(smpl_pmds),
++					max_pmd);
++
++			set->pmds[cnum].eventid = req->reg_smpl_eventid;
++
++			/*
++			 * Mark reset/smpl PMDS as used.
++			 *
++			 * We do not keep track of PMC because we have to
++			 * systematically restore ALL of them.
++			 */
++			bitmap_or(cast_ulp(set->used_pmds),
++					cast_ulp(set->used_pmds),
++					cast_ulp(reset_pmds), max_pmd);
++
++			bitmap_or(cast_ulp(set->used_pmds),
++					cast_ulp(set->used_pmds),
++					cast_ulp(smpl_pmds), max_pmd);
++
++			/*
++			 * we reprogram the PMD hence, we clear any pending
++			 * ovfl. Does affect ovfl switch on restart but new
++			 * value has already been established here
++			 */
++			if (test_bit(cnum, cast_ulp(set->povfl_pmds))) {
++				set->npend_ovfls--;
++				__clear_bit(cnum, cast_ulp(set->povfl_pmds));
++			}
++			__clear_bit(cnum, cast_ulp(set->ovfl_pmds));
++
++			/*
++			 * update ovfl_notify
++			 */
++			if (flags & PFM_REGFL_OVFL_NOTIFY)
++				__set_bit(cnum, cast_ulp(set->ovfl_notify));
++			else
++				__clear_bit(cnum, cast_ulp(set->ovfl_notify));
++		}
++		/*
++		 * establish new switch count
++		 */
++		set->pmds[cnum].ovflsw_thres = req->reg_ovfl_switch_cnt;
++		set->pmds[cnum].ovflsw_ref_thres = req->reg_ovfl_switch_cnt;
++
++		/*
++		 * set last value to new value for all types of PMD
++		 */
++		set->pmds[cnum].lval = value;
++
++		/*
++		 * update reset values (not just for counters)
++		 */
++		set->pmds[cnum].long_reset = req->reg_long_reset;
++		set->pmds[cnum].short_reset = req->reg_short_reset;
++
++		/*
++		 * update randomization mask
++		 */
++		set->pmds[cnum].mask = req->reg_random_mask;
++
++		/*
++		 * update set values
++		 */
++		set->pmds[cnum].value = value;
++
++		__set_bit(cnum, cast_ulp(set->used_pmds));
++
++		if (set == active_set) {
++			set->priv_flags |= PFM_SETFL_PRIV_MOD_PMDS;
++			if (can_access_pmu)
++				pfm_write_pmd(ctx, cnum, value);
++		}
++
++		/*
++		 * update number of used PMD registers
++		 */
++		set->nused_pmds = bitmap_weight(cast_ulp(set->used_pmds), max_pmd);
++
++		prev_set_id = set_id;
++
++		PFM_DBG("set%u pmd%u=0x%llx flags=0x%x a_pmu=%d "
++			"ctx_pmd=0x%llx s_reset=0x%llx "
++			"l_reset=0x%llx u_pmds=0x%llx nu_pmds=%u "
++			"s_pmds=0x%llx r_pmds=0x%llx o_pmds=0x%llx "
++			"o_thres=%llu compat=%d eventid=%llx",
++			set->id,
++			cnum,
++			(unsigned long long)value,
++			set->pmds[cnum].flags,
++			can_access_pmu,
++			(unsigned long long)set->pmds[cnum].value,
++			(unsigned long long)set->pmds[cnum].short_reset,
++			(unsigned long long)set->pmds[cnum].long_reset,
++			(unsigned long long)set->used_pmds[0],
++			set->nused_pmds,
++			(unsigned long long)set->pmds[cnum].smpl_pmds[0],
++			(unsigned long long)set->pmds[cnum].reset_pmds[0],
++			(unsigned long long)set->ovfl_pmds[0],
++			(unsigned long long)set->pmds[cnum].ovflsw_thres,
++			compat,
++			(unsigned long long)set->pmds[cnum].eventid);
++	}
++	ret = 0;
++
++error:
++	/*
++	 * make changes visible
++	 */
++	if (can_access_pmu)
++		pfm_arch_serialize();
++
++	return ret;
++}
++
++/*
++ * function called from sys_pfm_write_pmcs() to write the
++ * requested PMC registers. The function succeeds whether the context is
++ * attached or not. When attached to another thread, that thread must be
++ * stopped.
++ *
++ * The context is locked and interrupts are disabled.
++ */
++int __pfm_write_pmcs(struct pfm_context *ctx, struct pfarg_pmc *req, int count)
++{
++	struct pfm_event_set *set, *active_set;
++	u64 value, dfl_val, rsvd_msk;
++	u64 *impl_pmcs;
++	int i, can_access_pmu;
++	int ret;
++	u16 set_id, prev_set_id;
++	u16 cnum, pmc_type, max_pmc;
++	u32 flags;
++	pfm_pmc_check_t	wr_func;
++
++	active_set = ctx->active_set;
++
++	wr_func = pfm_pmu_conf->pmc_write_check;
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++	impl_pmcs = pfm_pmu_conf->regs.pmcs;
++
++	set = NULL;
++	prev_set_id = 0;
++	can_access_pmu = 0;
++
++	/*
++	 * we cannot access the actual PMC registers when monitoring is masked
++	 */
++	if (unlikely(ctx->state == PFM_CTX_LOADED))
++		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
++			        || ctx->flags.system;
++
++	ret = -EINVAL;
++
++	for (i = 0; i < count; i++, req++) {
++
++		cnum = req->reg_num;
++		set_id = req->reg_set;
++		value = req->reg_value;
++		flags = req->reg_flags;
++
++		/*
++		 * no access to unavailable PMC register
++		 */
++		if (unlikely(cnum >= max_pmc
++			     || !test_bit(cnum, cast_ulp(impl_pmcs)))) {
++			PFM_DBG("pmc%u is not available", cnum);
++			goto error;
++		}
++
++		pmc_type = pfm_pmu_conf->pmc_desc[cnum].type;
++		dfl_val = pfm_pmu_conf->pmc_desc[cnum].dfl_val;
++		rsvd_msk = pfm_pmu_conf->pmc_desc[cnum].rsvd_msk;
++
++		/*
++		 * ensure only valid flags are set
++		 */
++		if (flags & ~PFM_REGFL_PMC_ALL) {
++			PFM_DBG("pmc%u: invalid flags=0x%x", cnum, flags);
++			goto error;
++		}
++
++		/*
++		 * locate event set
++		 */
++		if (i == 0 || set_id != prev_set_id) {
++			set = pfm_find_set(ctx, set_id, 0);
++			if (set == NULL) {
++				PFM_DBG("event set%u does not exist",
++					set_id);
++				goto error;
++			}
++		}
++
++		/*
++		 * set reserved bits to default values
++		 * (reserved bits must be 1 in rsvd_msk)
++		 */
++		value = (value & ~rsvd_msk) | (dfl_val & rsvd_msk);
++
++		if (flags & PFM_REGFL_NO_EMUL64) {
++			if (!(pmc_type & PFM_REG_NO64)) {
++				PFM_DBG("pmc%u no support for "
++					"PFM_REGFL_NO_EMUL64", cnum);
++				goto error;
++			}
++			value &= ~pfm_pmu_conf->pmc_desc[cnum].no_emul64_msk;
++		}
++
++		/*
++		 * execute write checker, if any
++		 */
++		if (likely(wr_func && (pmc_type & PFM_REG_WC))) {
++			req->reg_value = value;
++			ret = (*wr_func)(ctx, set, req);
++			if (ret)
++				goto error;
++			value = req->reg_value;
++		}
++
++		/*
++		 * Now we commit the changes
++		 */
++
++		/*
++		 * mark PMC register as used
++		 * We do not track associated PMC register based on
++		 * the fact that they will likely need to be written
++		 * in order to become useful at which point the statement
++		 * below will catch that.
++		 *
++		 * The used_pmcs bitmask is only useful on architectures where
++		 * the PMC needs to be modified for particular bits, especially
++		 * on overflow or to stop/start.
++		 */
++		if (!test_bit(cnum, cast_ulp(set->used_pmcs))) {
++			__set_bit(cnum, cast_ulp(set->used_pmcs));
++			set->nused_pmcs++;
++		}
++
++		set->pmcs[cnum] = value;
++
++		if (set == active_set) {
++			set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
++			if (can_access_pmu)
++				pfm_arch_write_pmc(ctx, cnum, value);
++		}
++
++		prev_set_id = set_id;
++
++		PFM_DBG("set%u pmc%u=0x%llx a_pmu=%d "
++			"u_pmcs=0x%llx nu_pmcs=%u",
++			set->id,
++			cnum,
++			(unsigned long long)value,
++			can_access_pmu,
++			(unsigned long long)set->used_pmcs[0],
++			set->nused_pmcs);
++	}
++	ret = 0;
++error:
++	/*
++	 * make sure the changes are visible
++	 */
++	if (can_access_pmu)
++		pfm_arch_serialize();
++
++	return ret;
++}
++
++/*
++ * function called from sys_pfm_read_pmds() to read the 64-bit value of
++ * requested PMD registers. The function succeeds whether the context is
++ * attached or not. When attached to another thread, that thread must be
++ * stopped.
++ *
++ * The context is locked and interrupts are disabled.
++ */
++int __pfm_read_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count)
++{
++	u64 val = 0, lval, ovfl_mask, hw_val;
++	u64 sw_cnt;
++	u64 *impl_pmds;
++	struct pfm_event_set *set, *active_set;
++	int i, ret, can_access_pmu = 0;
++	u16 cnum, pmd_type, set_id, prev_set_id, max_pmd;
++
++	ovfl_mask = pfm_pmu_conf->ovfl_mask;
++	impl_pmds = pfm_pmu_conf->regs.pmds;
++	max_pmd   = pfm_pmu_conf->regs.max_pmd;
++	active_set = ctx->active_set;
++	set = NULL;
++	prev_set_id = 0;
++
++	if (likely(ctx->state == PFM_CTX_LOADED)) {
++		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
++			       || ctx->flags.system;
++
++		if (can_access_pmu)
++			pfm_arch_serialize();
++	}
++
++	/*
++	 * on both UP and SMP, we can only read the PMD from the hardware
++	 * register when the task is the owner of the local PMU.
++	 */
++	ret = -EINVAL;
++	for (i = 0; i < count; i++, req++) {
++
++		cnum = req->reg_num;
++		set_id = req->reg_set;
++
++		if (unlikely(cnum >= max_pmd
++			     || !test_bit(cnum, cast_ulp(impl_pmds)))) {
++			PFM_DBG("pmd%u is not implemented/unaccessible", cnum);
++			goto error;
++		}
++
++		pmd_type = pfm_pmu_conf->pmd_desc[cnum].type;
++
++		/*
++		 * locate event set
++		 */
++		if (i == 0 || set_id != prev_set_id) {
++			set = pfm_find_set(ctx, set_id, 0);
++			if (set == NULL) {
++				PFM_DBG("event set%u does not exist",
++					set_id);
++				goto error;
++			}
++		}
++		/*
++		 * it is not possible to read a PMD which was not requested:
++		 * 	- explicitly written via pfm_write_pmds()
++		 * 	- provided as a reg_smpl_pmds[] to another PMD during
++		 * 	  pfm_write_pmds()
++		 *
++		 * This is motivated by security and for optimization purposes:
++		 * 	- on context switch restore, we can restore only what we
++		 * 	  use (except when regs directly readable at user level,
++		 * 	  e.g., IA-64 self-monitoring, I386 RDPMC).
++		 * 	- do not need to maintain PMC -> PMD dependencies
++		 */
++		if (unlikely(!test_bit(cnum, cast_ulp(set->used_pmds)))) {
++			PFM_DBG("pmd%u cannot be read, because never "
++				"requested", cnum);
++			goto error;
++		}
++
++		val = set->pmds[cnum].value;
++		lval = set->pmds[cnum].lval;
++
++		/*
++		 * extract remaining ovfl to switch
++		 */
++		sw_cnt = set->pmds[cnum].ovflsw_thres;
++
++		/*
++		 * If the task is not the current one, then we check if the
++		 * PMU state is still in the local live register due to lazy
++		 * ctxsw. If true, then we read directly from the registers.
++		 */
++		if (set == active_set && can_access_pmu) {
++			hw_val = pfm_read_pmd(ctx, cnum);
++			if (pmd_type & PFM_REG_C64)
++				val = (val & ~ovfl_mask) | (hw_val & ovfl_mask);
++			else
++				val = hw_val;
++		}
++
++		PFM_DBG("set%u pmd%u=0x%llx sw_thr=%llu lval=0x%llx",
++			set->id,
++			cnum,
++			(unsigned long long)val,
++			(unsigned long long)sw_cnt,
++			(unsigned long long)lval);
++
++		req->reg_value = val;
++		req->reg_last_reset_val = lval;
++		req->reg_ovfl_switch_cnt = sw_cnt;
++
++		prev_set_id = set_id;
++	}
++	ret = 0;
++error:
++	return ret;
++}
+--- /dev/null
++++ b/perfmon/perfmon_sets.c
+@@ -0,0 +1,773 @@
++/*
++ * perfmon_sets.c: perfmon2 event sets and multiplexing functions
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/kernel.h>
++#include <linux/perfmon.h>
++
++static struct kmem_cache	*pfm_set_cachep;
++
++/*
++ * reload reference interrupt switch thresholds for PMD which
++ * can interrupt
++ */
++static void pfm_reload_switch_thresholds(struct pfm_event_set *set)
++{
++	u64 *used_pmds;
++	u16 i, max, first;
++
++	used_pmds = set->used_pmds;
++	first = pfm_pmu_conf->regs.first_intr_pmd;
++	max = pfm_pmu_conf->regs.max_intr_pmd;
++
++	for (i = first; i < max; i++) {
++		if (test_bit(i, cast_ulp(used_pmds))) {
++			set->pmds[i].ovflsw_thres = set->pmds[i].ovflsw_ref_thres;
++
++			PFM_DBG("set%u pmd%u ovflsw_thres=%llu",
++				set->id,
++				i,
++				(unsigned long long)set->pmds[i].ovflsw_thres);
++		}
++	}
++}
++
++/*
++ * connect all sets, reset internal fields
++ */
++int pfm_prepare_sets(struct pfm_context *ctx, struct pfm_event_set *act_set)
++{
++	struct pfm_event_set *set;
++	u16 max;
++
++	max = pfm_pmu_conf->regs.max_intr_pmd;
++
++	list_for_each_entry(set, &ctx->set_list, list) {
++		/*
++		 * cleanup bitvectors
++		 */
++		bitmap_zero(cast_ulp(set->ovfl_pmds), max);
++		bitmap_zero(cast_ulp(set->povfl_pmds), max);
++
++		set->npend_ovfls = 0;
++		/*
++		 * timer is reset when the context is loaded, so any
++		 * remainder timeout is cancelled
++		 */
++		set->hrtimer_rem.tv64 = 0;
++
++		/*
++		 * we cannot just use plain clear because of arch-specific flags
++		 */
++		set->priv_flags &= ~(PFM_SETFL_PRIV_MOD_BOTH|PFM_SETFL_PRIV_SWITCH);
++		/*
++		 * neither duration nor runs are reset because typically loading/unloading
++		 * does not mean counts are reset. To reset, the set must be modified
++		 */
++	}
++
++	if (act_set->flags & PFM_SETFL_OVFL_SWITCH)
++		pfm_reload_switch_thresholds(act_set);
++
++	return 0;
++}
++
++/*
++ * called by run_hrtimer_softirq()
++ */
++enum hrtimer_restart pfm_handle_switch_timeout(struct hrtimer *t)
++{
++	struct pfm_event_set *set;
++	struct pfm_context *ctx;
++	unsigned long flags;
++	enum hrtimer_restart ret = HRTIMER_NORESTART;
++
++	/*
++	 * prevent against race with unload
++	 */
++	ctx  = __get_cpu_var(pmu_ctx);
++	if (!ctx)
++		return HRTIMER_NORESTART;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	set = ctx->active_set;
++
++	/*
++	 * switching occurs only when context is attached
++	 */
++	switch(ctx->state) {
++		case PFM_CTX_LOADED:
++			break;
++		case PFM_CTX_ZOMBIE:
++		case PFM_CTX_UNLOADED:
++			goto done;
++		case PFM_CTX_MASKED:
++			/*
++			 * no switching occurs while masked
++			 * but timeout is re-armed
++			 */
++			ret = HRTIMER_RESTART;
++			goto done;
++	}
++	BUG_ON(ctx->flags.system && ctx->cpu != smp_processor_id());
++
++	/*
++	 * timer does not run while monitoring is inactive (not started)
++	 */
++	if (!pfm_arch_is_active(ctx))
++		goto done;
++
++	pfm_stats_inc(handle_timeout_count);
++
++	ret  = pfm_switch_sets(ctx, NULL, PFM_PMD_RESET_SHORT, 0);
++done:
++	spin_unlock_irqrestore(&ctx->lock, flags);
++	return ret;
++}
++
++/*
++ *
++ * always operating on the current task
++ * interrupts are masked
++ *
++ * input:
++ * 	- new_set: new set to switch to, if NULL follow normal chain
++ */
++enum hrtimer_restart pfm_switch_sets(struct pfm_context *ctx,
++				     struct pfm_event_set *new_set,
++				     int reset_mode,
++				     int no_restart)
++{
++	struct pfm_event_set *set;
++	u64 now, end;
++	u32 new_flags;
++	int is_system, is_active, nn;
++	enum hrtimer_restart ret = HRTIMER_NORESTART;
++
++	now = sched_clock();
++	set = ctx->active_set;
++	is_active = pfm_arch_is_active(ctx);
++
++	/*
++	 * if no set is explicitly requested,
++	 * use the set_switch_next field
++	 */
++	if (!new_set) {
++		/*
++		 * we use round-robin unless the user specified
++		 * a particular set to go to.
++		 */
++		new_set = list_first_entry(&set->list, struct pfm_event_set, list);
++		if (&new_set->list == &ctx->set_list)
++			new_set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
++	}
++
++	PFM_DBG_ovfl("state=%d act=%d cur_set=%u cur_runs=%llu cur_npend=%d next_set=%u "
++		  "next_runs=%llu new_npend=%d reset_mode=%d reset_pmds=%llx",
++		  ctx->state,
++		  is_active,
++		  set->id,
++		  (unsigned long long)set->runs,
++		  set->npend_ovfls,
++		  new_set->id,
++		  (unsigned long long)new_set->runs,
++		  new_set->npend_ovfls,
++		  reset_mode,
++		  (unsigned long long)new_set->reset_pmds[0]);
++
++	is_system = ctx->flags.system;
++	new_flags = new_set->flags;
++
++	/*
++	 * nothing more to do
++	 */
++	if (new_set == set)
++		goto skip_same_set;
++
++	if (is_active) {
++		pfm_arch_stop(current, ctx, set);
++		pfm_save_pmds(ctx, set);
++		/*
++		 * compute elapsed ns for active set
++		 */
++		set->duration += now - set->duration_start;
++	}
++
++	pfm_arch_restore_pmds(ctx, new_set);
++	/*
++	 * if masked, we must restore the pmcs such that they
++	 * do not capture anything.
++	 */
++	pfm_arch_restore_pmcs(ctx, new_set);
++
++	if (new_set->npend_ovfls) {
++		pfm_arch_resend_irq();
++		pfm_stats_inc(ovfl_intr_replay_count);
++	}
++
++	new_set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
++
++skip_same_set:
++	new_set->runs++;
++	/*
++	 * reset switch threshold
++	 */
++	if (new_flags & PFM_SETFL_OVFL_SWITCH)
++		pfm_reload_switch_thresholds(new_set);
++
++	/*
++	 * reset overflowed PMD registers
++	 */
++	nn = bitmap_weight(cast_ulp(new_set->reset_pmds),
++			   pfm_pmu_conf->regs.max_pmd);
++	if (nn)
++		pfm_reset_pmds(ctx, new_set, nn, reset_mode);
++	/*
++	 * this is needed when coming from pfm_start()
++	 */
++	if (no_restart)
++		goto skip_restart;
++
++	if (is_active) {
++		pfm_arch_start(current, ctx, new_set);
++		new_set->duration_start = now;
++
++		/*
++		 * install new timeout if necessary
++		 */
++		if (new_flags & PFM_SETFL_TIME_SWITCH) {
++			struct hrtimer *h;
++			h = &__get_cpu_var(pfm_hrtimer);
++			hrtimer_forward(h, h->base->get_time(), new_set->hrtimer_exp);
++			ret = HRTIMER_RESTART;
++		}
++	}
++
++skip_restart:
++	ctx->active_set = new_set;
++
++	end = sched_clock();
++
++	pfm_stats_inc(set_switch_count);
++	pfm_stats_add(set_switch_ns, end - now);
++
++	return ret;
++}
++
++/*
++ * called from __pfm_overflow_handler() to switch event sets.
++ * monitoring is stopped, task is current, interrupts are masked.
++ * compared to pfm_switch_sets(), this version is simplified because
++ * it knows about the call path. There is no need to stop monitoring
++ * because it is already frozen by PMU handler.
++ */
++void pfm_switch_sets_from_intr(struct pfm_context *ctx)
++{
++	struct pfm_event_set *set, *new_set;
++	u64 now, end;
++	u32 new_flags;
++	int is_system, n;
++
++	now = sched_clock();
++	set = ctx->active_set;
++	new_set = list_first_entry(&set->list, struct pfm_event_set, list);
++	if (&new_set->list == &ctx->set_list)
++		new_set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
++
++	PFM_DBG_ovfl("state=%d cur_set=%u cur_runs=%llu cur_npend=%d next_set=%u "
++		  "next_runs=%llu new_npend=%d new_r_pmds=%llx",
++		  ctx->state,
++		  set->id,
++		  (unsigned long long)set->runs,
++		  set->npend_ovfls,
++		  new_set->id,
++		  (unsigned long long)new_set->runs,
++		  new_set->npend_ovfls,
++		  (unsigned long long)new_set->reset_pmds[0]);
++
++	is_system = ctx->flags.system;
++	new_flags = new_set->flags;
++
++	/*
++	 * nothing more to do
++	 */
++	if (new_set == set)
++		goto skip_same_set;
++
++	if (set->flags & PFM_SETFL_TIME_SWITCH) {
++		hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
++		PFM_DBG_ovfl("cancelled timer for set%u", set->id);
++	}
++
++	/*
++	 * when called from PMU intr handler, monitoring
++	 * is already stopped
++	 *
++	 * save current PMD registers, we use a special
++	 * form for performance reason. On some architectures,
++	 * such as x86, the pmds are already saved when entering
++	 * the PMU interrupt handler via pfm-arch_intr_freeze()
++	 * so we don't need to save them again. On the contrary,
++	 * on IA-64, they are not saved by freeze, thus we have to
++	 * to it here.
++	 */
++	pfm_arch_save_pmds_from_intr(ctx, set);
++
++	/*
++	 * compute elapsed ns for active set
++	 */
++	set->duration += now - set->duration_start;
++
++	pfm_arch_restore_pmds(ctx, new_set);
++
++	/*
++	 * must not be restored active as we are still executing in the
++	 * PMU interrupt handler. activation is deferred to unfreeze PMU
++	 */
++	pfm_arch_restore_pmcs(ctx, new_set);
++
++	/*
++	 * check for pending interrupt on incoming set.
++	 * interrupts are masked so handler call deferred
++	 */
++	if (new_set->npend_ovfls) {
++		pfm_arch_resend_irq();
++		pfm_stats_inc(ovfl_intr_replay_count);
++	}
++	/*
++	 * no need to restore anything, that is already done
++	 */
++	new_set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
++	/*
++	 * reset duration counter
++	 */
++	new_set->duration_start = now;
++
++skip_same_set:
++	new_set->runs++;
++
++	/*
++	 * reset switch threshold
++	 */
++	if (new_flags & PFM_SETFL_OVFL_SWITCH)
++		pfm_reload_switch_thresholds(new_set);
++
++	/*
++	 * reset overflowed PMD registers
++	 */
++	n = bitmap_weight(cast_ulp(new_set->reset_pmds), pfm_pmu_conf->regs.max_pmd);
++	if (n)
++		pfm_reset_pmds(ctx, new_set, n, PFM_PMD_RESET_SHORT);
++
++	/*
++	 * XXX: isactive?
++	 *
++	 * Came here following a interrupt which triggered a switch, i.e.,
++	 * previous set was using OVFL_SWITCH, thus we just need to arm
++	 * check if the next set is using timeout, and if so arm the timer.
++	 */
++	if (new_flags & PFM_SETFL_TIME_SWITCH) {
++		hrtimer_start(&__get_cpu_var(pfm_hrtimer), set->hrtimer_exp, HRTIMER_MODE_REL);
++		PFM_DBG("armed new timeout for set%u", new_set->id);
++	}
++
++	ctx->active_set = new_set;
++
++	end = sched_clock();
++
++	pfm_stats_inc(set_switch_count);
++	pfm_stats_add(set_switch_ns, end - now);
++}
++
++
++static int pfm_setfl_sane(struct pfm_context *ctx, u32 flags)
++{
++#define PFM_SETFL_BOTH_SWITCH	(PFM_SETFL_OVFL_SWITCH|PFM_SETFL_TIME_SWITCH)
++	int ret;
++
++	ret = pfm_arch_setfl_sane(ctx, flags);
++	if (ret)
++		return ret;
++
++	if ((flags & PFM_SETFL_BOTH_SWITCH) == PFM_SETFL_BOTH_SWITCH) {
++		PFM_DBG("both switch ovfl and switch time are set");
++		return -EINVAL;
++	}
++	return 0;
++}
++
++/*
++ * it is never possible to change the identification of an existing set
++ */
++static int __pfm_change_evtset(struct pfm_context *ctx,
++			       struct pfm_event_set *set,
++			       struct pfarg_setdesc *req)
++{
++	struct timeval tv;
++	struct timespec ts;
++	ktime_t kt;
++	long d, rem, res_ns;
++	u32 flags;
++	int ret;
++	u16 set_id;
++
++	BUG_ON(ctx->state == PFM_CTX_LOADED);
++
++	set_id = req->set_id;
++	flags = req->set_flags;
++
++	ret = pfm_setfl_sane(ctx, flags);
++	if (ret) {
++		PFM_DBG("invalid flags 0x%x set %u", flags, set_id);
++		return -EINVAL;
++	}
++
++	hrtimer_get_res(CLOCK_MONOTONIC, &ts);
++	res_ns = (long)ktime_to_ns(timespec_to_ktime(ts));
++	PFM_DBG("clock_res=%ldns", res_ns);
++
++	/*
++	 * round-up to multiple of clock resolution
++	 * timeout = ((req->set_timeout+res_ns-1)/res_ns)*res_ns;
++	 *
++	 * u64 division missing on 32-bit arch, so use div_long_long
++	 */
++	d = div_long_long_rem_signed(req->set_timeout, res_ns, &rem);
++
++	PFM_DBG("set%u flags=0x%x req_timeout=%lluns "
++		"HZ=%u TICK_NSEC=%lu eff_timeout=%luns",
++		set_id,
++		flags,
++		(unsigned long long)req->set_timeout,
++		HZ, TICK_NSEC,
++		d * res_ns);
++
++	/*
++	 * Only accept timeout, we can actually achieve.
++	 * users can invoke clock_getres(CLOCK_MONOTONIC)
++	 * to figure out resolution and adjust timeout
++	 */
++	if (rem) {
++		PFM_DBG("set%u invalid timeout=%llu",
++			set_id,
++			(unsigned long long)req->set_timeout);
++		return -EINVAL;
++	}
++
++	tv = ns_to_timeval(req->set_timeout);
++	kt = timeval_to_ktime(tv);
++	set->hrtimer_exp = kt;
++
++	/*
++	 * commit changes
++	 */
++	set->id = set_id;
++	set->flags = flags;
++	set->priv_flags = 0;
++
++	/*
++	 * activation and duration counters are reset as
++	 * most likely major things will change in the set
++	 */
++	set->runs = 0;
++	set->duration = 0;
++
++	return 0;
++}
++
++/*
++ * this function does not modify the next field
++ */
++void pfm_init_evtset(struct pfm_event_set *set)
++{
++	u64 *impl_pmcs;
++	u16 i, max_pmc;
++
++	max_pmc = pfm_pmu_conf->regs.max_pmc;
++	impl_pmcs =  pfm_pmu_conf->regs.pmcs;
++
++	/*
++	 * install default values for all PMC  registers
++	 */
++	for (i=0; i < max_pmc;  i++) {
++		if (test_bit(i, cast_ulp(impl_pmcs))) {
++			set->pmcs[i] = pfm_pmu_conf->pmc_desc[i].dfl_val;
++			PFM_DBG("set%u pmc%u=0x%llx",
++				set->id,
++				i,
++				(unsigned long long)set->pmcs[i]);
++		}
++	}
++
++	/*
++	 * PMD registers are set to 0 when the event set is allocated,
++	 * hence we do not need to explicitly initialize them.
++	 *
++	 * For virtual PMD registers (i.e., those tied to a SW resource)
++	 * their value becomes meaningful once the context is attached.
++	 */
++}
++
++/*
++ * look for an event set using its identification. If the set does not
++ * exist:
++ * 	- if alloc == 0 then return error
++ * 	- if alloc == 1  then allocate set
++ *
++ * alloc is one ONLY when coming from pfm_create_evtsets() which can only
++ * be called when the context is detached, i.e. monitoring is stopped.
++ */
++struct pfm_event_set *pfm_find_set(struct pfm_context *ctx, u16 set_id, int alloc)
++{
++	struct pfm_event_set *set = NULL, *prev, *new_set;
++
++	PFM_DBG("looking for set=%u", set_id);
++
++	prev = NULL;
++	list_for_each_entry(set, &ctx->set_list, list) {
++		if (set->id == set_id)
++			return set;
++		if (set->id > set_id)
++			break;
++		prev = set;
++	}
++
++	if (!alloc)
++		return NULL;
++
++	/*
++	 * we are holding the context spinlock and interrupts
++	 * are unmasked. We must use GFP_ATOMIC as we cannot
++	 * sleep while holding a spin lock.
++	 */
++	new_set = kmem_cache_zalloc(pfm_set_cachep, GFP_ATOMIC);
++	if (!new_set)
++		return NULL;
++
++	new_set->id = set_id;
++
++	INIT_LIST_HEAD(&new_set->list);
++
++	if (prev == NULL) {
++		list_add(&(new_set->list), &ctx->set_list);
++	} else {
++		PFM_DBG("add after set=%u", prev->id);
++		list_add(&(new_set->list), &prev->list);
++	}
++	return new_set;
++}
++
++/*
++ * context is unloaded for this command. Interrupts are enabled
++ */
++int __pfm_create_evtsets(struct pfm_context *ctx, struct pfarg_setdesc *req,
++			int count)
++{
++	struct pfm_event_set *set;
++	u16 set_id;
++	int i, ret;
++
++	for (i = 0; i < count; i++, req++) {
++		set_id = req->set_id;
++
++		PFM_DBG("set_id=%u", set_id);
++
++		set = pfm_find_set(ctx, set_id, 1);
++		if (set == NULL)
++			goto error_mem;
++
++		ret = __pfm_change_evtset(ctx, set, req);
++		if (ret)
++			goto error_params;
++
++		pfm_init_evtset(set);
++	}
++	return 0;
++error_mem:
++	PFM_DBG("cannot allocate set %u", set_id);
++	return -ENOMEM;
++error_params:
++	return ret;
++}
++
++int __pfm_getinfo_evtsets(struct pfm_context *ctx, struct pfarg_setinfo *req,
++				 int count)
++{
++	struct pfm_event_set *set;
++	int i, is_system, is_loaded, ret;
++	u16 set_id, max, max_pmc, max_pmd;
++	u64 end;
++
++	end = sched_clock();
++
++	is_system = ctx->flags.system;
++	is_loaded = ctx->state == PFM_CTX_LOADED;
++
++	max = pfm_pmu_conf->regs.max_intr_pmd;
++	max_pmc  = pfm_pmu_conf->regs.max_pmc;
++	max_pmd = pfm_pmu_conf->regs.max_pmd;
++
++	ret = -EINVAL;
++	for (i = 0; i < count; i++, req++) {
++
++		set_id = req->set_id;
++
++		list_for_each_entry(set, &ctx->set_list, list) {
++			if (set->id == set_id)
++				goto found;
++			if (set->id > set_id)
++				goto error;
++		}
++found:
++		/*
++		 * compute leftover timeout
++		 */
++		req->set_flags = set->flags;
++
++		/*
++		 * XXX: fixme
++		 */
++		req->set_timeout = 0;
++
++		req->set_runs = set->runs;
++		req->set_act_duration = set->duration;
++
++		/*
++		 * adjust for active set if needed
++		 */
++		if (is_system && is_loaded && ctx->flags.started
++		    && set == ctx->active_set)
++			req->set_act_duration  += end - set->duration_start;
++
++		/*
++		 * copy the list of pmds which last overflowed for
++		 * the set
++		 */
++		bitmap_copy(cast_ulp(req->set_ovfl_pmds),
++			    cast_ulp(set->ovfl_pmds),
++			    max);
++
++		/*
++		 * copy bitmask of available PMU registers
++		 */
++		bitmap_copy(cast_ulp(req->set_avail_pmcs),
++			    cast_ulp(pfm_pmu_conf->regs.pmcs),
++			    max_pmc);
++
++		bitmap_copy(cast_ulp(req->set_avail_pmds),
++			    cast_ulp(pfm_pmu_conf->regs.pmds),
++			    max_pmd);
++
++		PFM_DBG("set%u flags=0x%x eff_usec=%llu runs=%llu "
++			"a_pmcs=0x%llx a_pmds=0x%llx",
++			set_id,
++			set->flags,
++			(unsigned long long)req->set_timeout,
++			(unsigned long long)set->runs,
++			(unsigned long long)pfm_pmu_conf->regs.pmcs[0],
++			(unsigned long long)pfm_pmu_conf->regs.pmds[0]);
++	}
++	ret = 0;
++error:
++	return ret;
++}
++
++/*
++ * context is unloaded for this command. Interrupts are enabled
++ */
++int __pfm_delete_evtsets(struct pfm_context *ctx, void *arg, int count)
++{
++	struct pfarg_setdesc *req = arg;
++	struct pfm_event_set *set;
++	u16 set_id;
++	int i, ret;
++
++	ret = -EINVAL;
++	for (i = 0; i < count; i++, req++) {
++		set_id = req->set_id;
++
++		list_for_each_entry(set, &ctx->set_list, list) {
++			if (set->id == set_id)
++				goto found;
++			if (set->id > set_id)
++				goto error;
++		}
++		goto error;
++found:
++		/*
++		 * clear active set if necessary.
++		 * will be updated when context is loaded
++		 */
++		if (set == ctx->active_set)
++			ctx->active_set = NULL;
++
++		list_del(&set->list);
++
++		kmem_cache_free(pfm_set_cachep, set);
++
++		PFM_DBG("set%u deleted", set_id);
++	}
++	ret = 0;
++error:
++	return ret;
++}
++
++/*
++ * called from pfm_context_free() to free all sets
++ */
++void pfm_free_sets(struct pfm_context *ctx)
++{
++	struct pfm_event_set *set, *tmp;
++
++	list_for_each_entry_safe(set, tmp, &ctx->set_list, list) {
++		list_del(&set->list);
++		kmem_cache_free(pfm_set_cachep, set);
++	}
++}
++
++int pfm_sets_init(void)
++{
++
++	pfm_set_cachep = kmem_cache_create("pfm_event_set",
++					   sizeof(struct pfm_event_set),
++					   SLAB_HWCACHE_ALIGN, 0, NULL);
++	if (pfm_set_cachep == NULL) {
++		PFM_ERR("cannot initialize event set slab");
++		return -ENOMEM;
++	}
++	return 0;
++}
+--- /dev/null
++++ b/perfmon/perfmon_syscalls.c
+@@ -0,0 +1,1061 @@
++/*
++ * perfmon_syscalls.c: perfmon2 system call interface
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/kernel.h>
++#include <linux/perfmon.h>
++#include <linux/fs.h>
++#include <linux/ptrace.h>
++#include <asm/uaccess.h>
++
++/*
++ * Context locking rules:
++ * ---------------------
++ * 	- any thread with access to the file descriptor of a context can
++ * 	  potentially issue perfmon calls
++ *
++ * 	- calls must be serialized to guarantee correctness
++ *
++ * 	- as soon as a context is attached to a thread or CPU, it may be
++ * 	  actively monitoring. On some architectures, such as IA-64, this
++ * 	  is true even though the pfm_start() call has not been made. This
++ * 	  comes from the fact that on some architectures, it is possible to
++ * 	  start/stop monitoring from userland.
++ *
++ *	- If monitoring is active, then there can PMU interrupts. Because
++ *	  context accesses must be serialized, the perfmon system calls
++ *	  must mask interrupts as soon as the context is attached.
++ *
++ *	- perfmon system calls that operate with the context unloaded cannot
++ *	  assume it is actually unloaded when they are called. They first need
++ *	  to check and for that they need interrupts masked. Then if the context
++ *	  is actually unloaded, they can unmask interrupts.
++ *
++ *	- interrupt masking holds true for other internal perfmon functions as
++ *	  well. Except for PMU interrupt handler because those interrupts cannot
++ *	  be nested.
++ *
++ * 	- we mask ALL interrupts instead of just the PMU interrupt because we
++ * 	  also need to protect against timer interrupts which could trigger
++ * 	  a set switch.
++ */
++
++/*
++ * cannot attach if :
++ * 	- kernel task
++ * 	- task not owned by caller (checked by ptrace_may_attach())
++ * 	- task is dead or zombie
++ * 	- cannot use blocking notification when self-monitoring
++ */
++static int pfm_task_incompatible(struct pfm_context *ctx, struct task_struct *task)
++{
++	/*
++	 * cannot attach to a kernel thread
++	 */
++	if (!task->mm) {
++		PFM_DBG("cannot attach to kernel thread [%d]", task->pid);
++		return -EPERM;
++	}
++
++	/*
++	 * cannot use block on notification when
++	 * self-monitoring.
++	 */
++	if (ctx->flags.block && task == current) {
++		PFM_DBG("cannot use block on notification when self-monitoring"
++			"[%d]", task->pid);
++		return -EINVAL;
++	}
++	/*
++	 * cannot attach to a zombie task
++	 */
++	if (task->exit_state == EXIT_ZOMBIE || task->exit_state == EXIT_DEAD) {
++		PFM_DBG("cannot attach to zombie/dead task [%d]", task->pid);
++		return -EBUSY;
++	}
++	return 0;
++}
++
++/*
++ * This function  is used in per-thread mode only AND when not
++ * self-monitoring. It finds the task to monitor and checks
++ * that the caller has persmissions to attach. It also checks
++ * that the task is stopped via ptrace so that we can safely
++ * modify its state.
++ *
++ * task refcount is increment when succesful.
++ * This function is not declared static because it is used by the
++ * IA-64 compatiblity module arch/ia64/perfmon/perfmon_comapt.c
++ */
++int pfm_get_task(struct pfm_context *ctx, pid_t pid, struct task_struct **task)
++{
++	struct task_struct *p;
++	int ret = 0, ret1 = 0;
++
++	/*
++	 * When attaching to another thread we must ensure
++	 * that the thread is actually stopped. Just like with
++	 * perfmon system calls, we enforce that the thread
++	 * be ptraced and STOPPED by using ptrace_check_attach().
++	 *
++	 * As a consequence, only the ptracing parent can actually
++	 * attach a context to a thread. Obviously, this constraint
++	 * does not exist for self-monitoring threads.
++	 *
++	 * We use ptrace_may_attach() to check for permission.
++	 * No permission checking is needed for self monitoring.
++	 */
++	read_lock(&tasklist_lock);
++
++	p = find_task_by_pid(pid);
++	if (p)
++		get_task_struct(p);
++
++	read_unlock(&tasklist_lock);
++
++	if (p == NULL)
++		return -ESRCH;
++
++	ret = -EPERM;
++
++	/*
++	 * returns 0 if cannot attach
++	 */
++	ret1 = ptrace_may_attach(p);
++	if (ret1)
++		ret = ptrace_check_attach(p, 0);
++
++	PFM_DBG("may_attach=%d check_attach=%d", ret1, ret);
++
++	if (ret || !ret1)
++		goto error;
++
++	ret = pfm_task_incompatible(ctx, p);
++	if (ret)
++		goto error;
++
++	*task = p;
++
++	return 0;
++error:
++	if (!(ret1 || ret))
++		ret = -EPERM;
++
++	put_task_struct(p);
++
++	return ret;
++}
++
++/*
++ * context must be locked when calling this function
++ */
++int pfm_check_task_state(struct pfm_context *ctx, int check_mask,
++			 unsigned long *flags)
++{
++	struct task_struct *task;
++	unsigned long local_flags, new_flags;
++	int state, ret;
++
++recheck:
++	/*
++	 * task is NULL for system-wide context
++	 */
++	task = ctx->task;
++	state = ctx->state;
++	local_flags = *flags;
++
++	PFM_DBG("state=%d check_mask=0x%x", state, check_mask);
++	/*
++	 * if the context is detached, then we do not touch
++	 * hardware, therefore there is not restriction on when we can
++	 * access it.
++	 */
++	if (state == PFM_CTX_UNLOADED)
++		return 0;
++	/*
++	 * no command can operate on a zombie context.
++	 * A context becomes zombie when the file that identifies
++	 * it is closed while the context is still attached to the
++	 * thread it monitors.
++	 */
++	if (state == PFM_CTX_ZOMBIE)
++		return -EINVAL;
++
++	/*
++	 * at this point, state is PFM_CTX_LOADED or PFM_CTX_MASKED
++	 */
++
++	/*
++	 * some commands require the context to be unloaded to operate
++	 */
++	if (check_mask & PFM_CMD_UNLOADED)  {
++		PFM_DBG("state=%d, cmd needs context unloaded", state);
++		return -EBUSY;
++	}
++
++	/*
++	 * self-monitoring always ok.
++	 */
++	if (task == current)
++		return 0;
++
++	/*
++	 * for syswide, the calling thread must be running on the cpu
++	 * the context is bound to.
++	 */
++	if (ctx->flags.system) {
++		if (ctx->cpu != smp_processor_id())
++			return -EBUSY;
++		return 0;
++	}
++
++	/*
++	 * at this point, monitoring another thread
++	 */
++
++	/*
++	 * the pfm_unload_context() command is allowed on masked context
++	 */
++	if (state == PFM_CTX_MASKED && !(check_mask & PFM_CMD_UNLOAD))
++		return 0;
++
++	/*
++	 * When we operate on another thread, we must wait for it to be
++	 * stopped and completely off any CPU as we need to access the
++	 * PMU state (or machine state).
++	 *
++	 * A thread can be put in the STOPPED state in various ways
++	 * including PTRACE_ATTACH, or when it receives a SIGSTOP signal.
++	 * We enforce that the thread must be ptraced, so it is stopped
++	 * AND it CANNOT wake up while we operate on it because this
++	 * would require an action from the ptracing parent which is the
++	 * thread that is calling this function.
++	 *
++	 * The dependency on ptrace, imposes that only the ptracing
++	 * parent can issue command on a thread. This is unfortunate
++	 * but we do not know of a better way of doing this.
++	 */
++	if (check_mask & PFM_CMD_STOPPED) {
++
++		spin_unlock_irqrestore(&ctx->lock, local_flags);
++
++		/*
++		 * check that the thread is ptraced AND STOPPED
++		 */
++		ret = ptrace_check_attach(task, 0);
++
++		spin_lock_irqsave(&ctx->lock, new_flags);
++
++		/*
++		 * flags may be different than when we released the lock
++		 */
++		*flags = new_flags;
++
++		if (ret)
++			return ret;
++		/*
++		 * we must recheck to verify if state has changed
++		 */
++		if (unlikely(ctx->state != state)) {
++			PFM_DBG("old_state=%d new_state=%d",
++				state,
++				ctx->state);
++			goto recheck;
++		}
++	}
++	return 0;
++}
++
++/*
++ * pfm_get_args - Function used to copy the syscall argument into kernel memory.
++ * @ureq: user argument
++ * @sz: user argument size
++ * @lsz: size of stack buffer
++ * @laddr: stack buffer address
++ * @req: point to start of kernel copy of the argument
++ * @ptr_free: address of kernel copy to free
++ *
++ * There are two options:
++ * 	- use a stack buffer described by laddr (addresses) and lsz (size)
++ * 	- allocate memory
++ *
++ * return:
++ * 	< 0 : in case of error (ptr_free may not be updated)
++ * 	  0 : success
++ *      - req: points to base of kernel copy of arguments
++ *	- ptr_free: address of buffer to free by caller on exit.
++ *		    NULL if using the stack buffer
++ *
++ * when ptr_free is not NULL upon return, the caller must kfree()
++ */
++int pfm_get_args(void __user *ureq, size_t sz, size_t lsz, void *laddr,
++		 void **req, void **ptr_free)
++{
++	void *addr;
++
++	/*
++	 * check syadmin argument limit
++	 */
++	if (unlikely(sz > pfm_controls.arg_mem_max)) {
++		PFM_DBG("argument too big %zu max=%zu",
++			sz,
++			pfm_controls.arg_mem_max);
++		return -E2BIG;
++	}
++
++	/*
++	 * check if vector fits on stack buffer
++	 */
++	if (sz > lsz) {
++		addr = kmalloc(sz, GFP_KERNEL);
++		if (unlikely(addr == NULL))
++			return -ENOMEM;
++		*ptr_free = addr;
++	} else {
++		addr = laddr;
++		*req = laddr;
++		*ptr_free = NULL;
++	}
++
++	/*
++	 * bring the data in
++	 */
++	if (unlikely(copy_from_user(addr, ureq, sz))) {
++		if (addr != laddr)
++			kfree(addr);
++		return -EFAULT;
++	}
++
++	/*
++	 * base address of kernel buffer
++	 */
++	*req = addr;
++
++	return 0;
++}
++
++/*
++ * arg is kmalloc'ed, thus it needs a kfree by caller
++ */
++int pfm_get_smpl_arg(char __user *fmt_uname, void __user *fmt_uarg, size_t usize, void **arg,
++		     struct pfm_smpl_fmt **fmt)
++{
++	struct pfm_smpl_fmt *f;
++	char *fmt_name;
++	void *addr = NULL;
++	size_t sz;
++	int ret;
++
++	fmt_name = getname(fmt_uname);
++	if (!fmt_name) {
++		PFM_DBG("getname failed");
++		return -ENOMEM;
++	}
++
++	/*
++	 * find fmt and increase refcount
++	 */
++	f = pfm_smpl_fmt_get(fmt_name);
++
++	putname(fmt_name);
++
++	if (f == NULL) {
++		PFM_DBG("buffer format not found");
++		return -EINVAL;
++	}
++
++	/*
++	 * expected format argument size
++	 */
++	sz = f->fmt_arg_size;
++
++	/*
++	 * check user size matches expected size
++	 * usize = -1 is for IA-64 backward compatibility
++	 */
++	ret = -EINVAL;
++	if (sz != usize && usize != -1) {
++		PFM_DBG("invalid arg size %zu, format expects %zu",
++			usize, sz);
++		goto error;
++	}
++
++	if (sz) {
++		ret = -ENOMEM;
++		addr = kmalloc(sz, GFP_KERNEL);
++		if (addr == NULL)
++			goto error;
++
++		ret = -EFAULT;
++		if (copy_from_user(addr, fmt_uarg, sz))
++			goto error;
++	}
++	*arg = addr;
++	*fmt = f;
++	return 0;
++
++error:
++	kfree(addr);
++	pfm_smpl_fmt_put(f);
++	return ret;
++}
++
++/*
++ * unlike the other perfmon system calls, this one return a file descriptor
++ * or a value < 0 in case of error, very much like open() or socket()
++ */
++asmlinkage long sys_pfm_create_context(struct pfarg_ctx __user *ureq,
++				       char __user *fmt_name,
++				       void __user *fmt_uarg, size_t fmt_size)
++{
++	struct pfarg_ctx req;
++	struct pfm_context *new_ctx;
++	struct pfm_smpl_fmt *fmt = NULL;
++	void *fmt_arg = NULL;
++	int ret;
++
++	PFM_DBG("req=%p fmt=%p fmt_arg=%p size=%zu",
++		ureq, fmt_name, fmt_uarg, fmt_size);
++
++	if (perfmon_disabled)
++		return -ENOSYS;
++
++	if (copy_from_user(&req, ureq, sizeof(req)))
++		return -EFAULT;
++
++	if (fmt_name) {
++		ret = pfm_get_smpl_arg(fmt_name, fmt_uarg, fmt_size, &fmt_arg, &fmt);
++		if (ret)
++			goto abort;
++	}
++
++	ret = __pfm_create_context(&req, fmt, fmt_arg, PFM_NORMAL, &new_ctx);
++
++	kfree(fmt_arg);
++abort:
++	return ret;
++}
++
++asmlinkage long sys_pfm_write_pmcs(int fd, struct pfarg_pmc __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_pmc pmcs[PFM_PMC_STK_ARG];
++	struct pfarg_pmc *req;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
++
++	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq)) {
++		PFM_DBG("invalid arg count %d", count);
++		return -EINVAL;
++	}
++
++	sz = count*sizeof(*ureq);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, sizeof(pmcs), pmcs, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (!ret)
++		ret = __pfm_write_pmcs(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	/*
++	 * This function may be on the critical path.
++	 * We want to avoid the branch if unecessary.
++	 */
++	if (fptr)
++		kfree(fptr);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_write_pmds(int fd, struct pfarg_pmd __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_pmd pmds[PFM_PMD_STK_ARG];
++	struct pfarg_pmd *req;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
++
++	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq)) {
++		PFM_DBG("invalid arg count %d", count);
++		return -EINVAL;
++	}
++
++	sz = count*sizeof(*ureq);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, sizeof(pmds), pmds, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (!ret)
++		ret = __pfm_write_pmds(ctx, req, count, 0);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	if (fptr)
++		kfree(fptr);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_read_pmds(int fd, struct pfarg_pmd __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_pmd pmds[PFM_PMD_STK_ARG];
++	struct pfarg_pmd *req;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
++
++	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
++		return -EINVAL;
++
++	sz = count*sizeof(*ureq);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, sizeof(pmds), pmds, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (!ret)
++		ret = __pfm_read_pmds(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	if (copy_to_user(ureq, req, sz))
++		ret = -EFAULT;
++
++	if (fptr)
++		kfree(fptr);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_restart(int fd)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	unsigned long flags;
++	int ret, fput_needed, unblock;
++
++	PFM_DBG("fd=%d", fd);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, 0, &flags);
++	if (!ret)
++		ret = __pfm_restart(ctx, &unblock);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++	/*
++	 * In per-thread mode with blocking notification, i.e.
++	 * ctx->flags.blocking=1, we need to defer issuing the
++	 * complete to unblock the blocked monitored thread.
++	 * Otherwise we have a potential deadlock due to a lock
++	 * inversion between the context lock and the task_rq_lock()
++	 * which can happen if one thread is in this call and the other
++	 * (the monitored thread) is in the context switch code.
++	 *
++	 * It is safe to access the context outside the critical section
++	 * because:
++	 * 	- we are protected by the fget_light(), thus the context
++	 * 	  cannot disappear
++	 */
++	if (ret == 0 && unblock)
++		complete(&ctx->restart_complete);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_stop(int fd)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	unsigned long flags;
++	int ret, fput_needed;
++	int release_info;
++
++	PFM_DBG("fd=%d", fd);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (!ret)
++		ret = __pfm_stop(ctx, &release_info);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++	/*
++	 * defer cancellation of timer to avoid race
++	 * with pfm_handle_switch_timeout()
++	 *
++	 * applies only when self-monitoring
++	 */
++	if (release_info & 0x2)
++		hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *ureq)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_start req;
++	unsigned long flags;
++	int ret, fput_needed;
++
++	PFM_DBG("fd=%d req=%p", fd, ureq);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	/*
++	 * the one argument is actually optional
++	 */
++	if (ureq && copy_from_user(&req, ureq, sizeof(req)))
++		return -EFAULT;
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
++	if (!ret)
++		ret = __pfm_start(ctx, ureq ? &req : NULL);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ureq)
++{
++	struct pfm_context *ctx;
++	struct task_struct *task;
++	struct file *filp;
++	unsigned long flags;
++	struct pfarg_load req;
++	int ret, fput_needed;
++
++	PFM_DBG("fd=%d req=%p", fd, ureq);
++
++	if (copy_from_user(&req, ureq, sizeof(req)))
++		return -EFAULT;
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	task = NULL;
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	/*
++	 * in per-thread mode (not self-monitoring), get a reference
++	 * on task to monitor. This must be done with interrupts enabled
++	 * Upon succesful return, refcount on task is increased.
++	 *
++	 * fget_light() is protecting the context.
++	 */
++	if (!ctx->flags.system) {
++		if (req.load_pid != current->pid) {
++			ret = pfm_get_task(ctx, req.load_pid, &task);
++			if (ret)
++				goto error;
++		} else
++			task = current;
++	}
++
++	/*
++	 * irqsave is required to avoid race in case context is already
++	 * loaded or with switch timeout in the case of self-monitoring
++	 */
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags);
++	if (!ret)
++		ret = __pfm_load_context(ctx, &req, task);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	/*
++	 * in per-thread mode (not self-monitoring), we need
++	 * to decrease refcount on task to monitor:
++	 *   - load successful: we have a reference to the task in ctx->task
++	 *   - load failed    : undo the effect of pfm_get_task()
++	 */
++	if (task && task != current)
++		put_task_struct(task);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_unload_context(int fd)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	unsigned long flags;
++	int ret, fput_needed;
++	int is_system, release_info = 0;
++	u32 cpu;
++
++	PFM_DBG("fd=%d", fd);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++	is_system = ctx->flags.system;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	cpu = ctx->cpu;
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED|PFM_CMD_UNLOAD, &flags);
++	if (!ret)
++		ret = __pfm_unload_context(ctx, &release_info);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	/*
++	 * cancel time now that context is unlocked
++	 * avoid race with pfm_handle_switch_timeout()
++	 */
++	if (release_info & 0x2) {
++		int r;
++		r = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
++		PFM_DBG("timeout cancel=%d", r);
++	}
++
++	if (release_info & 0x1)
++		pfm_release_session(is_system, cpu);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_create_evtsets(int fd, struct pfarg_setdesc __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_setdesc *req;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
++
++	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
++		return -EINVAL;
++
++	sz = count*sizeof(*ureq);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	/*
++	 * must mask interrupts because we do not know the state of context,
++	 * could be attached and we could be getting PMU interrupts. So
++	 * we mask and lock context and we check and possibly relax masking
++	 */
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags);
++	if (!ret)
++		ret = __pfm_create_evtsets(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	kfree(fptr);
++
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long  sys_pfm_getinfo_evtsets(int fd, struct pfarg_setinfo __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_setinfo *req;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
++
++	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
++		return -EINVAL;
++
++	sz = count*sizeof(*ureq);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	/*
++	 * this command operate even when context is loaded, so we need
++	 * to keep interrupts masked to avoid a race with PMU interrupt
++	 * which may switch active set
++	 */
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, 0, &flags);
++	if (!ret)
++		ret = __pfm_getinfo_evtsets(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	if (copy_to_user(ureq, req, sz))
++		ret = -EFAULT;
++
++	kfree(fptr);
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
++
++asmlinkage long sys_pfm_delete_evtsets(int fd, struct pfarg_setinfo __user *ureq, int count)
++{
++	struct pfm_context *ctx;
++	struct file *filp;
++	struct pfarg_setinfo *req;
++	void *fptr;
++	unsigned long flags;
++	size_t sz;
++	int ret, fput_needed;
++
++	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
++
++	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
++		return -EINVAL;
++
++	sz = count*sizeof(*ureq);
++
++	filp = fget_light(fd, &fput_needed);
++	if (unlikely(filp == NULL)) {
++		PFM_DBG("invalid fd %d", fd);
++		return -EBADF;
++	}
++
++	ctx = filp->private_data;
++	ret = -EBADF;
++
++	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
++		PFM_DBG("fd %d not related to perfmon", fd);
++		goto error;
++	}
++
++	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
++	if (ret)
++		goto error;
++
++	/*
++	 * must mask interrupts because we do not know the state of context,
++	 * could be attached and we could be getting PMU interrupts
++	 */
++	spin_lock_irqsave(&ctx->lock, flags);
++
++	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags);
++	if (!ret)
++		ret = __pfm_delete_evtsets(ctx, req, count);
++
++	spin_unlock_irqrestore(&ctx->lock, flags);
++
++	kfree(fptr);
++
++error:
++	fput_light(filp, fput_needed);
++	return ret;
++}
+--- /dev/null
++++ b/perfmon/perfmon_sysfs.c
+@@ -0,0 +1,677 @@
++/*
++ * perfmon_sysfs.c: perfmon2 sysfs interface
++ *
++ * This file implements the perfmon2 interface which
++ * provides access to the hardware performance counters
++ * of the host processor.
++ *
++ * The initial version of perfmon.c was written by
++ * Ganesh Venkitachalam, IBM Corp.
++ *
++ * Then it was modified for perfmon-1.x by Stephane Eranian and
++ * David Mosberger, Hewlett Packard Co.
++ *
++ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
++ * by Stephane Eranian, Hewlett Packard Co.
++ *
++ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
++ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
++ *                David Mosberger-Tang <davidm@hpl.hp.com>
++ *
++ * More information about perfmon available at:
++ * 	http://perfmon2.sf.net
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of version 2 of the GNU General Public
++ * License as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++#include <linux/kernel.h>
++#include <linux/perfmon.h>
++#include <linux/module.h> /* for EXPORT_SYMBOL */
++
++
++extern void pfm_reset_stats(int cpu);
++
++struct pfm_attribute {
++	struct attribute attr;
++	ssize_t (*show)(void *, char *);
++	ssize_t (*store)(void *, const char *, size_t);
++};
++#define to_attr(n) container_of(n, struct pfm_attribute, attr);
++
++#define PFM_RO_ATTR(_name) \
++struct pfm_attribute attr_##_name = __ATTR_RO(_name)
++
++#define PFM_RW_ATTR(_name,_mode,_show,_store) 			\
++struct pfm_attribute attr_##_name = __ATTR(_name,_mode,_show,_store);
++
++static int pfm_sysfs_init_done;	/* true when pfm_sysfs_init() completed */
++
++int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu);
++
++struct pfm_controls pfm_controls = {
++	.sys_group = PFM_GROUP_PERM_ANY,
++	.task_group = PFM_GROUP_PERM_ANY,
++	.arg_mem_max = PAGE_SIZE,
++	.smpl_buffer_mem_max = ~0,
++};
++EXPORT_SYMBOL(pfm_controls);
++
++DECLARE_PER_CPU(struct pfm_stats, pfm_stats);
++
++static struct kobject pfm_kernel_kobj, pfm_kernel_fmt_kobj;
++
++static ssize_t pfm_fmt_attr_show(struct kobject *kobj,
++		struct attribute *attr, char *buf)
++{
++	struct pfm_smpl_fmt *fmt = to_smpl_fmt(kobj);
++	struct pfm_attribute *attribute = to_attr(attr);
++	return attribute->show ? attribute->show(fmt, buf) : -EIO;
++}
++
++static ssize_t pfm_pmu_attr_show(struct kobject *kobj,
++		struct attribute *attr, char *buf)
++{
++	struct pfm_pmu_config *pmu= to_pmu(kobj);
++	struct pfm_attribute *attribute = to_attr(attr);
++	return attribute->show ? attribute->show(pmu, buf) : -EIO;
++}
++
++static ssize_t pfm_regs_attr_show(struct kobject *kobj,
++		struct attribute *attr, char *buf)
++{
++	struct pfm_regmap_desc *reg = to_reg(kobj);
++	struct pfm_attribute *attribute = to_attr(attr);
++	return attribute->show ? attribute->show(reg, buf) : -EIO;
++}
++
++static struct sysfs_ops pfm_fmt_sysfs_ops = {
++	.show = pfm_fmt_attr_show
++};
++
++static struct sysfs_ops pfm_pmu_sysfs_ops = {
++	.show = pfm_pmu_attr_show
++};
++
++static struct sysfs_ops pfm_regs_sysfs_ops = {
++	.show  = pfm_regs_attr_show
++};
++
++static struct kobj_type pfm_fmt_ktype = {
++	.sysfs_ops = &pfm_fmt_sysfs_ops,
++};
++
++static struct kobj_type pfm_pmu_ktype = {
++	.sysfs_ops = &pfm_pmu_sysfs_ops,
++};
++
++static struct kobj_type pfm_regs_ktype = {
++	.sysfs_ops = &pfm_regs_sysfs_ops,
++};
++
++static ssize_t version_show(void *info, char *buf)
++{
++	return snprintf(buf, PAGE_SIZE, "%u.%u\n",  PFM_VERSION_MAJ, PFM_VERSION_MIN);
++}
++
++static ssize_t task_sessions_count_show(void *info, char *buf)
++{
++	return pfm_sysfs_session_show(buf, PAGE_SIZE, 0);
++}
++
++static ssize_t sys_sessions_count_show(void *info, char *buf)
++{
++	return pfm_sysfs_session_show(buf, PAGE_SIZE, 1);
++}
++
++static ssize_t smpl_buffer_mem_cur_show(void *info, char *buf)
++{
++	return pfm_sysfs_session_show(buf, PAGE_SIZE, 2);
++}
++
++static ssize_t debug_show(void *info, char *buf)
++{
++	return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.debug);
++}
++
++static ssize_t debug_store(void *info, const char *buf, size_t sz)
++{
++	int d, i;
++
++	if (sscanf(buf,"%d", &d) != 1)
++		return -EINVAL;
++
++	pfm_controls.debug = d;
++
++	if (d == 0) {
++		for_each_online_cpu(i) {
++			pfm_reset_stats(i);
++		}
++	}
++	return sz;
++}
++
++static ssize_t debug_ovfl_show(void *info, char *buf)
++{
++	return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.debug_ovfl);
++}
++
++static ssize_t debug_ovfl_store(void *info, const char *buf, size_t sz)
++{
++	int d;
++
++	if (sscanf(buf,"%d", &d) != 1)
++		return -EINVAL;
++
++	pfm_controls.debug_ovfl = d;
++
++	return strnlen(buf, PAGE_SIZE);
++}
++
++static ssize_t reset_stats_show(void *info, char *buf)
++{
++	buf[0]='0';
++	buf[1]='\0';
++	return strnlen(buf, PAGE_SIZE);
++}
++
++static ssize_t reset_stats_store(void *info, const char *buf, size_t count)
++{
++	int i;
++
++	for_each_online_cpu(i) {
++		pfm_reset_stats(i);
++	}
++	return count;
++}
++
++static ssize_t sys_group_show(void *info, char *buf)
++{
++	return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.sys_group);
++}
++
++static ssize_t sys_group_store(void *info, const char *buf, size_t sz)
++{
++	int d;
++
++	if (sscanf(buf,"%d", &d) != 1)
++		return -EINVAL;
++
++	pfm_controls.sys_group = d;
++
++	return strnlen(buf, PAGE_SIZE);
++}
++
++static ssize_t task_group_show(void *info, char *buf)
++{
++	return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.task_group);
++}
++
++static ssize_t task_group_store(void *info, const char *buf, size_t sz)
++{
++	int d;
++
++	if (sscanf(buf,"%d", &d) != 1)
++		return -EINVAL;
++
++	pfm_controls.task_group = d;
++
++	return strnlen(buf, PAGE_SIZE);
++}
++
++static ssize_t buf_size_show(void *info, char *buf)
++{
++	return snprintf(buf, PAGE_SIZE, "%zu\n", pfm_controls.smpl_buffer_mem_max);
++}
++
++static ssize_t buf_size_store(void *info, const char *buf, size_t sz)
++{
++	size_t d;
++
++	if (sscanf(buf,"%zu", &d) != 1)
++		return -EINVAL;
++	/*
++	 * we impose a page as the minimum
++	 */
++	if (d < PAGE_SIZE)
++		return -EINVAL;
++
++	pfm_controls.smpl_buffer_mem_max = d;
++
++	return strnlen(buf, PAGE_SIZE);
++}
++
++static ssize_t arg_size_show(void *info, char *buf)
++{
++	return snprintf(buf, PAGE_SIZE, "%zu\n", pfm_controls.arg_mem_max);
++}
++
++static ssize_t arg_size_store(void *info, const char *buf, size_t sz)
++{
++	size_t d;
++
++	if (sscanf(buf,"%zu", &d) != 1)
++		return -EINVAL;
++
++	/*
++	 * we impose a page as the minimum.
++	 *
++	 * This limit may be smaller than the stack buffer
++	 * available and that is fine.
++	 */
++	if (d < PAGE_SIZE)
++		return -EINVAL;
++
++	pfm_controls.arg_mem_max = d;
++
++	return strnlen(buf, PAGE_SIZE);
++}
++
++/*
++ * /sys/kernel/perfmon attributes
++ */
++static PFM_RO_ATTR(version);
++static PFM_RO_ATTR(task_sessions_count);
++static PFM_RO_ATTR(sys_sessions_count);
++static PFM_RO_ATTR(smpl_buffer_mem_cur);
++
++static PFM_RW_ATTR(debug, 0644, debug_show, debug_store);
++static PFM_RW_ATTR(debug_ovfl, 0644, debug_ovfl_show, debug_ovfl_store);
++static PFM_RW_ATTR(reset_stats, 0644, reset_stats_show, reset_stats_store);
++static PFM_RW_ATTR(sys_group, 0644, sys_group_show, sys_group_store);
++static PFM_RW_ATTR(task_group, 0644, task_group_show, task_group_store);
++static PFM_RW_ATTR(smpl_buffer_mem_max, 0644, buf_size_show, buf_size_store);
++static PFM_RW_ATTR(arg_mem_max, 0644, arg_size_show, arg_size_store);
++
++static struct attribute *pfm_kernel_attrs[] = {
++	&attr_version.attr,
++	&attr_task_sessions_count.attr,
++	&attr_sys_sessions_count.attr,
++	&attr_smpl_buffer_mem_cur.attr,
++	&attr_debug.attr,
++	&attr_debug_ovfl.attr,
++	&attr_reset_stats.attr,
++	&attr_sys_group.attr,
++	&attr_task_group.attr,
++	&attr_smpl_buffer_mem_max.attr,
++	&attr_arg_mem_max.attr,
++	NULL
++};
++
++static struct attribute_group pfm_kernel_attr_group = {
++	.attrs = pfm_kernel_attrs,
++};
++
++int __init pfm_init_sysfs(void)
++{
++	int ret;
++
++	kobject_init(&pfm_kernel_kobj);
++	kobject_init(&pfm_kernel_fmt_kobj);
++
++	pfm_kernel_kobj.parent = &kernel_subsys.kobj;
++	kobject_set_name(&pfm_kernel_kobj, "perfmon");
++
++	pfm_kernel_fmt_kobj.parent = &pfm_kernel_kobj;
++	kobject_set_name(&pfm_kernel_fmt_kobj, "formats");
++
++	ret = kobject_add(&pfm_kernel_kobj);
++	if (ret) {
++		PFM_INFO("cannot add kernel object: %d", ret);
++		goto error;
++	}
++
++	ret = kobject_add(&pfm_kernel_fmt_kobj);
++	if (ret) {
++		PFM_INFO("cannot add fmt object: %d", ret);
++		goto error_fmt;
++	}
++
++	ret = sysfs_create_group(&pfm_kernel_kobj, &pfm_kernel_attr_group);
++	if (ret) {
++		PFM_INFO("cannot create kernel group");
++		goto error_group;
++	}
++
++	if (pfm_pmu_conf)
++		pfm_sysfs_add_pmu(pfm_pmu_conf);
++	/*
++	 * must be set before builtin_fmt and
++	 * add_pmu() calls
++	 */
++	pfm_sysfs_init_done = 1;
++	pfm_sysfs_builtin_fmt_add();
++	return 0;
++
++error_group:
++	kobject_del(&pfm_kernel_fmt_kobj);
++error_fmt:
++	kobject_del(&pfm_kernel_kobj);
++error:
++	pfm_sysfs_init_done = 0;
++	return ret;
++}
++
++/*
++ * per-cpu perfmon stats attributes
++ */
++#define PFM_DECL_STATS_ATTR(name) \
++static ssize_t name##_show(void *info, char *buf) \
++{ \
++	struct pfm_stats *st = info;\
++	return snprintf(buf, PAGE_SIZE, "%llu\n", \
++			(unsigned long long)st->name); \
++} \
++static PFM_RO_ATTR(name)
++
++/*
++ * per-reg attributes
++ */
++static ssize_t name_show(void *info, char *buf)
++{
++	struct pfm_regmap_desc *reg = info;
++	return snprintf(buf, PAGE_SIZE, "%s\n", reg->desc);
++}
++static PFM_RO_ATTR(name);
++
++static ssize_t dfl_val_show(void *info, char *buf)
++{
++	struct pfm_regmap_desc *reg = info;
++	return snprintf(buf, PAGE_SIZE, "0x%llx\n",
++			(unsigned long long)reg->dfl_val);
++}
++static PFM_RO_ATTR(dfl_val);
++
++static ssize_t rsvd_msk_show(void *info, char *buf)
++{
++	struct pfm_regmap_desc *reg = info;
++	return snprintf(buf, PAGE_SIZE, "0x%llx\n",
++			(unsigned long long)reg->rsvd_msk);
++}
++static PFM_RO_ATTR(rsvd_msk);
++
++static ssize_t width_show(void *info, char *buf)
++{
++	struct pfm_regmap_desc *reg = info;
++	int w;
++
++	w = (reg->type & PFM_REG_C64) ? pfm_pmu_conf->counter_width : 64;
++
++	return snprintf(buf, PAGE_SIZE, "%d\n", w);
++}
++static PFM_RO_ATTR(width);
++
++
++static ssize_t addr_show(void *info, char *buf)
++{
++	struct pfm_regmap_desc *reg = info;
++	return snprintf(buf, PAGE_SIZE, "0x%lx\n", reg->hw_addr);
++}
++static PFM_RO_ATTR(addr);
++
++static ssize_t fmt_version_show(void *data, char *buf)
++{
++	struct pfm_smpl_fmt *fmt = data;
++
++	return snprintf(buf, PAGE_SIZE, "%u.%u",
++		fmt->fmt_version >>16 & 0xffff,
++		fmt->fmt_version & 0xffff);
++}
++
++/*
++ * do not use predefined macros because of name conflict
++ * with /sys/kernel/perfmon/version
++ */
++struct pfm_attribute attr_fmt_version = {
++	.attr	= { .name = "version", .mode = 0444 },
++	.show	= fmt_version_show,
++};
++
++static struct attribute *pfm_fmt_attrs[] = {
++	&attr_fmt_version.attr,
++	NULL
++};
++
++static struct attribute_group pfm_fmt_attr_group = {
++	.attrs = pfm_fmt_attrs,
++};
++
++
++/*
++ * when a sampling format module is inserted, we populate
++ * sysfs with some information
++ */
++int pfm_sysfs_add_fmt(struct pfm_smpl_fmt *fmt)
++{
++	int ret;
++
++	if (pfm_sysfs_init_done == 0)
++		return 0;
++
++	fmt->kobj.ktype = &pfm_fmt_ktype;
++	kobject_init(&fmt->kobj);
++
++	kobject_set_name(&fmt->kobj, fmt->fmt_name);
++	//kobj_set_kset_s(fmt, pfm_fmt_subsys);
++	fmt->kobj.parent = &pfm_kernel_fmt_kobj;
++
++	ret = kobject_add(&fmt->kobj);
++	if (ret)
++		return ret;
++
++	ret = sysfs_create_group(&fmt->kobj, &pfm_fmt_attr_group);
++	if (ret)
++		kobject_del(&fmt->kobj);
++
++	return ret;
++}
++
++/*
++ * when a sampling format module is removed, its information
++ * must also be removed from sysfs
++ */
++int pfm_sysfs_remove_fmt(struct pfm_smpl_fmt *fmt)
++{
++	if (pfm_sysfs_init_done == 0)
++		return 0;
++
++	sysfs_remove_group(&fmt->kobj, &pfm_fmt_attr_group);
++	kobject_del(&fmt->kobj);
++
++	return 0;
++}
++
++static struct attribute *pfm_reg_attrs[] = {
++	&attr_name.attr,
++	&attr_dfl_val.attr,
++	&attr_rsvd_msk.attr,
++	&attr_width.attr,
++	&attr_addr.attr,
++	NULL
++};
++
++static struct attribute_group pfm_reg_attr_group = {
++	.attrs = pfm_reg_attrs,
++};
++
++static ssize_t model_show(void *info, char *buf)
++{
++	struct pfm_pmu_config *p = info;
++	return snprintf(buf, PAGE_SIZE, "%s\n", p->pmu_name);
++}
++static PFM_RO_ATTR(model);
++
++static struct attribute *pfm_pmu_desc_attrs[] = {
++	&attr_model.attr,
++	NULL
++};
++
++static struct attribute_group pfm_pmu_desc_attr_group = {
++	.attrs = pfm_pmu_desc_attrs,
++};
++
++static int pfm_sysfs_add_pmu_regs(struct pfm_pmu_config *pmu)
++{
++	struct pfm_regmap_desc *reg;
++	unsigned int i, k;
++	int ret;
++	char reg_name[8];
++
++	reg = pmu->pmc_desc;
++	for(i=0; i < pmu->num_pmc_entries; i++, reg++) {
++
++		if (!(reg->type & PFM_REG_I))
++			continue;
++
++		reg->kobj.ktype = &pfm_regs_ktype;
++		kobject_init(&reg->kobj);
++
++		reg->kobj.parent = &pmu->kobj;
++		snprintf(reg_name, sizeof(reg_name), "pmc%u", i);
++		kobject_set_name(&reg->kobj, reg_name);
++		//kobj_set_kset_s(reg, pfm_regs_subsys);
++
++		ret = kobject_add(&reg->kobj);
++		if (ret)
++			goto undo_pmcs;
++
++		ret = sysfs_create_group(&reg->kobj, &pfm_reg_attr_group);
++		if (ret) {
++			kobject_del(&reg->kobj);
++			goto undo_pmcs;
++		}
++	}
++
++	reg = pmu->pmd_desc;
++	for(i=0; i < pmu->num_pmd_entries; i++, reg++) {
++
++		if (!(reg->type & PFM_REG_I))
++			continue;
++
++		reg->kobj.ktype = &pfm_regs_ktype;
++		kobject_init(&reg->kobj);
++
++		reg->kobj.parent = &pmu->kobj;
++		snprintf(reg_name, sizeof(reg_name), "pmd%u", i);
++		kobject_set_name(&reg->kobj, reg_name);
++		//kobj_set_kset_s(reg, pfm_regs_subsys);
++
++		ret = kobject_add(&reg->kobj);
++		if (ret)
++			goto undo_pmds;
++
++		ret = sysfs_create_group(&reg->kobj, &pfm_reg_attr_group);
++		if (ret) {
++			kobject_del(&reg->kobj);
++			goto undo_pmds;
++		}
++	}
++	return 0;
++undo_pmds:
++	reg = pmu->pmd_desc;
++	for(k = 0; k < i; k++, reg++) {
++		if (!(reg->type & PFM_REG_I))
++			continue;
++		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
++		kobject_del(&reg->kobj);
++	}
++	i = pmu->num_pmc_entries;
++	/* fall through */
++undo_pmcs:
++	reg = pmu->pmc_desc;
++	for(k=0; k < i; k++, reg++) {
++		if (!(reg->type & PFM_REG_I))
++			continue;
++		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
++		kobject_del(&reg->kobj);
++	}
++	return ret;
++}
++
++static int pfm_sysfs_del_pmu_regs(struct pfm_pmu_config *pmu)
++{
++	struct pfm_regmap_desc *reg;
++	unsigned int i;
++
++	reg = pmu->pmc_desc;
++	for(i=0; i < pmu->regs.max_pmc; i++, reg++) {
++
++		if (!(reg->type & PFM_REG_I))
++			continue;
++
++		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
++		kobject_del(&reg->kobj);
++	}
++
++	reg = pmu->pmd_desc;
++	for(i=0; i < pmu->regs.max_pmd; i++, reg++) {
++
++		if (!(reg->type & PFM_REG_I))
++			continue;
++
++		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
++		kobject_del(&reg->kobj);
++	}
++	return 0;
++}
++
++/*
++ * when a PMU description module is inserted, we create
++ * a pmu_desc subdir in sysfs and we populate it with
++ * PMU specific information, such as register mappings
++ */
++int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu)
++{
++	int ret;
++
++	if (pfm_sysfs_init_done == 0)
++		return 0;
++
++	pmu->kobj.ktype = &pfm_pmu_ktype;
++	kobject_init(&pmu->kobj);
++	kobject_set_name(&pmu->kobj, "pmu_desc");
++	//kobj_set_kset_s(pmu, pfm_pmu_subsys);
++	pmu->kobj.parent = &pfm_kernel_kobj;
++
++	ret = kobject_add(&pmu->kobj);
++	if (ret)
++		return ret;
++
++	ret = sysfs_create_group(&pmu->kobj, &pfm_pmu_desc_attr_group);
++	if (ret)
++		kobject_del(&pmu->kobj);
++
++	ret = pfm_sysfs_add_pmu_regs(pmu);
++	if (ret) {
++		sysfs_remove_group(&pmu->kobj, &pfm_pmu_desc_attr_group);
++		kobject_del(&pmu->kobj);
++	}
++	return ret;
++}
++
++/*
++ * when a PMU description module is removed, we also remove
++ * all its information from sysfs, i.e., the pmu_desc subdir
++ * disappears
++ */
++int pfm_sysfs_remove_pmu(struct pfm_pmu_config *pmu)
++{
++	if (pfm_sysfs_init_done == 0)
++		return 0;
++
++	pfm_sysfs_del_pmu_regs(pmu);
++	sysfs_remove_group(&pmu->kobj, &pfm_pmu_desc_attr_group);
++	kobject_del(&pmu->kobj);
++
++	return 0;
++}
diff -Naur linux-2.6.25-org/patches/ps3-hacks/ps3-check-cd-eject.diff linux-2.6.25-id/patches/ps3-hacks/ps3-check-cd-eject.diff
--- linux-2.6.25-org/patches/ps3-hacks/ps3-check-cd-eject.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-hacks/ps3-check-cd-eject.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,23 @@
+Temporary hack to investigate unexpected CD eject during playback.
+
+Hacked-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ drivers/block/ps3_storage.c |    7 +++++++
+ 1 file changed, 7 insertions(+)
+
+--- ps3-linux-dev.orig/drivers/block/ps3_storage.c
++++ ps3-linux-dev/drivers/block/ps3_storage.c
+@@ -156,6 +156,13 @@ static void ps3_stor_process_srb(struct 
+ 	command_handler = dev_info->handler_info[srb->cmnd[0]].cmnd_handler;
+ 
+ 	if (command_handler) {
++		if (srb->cmnd[0] == TEST_UNIT_READY)
++			printk("%s: (%d) TEST_UNIT_READY\n", __func__,
++				current->pid);
++		if (srb->cmnd[0] == START_STOP)
++			//&& cmnd[4] == 0x02) /* eject */
++			printk("%s: (%d) START_STOP: %xh\n", __func__,
++				current->pid, srb->cmnd[4]);
+ 		(*command_handler)(dev_info, srb);
+ 	} else {
+ 		srb->result = (DID_ERROR << 16);
diff -Naur linux-2.6.25-org/patches/ps3-hacks/ps3-debug-zimage-initrd-in-userspace.diff linux-2.6.25-id/patches/ps3-hacks/ps3-debug-zimage-initrd-in-userspace.diff
--- linux-2.6.25-org/patches/ps3-hacks/ps3-debug-zimage-initrd-in-userspace.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-hacks/ps3-debug-zimage-initrd-in-userspace.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,77 @@
+From: David Woodhouse <dwmw2@infradead.org>
+
+FWIW this hack makes it slightly easier to debug the zImage.initrd in
+userspace... and if you run 'gdb zImage.initrd 3>dtblob' you can call
+wtree() from gdb to make it dump the device-tree to a file which can
+then be inspected with 'dtc -I dtb'.
+
+--- a/arch/powerpc/boot/Makefile
++++ b/arch/powerpc/boot/Makefile
+@@ -23,7 +23,7 @@
+ all: $(obj)/zImage
+ 
+ HOSTCC		:= gcc
+-BOOTCFLAGS	:= $(HOSTCFLAGS) -fno-builtin -nostdinc -isystem \
++BOOTCFLAGS	:= $(HOSTCFLAGS) -fno-builtin -nostdinc -g -isystem \
+ 		   $(shell $(CROSS32CC) -print-file-name=include) -fPIC
+ BOOTAFLAGS	:= -D__ASSEMBLY__ $(BOOTCFLAGS) -nostdinc
+ 
+--- a/arch/powerpc/boot/ps3-head.S
++++ b/arch/powerpc/boot/ps3-head.S
+@@ -45,3 +45,21 @@ __system_reset_entry:
+ 	b	smp_secondary_hold
+ 1:
+ 	b	_zimage_start
++
++.globl printhack
++printhack:
++	mr	5,4
++	mr	4,3
++	li	0,4
++	li	3,1
++	sc
++	blr
++
++	.globl wtree
++wtree:
++	mr	5,4
++	mr	4,3
++	li	0,4
++	li	3,3
++	sc
++	blr
+--- a/arch/powerpc/boot/ps3.c
++++ b/arch/powerpc/boot/ps3.c
+@@ -42,6 +42,8 @@ extern char _dtb_end[];
+ 
+ PLATFORM_STACK(4096);
+ 
++unsigned char fish[0x4000000];
++
+ static void ps3_console_write(const char *buf, int len)
+ {
+ }
+@@ -52,18 +54,20 @@ static void ps3_exit(void)
+ 	lv1_panic(0); /* zero = no reboot */
+ 	while(1);
+ }
++extern void printhack(const char *buf, int len);
+ 
+ int platform_init(void)
+ {
+ 	const u32 heapsize = 0x4000000 - (u32)_end; /* 64M */
+ 
+-	console_ops.write = ps3_console_write;
++	console_ops.write = printhack;//ps3_console_write;
+ 	platform_ops.secondary_release = smp_secondary_release;
+ 	platform_ops.exit = ps3_exit;
+ 
+ 	printf("\n-- PS3 bootwrapper --\n");
+ 
+-	simple_alloc_init(_end, heapsize, 32, 64);
++	//simple_alloc_init(_end, heapsize, 32, 64);
++	simple_alloc_init(fish, 0x4000000, 32, 64);
+ 	platform_ops.vmlinux_alloc = platform_ops.malloc;
+ 
+ 	ft_init(_dtb_start, 0, 4);
+
diff -Naur linux-2.6.25-org/patches/ps3-hacks/ps3-fix-spu-build-errors.diff linux-2.6.25-id/patches/ps3-hacks/ps3-fix-spu-build-errors.diff
--- linux-2.6.25-org/patches/ps3-hacks/ps3-fix-spu-build-errors.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-hacks/ps3-fix-spu-build-errors.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,168 @@
+---
+ arch/powerpc/platforms/cell/spu_base.c |  141 ---------------------------------
+ 1 file changed, 141 deletions(-)
+
+--- a/arch/powerpc/platforms/cell/spu_base.c
++++ b/arch/powerpc/platforms/cell/spu_base.c
+@@ -36,7 +36,6 @@
+ #include <asm/spu_priv1.h>
+ #include <asm/xmon.h>
+ #include <asm/prom.h>
+-#include "spu_priv1_mmio.h"
+ 
+ const struct spu_management_ops *spu_management_ops;
+ EXPORT_SYMBOL_GPL(spu_management_ops);
+@@ -636,138 +635,6 @@ static ssize_t spu_stat_show(struct sys_
+ 
+ static SYSDEV_ATTR(stat, 0644, spu_stat_show, NULL);
+ 
+-/* Hardcoded affinity idxs for QS20 */
+-#define SPES_PER_BE 8
+-static int QS20_reg_idxs[SPES_PER_BE] =   { 0, 2, 4, 6, 7, 5, 3, 1 };
+-static int QS20_reg_memory[SPES_PER_BE] = { 1, 1, 0, 0, 0, 0, 0, 0 };
+-
+-static struct spu *spu_lookup_reg(int node, u32 reg)
+-{
+-	struct spu *spu;
+-
+-	list_for_each_entry(spu, &cbe_spu_info[node].spus, cbe_list) {
+-		if (*(u32 *)get_property(spu_devnode(spu), "reg", NULL) == reg)
+-			return spu;
+-	}
+-	return NULL;
+-}
+-
+-static void init_aff_QS20_harcoded(void)
+-{
+-	int node, i;
+-	struct spu *last_spu, *spu;
+-	u32 reg;
+-
+-	for (node = 0; node < MAX_NUMNODES; node++) {
+-		last_spu = NULL;
+-		for (i = 0; i < SPES_PER_BE; i++) {
+-			reg = QS20_reg_idxs[i];
+-			spu = spu_lookup_reg(node, reg);
+-			if (!spu)
+-				continue;
+-			spu->has_mem_affinity = QS20_reg_memory[reg];
+-			if (last_spu)
+-				list_add_tail(&spu->aff_list,
+-						&last_spu->aff_list);
+-			last_spu = spu;
+-		}
+-	}
+-}
+-
+-static int of_has_vicinity(void)
+-{
+-	struct spu* spu;
+-
+-	spu = list_entry(cbe_spu_info[0].spus.next, struct spu, cbe_list);
+-	return of_find_property(spu_devnode(spu), "vicinity", NULL) != NULL;
+-}
+-
+-static struct spu *aff_devnode_spu(int cbe, struct device_node *dn)
+-{
+-	struct spu *spu;
+-
+-	list_for_each_entry(spu, &cbe_spu_info[cbe].spus, cbe_list)
+-		if (spu_devnode(spu) == dn)
+-			return spu;
+-	return NULL;
+-}
+-
+-static struct spu *
+-aff_node_next_to(int cbe, struct device_node *target, struct device_node *avoid)
+-{
+-	struct spu *spu;
+-	const phandle *vic_handles;
+-	int lenp, i;
+-
+-	list_for_each_entry(spu, &cbe_spu_info[cbe].spus, cbe_list) {
+-		if (spu_devnode(spu) == avoid)
+-			continue;
+-		vic_handles = get_property(spu_devnode(spu), "vicinity", &lenp);
+-		for (i=0; i < (lenp / sizeof(phandle)); i++) {
+-			if (vic_handles[i] == target->linux_phandle)
+-				return spu;
+-		}
+-	}
+-	return NULL;
+-}
+-
+-static void init_aff_fw_vicinity_node(int cbe)
+-{
+-	struct spu *spu, *last_spu;
+-	struct device_node *vic_dn, *last_spu_dn;
+-	phandle avoid_ph;
+-	const phandle *vic_handles;
+-	const char *name;
+-	int lenp, i, added, mem_aff;
+-
+-	last_spu = list_entry(cbe_spu_info[cbe].spus.next, struct spu, cbe_list);
+-	avoid_ph = 0;
+-	for (added = 1; added < cbe_spu_info[cbe].n_spus; added++) {
+-		last_spu_dn = spu_devnode(last_spu);
+-		vic_handles = get_property(last_spu_dn, "vicinity", &lenp);
+-
+-		for (i = 0; i < (lenp / sizeof(phandle)); i++) {
+-			if (vic_handles[i] == avoid_ph)
+-				continue;
+-
+-			vic_dn = of_find_node_by_phandle(vic_handles[i]);
+-			if (!vic_dn)
+-				continue;
+-
+-			name = get_property(vic_dn, "name", NULL);
+-			if (strcmp(name, "spe") == 0) {
+-				spu = aff_devnode_spu(cbe, vic_dn);
+-				avoid_ph = last_spu_dn->linux_phandle;
+-			}
+-			else {
+-				mem_aff = strcmp(name, "mic-tm") == 0;
+-				spu = aff_node_next_to(cbe, vic_dn, last_spu_dn);
+-				if (!spu)
+-					continue;
+-				if (mem_aff) {
+-					last_spu->has_mem_affinity = 1;
+-					spu->has_mem_affinity = 1;
+-				}
+-				avoid_ph = vic_dn->linux_phandle;
+-			}
+-			list_add_tail(&spu->aff_list, &last_spu->aff_list);
+-			last_spu = spu;
+-			break;
+-		}
+-	}
+-}
+-
+-static void init_aff_fw_vicinity(void)
+-{
+-	int cbe;
+-
+-	/* sets has_mem_affinity for each spu, as long as the
+-	 * spu->aff_list list, linking each spu to its neighbors
+-	 */
+-	for (cbe = 0; cbe < MAX_NUMNODES; cbe++)
+-		init_aff_fw_vicinity_node(cbe);
+-}
+-
+ static int __init init_spu_base(void)
+ {
+ 	int i, ret = 0;
+@@ -811,14 +678,6 @@ static int __init init_spu_base(void)
+ 	mutex_unlock(&spu_full_list_mutex);
+ 	spu_add_sysdev_attr(&attr_stat);
+ 
+-	if (of_has_vicinity()) {
+-		init_aff_fw_vicinity();
+-	} else {
+-		long root = of_get_flat_dt_root();
+-		if (of_flat_dt_is_compatible(root, "IBM,CPBW-1.0"))
+-			init_aff_QS20_harcoded();
+-	}
+-
+ 	return 0;
+ 
+  out_unregister_sysdev_class:
diff -Naur linux-2.6.25-org/patches/ps3-hacks/ps3-free-irq-check.patch linux-2.6.25-id/patches/ps3-hacks/ps3-free-irq-check.patch
--- linux-2.6.25-org/patches/ps3-hacks/ps3-free-irq-check.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-hacks/ps3-free-irq-check.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,28 @@
+A debugging hack.
+
+Check to make sure the dev_id passed to request_irq() and
+free_irq() match.  Since the args are void pointers, it is
+easy to pass the wrong variable.
+
+Hacked-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ kernel/irq/manage.c |    7 ++++++-
+ 1 file changed, 6 insertions(+), 1 deletion(-)
+
+--- a/kernel/irq/manage.c
++++ b/kernel/irq/manage.c
+@@ -436,8 +436,13 @@ void free_irq(unsigned int irq, void *de
+ 			struct irqaction **pp = p;
+ 
+ 			p = &action->next;
+-			if (action->dev_id != dev_id)
++			if (action->dev_id != dev_id) {
++				pr_debug("%s:%d: irq %u bad dev_id: request_irq(%p) != "
++					"free_irq(%p)\n" , __func__, __LINE__, irq, action->dev_id,
++					 dev_id);
++				BUG();
+ 				continue;
++			}
+ 
+ 			/* Found it - now remove it from the list of entries */
+ 			*pp = action->next;
diff -Naur linux-2.6.25-org/patches/ps3-hacks/ps3-oprofile-test-script.patch linux-2.6.25-id/patches/ps3-hacks/ps3-oprofile-test-script.patch
--- linux-2.6.25-org/patches/ps3-hacks/ps3-oprofile-test-script.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-hacks/ps3-oprofile-test-script.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,111 @@
+---
+ scripts/oprofile-test |  104 ++++++++++++++++++++++++++++++++++++++++++++++++++
+ 1 file changed, 104 insertions(+)
+
+--- /dev/null
++++ b/scripts/oprofile-test
+@@ -0,0 +1,104 @@
++#!/bin/sh -x
++
++name=oprofile
++prefixes="/usr"
++
++for prefix in ${prefixes}; do
++
++	if [ ! -d ${prefix} ]; then
++		echo "$0: bad prefix: '${prefix}'"
++		echo "$0: -- ${name} FAILED --"
++		exit 1
++	fi
++
++	dir=${prefix}/bin
++
++	if [ ! -d ${dir} ]; then
++		echo "$0: can't find root dir: '${dir}'"
++		echo "$0: -- ${name} FAILED --"
++		exit 1
++	fi
++
++	prog=${dir}/opcontrol
++
++	if [ ! -f ${prog} ] ; then
++		echo "$0: can't find progam: '${prog}'"
++		echo "$0: -- ${name} FAILED --"
++		exit 1
++	fi
++
++	prog=${dir}/opreport
++
++	if [ ! -f ${prog} ] ; then
++		echo "$0: can't find progam: '${prog}'"
++		echo "$0: -- ${name} FAILED --"
++		exit 1
++	fi
++
++	time=`date +%y.%m.%d-%H.%M.%S`
++	log=/root/${name}-dump-`hostname`-`uname -r`-${time}.dmp
++
++	echo "$0: root dir = '${dir}'"
++	echo "$0: log = '$log'"
++
++	${dir}/opcontrol --shutdown
++	${dir}/opcontrol --reset
++	${dir}/opcontrol --reset
++	rm -rf /var/lib/oprofile/*
++
++	if [ ! -f /boot/vmlinux ] ; then
++		echo "$0: can't find: '/boot/vmlinux'"
++		echo "$0: -- ${name} FAILED --"
++		exit 1
++	fi
++
++	${dir}/opcontrol --vmlinux=/boot/vmlinux
++
++	if [ $? -ne 0 ]; then
++		echo "$0: opcontrol --vmlinux failed."
++		echo "$0: -- ${name} FAILED --"
++		exit -1
++	fi
++
++	${dir}/opcontrol --event=cache_hit:10000 --event=cache_miss:10000
++
++	if [ $? -ne 0 ]; then
++		echo "$0: opcontrol --event=cache_hit:10000 --event=cache_miss:10000 failed."
++	fi
++
++	${dir}/opcontrol --verbose --start
++
++	if [ $? -ne 0 ]; then
++		echo "$0: opcontrol --start failed."
++		echo "$0: -- ${name} FAILED --"
++		exit -1
++	fi
++
++	echo sleeping 25 sec...
++	sleep 25
++
++	${dir}/opcontrol --dump
++
++	if [ $? -ne 0 ]; then
++		echo "$0: opcontrol --dump failed."
++		echo "$0: -- ${name} FAILED --"
++		exit -1
++	fi
++
++	${dir}/opreport
++
++	if [ $? -ne 0 ]; then
++		echo "$0: opreport failed."
++		echo "$0: -- ${name} FAILED --"
++		exit -1
++	fi
++
++	${dir}/opcontrol --shutdown
++	${dir}/opcontrol --reset
++	${dir}/opcontrol --reset
++	rm -rf /var/lib/oprofile/*
++
++	echo "$0: ++ ${name} OK ++"
++done
++
++exit 0
diff -Naur linux-2.6.25-org/patches/ps3-hacks/sync-ps3fb-on-panic.diff linux-2.6.25-id/patches/ps3-hacks/sync-ps3fb-on-panic.diff
--- linux-2.6.25-org/patches/ps3-hacks/sync-ps3fb-on-panic.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-hacks/sync-ps3fb-on-panic.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,57 @@
+Subject: [PATCH] Sync PS3 framebuffer on panic.
+From: David Woodhouse <dwmw2@infradead.org>
+
+There's not a lot of point in printing a nice message about the panic
+unless we then ask the hypervisor to update the screen for us. It's a
+hack, but in the absence of sensible devel units with a real console
+(thanks, Sony!) it's the best we can do.
+
+Signed-off-by: David Woodhouse <dwmw2@infradead.org>
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/setup.c |   14 ++++++++++++++
+ drivers/video/ps3fb.c              |    2 +-
+ 2 files changed, 15 insertions(+), 1 deletion(-)
+
+--- ps3-linux-2.6.21-rc5.orig/arch/powerpc/platforms/ps3/setup.c
++++ ps3-linux-2.6.21-rc5/arch/powerpc/platforms/ps3/setup.c
+@@ -46,6 +46,8 @@
+ static void smp_send_stop(void) {}
+ #endif
+ 
++extern void ps3fb_sync(int);
++
+ int ps3_get_firmware_version(union ps3_firmware_version *v)
+ {
+ 	int result = lv1_get_version_info(&v->raw);
+@@ -89,8 +91,18 @@ static void ps3_power_off(void)
+ 
+ static void ps3_panic(char *str)
+ {
++	static int panicked = 0;
++
+ 	DBG("%s:%d %s\n", __func__, __LINE__, str);
+ 
++#ifdef CONFIG_FB_PS3
++	/* Sync framebuffer to show panic info. */
++	if (!panicked) {
++		panicked = 1;
++		ps3fb_sync(0);
++	}
++#endif
++
+ 	smp_send_stop();
+ 	printk("\n");
+ 	printk("   System does not reboot automatically.\n");
+--- ps3-linux-2.6.21-rc5.orig/drivers/video/ps3fb.c
++++ ps3-linux-2.6.21-rc5/drivers/video/ps3fb.c
+@@ -384,7 +384,7 @@ static const struct fb_videomode *ps3fb_
+ 	return &ps3fb_modedb[mode - 1];
+ }
+ 
+-static int ps3fb_sync(u32 frame)
++int ps3fb_sync(u32 frame)
+ {
+ 	int i, status;
+ 	u32 xres, yres;
diff -Naur linux-2.6.25-org/patches/ps3-private/perfmon/cell-spe-add-get-speid-func.patch linux-2.6.25-id/patches/ps3-private/perfmon/cell-spe-add-get-speid-func.patch
--- linux-2.6.25-org/patches/ps3-private/perfmon/cell-spe-add-get-speid-func.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-private/perfmon/cell-spe-add-get-speid-func.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,31 @@
+
+This patch adds the ps3_get_speid() function which just returns the logical spe id.
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto at jp.sony.com>
+---
+ arch/powerpc/platforms/ps3/spu.c |    7 +++++++
+ 1 file changed, 7 insertions(+)
+
+--- a/arch/powerpc/platforms/ps3/spu.c
++++ b/arch/powerpc/platforms/ps3/spu.c
+@@ -27,6 +27,7 @@
+ #include <asm/spu.h>
+ #include <asm/spu_priv1.h>
+ #include <asm/lv1call.h>
++#include <asm/ps3.h>
+ 
+ #include "../cell/spufs/spufs.h"
+ #include "platform.h"
+@@ -140,6 +141,12 @@ static void _dump_areas(unsigned int spe
+ 	pr_debug("%s:%d: shadow:  %lxh\n", func, line, shadow);
+ }
+ 
++inline u64 ps3_get_spe_id(void *arg)
++{
++	return spu_pdata(arg)->spe_id;
++}
++EXPORT_SYMBOL_GPL(ps3_get_spe_id);
++
+ static unsigned long get_vas_id(void)
+ {
+ 	unsigned long id;
diff -Naur linux-2.6.25-org/patches/ps3-private/perfmon/cell-spe-add-hook-functions.patch linux-2.6.25-id/patches/ps3-private/perfmon/cell-spe-add-hook-functions.patch
--- linux-2.6.25-org/patches/ps3-private/perfmon/cell-spe-add-hook-functions.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-private/perfmon/cell-spe-add-hook-functions.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,388 @@
+This patch adds ctx switch hook functions to do pfm ctx switch by
+the spe scheduling instead of the ppe scheduling.
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto at jp.sony.com>
+---
+ arch/powerpc/perfmon/perfmon.c      |   32 ++++++
+ arch/powerpc/perfmon/perfmon_cell.c |  188 ++++++++++++++++++++++++++++++++++++
+ include/asm-powerpc/perfmon.h       |   54 ++++++++++
+ include/linux/perfmon.h             |    5 
+ perfmon/perfmon_ctxsw.c             |    2 
+ 5 files changed, 281 insertions(+)
+
+--- a/arch/powerpc/perfmon/perfmon.c
++++ b/arch/powerpc/perfmon/perfmon.c
+@@ -22,8 +22,12 @@
+  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+  * 02111-1307 USA
+  */
++
++#include <linux/kernel.h>
++#include <linux/module.h>
+ #include <linux/interrupt.h>
+ #include <linux/perfmon.h>
++#include <asm/perfmon.h>
+ 
+ static void pfm_stop_active(struct task_struct *task,
+ 			    struct pfm_context *ctx, struct pfm_event_set *set)
+@@ -220,6 +224,34 @@ void pfm_arch_restore_pmcs(struct pfm_co
+ 	}
+ }
+ 
++int pfm_arch_ctxsw(struct notifier_block *block,
++                   unsigned long object_id, void *arg)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++	struct task_struct *p;
++
++	if (!arch_info->get_pid || !arch_info->ctxsw) {
++		return 0;
++	}
++
++	read_lock(&tasklist_lock);
++
++	p = find_task_by_pid(arch_info->get_pid(arg));
++	if (p)
++		get_task_struct(p);
++
++	read_unlock(&tasklist_lock);
++
++	if (p == NULL)
++		return 0;
++
++	arch_info->ctxsw(block, object_id, p, arg);
++
++	put_task_struct(p);
++	return 0;
++}
++EXPORT_SYMBOL_GPL(pfm_arch_ctxsw);
++
+ char *pfm_arch_get_pmu_module_name(void)
+ {
+ 	unsigned int pvr = mfspr(SPRN_PVR);
+--- a/arch/powerpc/perfmon/perfmon_cell.c
++++ b/arch/powerpc/perfmon/perfmon_cell.c
+@@ -32,6 +32,8 @@
+ #include <asm/machdep.h>
+ #include <asm/rtas.h>
+ #include <asm/ps3.h>
++#include "../platforms/cell/spufs/spufs.h"
++#include <asm/perfmon.h>
+ 
+ MODULE_AUTHOR("Kevin Corry <kevcorry@us.ibm.com>, "
+ 	      "Carl Love <carll@us.ibm.com>");
+@@ -257,8 +259,27 @@ enum {
+ 	SIG_GROUP_NONE = 0,
+ 
+ 	/* 2.x PowerPC Processor Unit (PPU) Signal Groups */
++	SIG_GROUP_PPU_BASE = 20,
+ 	SIG_GROUP_PPU_IU1 = 21,
+ 	SIG_GROUP_PPU_XU = 22,
++
++	/* 3.x PowerPC Storage Subsystem (PPSS) Signal Groups */
++	SIG_GROUP_PPSS_BASE = 30,
++
++	/* 4.x Synergistic Processor Unit (SPU) Signal Groups */
++	SIG_GROUP_SPU_BASE = 40,
++
++	/* 5.x Memory Flow Controller (MFC) Signal Groups */
++	SIG_GROUP_MFC_BASE = 50,
++
++	/* 6.x Element )nterconnect Bus (EIB) Signal Groups */
++	SIG_GROUP_EIB_BASE = 60,
++
++	/* 7.x Memory Interface Controller (MIC) Signal Groups */
++	SIG_GROUP_MIC_BASE = 70,
++
++	/* 8.x Cell Broadband Engine Interface (BEI) Signal Groups */
++	SIG_GROUP_BEI_BASE = 80,
+ };
+ 
+ /**
+@@ -686,6 +707,9 @@ static void pfm_cell_enable_counters(str
+ 		((struct pfm_arch_pmu_info *)
+ 		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
++	if (set->priv_flags & PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE)
++	       return ;
++
+ 	info->enable_pm(smp_processor_id());
+ }
+ 
+@@ -1117,6 +1141,166 @@ static void pfm_cell_irq_handler(struct 
+ }
+ 
+ 
++static inline u64 update_sub_unit_field(u64 pm_event, u64 spe_id)
++{
++	return ((pm_event & 0xFFFF0000FFFFFFFF) | (spe_id << 32));
++}
++
++static void pfm_prepare_ctxswin_thread(struct pfm_context *ctx, u64 spe_id)
++{
++	struct pfm_event_set *set;
++	int i;
++	u64 signal_group;
++
++	spin_lock(&ctx->lock);
++
++	set = ctx->active_set;
++	while (set != NULL) {
++		for (i = 8; i < 16; i++) {
++			signal_group = set->pmcs[i] & 0x00000000FFFFFFFF;
++			signal_group = signal_group / 100;
++
++			/*
++			 * If the target event is included SPE signal group,
++			 * The sub_unit field in pmX_event pmc is changed to the
++			 * specified spe_id.
++			 */
++			if (SIG_GROUP_SPU_BASE < signal_group &&
++			    signal_group < SIG_GROUP_EIB_BASE) {
++				set->pmcs[i] = update_sub_unit_field(
++					set->pmcs[i], spe_id);
++				set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
++				set->priv_flags &=
++					~PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE;
++				PFM_DBG("pmcs[%d] : 0x%lx", i, set->pmcs[i]);
++			}
++		}
++		if (set->id_next == ctx->active_set->id)
++			break;
++
++		set = list_first_entry(&set->list, struct pfm_event_set, list);
++	}
++
++	spin_unlock(&ctx->lock);
++}
++
++static int pfm_spe_get_pid(void *arg)
++{
++	return ((struct spu *)arg)->pid;
++}
++
++static int pfm_spe_ctxsw(struct notifier_block *block,
++			 unsigned long object_id,
++			 struct task_struct *p, void *arg)
++{
++	struct spu *spu;
++	u64 spe_id;
++
++	spu = arg;
++
++#ifdef CONFIG_PPC_PS3
++	if (machine_is(ps3))
++		spe_id = ps3_get_spe_id(arg);
++	else
++		spe_id = spu->spe_id;
++#else
++	spe_id = spu->spe_id;
++#endif
++
++	if (!p->pfm_context) {
++		PFM_DBG("=== Ignore PFM SPE CTXSW ===");
++		return 0;
++	}
++
++	if (object_id) {
++		PFM_DBG("=== PFM SPE CTXSWIN === 0x%lx", spe_id);
++		if (p->pfm_context)
++			pfm_prepare_ctxswin_thread(p->pfm_context, spe_id);
++		pfm_ctxsw(NULL, p);
++
++	} else {
++		PFM_DBG("=== PFM SPE CTXSWOUT === 0x%lx", spe_id);
++		pfm_ctxsw(p, NULL);
++	}
++
++	return 0;
++}
++
++/*
++ * This function returns whether spe ctxsw should be used for
++ * the pfm context switch.
++ *
++ * The following implementation is very tentative.
++ *  If a pfm target event which is described in pmcs[8] - pmc[16](pmX_event reg)
++ *  is included in SPE signal group, this function returns 1.
++ */
++static int pfm_is_ctxsw_type_spe(struct pfm_event_set *set)
++{
++	int i;
++	u64 signal_group;
++
++	for (i = 8; i < 16; i++) {
++		signal_group = set->pmcs[i] & 0x00000000FFFFFFFF;
++		signal_group = signal_group / 100;
++		if (SIG_GROUP_SPU_BASE < signal_group &&
++		    signal_group < SIG_GROUP_EIB_BASE)
++			return 1;
++	}
++	return 0;
++}
++
++static int pfm_cell_add_ctxsw_hook(struct pfm_context *ctx)
++{
++	int ret;
++
++	if (!pfm_is_ctxsw_type_spe(ctx->active_set)) {
++		set_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
++		return 0;
++	}
++
++	if (ctx->ctxsw_notifier.notifier_call) {
++		set_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
++		return 0;
++	}
++	ctx->ctxsw_notifier.notifier_call = pfm_arch_ctxsw;
++	ctx->ctxsw_notifier.next = NULL;
++	ctx->ctxsw_notifier.priority = 0;
++	if (!ctx->flags.system)
++		ctx->active_set->priv_flags |=
++			PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE;
++
++	ret = spu_switch_event_register(&ctx->ctxsw_notifier);
++	if (ret) {
++		PFM_ERR("Can't register spe_notifier\n");
++		ctx->ctxsw_notifier.notifier_call = NULL;
++		ctx->active_set->priv_flags &=
++			~PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE;
++	}
++
++	return ret;
++}
++
++static int pfm_cell_remove_ctxsw_hook(struct pfm_context *ctx)
++{
++	int ret;
++
++	if (ctx->ctxsw_notifier.notifier_call == NULL) {
++		if (ctx->task)
++			clear_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
++		return 0;
++	}
++	ret = spu_switch_event_unregister(&ctx->ctxsw_notifier);
++	if (ret)
++		PFM_ERR("Can't unregister spe_notifier\n");
++	ctx->ctxsw_notifier.notifier_call = NULL;
++	ctx->ctxsw_notifier.next = NULL;
++	ctx->ctxsw_notifier.priority = 0;
++
++	return ret;
++}
++
++
++
+ static struct pfm_cell_platform_pmu_info ps3_platform_pmu_info = {
+ #ifdef CONFIG_PPC_PS3
+ 	.read_ctr                    = ps3_read_ctr,
+@@ -1157,6 +1341,10 @@ static struct pfm_cell_platform_pmu_info
+ 
+ static struct pfm_arch_pmu_info pfm_cell_pmu_info = {
+ 	.pmu_style        = PFM_POWERPC_PMU_CELL,
++	.add_ctxsw_hook   = pfm_cell_add_ctxsw_hook,
++	.remove_ctxsw_hook = pfm_cell_remove_ctxsw_hook,
++	.ctxsw             = pfm_spe_ctxsw,
++	.get_pid           = pfm_spe_get_pid,
+ 	.acquire_pmu      = pfm_cell_acquire_pmu,
+ 	.release_pmu      = pfm_cell_release_pmu,
+ 	.write_pmc        = pfm_cell_write_pmc,
+--- a/include/asm-powerpc/perfmon.h
++++ b/include/asm-powerpc/perfmon.h
+@@ -48,6 +48,12 @@ enum powerpc_pmu_type {
+ struct pfm_arch_pmu_info {
+ 	enum powerpc_pmu_type pmu_style;
+ 
++	int (*add_ctxsw_hook)(struct pfm_context *ctx);
++	int (*remove_ctxsw_hook)(struct pfm_context *ctx);
++        int (*get_pid)(void *arg);
++	int (*ctxsw)(struct notifier_block *block,
++		     unsigned long object_id, struct task_struct *p, void *arg);
++
+ 	void (*write_pmc)(unsigned int cnum, u64 value);
+ 	void (*write_pmd)(unsigned int cnum, u64 value);
+ 
+@@ -386,6 +392,54 @@ struct pfm_arch_context {
+  */
+ #define PFM_ARCH_SMPL_ALIGN_SIZE	0
+ 
++/*
++ * Hook perfmon context switch functions to get the information of
++ * the target task scheduling
++ */
++static inline void pfm_arch_add_ctxsw_hook(struct pfm_context *ctx)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	if (!ctx)
++		return;
++
++	if (arch_info->add_ctxsw_hook) {
++		arch_info->add_ctxsw_hook(ctx);
++		return;
++	}
++
++	set_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
++
++}
++
++static inline void pfm_arch_remove_ctxsw_hook(struct pfm_context *ctx)
++{
++	struct pfm_arch_pmu_info *arch_info = pfm_pmu_conf->arch_info;
++
++	if (!ctx)
++		return;
++
++	if (arch_info->remove_ctxsw_hook) {
++		arch_info->remove_ctxsw_hook(ctx);
++		return;
++	}
++
++	if (ctx->task)
++		clear_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW);
++}
++
++int pfm_arch_ctxsw(struct notifier_block *block,
++		   unsigned long object_id, void *arg);
++
++/*
++ * common private event set flags (priv_flags)
++ *
++ * upper 16 bits: for arch-specific use
++ * lower 16 bits: for common use
++ *
++ * See common perfmon.h
++ */
++#define PFM_SETFL_PRIV_WAIT_SUB_UNIT_FIELD_UPDATE 0x10000
+ 
+ #endif /* __KERNEL__ */
+ #endif /* _ASM_POWERPC_PERFMON_H_ */
+--- a/include/linux/perfmon.h
++++ b/include/linux/perfmon.h
+@@ -446,6 +446,11 @@ struct pfm_context {
+ 	 */
+ 	struct pfm_ovfl_arg 	ovfl_arg;
+ 	u64			ovfl_ovfl_notify[PFM_PMD_BV];
++
++	/*
++	 * task context switch notifier.
++	 */
++	struct notifier_block   ctxsw_notifier;
+ };
+ 
+ static inline struct pfm_arch_context *pfm_ctx_arch(struct pfm_context *c)
+--- a/perfmon/perfmon_ctxsw.c
++++ b/perfmon/perfmon_ctxsw.c
+@@ -36,6 +36,7 @@
+  * 02111-1307 USA
+  */
+ #include <linux/kernel.h>
++#include <linux/module.h>
+ #include <linux/perfmon.h>
+ 
+ /*
+@@ -369,3 +370,4 @@ void pfm_ctxsw(struct task_struct *prev,
+ 	pfm_stats_inc(ctxsw_count);
+ 	pfm_stats_add(ctxsw_ns, sched_clock() - now);
+ }
++EXPORT_SYMBOL_GPL(pfm_ctxsw);
diff -Naur linux-2.6.25-org/patches/ps3-private/perfmon/cell-spe-call-hook-functions.patch linux-2.6.25-id/patches/ps3-private/perfmon/cell-spe-call-hook-functions.patch
--- linux-2.6.25-org/patches/ps3-private/perfmon/cell-spe-call-hook-functions.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-private/perfmon/cell-spe-call-hook-functions.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,95 @@
+This patch changes the load ctx functions to call the arch depended
+hook functions for the ctx switch.
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto at jp.sony.com>
+---
+ perfmon/perfmon.c          |   20 ++++++++++++++++++++
+ perfmon/perfmon_ctxsw.c    |    9 +++++++--
+ perfmon/perfmon_syscalls.c |    6 ++++++
+ 3 files changed, 33 insertions(+), 2 deletions(-)
+
+--- a/perfmon/perfmon.c
++++ b/perfmon/perfmon.c
+@@ -102,6 +102,13 @@ void pfm_context_free(struct pfm_context
+ 
+ 	pfm_arch_context_free(ctx);
+ 
++#if CONFIG_PPC
++	pfm_arch_remove_ctxsw_hook(ctx);
++	if (ctx)
++		ctx->task = NULL;
++	PFM_DBG("unhook SPE ctxsw. context_free");
++#endif
++
+ 	fmt = ctx->smpl_fmt;
+ 
+ 	pfm_free_sets(ctx);
+@@ -854,6 +861,11 @@ void __pfm_exit_thread(struct task_struc
+ 	}
+ 	spin_unlock_irqrestore(&ctx->lock, flags);
+ 
++#ifdef CONFIG_PPC
++	pfm_arch_remove_ctxsw_hook(ctx);
++	PFM_DBG("unhook SPE ctxsw. exit_thread");
++#endif
++
+ 	/*
+ 	 * cancel timer now that context is unlocked
+ 	 */
+@@ -1445,8 +1457,12 @@ int __pfm_load_context(struct pfm_contex
+ 	}
+ 
+ 	if (!ctx->flags.system) {
++#ifdef CONFIG_PPC
++		/*pfm_arch_add_ctxsw_hook(ctx);*/
++#else
+ 		set_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
+ 		PFM_DBG("[%d] set TIF", task->pid);
++#endif
+ 	}
+ 
+ 	ctx->flags.work_type = PFM_WORK_NONE;
+@@ -1672,6 +1688,10 @@ int __pfm_create_context(struct pfarg_ct
+ 	ctx->flags.no_msg = (ctx_flags & PFM_FL_OVFL_NO_MSG) ? 1: 0;
+ 	ctx->flags.ia64_v20_compat = mode == PFM_COMPAT ? 1 : 0;
+ 
++	ctx->ctxsw_notifier.notifier_call = NULL;
++	ctx->ctxsw_notifier.next = NULL;
++	ctx->ctxsw_notifier.priority = 0;
++
+ 	/*
+ 	 * initialize arch-specific section
+ 	 * must be done before fmt_init()
+--- a/perfmon/perfmon_ctxsw.c
++++ b/perfmon/perfmon_ctxsw.c
+@@ -347,8 +347,13 @@ void pfm_ctxsw(struct task_struct *prev,
+ 
+ 	now = sched_clock();
+ 
+-	ctxp = prev->pfm_context;
+-	ctxn = next->pfm_context;
++	ctxp = NULL;
++	ctxn = NULL;
++	if (prev)
++		ctxp = prev->pfm_context;
++
++	if (next)
++		ctxn = next->pfm_context;
+ 
+ 	if (ctxp)
+ 		__pfm_ctxswout_thread(prev, ctxp, now);
+--- a/perfmon/perfmon_syscalls.c
++++ b/perfmon/perfmon_syscalls.c
+@@ -828,6 +828,12 @@ asmlinkage long sys_pfm_load_context(int
+ 
+ 	spin_unlock_irqrestore(&ctx->lock, flags);
+ 
++#ifdef CONFIG_PPC
++	if (!ctx->flags.system && ret == 0) {
++		pfm_arch_add_ctxsw_hook(ctx);
++	}
++#endif
++
+ 	/*
+ 	 * in per-thread mode (not self-monitoring), we need
+ 	 * to decrease refcount on task to monitor:
diff -Naur linux-2.6.25-org/patches/ps3-private/perfmon/cell-spe-other.patch linux-2.6.25-id/patches/ps3-private/perfmon/cell-spe-other.patch
--- linux-2.6.25-org/patches/ps3-private/perfmon/cell-spe-other.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-private/perfmon/cell-spe-other.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,100 @@
+
+This patch fixes some problems which happen at ctxsw by spe scheduling.
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto at jp.sony.com>
+---
+ arch/powerpc/perfmon/perfmon_cell.c |   14 ++++++++++++++
+ perfmon/perfmon_file.c              |   11 +++++++++++
+ perfmon/perfmon_rw.c                |   24 ++++++++++++++++++++++--
+ 3 files changed, 47 insertions(+), 2 deletions(-)
+
+--- a/arch/powerpc/perfmon/perfmon_cell.c
++++ b/arch/powerpc/perfmon/perfmon_cell.c
+@@ -896,9 +896,23 @@ static int pfm_cell_load_context(struct 
+ static int pfm_cell_unload_context(struct pfm_context *ctx,
+ 				   struct task_struct *task)
+ {
++	struct pfm_cell_platform_pmu_info *info =
++		((struct pfm_arch_pmu_info *)
++		 (pfm_pmu_conf->arch_info))->platform_info;
++
+ 	if (task == current || ctx->flags.system) {
+ 		reset_signals(smp_processor_id());
+ 	}
++
++	/*
++	 * At the end of monitoring SPE events,
++	 * detaching pfm_ctx from the target task may not be done correctly.
++	 * So, Clear all signals and disable PM
++	 * at here.
++	 */
++	info->disable_pm(smp_processor_id());
++	reset_signals(smp_processor_id());
++
+ 	return 0;
+ }
+ 
+--- a/perfmon/perfmon_file.c
++++ b/perfmon/perfmon_file.c
+@@ -596,6 +596,17 @@ int __pfm_close(struct pfm_context *ctx,
+ 		 * ZOMBIE state as part of pfm_unload_context()
+ 		 */
+ 		can_unload = can_free = 0;
++
++		if (!test_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW)) {
++			/*
++			 * Cell specific change
++			 * The normal perfmon ctxsw mechanism is not used.
++			 * unload & free is executed in this function.
++			 *
++			 */
++			can_unload = 1;
++			can_free = 1;
++		}
+ 	}
+ #endif
+ 	if (can_unload)
+--- a/perfmon/perfmon_rw.c
++++ b/perfmon/perfmon_rw.c
+@@ -502,6 +502,25 @@ int __pfm_read_pmds(struct pfm_context *
+ 
+ 		if (can_access_pmu)
+ 			pfm_arch_serialize();
++
++		/*
++		 * for CBE SPE events
++		 * The workaround for the read_pmds problem
++		 * which the result pmd value is 0.
++		 * Because can_access_pmu is 0.
++		 * This problem happnens under the following condition.
++		 *  --attach-task option
++		 *  Ctrl-C
++		 *  The target events are SPE event.
++		 *  no ctxsw happened while before Ctrl-C.
++		 *
++		 * The cause of this problem is that the pmd reader HW thread
++		 * is not same as the starter HW thread of the measurement.
++		 *
++		 * Save pmds.
++		 */
++		if (!can_access_pmu/* && pfm_is_ctxsw_type_spe(ctx->active_set)*/)
++			pfm_save_pmds(ctx, ctx->active_set);
+ 	}
+ 
+ 	/*
+@@ -572,12 +591,13 @@ int __pfm_read_pmds(struct pfm_context *
+ 				val = hw_val;
+ 		}
+ 
+-		PFM_DBG("set%u pmd%u=0x%llx sw_thr=%llu lval=0x%llx",
++		PFM_DBG("set%u pmd%u=0x%llx sw_thr=%llu lval=0x%llx"
++			" can_access=%d",
+ 			set->id,
+ 			cnum,
+ 			(unsigned long long)val,
+ 			(unsigned long long)sw_cnt,
+-			(unsigned long long)lval);
++			(unsigned long long)lval, can_access_pmu);
+ 
+ 		req->reg_value = val;
+ 		req->reg_last_reset_val = lval;
diff -Naur linux-2.6.25-org/patches/ps3-private/perfmon/ps3-print-spe-id.patch linux-2.6.25-id/patches/ps3-private/perfmon/ps3-print-spe-id.patch
--- linux-2.6.25-org/patches/ps3-private/perfmon/ps3-print-spe-id.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-private/perfmon/ps3-print-spe-id.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,14 @@
+---
+ arch/powerpc/platforms/ps3/spu.c |    1 +
+ 1 file changed, 1 insertion(+)
+
+--- a/arch/powerpc/platforms/ps3/spu.c
++++ b/arch/powerpc/platforms/ps3/spu.c
+@@ -175,6 +175,7 @@ static int __init construct_spu(struct s
+ 		return result;
+ 	}
+ 
++	pr_info("spe_id[%d] : 0x%lx\n", spu->number, ps3_get_spe_id(spu));
+ 	return result;
+ }
+ 
diff -Naur linux-2.6.25-org/patches/ps3-stable/kill-unused-ps3_repository_bump_device.diff linux-2.6.25-id/patches/ps3-stable/kill-unused-ps3_repository_bump_device.diff
--- linux-2.6.25-org/patches/ps3-stable/kill-unused-ps3_repository_bump_device.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/kill-unused-ps3_repository_bump_device.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,27 @@
+Subject: PS3: Kill unused ps3_repository_bump_device()
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+PS3: Kill unused routine ps3_repository_bump_device().
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/platform.h |    6 ------
+ 1 files changed, 6 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/platform.h
++++ b/arch/powerpc/platforms/ps3/platform.h
+@@ -144,12 +144,6 @@ struct ps3_repository_device {
+ 	u64 dev_id;
+ };
+ 
+-static inline struct ps3_repository_device *ps3_repository_bump_device(
+-	struct ps3_repository_device *repo)
+-{
+-	repo->dev_index++;
+-	return repo;
+-}
+ int ps3_repository_find_device(struct ps3_repository_device *repo);
+ int ps3_repository_find_device_by_id(struct ps3_repository_device *repo,
+ 				     u64 bus_id, u64 dev_id);
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-checkpatch-repository.diff linux-2.6.25-id/patches/ps3-stable/ps3-checkpatch-repository.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3-checkpatch-repository.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-checkpatch-repository.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,294 @@
+Subject: PS3: Checkpatch cleanups for arch/powerpc/platforms/ps3/repository.c
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+Cleanup coding errors in arch/powerpc/platforms/ps3/repository.c as reported
+by sparse and checkpatch.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/repository.c |   64 ++++++++++++++++----------------
+ 1 files changed, 32 insertions(+), 32 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/repository.c
++++ b/arch/powerpc/platforms/ps3/repository.c
+@@ -33,7 +33,7 @@ enum ps3_lpar_id {
+ };
+ 
+ #define dump_field(_a, _b) _dump_field(_a, _b, __func__, __LINE__)
+-static void _dump_field(const char *hdr, u64 n, const char* func, int line)
++static void _dump_field(const char *hdr, u64 n, const char *func, int line)
+ {
+ #if defined(DEBUG)
+ 	char s[16];
+@@ -50,8 +50,8 @@ static void _dump_field(const char *hdr,
+ 
+ #define dump_node_name(_a, _b, _c, _d, _e) \
+ 	_dump_node_name(_a, _b, _c, _d, _e, __func__, __LINE__)
+-static void _dump_node_name (unsigned int lpar_id, u64 n1, u64 n2, u64 n3,
+-	u64 n4, const char* func, int line)
++static void _dump_node_name(unsigned int lpar_id, u64 n1, u64 n2, u64 n3,
++	u64 n4, const char *func, int line)
+ {
+ 	pr_debug("%s:%d: lpar: %u\n", func, line, lpar_id);
+ 	_dump_field("n1: ", n1, func, line);
+@@ -63,7 +63,7 @@ static void _dump_node_name (unsigned in
+ #define dump_node(_a, _b, _c, _d, _e, _f, _g) \
+ 	_dump_node(_a, _b, _c, _d, _e, _f, _g, __func__, __LINE__)
+ static void _dump_node(unsigned int lpar_id, u64 n1, u64 n2, u64 n3, u64 n4,
+-	u64 v1, u64 v2, const char* func, int line)
++	u64 v1, u64 v2, const char *func, int line)
+ {
+ 	pr_debug("%s:%d: lpar: %u\n", func, line, lpar_id);
+ 	_dump_field("n1: ", n1, func, line);
+@@ -165,7 +165,7 @@ int ps3_repository_read_bus_str(unsigned
+ 		make_first_field("bus", bus_index),
+ 		make_field(bus_str, 0),
+ 		0, 0,
+-		value, 0);
++		value, NULL);
+ }
+ 
+ int ps3_repository_read_bus_id(unsigned int bus_index, u64 *bus_id)
+@@ -190,7 +190,7 @@ int ps3_repository_read_bus_type(unsigne
+ 		make_first_field("bus", bus_index),
+ 		make_field("type", 0),
+ 		0, 0,
+-		&v1, 0);
++		&v1, NULL);
+ 	*bus_type = v1;
+ 	return result;
+ }
+@@ -205,7 +205,7 @@ int ps3_repository_read_bus_num_dev(unsi
+ 		make_first_field("bus", bus_index),
+ 		make_field("num_dev", 0),
+ 		0, 0,
+-		&v1, 0);
++		&v1, NULL);
+ 	*num_dev = v1;
+ 	return result;
+ }
+@@ -218,7 +218,7 @@ int ps3_repository_read_dev_str(unsigned
+ 		make_field("dev", dev_index),
+ 		make_field(dev_str, 0),
+ 		0,
+-		value, 0);
++		value, NULL);
+ }
+ 
+ int ps3_repository_read_dev_id(unsigned int bus_index, unsigned int dev_index,
+@@ -231,7 +231,7 @@ int ps3_repository_read_dev_id(unsigned 
+ 		make_field("dev", dev_index),
+ 		make_field("id", 0),
+ 		0,
+-		dev_id, 0);
++		dev_id, NULL);
+ 	return result;
+ }
+ 
+@@ -246,14 +246,14 @@ int ps3_repository_read_dev_type(unsigne
+ 		make_field("dev", dev_index),
+ 		make_field("type", 0),
+ 		0,
+-		&v1, 0);
++		&v1, NULL);
+ 	*dev_type = v1;
+ 	return result;
+ }
+ 
+ int ps3_repository_read_dev_intr(unsigned int bus_index,
+ 	unsigned int dev_index, unsigned int intr_index,
+-	enum ps3_interrupt_type *intr_type, unsigned int* interrupt_id)
++	enum ps3_interrupt_type *intr_type, unsigned int *interrupt_id)
+ {
+ 	int result;
+ 	u64 v1;
+@@ -282,7 +282,7 @@ int ps3_repository_read_dev_reg_type(uns
+ 		make_field("dev", dev_index),
+ 		make_field("reg", reg_index),
+ 		make_field("type", 0),
+-		&v1, 0);
++		&v1, NULL);
+ 	*reg_type = v1;
+ 	return result;
+ }
+@@ -588,7 +588,7 @@ int ps3_repository_read_stor_dev_port(un
+ 		make_first_field("bus", bus_index),
+ 		make_field("dev", dev_index),
+ 		make_field("port", 0),
+-		0, port, 0);
++		0, port, NULL);
+ }
+ 
+ int ps3_repository_read_stor_dev_blk_size(unsigned int bus_index,
+@@ -598,7 +598,7 @@ int ps3_repository_read_stor_dev_blk_siz
+ 		make_first_field("bus", bus_index),
+ 		make_field("dev", dev_index),
+ 		make_field("blk_size", 0),
+-		0, blk_size, 0);
++		0, blk_size, NULL);
+ }
+ 
+ int ps3_repository_read_stor_dev_num_blocks(unsigned int bus_index,
+@@ -608,7 +608,7 @@ int ps3_repository_read_stor_dev_num_blo
+ 		make_first_field("bus", bus_index),
+ 		make_field("dev", dev_index),
+ 		make_field("n_blocks", 0),
+-		0, num_blocks, 0);
++		0, num_blocks, NULL);
+ }
+ 
+ int ps3_repository_read_stor_dev_num_regions(unsigned int bus_index,
+@@ -621,7 +621,7 @@ int ps3_repository_read_stor_dev_num_reg
+ 		make_first_field("bus", bus_index),
+ 		make_field("dev", dev_index),
+ 		make_field("n_regs", 0),
+-		0, &v1, 0);
++		0, &v1, NULL);
+ 	*num_regions = v1;
+ 	return result;
+ }
+@@ -638,7 +638,7 @@ int ps3_repository_read_stor_dev_region_
+ 	    make_field("dev", dev_index),
+ 	    make_field("region", region_index),
+ 	    make_field("id", 0),
+-	    &v1, 0);
++	    &v1, NULL);
+ 	*region_id = v1;
+ 	return result;
+ }
+@@ -651,7 +651,7 @@ int ps3_repository_read_stor_dev_region_
+ 	    make_field("dev", dev_index),
+ 	    make_field("region", region_index),
+ 	    make_field("size", 0),
+-	    region_size, 0);
++	    region_size, NULL);
+ }
+ 
+ int ps3_repository_read_stor_dev_region_start(unsigned int bus_index,
+@@ -662,7 +662,7 @@ int ps3_repository_read_stor_dev_region_
+ 	    make_field("dev", dev_index),
+ 	    make_field("region", region_index),
+ 	    make_field("start", 0),
+-	    region_start, 0);
++	    region_start, NULL);
+ }
+ 
+ int ps3_repository_read_stor_dev_info(unsigned int bus_index,
+@@ -718,7 +718,7 @@ int ps3_repository_read_rm_size(unsigned
+ 		make_field("pu", 0),
+ 		ppe_id,
+ 		make_field("rm_size", 0),
+-		rm_size, 0);
++		rm_size, NULL);
+ }
+ 
+ int ps3_repository_read_region_total(u64 *region_total)
+@@ -727,7 +727,7 @@ int ps3_repository_read_region_total(u64
+ 		make_first_field("bi", 0),
+ 		make_field("rgntotal", 0),
+ 		0, 0,
+-		region_total, 0);
++		region_total, NULL);
+ }
+ 
+ /**
+@@ -763,7 +763,7 @@ int ps3_repository_read_num_spu_reserved
+ 		make_first_field("bi", 0),
+ 		make_field("spun", 0),
+ 		0, 0,
+-		&v1, 0);
++		&v1, NULL);
+ 	*num_spu_reserved = v1;
+ 	return result;
+ }
+@@ -782,7 +782,7 @@ int ps3_repository_read_num_spu_resource
+ 		make_first_field("bi", 0),
+ 		make_field("spursvn", 0),
+ 		0, 0,
+-		&v1, 0);
++		&v1, NULL);
+ 	*num_resource_id = v1;
+ 	return result;
+ }
+@@ -795,7 +795,7 @@ int ps3_repository_read_num_spu_resource
+  */
+ 
+ int ps3_repository_read_spu_resource_id(unsigned int res_index,
+-	enum ps3_spu_resource_type* resource_type, unsigned int *resource_id)
++	enum ps3_spu_resource_type *resource_type, unsigned int *resource_id)
+ {
+ 	int result;
+ 	u64 v1;
+@@ -812,14 +812,14 @@ int ps3_repository_read_spu_resource_id(
+ 	return result;
+ }
+ 
+-int ps3_repository_read_boot_dat_address(u64 *address)
++static int ps3_repository_read_boot_dat_address(u64 *address)
+ {
+ 	return read_node(PS3_LPAR_ID_CURRENT,
+ 		make_first_field("bi", 0),
+ 		make_field("boot_dat", 0),
+ 		make_field("address", 0),
+ 		0,
+-		address, 0);
++		address, NULL);
+ }
+ 
+ int ps3_repository_read_boot_dat_size(unsigned int *size)
+@@ -832,7 +832,7 @@ int ps3_repository_read_boot_dat_size(un
+ 		make_field("boot_dat", 0),
+ 		make_field("size", 0),
+ 		0,
+-		&v1, 0);
++		&v1, NULL);
+ 	*size = v1;
+ 	return result;
+ }
+@@ -847,7 +847,7 @@ int ps3_repository_read_vuart_av_port(un
+ 		make_field("vir_uart", 0),
+ 		make_field("port", 0),
+ 		make_field("avset", 0),
+-		&v1, 0);
++		&v1, NULL);
+ 	*port = v1;
+ 	return result;
+ }
+@@ -862,7 +862,7 @@ int ps3_repository_read_vuart_sysmgr_por
+ 		make_field("vir_uart", 0),
+ 		make_field("port", 0),
+ 		make_field("sysmgr", 0),
+-		&v1, 0);
++		&v1, NULL);
+ 	*port = v1;
+ 	return result;
+ }
+@@ -893,7 +893,7 @@ int ps3_repository_read_num_be(unsigned 
+ 		0,
+ 		0,
+ 		0,
+-		&v1, 0);
++		&v1, NULL);
+ 	*num_be = v1;
+ 	return result;
+ }
+@@ -905,7 +905,7 @@ int ps3_repository_read_be_node_id(unsig
+ 		0,
+ 		0,
+ 		0,
+-		node_id, 0);
++		node_id, NULL);
+ }
+ 
+ int ps3_repository_read_tb_freq(u64 node_id, u64 *tb_freq)
+@@ -915,7 +915,7 @@ int ps3_repository_read_tb_freq(u64 node
+ 		node_id,
+ 		make_field("clock", 0),
+ 		0,
+-		tb_freq, 0);
++		tb_freq, NULL);
+ }
+ 
+ int ps3_repository_read_be_tb_freq(unsigned int be_index, u64 *tb_freq)
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-checkpatch-sys-manager.diff linux-2.6.25-id/patches/ps3-stable/ps3-checkpatch-sys-manager.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3-checkpatch-sys-manager.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-checkpatch-sys-manager.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,66 @@
+Subject: PS3: Checkpatch cleanups for drivers/ps3/ps3-sys-manager.c
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+Cleanup coding errors in drivers/ps3/ps3-sys-manager.c as reported
+by checkpatch.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ drivers/ps3/ps3-sys-manager.c |   14 +++++++-------
+ 1 files changed, 7 insertions(+), 7 deletions(-)
+
+--- a/drivers/ps3/ps3-sys-manager.c
++++ b/drivers/ps3/ps3-sys-manager.c
+@@ -452,7 +452,7 @@ static int ps3_sys_manager_handle_event(
+ 	case PS3_SM_EVENT_THERMAL_ALERT:
+ 		dev_dbg(&dev->core, "%s:%d: THERMAL_ALERT (zone %u)\n",
+ 			__func__, __LINE__, event.value);
+-		printk(KERN_INFO "PS3 Thermal Alert Zone %u\n", event.value);
++		pr_info("PS3 Thermal Alert Zone %u\n", event.value);
+ 		break;
+ 	case PS3_SM_EVENT_THERMAL_CLEARED:
+ 		dev_dbg(&dev->core, "%s:%d: THERMAL_CLEARED (zone %u)\n",
+@@ -488,7 +488,7 @@ static int ps3_sys_manager_handle_cmd(st
+ 	result = ps3_vuart_read(dev, &cmd, sizeof(cmd));
+ 	BUG_ON(result && "need to retry here");
+ 
+-	if(result)
++	if (result)
+ 		return result;
+ 
+ 	if (cmd.version != 1) {
+@@ -521,7 +521,7 @@ static int ps3_sys_manager_handle_msg(st
+ 	result = ps3_vuart_read(dev, &header,
+ 		sizeof(struct ps3_sys_manager_header));
+ 
+-	if(result)
++	if (result)
+ 		return result;
+ 
+ 	if (header.version != 1) {
+@@ -589,9 +589,9 @@ static void ps3_sys_manager_final_power_
+ 		PS3_SM_WAKE_DEFAULT);
+ 	ps3_sys_manager_send_request_shutdown(dev);
+ 
+-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
++	pr_emerg("System Halted, OK to turn off power\n");
+ 
+-	while(1)
++	while (1)
+ 		ps3_sys_manager_handle_msg(dev);
+ }
+ 
+@@ -626,9 +626,9 @@ static void ps3_sys_manager_final_restar
+ 		PS3_SM_WAKE_DEFAULT);
+ 	ps3_sys_manager_send_request_shutdown(dev);
+ 
+-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
++	pr_emerg("System Halted, OK to turn off power\n");
+ 
+-	while(1)
++	while (1)
+ 		ps3_sys_manager_handle_msg(dev);
+ }
+ 
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-checkpatch-vuart.diff linux-2.6.25-id/patches/ps3-stable/ps3-checkpatch-vuart.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3-checkpatch-vuart.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-checkpatch-vuart.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,77 @@
+Subject: PS3: Checkpatch cleanups for drivers/ps3/ps3-vuart.c
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+Cleanup coding errors in drivers/ps3/ps3-vuart.c as reported by
+checkpatch.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ drivers/ps3/ps3-vuart.c |   16 +++++++---------
+ 1 files changed, 7 insertions(+), 9 deletions(-)
+
+--- a/drivers/ps3/ps3-vuart.c
++++ b/drivers/ps3/ps3-vuart.c
+@@ -108,18 +108,18 @@ static struct ps3_vuart_port_priv *to_po
+ struct ports_bmp {
+ 	u64 status;
+ 	u64 unused[3];
+-} __attribute__ ((aligned (32)));
++} __attribute__((aligned(32)));
+ 
+ #define dump_ports_bmp(_b) _dump_ports_bmp(_b, __func__, __LINE__)
+ static void __maybe_unused _dump_ports_bmp(
+-	const struct ports_bmp* bmp, const char* func, int line)
++	const struct ports_bmp *bmp, const char *func, int line)
+ {
+ 	pr_debug("%s:%d: ports_bmp: %016lxh\n", func, line, bmp->status);
+ }
+ 
+ #define dump_port_params(_b) _dump_port_params(_b, __func__, __LINE__)
+ static void __maybe_unused _dump_port_params(unsigned int port_number,
+-	const char* func, int line)
++	const char *func, int line)
+ {
+ #if defined(DEBUG)
+ 	static const char *strings[] = {
+@@ -363,7 +363,7 @@ int ps3_vuart_disable_interrupt_disconne
+  */
+ 
+ static int ps3_vuart_raw_write(struct ps3_system_bus_device *dev,
+-	const void* buf, unsigned int bytes, unsigned long *bytes_written)
++	const void *buf, unsigned int bytes, unsigned long *bytes_written)
+ {
+ 	int result;
+ 	struct ps3_vuart_port_priv *priv = to_port_priv(dev);
+@@ -431,7 +431,7 @@ void ps3_vuart_clear_rx_bytes(struct ps3
+ 	int result;
+ 	struct ps3_vuart_port_priv *priv = to_port_priv(dev);
+ 	u64 bytes_waiting;
+-	void* tmp;
++	void *tmp;
+ 
+ 	result = ps3_vuart_get_rx_bytes_waiting(dev, &bytes_waiting);
+ 
+@@ -526,9 +526,8 @@ int ps3_vuart_write(struct ps3_system_bu
+ 
+ 	lb = kmalloc(sizeof(struct list_buffer) + bytes, GFP_KERNEL);
+ 
+-	if (!lb) {
++	if (!lb)
+ 		return -ENOMEM;
+-	}
+ 
+ 	memcpy(lb->data, buf, bytes);
+ 	lb->head = lb->data;
+@@ -926,9 +925,8 @@ static int ps3_vuart_bus_interrupt_get(v
+ 
+ 	BUG_ON(vuart_bus_priv.use_count > 2);
+ 
+-	if (vuart_bus_priv.use_count != 1) {
++	if (vuart_bus_priv.use_count != 1)
+ 		return 0;
+-	}
+ 
+ 	BUG_ON(vuart_bus_priv.bmp);
+ 
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-defconfig-updates.patch linux-2.6.25-id/patches/ps3-stable/ps3-defconfig-updates.patch
--- linux-2.6.25-org/patches/ps3-stable/ps3-defconfig-updates.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-defconfig-updates.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,145 @@
+Subject: PS3: Update ps3_defconfig
+
+Update ps3_defconfig.
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/configs/ps3_defconfig |   49 ++++++++++++++++++++++++++-----------
+ 1 file changed, 35 insertions(+), 14 deletions(-)
+
+--- a/arch/powerpc/configs/ps3_defconfig
++++ b/arch/powerpc/configs/ps3_defconfig
+@@ -1,7 +1,7 @@
+ #
+ # Automatically generated make config: don't edit
+-# Linux kernel version: 2.6.24-rc4
+-# Tue Dec  4 22:49:57 2007
++# Linux kernel version: 2.6.24-rc8
++# Wed Jan 16 14:31:21 2008
+ #
+ CONFIG_PPC64=y
+ 
+@@ -103,6 +103,7 @@ CONFIG_VM_EVENT_COUNTERS=y
+ CONFIG_SLAB=y
+ # CONFIG_SLUB is not set
+ # CONFIG_SLOB is not set
++CONFIG_SLABINFO=y
+ CONFIG_RT_MUTEXES=y
+ # CONFIG_TINY_SHMEM is not set
+ CONFIG_BASE_SMALL=0
+@@ -154,7 +155,6 @@ CONFIG_PPC_PS3=y
+ # CONFIG_PS3_ADVANCED is not set
+ CONFIG_PS3_HTAB_SIZE=20
+ # CONFIG_PS3_DYNAMIC_DMA is not set
+-CONFIG_PS3_USE_LPAR_ADDR=y
+ CONFIG_PS3_VUART=y
+ CONFIG_PS3_PS3AV=y
+ CONFIG_PS3_SYS_MANAGER=y
+@@ -162,6 +162,7 @@ CONFIG_PS3_STORAGE=y
+ CONFIG_PS3_DISK=y
+ CONFIG_PS3_ROM=y
+ CONFIG_PS3_FLASH=y
++CONFIG_PS3_LPM=m
+ CONFIG_PPC_CELL=y
+ # CONFIG_PPC_CELL_NATIVE is not set
+ # CONFIG_PPC_IBM_CELL_BLADE is not set
+@@ -225,7 +226,7 @@ CONFIG_HAVE_MEMORY_PRESENT=y
+ # CONFIG_SPARSEMEM_STATIC is not set
+ CONFIG_SPARSEMEM_EXTREME=y
+ CONFIG_SPARSEMEM_VMEMMAP_ENABLE=y
+-CONFIG_SPARSEMEM_VMEMMAP=y
++# CONFIG_SPARSEMEM_VMEMMAP is not set
+ CONFIG_MEMORY_HOTPLUG=y
+ CONFIG_MEMORY_HOTPLUG_SPARSE=y
+ CONFIG_SPLIT_PTLOCK_CPUS=4
+@@ -338,7 +339,26 @@ CONFIG_IPV6_SIT=y
+ # CONFIG_NET_PKTGEN is not set
+ # CONFIG_HAMRADIO is not set
+ # CONFIG_IRDA is not set
+-# CONFIG_BT is not set
++CONFIG_BT=m
++CONFIG_BT_L2CAP=m
++CONFIG_BT_SCO=m
++CONFIG_BT_RFCOMM=m
++CONFIG_BT_RFCOMM_TTY=y
++CONFIG_BT_BNEP=m
++CONFIG_BT_BNEP_MC_FILTER=y
++CONFIG_BT_BNEP_PROTO_FILTER=y
++CONFIG_BT_HIDP=m
++
++#
++# Bluetooth device drivers
++#
++CONFIG_BT_HCIUSB=m
++CONFIG_BT_HCIUSB_SCO=y
++# CONFIG_BT_HCIUART is not set
++# CONFIG_BT_HCIBCM203X is not set
++# CONFIG_BT_HCIBPA10X is not set
++# CONFIG_BT_HCIBFUSB is not set
++# CONFIG_BT_HCIVHCI is not set
+ # CONFIG_AF_RXRPC is not set
+ 
+ #
+@@ -666,14 +686,14 @@ CONFIG_LOGO_LINUX_CLUT224=y
+ #
+ # Sound
+ #
+-CONFIG_SOUND=y
++CONFIG_SOUND=m
+ 
+ #
+ # Advanced Linux Sound Architecture
+ #
+-CONFIG_SND=y
+-CONFIG_SND_TIMER=y
+-CONFIG_SND_PCM=y
++CONFIG_SND=m
++CONFIG_SND_TIMER=m
++CONFIG_SND_PCM=m
+ # CONFIG_SND_SEQUENCER is not set
+ # CONFIG_SND_MIXER_OSS is not set
+ # CONFIG_SND_PCM_OSS is not set
+@@ -702,7 +722,7 @@ CONFIG_SND_VERBOSE_PROCFS=y
+ #
+ # ALSA PowerPC devices
+ #
+-CONFIG_SND_PS3=y
++CONFIG_SND_PS3=m
+ CONFIG_SND_PS3_DEFAULT_START_DELAY=2000
+ 
+ #
+@@ -747,7 +767,7 @@ CONFIG_USB_SUPPORT=y
+ CONFIG_USB_ARCH_HAS_HCD=y
+ CONFIG_USB_ARCH_HAS_OHCI=y
+ CONFIG_USB_ARCH_HAS_EHCI=y
+-CONFIG_USB=y
++CONFIG_USB=m
+ # CONFIG_USB_DEBUG is not set
+ 
+ #
+@@ -761,13 +781,13 @@ CONFIG_USB_DEVICEFS=y
+ #
+ # USB Host Controller Drivers
+ #
+-CONFIG_USB_EHCI_HCD=y
++CONFIG_USB_EHCI_HCD=m
+ # CONFIG_USB_EHCI_SPLIT_ISO is not set
+ # CONFIG_USB_EHCI_ROOT_HUB_TT is not set
+ # CONFIG_USB_EHCI_TT_NEWSCHED is not set
+ CONFIG_USB_EHCI_BIG_ENDIAN_MMIO=y
+ # CONFIG_USB_ISP116X_HCD is not set
+-CONFIG_USB_OHCI_HCD=y
++CONFIG_USB_OHCI_HCD=m
+ # CONFIG_USB_OHCI_HCD_PPC_OF is not set
+ # CONFIG_USB_OHCI_BIG_ENDIAN_DESC is not set
+ CONFIG_USB_OHCI_BIG_ENDIAN_MMIO=y
+@@ -1033,7 +1053,8 @@ CONFIG_HAS_IOMEM=y
+ CONFIG_HAS_IOPORT=y
+ CONFIG_HAS_DMA=y
+ CONFIG_INSTRUMENTATION=y
+-# CONFIG_PROFILING is not set
++CONFIG_PROFILING=y
++CONFIG_OPROFILE=m
+ # CONFIG_KPROBES is not set
+ # CONFIG_MARKERS is not set
+ 
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-lpm-device-support.patch linux-2.6.25-id/patches/ps3-stable/ps3-lpm-device-support.patch
--- linux-2.6.25-org/patches/ps3-stable/ps3-lpm-device-support.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-lpm-device-support.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,178 @@
+Subject: PS3: Add logical performance monitor device support
+
+Add PS3 logical performance monitor device support to the
+PS3 system-bus and platform device registration routines.
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+v2: o Add enum ps3_lpm_tb_type.
+    o Remove redundant enclosing structure and return proper
+      error codes from ps3_register_lpm_devices().
+v3: o Add lpm.node_id to struct ps3_system_bus_device.
+    o Cleanup repository discovery logic.
+
+
+ arch/powerpc/platforms/ps3/device-init.c |   85 +++++++++++++++++++++++++++++++
+ arch/powerpc/platforms/ps3/system-bus.c  |    5 +
+ include/asm-powerpc/ps3.h                |    8 ++
+ 3 files changed, 98 insertions(+)
+ create mode 100644 arch/powerpc/platforms/ps3/lpm.c
+
+--- a/arch/powerpc/platforms/ps3/device-init.c
++++ b/arch/powerpc/platforms/ps3/device-init.c
+@@ -31,6 +31,89 @@
+ 
+ #include "platform.h"
+ 
++static int __init ps3_register_lpm_devices(void)
++{
++	int result;
++	u64 tmp1;
++	u64 tmp2;
++	struct ps3_system_bus_device *dev;
++
++	pr_debug(" -> %s:%d\n", __func__, __LINE__);
++
++	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
++	if (!dev)
++		return -ENOMEM;
++
++	dev->match_id = PS3_MATCH_ID_LPM;
++	dev->dev_type = PS3_DEVICE_TYPE_LPM;
++
++	/* The current lpm driver only supports a single BE processor. */
++
++	result = ps3_repository_read_be_node_id(0, &dev->lpm.node_id);
++
++	if (result) {
++		pr_debug("%s:%d: ps3_repository_read_be_node_id failed \n",
++			__func__, __LINE__);
++		goto fail_read_repo;
++	}
++
++	result = ps3_repository_read_lpm_privileges(dev->lpm.node_id, &tmp1,
++		&dev->lpm.rights);
++
++	if (result) {
++		pr_debug("%s:%d: ps3_repository_read_lpm_privleges failed \n",
++			__func__, __LINE__);
++		goto fail_read_repo;
++	}
++
++	lv1_get_logical_partition_id(&tmp2);
++
++	if (tmp1 != tmp2) {
++		pr_debug("%s:%d: wrong lpar\n",
++			__func__, __LINE__);
++		result = -ENODEV;
++		goto fail_rights;
++	}
++
++	if (!(dev->lpm.rights & PS3_LPM_RIGHTS_USE_LPM)) {
++		pr_debug("%s:%d: don't have rights to use lpm\n",
++			__func__, __LINE__);
++		result = -EPERM;
++		goto fail_rights;
++	}
++
++	pr_debug("%s:%d: pu_id %lu, rights %lu(%lxh)\n",
++		__func__, __LINE__, dev->lpm.pu_id, dev->lpm.rights,
++		dev->lpm.rights);
++
++	result = ps3_repository_read_pu_id(0, &dev->lpm.pu_id);
++
++	if (result) {
++		pr_debug("%s:%d: ps3_repository_read_pu_id failed \n",
++			__func__, __LINE__);
++		goto fail_read_repo;
++	}
++
++	result = ps3_system_bus_device_register(dev);
++
++	if (result) {
++		pr_debug("%s:%d ps3_system_bus_device_register failed\n",
++			__func__, __LINE__);
++		goto fail_register;
++	}
++
++	pr_debug(" <- %s:%d\n", __func__, __LINE__);
++	return 0;
++
++
++fail_register:
++fail_rights:
++fail_read_repo:
++	kfree(dev);
++	pr_debug(" <- %s:%d: failed\n", __func__, __LINE__);
++	return result;
++}
++
+ /**
+  * ps3_setup_gelic_device - Setup and register a gelic device instance.
+  *
+@@ -827,6 +910,8 @@ static int __init ps3_register_devices(v
+ 
+ 	ps3_register_sound_devices();
+ 
++	ps3_register_lpm_devices();
++
+ 	pr_debug(" <- %s:%d\n", __func__, __LINE__);
+ 	return 0;
+ }
+--- a/arch/powerpc/platforms/ps3/system-bus.c
++++ b/arch/powerpc/platforms/ps3/system-bus.c
+@@ -715,6 +715,7 @@ int ps3_system_bus_device_register(struc
+ 	static unsigned int dev_ioc0_count;
+ 	static unsigned int dev_sb_count;
+ 	static unsigned int dev_vuart_count;
++	static unsigned int dev_lpm_count;
+ 
+ 	if (!dev->core.parent)
+ 		dev->core.parent = &ps3_system_bus;
+@@ -737,6 +738,10 @@ int ps3_system_bus_device_register(struc
+ 		snprintf(dev->core.bus_id, sizeof(dev->core.bus_id),
+ 			"vuart_%02x", ++dev_vuart_count);
+ 		break;
++	case PS3_DEVICE_TYPE_LPM:
++		snprintf(dev->core.bus_id, sizeof(dev->core.bus_id),
++			"lpm_%02x", ++dev_lpm_count);
++		break;
+ 	default:
+ 		BUG();
+ 	};
+--- a/include/asm-powerpc/ps3.h
++++ b/include/asm-powerpc/ps3.h
+@@ -317,6 +317,7 @@ enum ps3_match_id {
+ 	PS3_MATCH_ID_STOR_FLASH     = 8,
+ 	PS3_MATCH_ID_SOUND          = 9,
+ 	PS3_MATCH_ID_GRAPHICS       = 10,
++	PS3_MATCH_ID_LPM            = 11,
+ };
+ 
+ #define PS3_MODULE_ALIAS_EHCI           "ps3:1"
+@@ -329,11 +330,13 @@ enum ps3_match_id {
+ #define PS3_MODULE_ALIAS_STOR_FLASH     "ps3:8"
+ #define PS3_MODULE_ALIAS_SOUND          "ps3:9"
+ #define PS3_MODULE_ALIAS_GRAPHICS       "ps3:10"
++#define PS3_MODULE_ALIAS_LPM            "ps3:11"
+ 
+ enum ps3_system_bus_device_type {
+ 	PS3_DEVICE_TYPE_IOC0 = 1,
+ 	PS3_DEVICE_TYPE_SB,
+ 	PS3_DEVICE_TYPE_VUART,
++	PS3_DEVICE_TYPE_LPM,
+ };
+ 
+ /**
+@@ -350,6 +353,11 @@ struct ps3_system_bus_device {
+ 	struct ps3_dma_region *d_region;  /* SB, IOC0 */
+ 	struct ps3_mmio_region *m_region; /* SB, IOC0*/
+ 	unsigned int port_number;         /* VUART */
++	struct {                          /* LPM */
++		u64 node_id;
++		u64 pu_id;
++		u64 rights;
++	} lpm;
+ 
+ /*	struct iommu_table *iommu_table; -- waiting for BenH's cleanups */
+ 	struct device core;
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-lpm-driver-support.patch linux-2.6.25-id/patches/ps3-stable/ps3-lpm-driver-support.patch
--- linux-2.6.25-org/patches/ps3-stable/ps3-lpm-driver-support.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-lpm-driver-support.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,1390 @@
+Subject: PS3: Add logical performance monitor driver support
+
+From: Takashi Yamamoto <TakashiA.Yamamoto@jp.sony.com>
+
+Add PS3 logical performance monitor (lpm) device driver.
+
+The PS3's LV1 hypervisor provides a Logical Performance Monitor that
+abstracts the Cell processor's performance monitor features for use
+by guest operating systems.
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto@jp.sony.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+v2: o Correct Yamamoto-san's mail addr.
+    o Text cleanups.
+    o Added more comments explaining lpm operation.
+    o Split SRPN_BKMK into separate patch.
+    o Use get_hard_smp_processor_id() in ps3_get_hw_thread_id().
+    o Split ps3_copy_trace_buffer() into ps3_lpm_copy_tb() and ps3_lpm_copy_tb_to_user().
+    o Replace mutex with atomic_t in ps3_lpm_open()/ps3_lpm_close().
+    o General cleanup of ps3_lpm_open()/ps3_lpm_close().
+v3:
+   o Add BE node_id to struct lpm_priv.
+   o Change some dev_err() to dev_dbg().
+   o Fix kzalloc() bug.
+   o Text fix 'lost' -> 'loss'.
+   o Use lpm_priv->node_id with lv1_construct_lpm().
+
+ arch/powerpc/platforms/ps3/Kconfig |   13 
+ drivers/ps3/Makefile               |    1 
+ drivers/ps3/ps3-lpm.c              | 1248 +++++++++++++++++++++++++++++++++++++
+ include/asm-powerpc/ps3.h          |   62 +
+ 4 files changed, 1324 insertions(+)
+ create mode 100644 arch/powerpc/platforms/ps3/ps3-lpm.c
+
+--- a/arch/powerpc/platforms/ps3/Kconfig
++++ b/arch/powerpc/platforms/ps3/Kconfig
+@@ -138,4 +138,17 @@ config PS3_FLASH
+ 	  be disabled on the kernel command line using "ps3flash=off", to
+ 	  not allocate this fixed buffer.
+ 
++config PS3_LPM
++	tristate "PS3 Logical Performance Monitor support"
++	depends on PPC_PS3
++	help
++	  Include support for the PS3 Logical Performance Monitor.
++
++	  This support is required to use the logical performance monitor
++	  of the PS3's LV1 hypervisor.
++
++	  If you intend to use the advanced performance monitoring and
++	  profiling support of the Cell processor with programs like
++	  oprofile and perfmon2, then say Y or M, otherwise say N.
++
+ endmenu
+--- a/drivers/ps3/Makefile
++++ b/drivers/ps3/Makefile
+@@ -4,3 +4,4 @@ ps3av_mod-objs		+= ps3av.o ps3av_cmd.o
+ obj-$(CONFIG_PPC_PS3) += sys-manager-core.o
+ obj-$(CONFIG_PS3_SYS_MANAGER) += ps3-sys-manager.o
+ obj-$(CONFIG_PS3_STORAGE) += ps3stor_lib.o
++obj-$(CONFIG_PS3_LPM) += ps3-lpm.o
+--- /dev/null
++++ b/drivers/ps3/ps3-lpm.c
+@@ -0,0 +1,1248 @@
++/*
++ * PS3 Logical Performance Monitor.
++ *
++ *  Copyright (C) 2007 Sony Computer Entertainment Inc.
++ *  Copyright 2007 Sony Corp.
++ *
++ *  This program is free software; you can redistribute it and/or modify
++ *  it under the terms of the GNU General Public License as published by
++ *  the Free Software Foundation; version 2 of the License.
++ *
++ *  This program is distributed in the hope that it will be useful,
++ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
++ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ *  GNU General Public License for more details.
++ *
++ *  You should have received a copy of the GNU General Public License
++ *  along with this program; if not, write to the Free Software
++ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
++ */
++
++#include <linux/kernel.h>
++#include <linux/module.h>
++#include <linux/interrupt.h>
++#include <linux/uaccess.h>
++#include <asm/ps3.h>
++#include <asm/lv1call.h>
++#include <asm/cell-pmu.h>
++
++
++/* BOOKMARK tag macros */
++#define PS3_PM_BOOKMARK_START                    0x8000000000000000ULL
++#define PS3_PM_BOOKMARK_STOP                     0x4000000000000000ULL
++#define PS3_PM_BOOKMARK_TAG_KERNEL               0x1000000000000000ULL
++#define PS3_PM_BOOKMARK_TAG_USER                 0x3000000000000000ULL
++#define PS3_PM_BOOKMARK_TAG_MASK_HI              0xF000000000000000ULL
++#define PS3_PM_BOOKMARK_TAG_MASK_LO              0x0F00000000000000ULL
++
++/* CBE PM CONTROL register macros */
++#define PS3_PM_CONTROL_PPU_TH0_BOOKMARK          0x00001000
++#define PS3_PM_CONTROL_PPU_TH1_BOOKMARK          0x00000800
++#define PS3_PM_CONTROL_PPU_COUNT_MODE_MASK       0x000C0000
++#define PS3_PM_CONTROL_PPU_COUNT_MODE_PROBLEM    0x00080000
++#define PS3_WRITE_PM_MASK                        0xFFFFFFFFFFFFFFFFULL
++
++/* CBE PM START STOP register macros */
++#define PS3_PM_START_STOP_PPU_TH0_BOOKMARK_START 0x02000000
++#define PS3_PM_START_STOP_PPU_TH1_BOOKMARK_START 0x01000000
++#define PS3_PM_START_STOP_PPU_TH0_BOOKMARK_STOP  0x00020000
++#define PS3_PM_START_STOP_PPU_TH1_BOOKMARK_STOP  0x00010000
++#define PS3_PM_START_STOP_START_MASK             0xFF000000
++#define PS3_PM_START_STOP_STOP_MASK              0x00FF0000
++
++/* CBE PM COUNTER register macres */
++#define PS3_PM_COUNTER_MASK_HI                   0xFFFFFFFF00000000ULL
++#define PS3_PM_COUNTER_MASK_LO                   0x00000000FFFFFFFFULL
++
++/* BASE SIGNAL GROUP NUMBER macros */
++#define PM_ISLAND2_BASE_SIGNAL_GROUP_NUMBER  0
++#define PM_ISLAND2_SIGNAL_GROUP_NUMBER1      6
++#define PM_ISLAND2_SIGNAL_GROUP_NUMBER2      7
++#define PM_ISLAND3_BASE_SIGNAL_GROUP_NUMBER  7
++#define PM_ISLAND4_BASE_SIGNAL_GROUP_NUMBER  15
++#define PM_SPU_TRIGGER_SIGNAL_GROUP_NUMBER   17
++#define PM_SPU_EVENT_SIGNAL_GROUP_NUMBER     18
++#define PM_ISLAND5_BASE_SIGNAL_GROUP_NUMBER  18
++#define PM_ISLAND6_BASE_SIGNAL_GROUP_NUMBER  24
++#define PM_ISLAND7_BASE_SIGNAL_GROUP_NUMBER  49
++#define PM_ISLAND8_BASE_SIGNAL_GROUP_NUMBER  52
++#define PM_SIG_GROUP_SPU                     41
++#define PM_SIG_GROUP_SPU_TRIGGER             42
++#define PM_SIG_GROUP_SPU_EVENT               43
++#define PM_SIG_GROUP_MFC_MAX                 60
++
++/**
++ * struct ps3_lpm_shadow_regs - Performance monitor shadow registers.
++ *
++ * @pm_control: Shadow of the processor's pm_control register.
++ * @pm_start_stop: Shadow of the processor's pm_start_stop register.
++ * @pm_interval: Shadow of the processor's pm_interval register.
++ * @group_control: Shadow of the processor's group_control register.
++ * @debug_bus_control: Shadow of the processor's debug_bus_control register.
++ *
++ * The logical performance monitor provides a write-only interface to
++ * these processor registers.  These shadow variables cache the processor
++ * register values for reading.
++ *
++ * The initial value of the shadow registers at lpm creation is
++ * PS3_LPM_SHADOW_REG_INIT.
++ */
++
++struct ps3_lpm_shadow_regs {
++	u64 pm_control;
++	u64 pm_start_stop;
++	u64 pm_interval;
++	u64 group_control;
++	u64 debug_bus_control;
++};
++
++#define PS3_LPM_SHADOW_REG_INIT 0xFFFFFFFF00000000ULL
++
++/**
++ * struct ps3_lpm_priv - Private lpm device data.
++ *
++ * @open: An atomic variable indicating the lpm driver has been opened.
++ * @rights: The lpm rigths granted by the system policy module.  A logical
++ *  OR of enum ps3_lpm_rights.
++ * @node_id: The node id of a BE prosessor whose performance monitor this
++ *  lpar has the right to use.
++ * @pu_id: The lv1 id of the logical PU.
++ * @lpm_id: The lv1 id of this lpm instance.
++ * @outlet_id: The outlet created by lv1 for this lpm instance.
++ * @tb_count: The number of bytes of data held in the lv1 trace buffer.
++ * @tb_cache: Kernel buffer to receive the data from the lv1 trace buffer.
++ *  Must be 128 byte aligned.
++ * @tb_cache_size: Size of the kernel @tb_cache buffer.  Must be 128 byte
++ *  aligned.
++ * @tb_cache_internal: An unaligned buffer allocated by this driver to be
++ *  used for the trace buffer cache when ps3_lpm_open() is called with a
++ *  NULL tb_cache argument.  Otherwise unused.
++ * @shadow: Processor register shadow of type struct ps3_lpm_shadow_regs.
++ * @sbd: The struct ps3_system_bus_device attached to this driver.
++ *
++ * The trace buffer is a buffer allocated and used internally to the lv1
++ * hypervisor to collect trace data.  The trace buffer cache is a guest
++ * buffer that accepts the trace data from the trace buffer.
++ */
++
++struct ps3_lpm_priv {
++	atomic_t open;
++	u64 rights;
++	u64 node_id;
++	u64 pu_id;
++	u64 lpm_id;
++	u64 outlet_id;
++	u64 tb_count;
++	void *tb_cache;
++	u64 tb_cache_size;
++	void *tb_cache_internal;
++	struct ps3_lpm_shadow_regs shadow;
++	struct ps3_system_bus_device *sbd;
++};
++
++enum {
++	PS3_LPM_DEFAULT_TB_CACHE_SIZE = 0x4000,
++};
++
++/**
++ * lpm_priv - Static instance of the lpm data.
++ *
++ * Since the exported routines don't support the notion of a device
++ * instance we need to hold the instance in this static variable
++ * and then only allow at most one instance at a time to be created.
++ */
++
++static struct ps3_lpm_priv *lpm_priv;
++
++static struct device *sbd_core(void)
++{
++	BUG_ON(!lpm_priv || !lpm_priv->sbd);
++	return &lpm_priv->sbd->core;
++}
++
++/**
++ * use_start_stop_bookmark - Enable the PPU bookmark trace.
++ *
++ * And it enables PPU bookmark triggers ONLY if the other triggers are not set.
++ * The start/stop bookmarks are inserted at ps3_enable_pm() and ps3_disable_pm()
++ * to start/stop LPM.
++ *
++ * Used to get good quality of the performance counter.
++ */
++
++enum {use_start_stop_bookmark = 1,};
++
++void ps3_set_bookmark(u64 bookmark)
++{
++	/*
++	 * As per the PPE book IV, to avoid bookmark loss there must
++	 * not be a traced branch within 10 cycles of setting the
++	 * SPRN_BKMK register.  The actual text is unclear if 'within'
++	 * includes cycles before the call.
++	 */
++
++	asm volatile("or 29, 29, 29;"); /* db10cyc */
++	mtspr(SPRN_BKMK, bookmark);
++	asm volatile("or 29, 29, 29;"); /* db10cyc */
++}
++EXPORT_SYMBOL_GPL(ps3_set_bookmark);
++
++void ps3_set_pm_bookmark(u64 tag, u64 incident, u64 th_id)
++{
++	u64 bookmark;
++
++	bookmark = (get_tb() & 0x00000000FFFFFFFFULL) |
++		PS3_PM_BOOKMARK_TAG_KERNEL;
++	bookmark = ((tag << 56) & PS3_PM_BOOKMARK_TAG_MASK_LO) |
++		(incident << 48) | (th_id << 32) | bookmark;
++	ps3_set_bookmark(bookmark);
++}
++EXPORT_SYMBOL_GPL(ps3_set_pm_bookmark);
++
++/**
++ * ps3_read_phys_ctr - Read physical counter registers.
++ *
++ * Each physical counter can act as one 32 bit counter or as two 16 bit
++ * counters.
++ */
++
++u32 ps3_read_phys_ctr(u32 cpu, u32 phys_ctr)
++{
++	int result;
++	u64 counter0415;
++	u64 counter2637;
++
++	if (phys_ctr >= NR_PHYS_CTRS) {
++		dev_dbg(sbd_core(), "%s:%u: phys_ctr too big: %u\n", __func__,
++			__LINE__, phys_ctr);
++		return 0;
++	}
++
++	result = lv1_set_lpm_counter(lpm_priv->lpm_id, 0, 0, 0, 0, &counter0415,
++				     &counter2637);
++	if (result) {
++		dev_err(sbd_core(), "%s:%u: lv1_set_lpm_counter failed: "
++			"phys_ctr %u, %s\n", __func__, __LINE__, phys_ctr,
++			ps3_result(result));
++		return 0;
++	}
++
++	switch (phys_ctr) {
++	case 0:
++		return counter0415 >> 32;
++	case 1:
++		return counter0415 & PS3_PM_COUNTER_MASK_LO;
++	case 2:
++		return counter2637 >> 32;
++	case 3:
++		return counter2637 & PS3_PM_COUNTER_MASK_LO;
++	default:
++		BUG();
++	}
++	return 0;
++}
++EXPORT_SYMBOL_GPL(ps3_read_phys_ctr);
++
++/**
++ * ps3_write_phys_ctr - Write physical counter registers.
++ *
++ * Each physical counter can act as one 32 bit counter or as two 16 bit
++ * counters.
++ */
++
++void ps3_write_phys_ctr(u32 cpu, u32 phys_ctr, u32 val)
++{
++	u64 counter0415;
++	u64 counter0415_mask;
++	u64 counter2637;
++	u64 counter2637_mask;
++	int result;
++
++	if (phys_ctr >= NR_PHYS_CTRS) {
++		dev_dbg(sbd_core(), "%s:%u: phys_ctr too big: %u\n", __func__,
++			__LINE__, phys_ctr);
++		return;
++	}
++
++	switch (phys_ctr) {
++	case 0:
++		counter0415 = (u64)val << 32;
++		counter0415_mask = PS3_PM_COUNTER_MASK_HI;
++		counter2637 = 0x0;
++		counter2637_mask = 0x0;
++		break;
++	case 1:
++		counter0415 = (u64)val;
++		counter0415_mask = PS3_PM_COUNTER_MASK_LO;
++		counter2637 = 0x0;
++		counter2637_mask = 0x0;
++		break;
++	case 2:
++		counter0415 = 0x0;
++		counter0415_mask = 0x0;
++		counter2637 = (u64)val << 32;
++		counter2637_mask = PS3_PM_COUNTER_MASK_HI;
++		break;
++	case 3:
++		counter0415 = 0x0;
++		counter0415_mask = 0x0;
++		counter2637 = (u64)val;
++		counter2637_mask = PS3_PM_COUNTER_MASK_LO;
++		break;
++	default:
++		BUG();
++	}
++
++	result = lv1_set_lpm_counter(lpm_priv->lpm_id,
++				     counter0415, counter0415_mask,
++				     counter2637, counter2637_mask,
++				     &counter0415, &counter2637);
++	if (result)
++		dev_err(sbd_core(), "%s:%u: lv1_set_lpm_counter failed: "
++			"phys_ctr %u, val %u, %s\n", __func__, __LINE__,
++			phys_ctr, val, ps3_result(result));
++}
++EXPORT_SYMBOL_GPL(ps3_write_phys_ctr);
++
++/**
++ * ps3_read_ctr - Read counter.
++ *
++ * Read 16 or 32 bits depending on the current size of the counter.
++ * Counters 4, 5, 6 & 7 are always 16 bit.
++ */
++
++u32 ps3_read_ctr(u32 cpu, u32 ctr)
++{
++	u32 val;
++	u32 phys_ctr = ctr & (NR_PHYS_CTRS - 1);
++
++	val = ps3_read_phys_ctr(cpu, phys_ctr);
++
++	if (ps3_get_ctr_size(cpu, phys_ctr) == 16)
++		val = (ctr < NR_PHYS_CTRS) ? (val >> 16) : (val & 0xffff);
++
++	return val;
++}
++EXPORT_SYMBOL_GPL(ps3_read_ctr);
++
++/**
++ * ps3_write_ctr - Write counter.
++ *
++ * Write 16 or 32 bits depending on the current size of the counter.
++ * Counters 4, 5, 6 & 7 are always 16 bit.
++ */
++
++void ps3_write_ctr(u32 cpu, u32 ctr, u32 val)
++{
++	u32 phys_ctr;
++	u32 phys_val;
++
++	phys_ctr = ctr & (NR_PHYS_CTRS - 1);
++
++	if (ps3_get_ctr_size(cpu, phys_ctr) == 16) {
++		phys_val = ps3_read_phys_ctr(cpu, phys_ctr);
++
++		if (ctr < NR_PHYS_CTRS)
++			val = (val << 16) | (phys_val & 0xffff);
++		else
++			val = (val & 0xffff) | (phys_val & 0xffff0000);
++	}
++
++	ps3_write_phys_ctr(cpu, phys_ctr, val);
++}
++EXPORT_SYMBOL_GPL(ps3_write_ctr);
++
++/**
++ * ps3_read_pm07_control - Read counter control registers.
++ *
++ * Each logical counter has a corresponding control register.
++ */
++
++u32 ps3_read_pm07_control(u32 cpu, u32 ctr)
++{
++	return 0;
++}
++EXPORT_SYMBOL_GPL(ps3_read_pm07_control);
++
++/**
++ * ps3_write_pm07_control - Write counter control registers.
++ *
++ * Each logical counter has a corresponding control register.
++ */
++
++void ps3_write_pm07_control(u32 cpu, u32 ctr, u32 val)
++{
++	int result;
++	static const u64 mask = 0xFFFFFFFFFFFFFFFFULL;
++	u64 old_value;
++
++	if (ctr >= NR_CTRS) {
++		dev_dbg(sbd_core(), "%s:%u: ctr too big: %u\n", __func__,
++			__LINE__, ctr);
++		return;
++	}
++
++	result = lv1_set_lpm_counter_control(lpm_priv->lpm_id, ctr, val, mask,
++					     &old_value);
++	if (result)
++		dev_err(sbd_core(), "%s:%u: lv1_set_lpm_counter_control "
++			"failed: ctr %u, %s\n", __func__, __LINE__, ctr,
++			ps3_result(result));
++}
++EXPORT_SYMBOL_GPL(ps3_write_pm07_control);
++
++/**
++ * ps3_read_pm - Read Other LPM control registers.
++ */
++
++u32 ps3_read_pm(u32 cpu, enum pm_reg_name reg)
++{
++	int result = 0;
++	u64 val = 0;
++
++	switch (reg) {
++	case pm_control:
++		return lpm_priv->shadow.pm_control;
++	case trace_address:
++		return CBE_PM_TRACE_BUF_EMPTY;
++	case pm_start_stop:
++		return lpm_priv->shadow.pm_start_stop;
++	case pm_interval:
++		return lpm_priv->shadow.pm_interval;
++	case group_control:
++		return lpm_priv->shadow.group_control;
++	case debug_bus_control:
++		return lpm_priv->shadow.debug_bus_control;
++	case pm_status:
++		result = lv1_get_lpm_interrupt_status(lpm_priv->lpm_id,
++						      &val);
++		if (result) {
++			val = 0;
++			dev_dbg(sbd_core(), "%s:%u: lv1 get_lpm_status failed: "
++				"reg %u, %s\n", __func__, __LINE__, reg,
++				ps3_result(result));
++		}
++		return (u32)val;
++	case ext_tr_timer:
++		return 0;
++	default:
++		dev_dbg(sbd_core(), "%s:%u: unknown reg: %d\n", __func__,
++			__LINE__, reg);
++		BUG();
++		break;
++	}
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(ps3_read_pm);
++
++/**
++ * ps3_write_pm - Write Other LPM control registers.
++ */
++
++void ps3_write_pm(u32 cpu, enum pm_reg_name reg, u32 val)
++{
++	int result = 0;
++	u64 dummy;
++
++	switch (reg) {
++	case group_control:
++		if (val != lpm_priv->shadow.group_control)
++			result = lv1_set_lpm_group_control(lpm_priv->lpm_id,
++							   val,
++							   PS3_WRITE_PM_MASK,
++							   &dummy);
++		lpm_priv->shadow.group_control = val;
++		break;
++	case debug_bus_control:
++		if (val != lpm_priv->shadow.debug_bus_control)
++			result = lv1_set_lpm_debug_bus_control(lpm_priv->lpm_id,
++							      val,
++							      PS3_WRITE_PM_MASK,
++							      &dummy);
++		lpm_priv->shadow.debug_bus_control = val;
++		break;
++	case pm_control:
++		if (use_start_stop_bookmark)
++			val |= (PS3_PM_CONTROL_PPU_TH0_BOOKMARK |
++				PS3_PM_CONTROL_PPU_TH1_BOOKMARK);
++		if (val != lpm_priv->shadow.pm_control)
++			result = lv1_set_lpm_general_control(lpm_priv->lpm_id,
++							     val,
++							     PS3_WRITE_PM_MASK,
++							     0, 0, &dummy,
++							     &dummy);
++		lpm_priv->shadow.pm_control = val;
++		break;
++	case pm_interval:
++		if (val != lpm_priv->shadow.pm_interval)
++			result = lv1_set_lpm_interval(lpm_priv->lpm_id, val,
++						   PS3_WRITE_PM_MASK, &dummy);
++		lpm_priv->shadow.pm_interval = val;
++		break;
++	case pm_start_stop:
++		if (val != lpm_priv->shadow.pm_start_stop)
++			result = lv1_set_lpm_trigger_control(lpm_priv->lpm_id,
++							     val,
++							     PS3_WRITE_PM_MASK,
++							     &dummy);
++		lpm_priv->shadow.pm_start_stop = val;
++		break;
++	case trace_address:
++	case ext_tr_timer:
++	case pm_status:
++		break;
++	default:
++		dev_dbg(sbd_core(), "%s:%u: unknown reg: %d\n", __func__,
++			__LINE__, reg);
++		BUG();
++		break;
++	}
++
++	if (result)
++		dev_err(sbd_core(), "%s:%u: lv1 set_control failed: "
++			"reg %u, %s\n", __func__, __LINE__, reg,
++			ps3_result(result));
++}
++EXPORT_SYMBOL_GPL(ps3_write_pm);
++
++/**
++ * ps3_get_ctr_size - Get the size of a physical counter.
++ *
++ * Returns either 16 or 32.
++ */
++
++u32 ps3_get_ctr_size(u32 cpu, u32 phys_ctr)
++{
++	u32 pm_ctrl;
++
++	if (phys_ctr >= NR_PHYS_CTRS) {
++		dev_dbg(sbd_core(), "%s:%u: phys_ctr too big: %u\n", __func__,
++			__LINE__, phys_ctr);
++		return 0;
++	}
++
++	pm_ctrl = ps3_read_pm(cpu, pm_control);
++	return (pm_ctrl & CBE_PM_16BIT_CTR(phys_ctr)) ? 16 : 32;
++}
++EXPORT_SYMBOL_GPL(ps3_get_ctr_size);
++
++/**
++ * ps3_set_ctr_size - Set the size of a physical counter to 16 or 32 bits.
++ */
++
++void ps3_set_ctr_size(u32 cpu, u32 phys_ctr, u32 ctr_size)
++{
++	u32 pm_ctrl;
++
++	if (phys_ctr >= NR_PHYS_CTRS) {
++		dev_dbg(sbd_core(), "%s:%u: phys_ctr too big: %u\n", __func__,
++			__LINE__, phys_ctr);
++		return;
++	}
++
++	pm_ctrl = ps3_read_pm(cpu, pm_control);
++
++	switch (ctr_size) {
++	case 16:
++		pm_ctrl |= CBE_PM_16BIT_CTR(phys_ctr);
++		ps3_write_pm(cpu, pm_control, pm_ctrl);
++		break;
++
++	case 32:
++		pm_ctrl &= ~CBE_PM_16BIT_CTR(phys_ctr);
++		ps3_write_pm(cpu, pm_control, pm_ctrl);
++		break;
++	default:
++		BUG();
++	}
++}
++EXPORT_SYMBOL_GPL(ps3_set_ctr_size);
++
++static u64 pm_translate_signal_group_number_on_island2(u64 subgroup)
++{
++
++	if (subgroup == 2)
++		subgroup = 3;
++
++	if (subgroup <= 6)
++		return PM_ISLAND2_BASE_SIGNAL_GROUP_NUMBER + subgroup;
++	else if (subgroup == 7)
++		return PM_ISLAND2_SIGNAL_GROUP_NUMBER1;
++	else
++		return PM_ISLAND2_SIGNAL_GROUP_NUMBER2;
++}
++
++static u64 pm_translate_signal_group_number_on_island3(u64 subgroup)
++{
++
++	switch (subgroup) {
++	case 2:
++	case 3:
++	case 4:
++		subgroup += 2;
++		break;
++	case 5:
++		subgroup = 8;
++		break;
++	default:
++		break;
++	}
++	return PM_ISLAND3_BASE_SIGNAL_GROUP_NUMBER + subgroup;
++}
++
++static u64 pm_translate_signal_group_number_on_island4(u64 subgroup)
++{
++	return PM_ISLAND4_BASE_SIGNAL_GROUP_NUMBER + subgroup;
++}
++
++static u64 pm_translate_signal_group_number_on_island5(u64 subgroup)
++{
++
++	switch (subgroup) {
++	case 3:
++		subgroup = 4;
++		break;
++	case 4:
++		subgroup = 6;
++		break;
++	default:
++		break;
++	}
++	return PM_ISLAND5_BASE_SIGNAL_GROUP_NUMBER + subgroup;
++}
++
++static u64 pm_translate_signal_group_number_on_island6(u64 subgroup,
++						       u64 subsubgroup)
++{
++	switch (subgroup) {
++	case 3:
++	case 4:
++	case 5:
++		subgroup += 1;
++		break;
++	default:
++		break;
++	}
++
++	switch (subsubgroup) {
++	case 4:
++	case 5:
++	case 6:
++		subsubgroup += 2;
++		break;
++	case 7:
++	case 8:
++	case 9:
++	case 10:
++		subsubgroup += 4;
++		break;
++	case 11:
++	case 12:
++	case 13:
++		subsubgroup += 5;
++		break;
++	default:
++		break;
++	}
++
++	if (subgroup <= 5)
++		return (PM_ISLAND6_BASE_SIGNAL_GROUP_NUMBER + subgroup);
++	else
++		return (PM_ISLAND6_BASE_SIGNAL_GROUP_NUMBER + subgroup
++			+ subsubgroup - 1);
++}
++
++static u64 pm_translate_signal_group_number_on_island7(u64 subgroup)
++{
++	return PM_ISLAND7_BASE_SIGNAL_GROUP_NUMBER + subgroup;
++}
++
++static u64 pm_translate_signal_group_number_on_island8(u64 subgroup)
++{
++	return PM_ISLAND8_BASE_SIGNAL_GROUP_NUMBER + subgroup;
++}
++
++static u64 pm_signal_group_to_ps3_lv1_signal_group(u64 group)
++{
++	u64 island;
++	u64 subgroup;
++	u64 subsubgroup;
++
++	subgroup = 0;
++	subsubgroup = 0;
++	island = 0;
++	if (group < 1000) {
++		if (group < 100) {
++			if (20 <= group && group < 30) {
++				island = 2;
++				subgroup = group - 20;
++			} else if (30 <= group && group < 40) {
++				island = 3;
++				subgroup = group - 30;
++			} else if (40 <= group && group < 50) {
++				island = 4;
++				subgroup = group - 40;
++			} else if (50 <= group && group < 60) {
++				island = 5;
++				subgroup = group - 50;
++			} else if (60 <= group && group < 70) {
++				island = 6;
++				subgroup = group - 60;
++			} else if (70 <= group && group < 80) {
++				island = 7;
++				subgroup = group - 70;
++			} else if (80 <= group && group < 90) {
++				island = 8;
++				subgroup = group - 80;
++			}
++		} else if (200 <= group && group < 300) {
++			island = 2;
++			subgroup = group - 200;
++		} else if (600 <= group && group < 700) {
++			island = 6;
++			subgroup = 5;
++			subsubgroup = group - 650;
++		}
++	} else if (6000 <= group && group < 7000) {
++		island = 6;
++		subgroup = 5;
++		subsubgroup = group - 6500;
++	}
++
++	switch (island) {
++	case 2:
++		return pm_translate_signal_group_number_on_island2(subgroup);
++	case 3:
++		return pm_translate_signal_group_number_on_island3(subgroup);
++	case 4:
++		return pm_translate_signal_group_number_on_island4(subgroup);
++	case 5:
++		return pm_translate_signal_group_number_on_island5(subgroup);
++	case 6:
++		return pm_translate_signal_group_number_on_island6(subgroup,
++								   subsubgroup);
++	case 7:
++		return pm_translate_signal_group_number_on_island7(subgroup);
++	case 8:
++		return pm_translate_signal_group_number_on_island8(subgroup);
++	default:
++		dev_dbg(sbd_core(), "%s:%u: island not found: %lu\n", __func__,
++			__LINE__, group);
++		BUG();
++		break;
++	}
++	return 0;
++}
++
++static u64 pm_bus_word_to_ps3_lv1_bus_word(u8 word)
++{
++
++	switch (word) {
++	case 1:
++		return 0xF000;
++	case 2:
++		return 0x0F00;
++	case 4:
++		return 0x00F0;
++	case 8:
++	default:
++		return 0x000F;
++	}
++}
++
++static int __ps3_set_signal(u64 lv1_signal_group, u64 bus_select,
++			    u64 signal_select, u64 attr1, u64 attr2, u64 attr3)
++{
++	int ret;
++
++	ret = lv1_set_lpm_signal(lpm_priv->lpm_id, lv1_signal_group, bus_select,
++				 signal_select, attr1, attr2, attr3);
++	if (ret)
++		dev_err(sbd_core(),
++			"%s:%u: error:%d 0x%lx 0x%lx 0x%lx 0x%lx 0x%lx 0x%lx\n",
++			__func__, __LINE__, ret, lv1_signal_group, bus_select,
++			signal_select, attr1, attr2, attr3);
++
++	return ret;
++}
++
++int ps3_set_signal(u64 signal_group, u8 signal_bit, u16 sub_unit,
++		   u8 bus_word)
++{
++	int ret;
++	u64 lv1_signal_group;
++	u64 bus_select;
++	u64 signal_select;
++	u64 attr1, attr2, attr3;
++
++	if (signal_group == 0)
++		return __ps3_set_signal(0, 0, 0, 0, 0, 0);
++
++	lv1_signal_group =
++		pm_signal_group_to_ps3_lv1_signal_group(signal_group);
++	bus_select = pm_bus_word_to_ps3_lv1_bus_word(bus_word);
++
++	switch (signal_group) {
++	case PM_SIG_GROUP_SPU_TRIGGER:
++		signal_select = 1;
++		signal_select = signal_select << (63 - signal_bit);
++		break;
++	case PM_SIG_GROUP_SPU_EVENT:
++		signal_select = 1;
++		signal_select = (signal_select << (63 - signal_bit)) | 0x3;
++		break;
++	default:
++		signal_select = 0;
++		break;
++	}
++
++	/*
++	 * 0: physical object.
++	 * 1: logical object.
++	 * This parameter is only used for the PPE and SPE signals.
++	 */
++	attr1 = 1;
++
++	/*
++	 * This parameter is used to specify the target physical/logical
++	 * PPE/SPE object.
++	 */
++	if (PM_SIG_GROUP_SPU <= signal_group &&
++		signal_group < PM_SIG_GROUP_MFC_MAX)
++		attr2 = sub_unit;
++	else
++		attr2 = lpm_priv->pu_id;
++
++	/*
++	 * This parameter is only used for setting the SPE signal.
++	 */
++	attr3 = 0;
++
++	ret = __ps3_set_signal(lv1_signal_group, bus_select, signal_select,
++			       attr1, attr2, attr3);
++	if (ret)
++		dev_err(sbd_core(), "%s:%u: __ps3_set_signal failed: %d\n",
++			__func__, __LINE__, ret);
++
++	return ret;
++}
++EXPORT_SYMBOL_GPL(ps3_set_signal);
++
++u32 ps3_get_hw_thread_id(int cpu)
++{
++	return get_hard_smp_processor_id(cpu);
++}
++EXPORT_SYMBOL_GPL(ps3_get_hw_thread_id);
++
++/**
++ * ps3_enable_pm - Enable the entire performance monitoring unit.
++ *
++ * When we enable the LPM, all pending writes to counters get committed.
++ */
++
++void ps3_enable_pm(u32 cpu)
++{
++	int result;
++	u64 tmp;
++	int insert_bookmark = 0;
++
++	lpm_priv->tb_count = 0;
++
++	if (use_start_stop_bookmark) {
++		if (!(lpm_priv->shadow.pm_start_stop &
++			(PS3_PM_START_STOP_START_MASK
++			| PS3_PM_START_STOP_STOP_MASK))) {
++			result = lv1_set_lpm_trigger_control(lpm_priv->lpm_id,
++				(PS3_PM_START_STOP_PPU_TH0_BOOKMARK_START |
++				PS3_PM_START_STOP_PPU_TH1_BOOKMARK_START |
++				PS3_PM_START_STOP_PPU_TH0_BOOKMARK_STOP |
++				PS3_PM_START_STOP_PPU_TH1_BOOKMARK_STOP),
++				0xFFFFFFFFFFFFFFFFULL, &tmp);
++
++			if (result)
++				dev_err(sbd_core(), "%s:%u: "
++					"lv1_set_lpm_trigger_control failed: "
++					"%s\n", __func__, __LINE__,
++					ps3_result(result));
++
++			insert_bookmark = !result;
++		}
++	}
++
++	result = lv1_start_lpm(lpm_priv->lpm_id);
++
++	if (result)
++		dev_err(sbd_core(), "%s:%u: lv1_start_lpm failed: %s\n",
++			__func__, __LINE__, ps3_result(result));
++
++	if (use_start_stop_bookmark && !result && insert_bookmark)
++		ps3_set_bookmark(get_tb() | PS3_PM_BOOKMARK_START);
++}
++EXPORT_SYMBOL_GPL(ps3_enable_pm);
++
++/**
++ * ps3_disable_pm - Disable the entire performance monitoring unit.
++ */
++
++void ps3_disable_pm(u32 cpu)
++{
++	int result;
++	u64 tmp;
++
++	ps3_set_bookmark(get_tb() | PS3_PM_BOOKMARK_STOP);
++
++	result = lv1_stop_lpm(lpm_priv->lpm_id, &tmp);
++
++	if (result) {
++		if(result != LV1_WRONG_STATE)
++			dev_err(sbd_core(), "%s:%u: lv1_stop_lpm failed: %s\n",
++				__func__, __LINE__, ps3_result(result));
++		return;
++	}
++
++	lpm_priv->tb_count = tmp;
++
++	dev_dbg(sbd_core(), "%s:%u: tb_count %lu (%lxh)\n", __func__, __LINE__,
++		lpm_priv->tb_count, lpm_priv->tb_count);
++}
++EXPORT_SYMBOL_GPL(ps3_disable_pm);
++
++/**
++ * ps3_lpm_copy_tb - Copy data from the trace buffer to a kernel buffer.
++ * @offset: Offset in bytes from the start of the trace buffer.
++ * @buf: Copy destination.
++ * @count: Maximum count of bytes to copy.
++ * @bytes_copied: Pointer to a variable that will recieve the number of
++ *  bytes copied to @buf.
++ *
++ * On error @buf will contain any successfully copied trace buffer data
++ * and bytes_copied will be set to the number of bytes successfully copied.
++ */
++
++int ps3_lpm_copy_tb(unsigned long offset, void *buf, unsigned long count,
++		    unsigned long *bytes_copied)
++{
++	int result;
++
++	*bytes_copied = 0;
++
++	if (!lpm_priv->tb_cache)
++		return -EPERM;
++
++	if (offset >= lpm_priv->tb_count)
++		return 0;
++
++	count = min(count, lpm_priv->tb_count - offset);
++
++	while (*bytes_copied < count) {
++		const unsigned long request = count - *bytes_copied;
++		u64 tmp;
++
++		result = lv1_copy_lpm_trace_buffer(lpm_priv->lpm_id, offset,
++						   request, &tmp);
++		if (result) {
++			dev_dbg(sbd_core(), "%s:%u: 0x%lx bytes at 0x%lx\n",
++				__func__, __LINE__, request, offset);
++
++			dev_err(sbd_core(), "%s:%u: lv1_copy_lpm_trace_buffer "
++				"failed: %s\n", __func__, __LINE__,
++				ps3_result(result));
++			return result == LV1_WRONG_STATE ? -EBUSY : -EINVAL;
++		}
++
++		memcpy(buf, lpm_priv->tb_cache, tmp);
++		buf += tmp;
++		*bytes_copied += tmp;
++		offset += tmp;
++	}
++	dev_dbg(sbd_core(), "%s:%u: copied %lxh bytes\n", __func__, __LINE__,
++		*bytes_copied);
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(ps3_lpm_copy_tb);
++
++/**
++ * ps3_lpm_copy_tb_to_user - Copy data from the trace buffer to a user buffer.
++ * @offset: Offset in bytes from the start of the trace buffer.
++ * @buf: A __user copy destination.
++ * @count: Maximum count of bytes to copy.
++ * @bytes_copied: Pointer to a variable that will recieve the number of
++ *  bytes copied to @buf.
++ *
++ * On error @buf will contain any successfully copied trace buffer data
++ * and bytes_copied will be set to the number of bytes successfully copied.
++ */
++
++int ps3_lpm_copy_tb_to_user(unsigned long offset, void __user *buf,
++			    unsigned long count, unsigned long *bytes_copied)
++{
++	int result;
++
++	*bytes_copied = 0;
++
++	if (!lpm_priv->tb_cache)
++		return -EPERM;
++
++	if (offset >= lpm_priv->tb_count)
++		return 0;
++
++	count = min(count, lpm_priv->tb_count - offset);
++
++	while (*bytes_copied < count) {
++		const unsigned long request = count - *bytes_copied;
++		u64 tmp;
++
++		result = lv1_copy_lpm_trace_buffer(lpm_priv->lpm_id, offset,
++						   request, &tmp);
++		if (result) {
++			dev_dbg(sbd_core(), "%s:%u: 0x%lx bytes at 0x%lx\n",
++				__func__, __LINE__, request, offset);
++			dev_err(sbd_core(), "%s:%u: lv1_copy_lpm_trace_buffer "
++				"failed: %s\n", __func__, __LINE__,
++				ps3_result(result));
++			return result == LV1_WRONG_STATE ? -EBUSY : -EINVAL;
++		}
++
++		result = copy_to_user(buf, lpm_priv->tb_cache, tmp);
++
++		if (result) {
++			dev_dbg(sbd_core(), "%s:%u: 0x%lx bytes at 0x%p\n",
++				__func__, __LINE__, tmp, buf);
++			dev_err(sbd_core(), "%s:%u: copy_to_user failed: %d\n",
++				__func__, __LINE__, result);
++			return -EFAULT;
++		}
++
++		buf += tmp;
++		*bytes_copied += tmp;
++		offset += tmp;
++	}
++	dev_dbg(sbd_core(), "%s:%u: copied %lxh bytes\n", __func__, __LINE__,
++		*bytes_copied);
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(ps3_lpm_copy_tb_to_user);
++
++/**
++ * ps3_get_and_clear_pm_interrupts -
++ *
++ * Clearing interrupts for the entire performance monitoring unit.
++ * Reading pm_status clears the interrupt bits.
++ */
++
++u32 ps3_get_and_clear_pm_interrupts(u32 cpu)
++{
++	return ps3_read_pm(cpu, pm_status);
++}
++EXPORT_SYMBOL_GPL(ps3_get_and_clear_pm_interrupts);
++
++/**
++ * ps3_enable_pm_interrupts -
++ *
++ * Enabling interrupts for the entire performance monitoring unit.
++ * Enables the interrupt bits in the pm_status register.
++ */
++
++void ps3_enable_pm_interrupts(u32 cpu, u32 thread, u32 mask)
++{
++	if (mask)
++		ps3_write_pm(cpu, pm_status, mask);
++}
++EXPORT_SYMBOL_GPL(ps3_enable_pm_interrupts);
++
++/**
++ * ps3_enable_pm_interrupts -
++ *
++ * Disabling interrupts for the entire performance monitoring unit.
++ */
++
++void ps3_disable_pm_interrupts(u32 cpu)
++{
++	ps3_get_and_clear_pm_interrupts(cpu);
++	ps3_write_pm(cpu, pm_status, 0);
++}
++EXPORT_SYMBOL_GPL(ps3_disable_pm_interrupts);
++
++/**
++ * ps3_lpm_open - Open the logical performance monitor device.
++ * @tb_type: Specifies the type of trace buffer lv1 sould use for this lpm
++ *  instance, specified by one of enum ps3_lpm_tb_type.
++ * @tb_cache: Optional user supplied buffer to use as the trace buffer cache.
++ *  If NULL, the driver will allocate and manage an internal buffer.
++ *  Unused when when @tb_type is PS3_LPM_TB_TYPE_NONE.
++ * @tb_cache_size: The size in bytes of the user supplied @tb_cache buffer.
++ *  Unused when @tb_cache is NULL or @tb_type is PS3_LPM_TB_TYPE_NONE.
++ */
++
++int ps3_lpm_open(enum ps3_lpm_tb_type tb_type, void *tb_cache,
++	u64 tb_cache_size)
++{
++	int result;
++	u64 tb_size;
++
++	BUG_ON(!lpm_priv);
++	BUG_ON(tb_type != PS3_LPM_TB_TYPE_NONE
++		&& tb_type != PS3_LPM_TB_TYPE_INTERNAL);
++
++	if (tb_type == PS3_LPM_TB_TYPE_NONE && tb_cache)
++		dev_dbg(sbd_core(), "%s:%u: bad in vals\n", __func__, __LINE__);
++
++	if (!atomic_add_unless(&lpm_priv->open, 1, 1)) {
++		dev_dbg(sbd_core(), "%s:%u: busy\n", __func__, __LINE__);
++		return -EBUSY;
++	}
++
++	/* Note tb_cache needs 128 byte alignment. */
++
++	if (tb_type == PS3_LPM_TB_TYPE_NONE) {
++		lpm_priv->tb_cache_size = 0;
++		lpm_priv->tb_cache_internal = NULL;
++		lpm_priv->tb_cache = NULL;
++	} else if (tb_cache) {
++		if (tb_cache != (void *)_ALIGN_UP((unsigned long)tb_cache, 128)
++			|| tb_cache_size != _ALIGN_UP(tb_cache_size, 128)) {
++			dev_err(sbd_core(), "%s:%u: unaligned tb_cache\n",
++				__func__, __LINE__);
++			result = -EINVAL;
++			goto fail_align;
++		}
++		lpm_priv->tb_cache_size = tb_cache_size;
++		lpm_priv->tb_cache_internal = NULL;
++		lpm_priv->tb_cache = tb_cache;
++	} else {
++		lpm_priv->tb_cache_size = PS3_LPM_DEFAULT_TB_CACHE_SIZE;
++		lpm_priv->tb_cache_internal = kzalloc(
++			lpm_priv->tb_cache_size + 127, GFP_KERNEL);
++		if (!lpm_priv->tb_cache_internal) {
++			dev_err(sbd_core(), "%s:%u: alloc internal tb_cache "
++				"failed\n", __func__, __LINE__);
++			result = -ENOMEM;
++			goto fail_malloc;
++		}
++		lpm_priv->tb_cache = (void *)_ALIGN_UP(
++			(unsigned long)lpm_priv->tb_cache_internal, 128);
++	}
++
++	result = lv1_construct_lpm(lpm_priv->node_id, tb_type, 0, 0,
++				ps3_mm_phys_to_lpar(__pa(lpm_priv->tb_cache)),
++				lpm_priv->tb_cache_size, &lpm_priv->lpm_id,
++				&lpm_priv->outlet_id, &tb_size);
++
++	if (result) {
++		dev_err(sbd_core(), "%s:%u: lv1_construct_lpm failed: %s\n",
++			__func__, __LINE__, ps3_result(result));
++		result = -EINVAL;
++		goto fail_construct;
++	}
++
++	lpm_priv->shadow.pm_control = PS3_LPM_SHADOW_REG_INIT;
++	lpm_priv->shadow.pm_start_stop = PS3_LPM_SHADOW_REG_INIT;
++	lpm_priv->shadow.pm_interval = PS3_LPM_SHADOW_REG_INIT;
++	lpm_priv->shadow.group_control = PS3_LPM_SHADOW_REG_INIT;
++	lpm_priv->shadow.debug_bus_control = PS3_LPM_SHADOW_REG_INIT;
++
++	dev_dbg(sbd_core(), "%s:%u: lpm_id 0x%lx, outlet_id 0x%lx, "
++		"tb_size 0x%lx\n", __func__, __LINE__, lpm_priv->lpm_id,
++		lpm_priv->outlet_id, tb_size);
++
++	return 0;
++
++fail_construct:
++	kfree(lpm_priv->tb_cache_internal);
++	lpm_priv->tb_cache_internal = NULL;
++fail_malloc:
++fail_align:
++	atomic_dec(&lpm_priv->open);
++	return result;
++}
++EXPORT_SYMBOL_GPL(ps3_lpm_open);
++
++/**
++ * ps3_lpm_close - Close the lpm device.
++ *
++ */
++
++int ps3_lpm_close(void)
++{
++	dev_dbg(sbd_core(), "%s:%u\n", __func__, __LINE__);
++
++	lv1_destruct_lpm(lpm_priv->lpm_id);
++	lpm_priv->lpm_id = 0;
++
++	kfree(lpm_priv->tb_cache_internal);
++	lpm_priv->tb_cache_internal = NULL;
++
++	atomic_dec(&lpm_priv->open);
++	return 0;
++}
++EXPORT_SYMBOL_GPL(ps3_lpm_close);
++
++static int __devinit ps3_lpm_probe(struct ps3_system_bus_device *dev)
++{
++	dev_dbg(&dev->core, " -> %s:%u\n", __func__, __LINE__);
++
++	if (lpm_priv) {
++		dev_info(&dev->core, "%s:%u: called twice\n",
++			__func__, __LINE__);
++		return -EBUSY;
++	}
++
++	lpm_priv = kzalloc(sizeof(*lpm_priv), GFP_KERNEL);
++
++	if (!lpm_priv)
++		return -ENOMEM;
++
++	lpm_priv->sbd = dev;
++	lpm_priv->node_id = dev->lpm.node_id;
++	lpm_priv->pu_id = dev->lpm.pu_id;
++	lpm_priv->rights = dev->lpm.rights;
++
++	dev_info(&dev->core, " <- %s:%u:\n", __func__, __LINE__);
++
++	return 0;
++}
++
++static int ps3_lpm_remove(struct ps3_system_bus_device *dev)
++{
++	dev_dbg(&dev->core, " -> %s:%u:\n", __func__, __LINE__);
++
++	ps3_lpm_close();
++
++	kfree(lpm_priv);
++	lpm_priv = NULL;
++
++	dev_info(&dev->core, " <- %s:%u:\n", __func__, __LINE__);
++	return 0;
++}
++
++static struct ps3_system_bus_driver ps3_lpm_driver = {
++	.match_id = PS3_MATCH_ID_LPM,
++	.core.name	= "ps3-lpm",
++	.core.owner	= THIS_MODULE,
++	.probe		= ps3_lpm_probe,
++	.remove		= ps3_lpm_remove,
++	.shutdown	= ps3_lpm_remove,
++};
++
++static int __init ps3_lpm_init(void)
++{
++	pr_debug("%s:%d:\n", __func__, __LINE__);
++	return ps3_system_bus_driver_register(&ps3_lpm_driver);
++}
++
++static void __exit ps3_lpm_exit(void)
++{
++	pr_debug("%s:%d:\n", __func__, __LINE__);
++	ps3_system_bus_driver_unregister(&ps3_lpm_driver);
++}
++
++module_init(ps3_lpm_init);
++module_exit(ps3_lpm_exit);
++
++MODULE_LICENSE("GPL v2");
++MODULE_DESCRIPTION("PS3 Logical Performance Monitor Driver");
++MODULE_AUTHOR("Sony Corporation");
++MODULE_ALIAS(PS3_MODULE_ALIAS_LPM);
+--- a/include/asm-powerpc/ps3.h
++++ b/include/asm-powerpc/ps3.h
+@@ -24,6 +24,7 @@
+ #include <linux/init.h>
+ #include <linux/types.h>
+ #include <linux/device.h>
++#include "cell-pmu.h"
+ 
+ union ps3_firmware_version {
+ 	u64 raw;
+@@ -446,5 +447,66 @@ struct ps3_prealloc {
+ extern struct ps3_prealloc ps3fb_videomemory;
+ extern struct ps3_prealloc ps3flash_bounce_buffer;
+ 
++/* logical performance monitor */
++
++/**
++ * enum ps3_lpm_rights - Rigths granted by the system policy module.
++ *
++ * @PS3_LPM_RIGHTS_USE_LPM: The right to use the lpm.
++ * @PS3_LPM_RIGHTS_USE_TB: The right to use the internal trace buffer.
++ */
++
++enum ps3_lpm_rights {
++	PS3_LPM_RIGHTS_USE_LPM = 0x001,
++	PS3_LPM_RIGHTS_USE_TB = 0x100,
++};
++
++/**
++ * enum ps3_lpm_tb_type - Type of trace buffer lv1 should use.
++ *
++ * @PS3_LPM_TB_TYPE_NONE: Do not use a trace buffer.
++ * @PS3_LPM_RIGHTS_USE_TB: Use the lv1 internal trace buffer.  Must have
++ *  rights @PS3_LPM_RIGHTS_USE_TB.
++ */
++
++enum ps3_lpm_tb_type {
++	PS3_LPM_TB_TYPE_NONE = 0,
++	PS3_LPM_TB_TYPE_INTERNAL = 1,
++};
++
++int ps3_lpm_open(enum ps3_lpm_tb_type tb_type, void *tb_cache,
++	u64 tb_cache_size);
++int ps3_lpm_close(void);
++int ps3_lpm_copy_tb(unsigned long offset, void *buf, unsigned long count,
++	unsigned long *bytes_copied);
++int ps3_lpm_copy_tb_to_user(unsigned long offset, void __user *buf,
++	unsigned long count, unsigned long *bytes_copied);
++void ps3_set_bookmark(u64 bookmark);
++void ps3_set_pm_bookmark(u64 tag, u64 incident, u64 th_id);
++int ps3_set_signal(u64 rtas_signal_group, u8 signal_bit, u16 sub_unit,
++	u8 bus_word);
++
++u32 ps3_read_phys_ctr(u32 cpu, u32 phys_ctr);
++void ps3_write_phys_ctr(u32 cpu, u32 phys_ctr, u32 val);
++u32 ps3_read_ctr(u32 cpu, u32 ctr);
++void ps3_write_ctr(u32 cpu, u32 ctr, u32 val);
++
++u32 ps3_read_pm07_control(u32 cpu, u32 ctr);
++void ps3_write_pm07_control(u32 cpu, u32 ctr, u32 val);
++u32 ps3_read_pm(u32 cpu, enum pm_reg_name reg);
++void ps3_write_pm(u32 cpu, enum pm_reg_name reg, u32 val);
++
++u32 ps3_get_ctr_size(u32 cpu, u32 phys_ctr);
++void ps3_set_ctr_size(u32 cpu, u32 phys_ctr, u32 ctr_size);
++
++void ps3_enable_pm(u32 cpu);
++void ps3_disable_pm(u32 cpu);
++void ps3_enable_pm_interrupts(u32 cpu, u32 thread, u32 mask);
++void ps3_disable_pm_interrupts(u32 cpu);
++
++u32 ps3_get_and_clear_pm_interrupts(u32 cpu);
++void ps3_sync_irq(int node);
++u32 ps3_get_hw_thread_id(int cpu);
++u64 ps3_get_spe_id(void *arg);
+ 
+ #endif
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-lpm-fix-set-bookmark.patch linux-2.6.25-id/patches/ps3-stable/ps3-lpm-fix-set-bookmark.patch
--- linux-2.6.25-org/patches/ps3-stable/ps3-lpm-fix-set-bookmark.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-lpm-fix-set-bookmark.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,33 @@
+Subject: PS3: Fix lpm set bookmark
+
+From: Takashi Yamamoto <TakashiA.Yamamoto@jp.sony.com>
+
+Fix the ps3_set_bookmark() routine of the PS3 logical performance
+monitor driver.
+
+To properly set a performance monitor bookmark the Cell processor
+requires no instruction branches near the setting of the bookmark
+SPR.  Testing showed that the use of the db10cyc instruction did
+not work correctly.  This change replaces the db10cyc instruction
+with 10 nop instructions.
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto@jp.sony.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ drivers/ps3/ps3-lpm.c |    4 ++--
+ 1 file changed, 2 insertions(+), 2 deletions(-)
+
+--- a/drivers/ps3/ps3-lpm.c
++++ b/drivers/ps3/ps3-lpm.c
+@@ -181,9 +181,9 @@ void ps3_set_bookmark(u64 bookmark)
+ 	 * includes cycles before the call.
+ 	 */
+ 
+-	asm volatile("or 29, 29, 29;"); /* db10cyc */
++	asm volatile("nop;nop;nop;nop;nop;nop;nop;nop;nop;");
+ 	mtspr(SPRN_BKMK, bookmark);
+-	asm volatile("or 29, 29, 29;"); /* db10cyc */
++	asm volatile("nop;nop;nop;nop;nop;nop;nop;nop;nop;");
+ }
+ EXPORT_SYMBOL_GPL(ps3_set_bookmark);
+ 
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-lpm-repository-support.patch linux-2.6.25-id/patches/ps3-stable/ps3-lpm-repository-support.patch
--- linux-2.6.25-org/patches/ps3-stable/ps3-lpm-repository-support.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-lpm-repository-support.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,166 @@
+Subject: PS3: Add logical performance monitor repository routines
+
+From: Takashi Yamamoto <TakashiA.Yamamoto@jp.sony.com>
+
+Add repository routines for the PS3 Logical Performance Monitor.
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto@jp.sony.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+v2: o Correct Yamamoto-san's mail addr.
+
+v3: o Change num_pu and pu_id to type u64.
+    o Add comments to describe some symbol names.
+
+ arch/powerpc/platforms/ps3/platform.h   |   12 +++--
+ arch/powerpc/platforms/ps3/repository.c |   75 +++++++++++++++++++++++++++++++-
+ 2 files changed, 83 insertions(+), 4 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/platform.h
++++ b/arch/powerpc/platforms/ps3/platform.h
+@@ -180,10 +180,10 @@ int ps3_repository_read_stor_dev_region(
+ 	unsigned int dev_index, unsigned int region_index,
+ 	unsigned int *region_id, u64 *region_start, u64 *region_size);
+ 
+-/* repository pu and memory info */
++/* repository logical pu and memory info */
+ 
+-int ps3_repository_read_num_pu(unsigned int *num_pu);
+-int ps3_repository_read_ppe_id(unsigned int *pu_index, unsigned int *ppe_id);
++int ps3_repository_read_num_pu(u64 *num_pu);
++int ps3_repository_read_pu_id(unsigned int pu_index, u64 *pu_id);
+ int ps3_repository_read_rm_base(unsigned int ppe_id, u64 *rm_base);
+ int ps3_repository_read_rm_size(unsigned int ppe_id, u64 *rm_size);
+ int ps3_repository_read_region_total(u64 *region_total);
+@@ -194,9 +194,15 @@ int ps3_repository_read_mm_info(u64 *rm_
+ 
+ int ps3_repository_read_num_be(unsigned int *num_be);
+ int ps3_repository_read_be_node_id(unsigned int be_index, u64 *node_id);
++int ps3_repository_read_be_id(u64 node_id, u64 *be_id);
+ int ps3_repository_read_tb_freq(u64 node_id, u64 *tb_freq);
+ int ps3_repository_read_be_tb_freq(unsigned int be_index, u64 *tb_freq);
+ 
++/* repository performance monitor info */
++
++int ps3_repository_read_lpm_privileges(unsigned int be_index, u64 *lpar,
++	u64 *rights);
++
+ /* repository 'Other OS' area */
+ 
+ int ps3_repository_read_boot_dat_addr(u64 *lpar_addr);
+--- a/arch/powerpc/platforms/ps3/repository.c
++++ b/arch/powerpc/platforms/ps3/repository.c
+@@ -711,6 +711,35 @@ int ps3_repository_read_stor_dev_region(
+ 	return result;
+ }
+ 
++/**
++ * ps3_repository_read_num_pu - Number of logical PU processors for this lpar.
++ */
++
++int ps3_repository_read_num_pu(u64 *num_pu)
++{
++	*num_pu = 0;
++	return read_node(PS3_LPAR_ID_CURRENT,
++			   make_first_field("bi", 0),
++			   make_field("pun", 0),
++			   0, 0,
++			   num_pu, NULL);
++}
++
++/**
++ * ps3_repository_read_pu_id - Read the logical PU id.
++ * @pu_index: Zero based index.
++ * @pu_id: The logical PU id.
++ */
++
++int ps3_repository_read_pu_id(unsigned int pu_index, u64 *pu_id)
++{
++	return read_node(PS3_LPAR_ID_CURRENT,
++		make_first_field("bi", 0),
++		make_field("pu", pu_index),
++		0, 0,
++		pu_id, NULL);
++}
++
+ int ps3_repository_read_rm_size(unsigned int ppe_id, u64 *rm_size)
+ {
+ 	return read_node(PS3_LPAR_ID_CURRENT,
+@@ -883,6 +912,10 @@ int ps3_repository_read_boot_dat_info(u6
+ 		: ps3_repository_read_boot_dat_size(size);
+ }
+ 
++/**
++ * ps3_repository_read_num_be - Number of physical BE processors in the system.
++ */
++
+ int ps3_repository_read_num_be(unsigned int *num_be)
+ {
+ 	int result;
+@@ -898,6 +931,12 @@ int ps3_repository_read_num_be(unsigned 
+ 	return result;
+ }
+ 
++/**
++ * ps3_repository_read_be_node_id - Read the physical BE processor node id.
++ * @be_index: Zero based index.
++ * @node_id: The BE processor node id.
++ */
++
+ int ps3_repository_read_be_node_id(unsigned int be_index, u64 *node_id)
+ {
+ 	return read_node(PS3_LPAR_ID_PME,
+@@ -908,6 +947,22 @@ int ps3_repository_read_be_node_id(unsig
+ 		node_id, NULL);
+ }
+ 
++/**
++ * ps3_repository_read_be_id - Read the physical BE processor id.
++ * @node_id: The BE processor node id.
++ * @be_id: The BE processor id.
++ */
++
++int ps3_repository_read_be_id(u64 node_id, u64 *be_id)
++{
++	return read_node(PS3_LPAR_ID_PME,
++		make_first_field("be", 0),
++		node_id,
++		0,
++		0,
++		be_id, NULL);
++}
++
+ int ps3_repository_read_tb_freq(u64 node_id, u64 *tb_freq)
+ {
+ 	return read_node(PS3_LPAR_ID_PME,
+@@ -924,11 +979,29 @@ int ps3_repository_read_be_tb_freq(unsig
+ 	u64 node_id;
+ 
+ 	*tb_freq = 0;
+-	result = ps3_repository_read_be_node_id(0, &node_id);
++	result = ps3_repository_read_be_node_id(be_index, &node_id);
+ 	return result ? result
+ 		: ps3_repository_read_tb_freq(node_id, tb_freq);
+ }
+ 
++int ps3_repository_read_lpm_privileges(unsigned int be_index, u64 *lpar,
++	u64 *rights)
++{
++	int result;
++	u64 node_id;
++
++	*lpar = 0;
++	*rights = 0;
++	result = ps3_repository_read_be_node_id(be_index, &node_id);
++	return result ? result
++		: read_node(PS3_LPAR_ID_PME,
++			    make_first_field("be", 0),
++			    node_id,
++			    make_field("lpm", 0),
++			    make_field("priv", 0),
++			    lpar, rights);
++}
++
+ #if defined(DEBUG)
+ 
+ int ps3_repository_dump_resource_info(const struct ps3_repository_device *repo)
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-make-dev_id-and-bus_id-u64.diff linux-2.6.25-id/patches/ps3-stable/ps3-make-dev_id-and-bus_id-u64.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3-make-dev_id-and-bus_id-u64.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-make-dev_id-and-bus_id-u64.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,268 @@
+Subject: PS3: Make bus_id and dev_id u64
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+Change the PS3 bus_id and dev_id from type unsigned int to u64.  These
+IDs are 64-bit in the repository, and the special storage notification
+device has a device ID of ULONG_MAX.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/device-init.c |    4 ++--
+ arch/powerpc/platforms/ps3/mm.c          |    8 ++++----
+ arch/powerpc/platforms/ps3/platform.h    |   12 ++++++------
+ arch/powerpc/platforms/ps3/repository.c  |   21 ++++++++-------------
+ arch/powerpc/platforms/ps3/system-bus.c  |   14 +++++++-------
+ drivers/net/ps3_gelic_net.c              |    4 ++--
+ include/asm-powerpc/ps3.h                |    4 ++--
+ 7 files changed, 31 insertions(+), 36 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/device-init.c
++++ b/arch/powerpc/platforms/ps3/device-init.c
+@@ -297,7 +297,7 @@ static int ps3_storage_wait_for_device(c
+ 		u64 dev_port;
+ 	} *notify_event;
+ 
+-	pr_debug(" -> %s:%u: (%u:%u:%u)\n", __func__, __LINE__, repo->bus_id,
++	pr_debug(" -> %s:%u: (%lu:%lu:%u)\n", __func__, __LINE__, repo->bus_id,
+ 		 repo->dev_id, repo->dev_type);
+ 
+ 	buf = kzalloc(512, GFP_KERNEL);
+@@ -384,7 +384,7 @@ static int ps3_storage_wait_for_device(c
+ 
+ 		if (notify_event->dev_id == repo->dev_id &&
+ 		    notify_event->dev_type == PS3_DEV_TYPE_NOACCESS) {
+-			pr_debug("%s:%u: no access: dev_id %u\n", __func__,
++			pr_debug("%s:%u: no access: dev_id %lu\n", __func__,
+ 				 __LINE__, repo->dev_id);
+ 			break;
+ 		}
+--- a/arch/powerpc/platforms/ps3/mm.c
++++ b/arch/powerpc/platforms/ps3/mm.c
+@@ -359,7 +359,7 @@ static unsigned long dma_sb_lpar_to_bus(
+ static void  __maybe_unused _dma_dump_region(const struct ps3_dma_region *r,
+ 	const char *func, int line)
+ {
+-	DBG("%s:%d: dev        %u:%u\n", func, line, r->dev->bus_id,
++	DBG("%s:%d: dev        %lu:%lu\n", func, line, r->dev->bus_id,
+ 		r->dev->dev_id);
+ 	DBG("%s:%d: page_size  %u\n", func, line, r->page_size);
+ 	DBG("%s:%d: bus_addr   %lxh\n", func, line, r->bus_addr);
+@@ -394,7 +394,7 @@ struct dma_chunk {
+ static void _dma_dump_chunk (const struct dma_chunk* c, const char* func,
+ 	int line)
+ {
+-	DBG("%s:%d: r.dev        %u:%u\n", func, line,
++	DBG("%s:%d: r.dev        %lu:%lu\n", func, line,
+ 		c->region->dev->bus_id, c->region->dev->dev_id);
+ 	DBG("%s:%d: r.bus_addr   %lxh\n", func, line, c->region->bus_addr);
+ 	DBG("%s:%d: r.page_size  %u\n", func, line, c->region->page_size);
+@@ -658,7 +658,7 @@ static int dma_sb_region_create(struct p
+ 	BUG_ON(!r);
+ 
+ 	if (!r->dev->bus_id) {
+-		pr_info("%s:%d: %u:%u no dma\n", __func__, __LINE__,
++		pr_info("%s:%d: %lu:%lu no dma\n", __func__, __LINE__,
+ 			r->dev->bus_id, r->dev->dev_id);
+ 		return 0;
+ 	}
+@@ -724,7 +724,7 @@ static int dma_sb_region_free(struct ps3
+ 	BUG_ON(!r);
+ 
+ 	if (!r->dev->bus_id) {
+-		pr_info("%s:%d: %u:%u no dma\n", __func__, __LINE__,
++		pr_info("%s:%d: %lu:%lu no dma\n", __func__, __LINE__,
+ 			r->dev->bus_id, r->dev->dev_id);
+ 		return 0;
+ 	}
+--- a/arch/powerpc/platforms/ps3/platform.h
++++ b/arch/powerpc/platforms/ps3/platform.h
+@@ -95,7 +95,7 @@ enum ps3_dev_type {
+ 
+ int ps3_repository_read_bus_str(unsigned int bus_index, const char *bus_str,
+ 	u64 *value);
+-int ps3_repository_read_bus_id(unsigned int bus_index, unsigned int *bus_id);
++int ps3_repository_read_bus_id(unsigned int bus_index, u64 *bus_id);
+ int ps3_repository_read_bus_type(unsigned int bus_index,
+ 	enum ps3_bus_type *bus_type);
+ int ps3_repository_read_bus_num_dev(unsigned int bus_index,
+@@ -119,7 +119,7 @@ enum ps3_reg_type {
+ int ps3_repository_read_dev_str(unsigned int bus_index,
+ 	unsigned int dev_index, const char *dev_str, u64 *value);
+ int ps3_repository_read_dev_id(unsigned int bus_index, unsigned int dev_index,
+-	unsigned int *dev_id);
++	u64 *dev_id);
+ int ps3_repository_read_dev_type(unsigned int bus_index,
+ 	unsigned int dev_index, enum ps3_dev_type *dev_type);
+ int ps3_repository_read_dev_intr(unsigned int bus_index,
+@@ -138,12 +138,12 @@ int ps3_repository_read_dev_reg(unsigned
+ /* repository bus enumerators */
+ 
+ struct ps3_repository_device {
+-	enum ps3_bus_type bus_type;
+ 	unsigned int bus_index;
+-	unsigned int bus_id;
+-	enum ps3_dev_type dev_type;
+ 	unsigned int dev_index;
+-	unsigned int dev_id;
++	enum ps3_bus_type bus_type;
++	enum ps3_dev_type dev_type;
++	u64 bus_id;
++	u64 dev_id;
+ };
+ 
+ static inline struct ps3_repository_device *ps3_repository_bump_device(
+--- a/arch/powerpc/platforms/ps3/repository.c
++++ b/arch/powerpc/platforms/ps3/repository.c
+@@ -168,18 +168,15 @@ int ps3_repository_read_bus_str(unsigned
+ 		value, 0);
+ }
+ 
+-int ps3_repository_read_bus_id(unsigned int bus_index, unsigned int *bus_id)
++int ps3_repository_read_bus_id(unsigned int bus_index, u64 *bus_id)
+ {
+ 	int result;
+-	u64 v1;
+-	u64 v2; /* unused */
+ 
+ 	result = read_node(PS3_LPAR_ID_PME,
+ 		make_first_field("bus", bus_index),
+ 		make_field("id", 0),
+ 		0, 0,
+-		&v1, &v2);
+-	*bus_id = v1;
++		bus_id, NULL);
+ 	return result;
+ }
+ 
+@@ -225,18 +222,16 @@ int ps3_repository_read_dev_str(unsigned
+ }
+ 
+ int ps3_repository_read_dev_id(unsigned int bus_index, unsigned int dev_index,
+-	unsigned int *dev_id)
++	u64 *dev_id)
+ {
+ 	int result;
+-	u64 v1;
+ 
+ 	result = read_node(PS3_LPAR_ID_PME,
+ 		make_first_field("bus", bus_index),
+ 		make_field("dev", dev_index),
+ 		make_field("id", 0),
+ 		0,
+-		&v1, 0);
+-	*dev_id = v1;
++		dev_id, 0);
+ 	return result;
+ }
+ 
+@@ -332,7 +327,7 @@ int ps3_repository_find_device(struct ps
+ 		return result;
+ 	}
+ 
+-	pr_debug("%s:%d: bus_type %u, bus_index %u, bus_id %u, num_dev %u\n",
++	pr_debug("%s:%d: bus_type %u, bus_index %u, bus_id %lu, num_dev %u\n",
+ 		__func__, __LINE__, tmp.bus_type, tmp.bus_index, tmp.bus_id,
+ 		num_dev);
+ 
+@@ -387,7 +382,7 @@ int ps3_repository_find_device(struct ps
+ 		return result;
+ 	}
+ 
+-	pr_debug("%s:%d: found: dev_type %u, dev_index %u, dev_id %u\n",
++	pr_debug("%s:%d: found: dev_type %u, dev_index %u, dev_id %lu\n",
+ 		__func__, __LINE__, tmp.dev_type, tmp.dev_index, tmp.dev_id);
+ 
+ 	*repo = tmp;
+@@ -1034,7 +1029,7 @@ static int dump_device_info(struct ps3_r
+ 			continue;
+ 		}
+ 
+-		pr_debug("%s:%d  (%u:%u): dev_type %u, dev_id %u\n", __func__,
++		pr_debug("%s:%d  (%u:%u): dev_type %u, dev_id %lu\n", __func__,
+ 			__LINE__, repo->bus_index, repo->dev_index,
+ 			repo->dev_type, repo->dev_id);
+ 
+@@ -1091,7 +1086,7 @@ int ps3_repository_dump_bus_info(void)
+ 			continue;
+ 		}
+ 
+-		pr_debug("%s:%d bus_%u: bus_type %u, bus_id %u, num_dev %u\n",
++		pr_debug("%s:%d bus_%u: bus_type %u, bus_id %lu, num_dev %u\n",
+ 			__func__, __LINE__, repo.bus_index, repo.bus_type,
+ 			repo.bus_id, num_dev);
+ 
+--- a/arch/powerpc/platforms/ps3/system-bus.c
++++ b/arch/powerpc/platforms/ps3/system-bus.c
+@@ -42,8 +42,8 @@ struct {
+ 	int gpu;
+ } static usage_hack;
+ 
+-static int ps3_is_device(struct ps3_system_bus_device *dev,
+-			 unsigned int bus_id, unsigned int dev_id)
++static int ps3_is_device(struct ps3_system_bus_device *dev, u64 bus_id,
++			 u64 dev_id)
+ {
+ 	return dev->bus_id == bus_id && dev->dev_id == dev_id;
+ }
+@@ -182,8 +182,8 @@ int ps3_open_hv_device(struct ps3_system
+ 	case PS3_MATCH_ID_SYSTEM_MANAGER:
+ 		pr_debug("%s:%d: unsupported match_id: %u\n", __func__,
+ 			__LINE__, dev->match_id);
+-		pr_debug("%s:%d: bus_id: %u\n", __func__,
+-			__LINE__, dev->bus_id);
++		pr_debug("%s:%d: bus_id: %lu\n", __func__, __LINE__,
++			dev->bus_id);
+ 		BUG();
+ 		return -EINVAL;
+ 
+@@ -220,8 +220,8 @@ int ps3_close_hv_device(struct ps3_syste
+ 	case PS3_MATCH_ID_SYSTEM_MANAGER:
+ 		pr_debug("%s:%d: unsupported match_id: %u\n", __func__,
+ 			__LINE__, dev->match_id);
+-		pr_debug("%s:%d: bus_id: %u\n", __func__,
+-			__LINE__, dev->bus_id);
++		pr_debug("%s:%d: bus_id: %lu\n", __func__, __LINE__,
++			dev->bus_id);
+ 		BUG();
+ 		return -EINVAL;
+ 
+@@ -240,7 +240,7 @@ EXPORT_SYMBOL_GPL(ps3_close_hv_device);
+ static void _dump_mmio_region(const struct ps3_mmio_region* r,
+ 	const char* func, int line)
+ {
+-	pr_debug("%s:%d: dev       %u:%u\n", func, line, r->dev->bus_id,
++	pr_debug("%s:%d: dev       %lu:%lu\n", func, line, r->dev->bus_id,
+ 		r->dev->dev_id);
+ 	pr_debug("%s:%d: bus_addr  %lxh\n", func, line, r->bus_addr);
+ 	pr_debug("%s:%d: len       %lxh\n", func, line, r->len);
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -58,11 +58,11 @@ static inline struct device *ctodev(stru
+ {
+ 	return &card->dev->core;
+ }
+-static inline unsigned int bus_id(struct gelic_net_card *card)
++static inline u64 bus_id(struct gelic_net_card *card)
+ {
+ 	return card->dev->bus_id;
+ }
+-static inline unsigned int dev_id(struct gelic_net_card *card)
++static inline u64 dev_id(struct gelic_net_card *card)
+ {
+ 	return card->dev->dev_id;
+ }
+--- a/include/asm-powerpc/ps3.h
++++ b/include/asm-powerpc/ps3.h
+@@ -344,8 +344,8 @@ struct ps3_system_bus_device {
+ 	enum ps3_match_id match_id;
+ 	enum ps3_system_bus_device_type dev_type;
+ 
+-	unsigned int bus_id;              /* SB */
+-	unsigned int dev_id;              /* SB */
++	u64 bus_id;                       /* SB */
++	u64 dev_id;                       /* SB */
+ 	unsigned int interrupt_id;        /* SB */
+ 	struct ps3_dma_region *d_region;  /* SB, IOC0 */
+ 	struct ps3_mmio_region *m_region; /* SB, IOC0*/
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-refactor-ps3_repository_find_device.diff linux-2.6.25-id/patches/ps3-stable/ps3-refactor-ps3_repository_find_device.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3-refactor-ps3_repository_find_device.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-refactor-ps3_repository_find_device.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,88 @@
+Subject: PS3: Refactor ps3_repository_find_device()
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+PS3: Refactor ps3_repository_find_device() to use the existing
+ps3_repository_read_bus_id() routine.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/repository.c |   60 +++++++++++---------------------
+ 1 files changed, 22 insertions(+), 38 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/repository.c
++++ b/arch/powerpc/platforms/ps3/repository.c
+@@ -445,50 +445,34 @@ int __devinit ps3_repository_find_device
+ 
+ 	pr_debug(" -> %s:%d: find bus_type %u\n", __func__, __LINE__, bus_type);
+ 
+-	for (repo.bus_index = 0; repo.bus_index < 10; repo.bus_index++) {
++	repo.bus_type = bus_type;
++	result = ps3_repository_find_bus(repo.bus_type, 0, &repo.bus_index);
++	if (result) {
++		pr_debug(" <- %s:%u: bus not found\n", __func__, __LINE__);
++		return result;
++	}
+ 
+-		result = ps3_repository_read_bus_type(repo.bus_index,
+-			&repo.bus_type);
++	result = ps3_repository_read_bus_id(repo.bus_index, &repo.bus_id);
++	if (result) {
++		pr_debug("%s:%d read_bus_id(%u) failed\n", __func__, __LINE__,
++			 repo.bus_index);
++		return result;
++	}
+ 
+-		if (result) {
+-			pr_debug("%s:%d read_bus_type(%u) failed\n",
+-				__func__, __LINE__, repo.bus_index);
++	for (repo.dev_index = 0; ; repo.dev_index++) {
++		result = ps3_repository_find_device(&repo);
++		if (result == -ENODEV) {
++			result = 0;
++			break;
++		} else if (result)
+ 			break;
+-		}
+-
+-		if (repo.bus_type != bus_type) {
+-			pr_debug("%s:%d: skip, bus_type %u\n", __func__,
+-				__LINE__, repo.bus_type);
+-			continue;
+-		}
+-
+-		result = ps3_repository_read_bus_id(repo.bus_index,
+-			&repo.bus_id);
+ 
++		result = callback(&repo);
+ 		if (result) {
+-			pr_debug("%s:%d read_bus_id(%u) failed\n",
+-				__func__, __LINE__, repo.bus_index);
+-			continue;
+-		}
+-
+-		for (repo.dev_index = 0; ; repo.dev_index++) {
+-			result = ps3_repository_find_device(&repo);
+-
+-			if (result == -ENODEV) {
+-				result = 0;
+-				break;
+-			} else if (result)
+-				break;
+-
+-			result = callback(&repo);
+-
+-			if (result) {
+-				pr_debug("%s:%d: abort at callback\n", __func__,
+-					__LINE__);
+-				break;
+-			}
++			pr_debug("%s:%d: abort at callback\n", __func__,
++				__LINE__);
++			break;
+ 		}
+-		break;
+ 	}
+ 
+ 	pr_debug(" <- %s:%d\n", __func__, __LINE__);
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-remove-use-lpar-addr.patch linux-2.6.25-id/patches/ps3-stable/ps3-remove-use-lpar-addr.patch
--- linux-2.6.25-org/patches/ps3-stable/ps3-remove-use-lpar-addr.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-remove-use-lpar-addr.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,95 @@
+Subject: PS3: Revove use lpar address workaround
+
+Remove the PS3 workaround needed to support sparsemem SPU mappings.
+The SPU mappings no longer use sparsemem, so this workaround is no
+longer needed.
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/Kconfig |   11 -----------
+ arch/powerpc/platforms/ps3/mm.c    |   16 ++++------------
+ include/asm-powerpc/sparsemem.h    |    5 -----
+ 3 files changed, 4 insertions(+), 28 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/Kconfig
++++ b/arch/powerpc/platforms/ps3/Kconfig
+@@ -61,17 +61,6 @@ config PS3_DYNAMIC_DMA
+ 	  This support is mainly for Linux kernel development.  If unsure,
+ 	  say N.
+ 
+-config PS3_USE_LPAR_ADDR
+-	depends on PPC_PS3 && EXPERIMENTAL
+-	bool "PS3 use lpar address space"
+-	default y
+-	help
+-	  This option is solely for experimentation by experts.  Disables
+-	  translation of lpar addresses.  SPE support currently won't work
+-	  without this set to y.
+-
+-	  If you have any doubt, choose the default y.
+-
+ config PS3_VUART
+ 	depends on PPC_PS3
+ 	tristate
+--- a/arch/powerpc/platforms/ps3/mm.c
++++ b/arch/powerpc/platforms/ps3/mm.c
+@@ -36,11 +36,6 @@
+ #endif
+ 
+ enum {
+-#if defined(CONFIG_PS3_USE_LPAR_ADDR)
+-	USE_LPAR_ADDR = 1,
+-#else
+-	USE_LPAR_ADDR = 0,
+-#endif
+ #if defined(CONFIG_PS3_DYNAMIC_DMA)
+ 	USE_DYNAMIC_DMA = 1,
+ #else
+@@ -137,11 +132,8 @@ static struct map map;
+ unsigned long ps3_mm_phys_to_lpar(unsigned long phys_addr)
+ {
+ 	BUG_ON(is_kernel_addr(phys_addr));
+-	if (USE_LPAR_ADDR)
+-		return phys_addr;
+-	else
+-		return (phys_addr < map.rm.size || phys_addr >= map.total)
+-			? phys_addr : phys_addr + map.r1.offset;
++	return (phys_addr < map.rm.size || phys_addr >= map.total)
++		? phys_addr : phys_addr + map.r1.offset;
+ }
+ 
+ EXPORT_SYMBOL(ps3_mm_phys_to_lpar);
+@@ -309,7 +301,7 @@ static int __init ps3_mm_add_memory(void
+ 
+ 	BUG_ON(!mem_init_done);
+ 
+-	start_addr = USE_LPAR_ADDR ? map.r1.base : map.rm.size;
++	start_addr = map.rm.size;
+ 	start_pfn = start_addr >> PAGE_SHIFT;
+ 	nr_pages = (map.r1.size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+ 
+@@ -1007,7 +999,7 @@ static int dma_sb_region_create_linear(s
+ 
+ 	if (r->offset + r->len > map.rm.size) {
+ 		/* Map (part of) 2nd RAM chunk */
+-		virt_addr = USE_LPAR_ADDR ? map.r1.base : map.rm.size;
++		virt_addr = map.rm.size;
+ 		len = r->len;
+ 		if (r->offset >= map.rm.size)
+ 			virt_addr += r->offset - map.rm.size;
+--- a/include/asm-powerpc/sparsemem.h
++++ b/include/asm-powerpc/sparsemem.h
+@@ -10,13 +10,8 @@
+  */
+ #define SECTION_SIZE_BITS       24
+ 
+-#if defined(CONFIG_PS3_USE_LPAR_ADDR)
+-#define MAX_PHYSADDR_BITS       47
+-#define MAX_PHYSMEM_BITS        47
+-#else
+ #define MAX_PHYSADDR_BITS       44
+ #define MAX_PHYSMEM_BITS        44
+-#endif
+ 
+ #ifdef CONFIG_MEMORY_HOTPLUG
+ extern void create_section_mapping(unsigned long start, unsigned long end);
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-storage-correct-notification-mechanism.diff linux-2.6.25-id/patches/ps3-stable/ps3-storage-correct-notification-mechanism.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3-storage-correct-notification-mechanism.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-storage-correct-notification-mechanism.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,582 @@
+Subject: PS3: Use the HV's storage device notification mechanism properly
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+The PS3 hypervisor has a storage device notification mechanism to wait until a
+storage device is ready. Unfortunately the storage device probing code used
+this mechanism in an incorrect way, needing a polling loop and handling of
+devices that are not yet ready.
+
+This change corrects this by:
+  - First waiting for the reception of an asynchronous notification that a new
+    storage device became ready,
+  - Then looking up the storage device in the device repository.
+
+On shutdown, the storage probe thread is stopped and the storage notification
+device is closed using a reboot notifier.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/device-init.c |  422 ++++++++++++++++---------------
+ arch/powerpc/platforms/ps3/platform.h    |    2 
+ arch/powerpc/platforms/ps3/repository.c  |   29 --
+ 3 files changed, 221 insertions(+), 232 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/device-init.c
++++ b/arch/powerpc/platforms/ps3/device-init.c
+@@ -23,6 +23,7 @@
+ #include <linux/kernel.h>
+ #include <linux/kthread.h>
+ #include <linux/init.h>
++#include <linux/reboot.h>
+ 
+ #include <asm/firmware.h>
+ #include <asm/lv1call.h>
+@@ -238,166 +239,6 @@ static int __init ps3_setup_vuart_device
+ 	return result;
+ }
+ 
+-static int ps3stor_wait_for_completion(u64 dev_id, u64 tag,
+-				       unsigned int timeout)
+-{
+-	int result = -1;
+-	unsigned int retries = 0;
+-	u64 status;
+-
+-	for (retries = 0; retries < timeout; retries++) {
+-		result = lv1_storage_check_async_status(dev_id, tag, &status);
+-		if (!result)
+-			break;
+-
+-		msleep(1);
+-	}
+-
+-	if (result)
+-		pr_debug("%s:%u: check_async_status: %s, status %lx\n",
+-			 __func__, __LINE__, ps3_result(result), status);
+-
+-	return result;
+-}
+-
+-/**
+- * ps3_storage_wait_for_device - Wait for a storage device to become ready.
+- * @repo: The repository device to wait for.
+- *
+- * Uses the hypervisor's storage device notification mechanism to wait until
+- * a storage device is ready.  The device notification mechanism uses a
+- * psuedo device (id = -1) to asynchronously notify the guest when storage
+- * devices become ready.  The notification device has a block size of 512
+- * bytes.
+- */
+-
+-static int ps3_storage_wait_for_device(const struct ps3_repository_device *repo)
+-{
+-	int error = -ENODEV;
+-	int result;
+-	const u64 notification_dev_id = (u64)-1LL;
+-	const unsigned int timeout = HZ;
+-	u64 lpar;
+-	u64 tag;
+-	void *buf;
+-	enum ps3_notify_type {
+-		notify_device_ready = 0,
+-		notify_region_probe = 1,
+-		notify_region_update = 2,
+-	};
+-	struct {
+-		u64 operation_code;	/* must be zero */
+-		u64 event_mask;		/* OR of 1UL << enum ps3_notify_type */
+-	} *notify_cmd;
+-	struct {
+-		u64 event_type;		/* enum ps3_notify_type */
+-		u64 bus_id;
+-		u64 dev_id;
+-		u64 dev_type;
+-		u64 dev_port;
+-	} *notify_event;
+-
+-	pr_debug(" -> %s:%u: (%lu:%lu:%u)\n", __func__, __LINE__, repo->bus_id,
+-		 repo->dev_id, repo->dev_type);
+-
+-	buf = kzalloc(512, GFP_KERNEL);
+-	if (!buf)
+-		return -ENOMEM;
+-
+-	lpar = ps3_mm_phys_to_lpar(__pa(buf));
+-	notify_cmd = buf;
+-	notify_event = buf;
+-
+-	result = lv1_open_device(repo->bus_id, notification_dev_id, 0);
+-	if (result) {
+-		printk(KERN_ERR "%s:%u: lv1_open_device %s\n", __func__,
+-		       __LINE__, ps3_result(result));
+-		goto fail_free;
+-	}
+-
+-	/* Setup and write the request for device notification. */
+-
+-	notify_cmd->operation_code = 0; /* must be zero */
+-	notify_cmd->event_mask = 1UL << notify_region_probe;
+-
+-	result = lv1_storage_write(notification_dev_id, 0, 0, 1, 0, lpar,
+-				   &tag);
+-	if (result) {
+-		printk(KERN_ERR "%s:%u: write failed %s\n", __func__, __LINE__,
+-		       ps3_result(result));
+-		goto fail_close;
+-	}
+-
+-	/* Wait for the write completion */
+-
+-	result = ps3stor_wait_for_completion(notification_dev_id, tag,
+-					     timeout);
+-	if (result) {
+-		printk(KERN_ERR "%s:%u: write not completed %s\n", __func__,
+-		       __LINE__, ps3_result(result));
+-		goto fail_close;
+-	}
+-
+-	/* Loop here processing the requested notification events. */
+-
+-	while (1) {
+-		memset(notify_event, 0, sizeof(*notify_event));
+-
+-		result = lv1_storage_read(notification_dev_id, 0, 0, 1, 0,
+-					  lpar, &tag);
+-		if (result) {
+-			printk(KERN_ERR "%s:%u: write failed %s\n", __func__,
+-			       __LINE__, ps3_result(result));
+-			break;
+-		}
+-
+-		result = ps3stor_wait_for_completion(notification_dev_id, tag,
+-						     timeout);
+-		if (result) {
+-			printk(KERN_ERR "%s:%u: read not completed %s\n",
+-			       __func__, __LINE__, ps3_result(result));
+-			break;
+-		}
+-
+-		pr_debug("%s:%d: notify event (%u:%u:%u): event_type 0x%lx, "
+-			 "port %lu\n", __func__, __LINE__, repo->bus_index,
+-			 repo->dev_index, repo->dev_type,
+-			 notify_event->event_type, notify_event->dev_port);
+-
+-		if (notify_event->event_type != notify_region_probe ||
+-		    notify_event->bus_id != repo->bus_id) {
+-			pr_debug("%s:%u: bad notify_event: event %lu, "
+-				 "dev_id %lu, dev_type %lu\n",
+-				 __func__, __LINE__, notify_event->event_type,
+-				 notify_event->dev_id, notify_event->dev_type);
+-			break;
+-		}
+-
+-		if (notify_event->dev_id == repo->dev_id &&
+-		    notify_event->dev_type == repo->dev_type) {
+-			pr_debug("%s:%u: device ready (%u:%u:%u)\n", __func__,
+-				 __LINE__, repo->bus_index, repo->dev_index,
+-				 repo->dev_type);
+-			error = 0;
+-			break;
+-		}
+-
+-		if (notify_event->dev_id == repo->dev_id &&
+-		    notify_event->dev_type == PS3_DEV_TYPE_NOACCESS) {
+-			pr_debug("%s:%u: no access: dev_id %lu\n", __func__,
+-				 __LINE__, repo->dev_id);
+-			break;
+-		}
+-	}
+-
+-fail_close:
+-	lv1_close_device(repo->bus_id, notification_dev_id);
+-fail_free:
+-	kfree(buf);
+-	pr_debug(" <- %s:%u\n", __func__, __LINE__);
+-	return error;
+-}
+-
+ static int ps3_setup_storage_dev(const struct ps3_repository_device *repo,
+ 				 enum ps3_match_id match_id)
+ {
+@@ -449,16 +290,6 @@ static int ps3_setup_storage_dev(const s
+ 		goto fail_find_interrupt;
+ 	}
+ 
+-	/* FIXME: Arrange to only do this on a 'cold' boot */
+-
+-	result = ps3_storage_wait_for_device(repo);
+-	if (result) {
+-		printk(KERN_ERR "%s:%u: storage_notification failed %d\n",
+-		       __func__, __LINE__, result);
+-		result = -ENODEV;
+-		goto fail_probe_notification;
+-	}
+-
+ 	for (i = 0; i < num_regions; i++) {
+ 		unsigned int id;
+ 		u64 start, size;
+@@ -494,7 +325,6 @@ static int ps3_setup_storage_dev(const s
+ 
+ fail_device_register:
+ fail_read_region:
+-fail_probe_notification:
+ fail_find_interrupt:
+ 	kfree(p);
+ fail_malloc:
+@@ -659,62 +489,248 @@ static int ps3_register_repository_devic
+ 	return result;
+ }
+ 
++
++#define PS3_NOTIFICATION_DEV_ID		ULONG_MAX
++#define PS3_NOTIFICATION_INTERRUPT_ID	0
++
++struct ps3_notification_device {
++	struct ps3_system_bus_device sbd;
++	spinlock_t lock;
++	u64 tag;
++	u64 lv1_status;
++	struct completion done;
++};
++
++enum ps3_notify_type {
++	notify_device_ready = 0,
++	notify_region_probe = 1,
++	notify_region_update = 2,
++};
++
++struct ps3_notify_cmd {
++	u64 operation_code;		/* must be zero */
++	u64 event_mask;			/* OR of 1UL << enum ps3_notify_type */
++};
++
++struct ps3_notify_event {
++	u64 event_type;			/* enum ps3_notify_type */
++	u64 bus_id;
++	u64 dev_id;
++	u64 dev_type;
++	u64 dev_port;
++};
++
++static irqreturn_t ps3_notification_interrupt(int irq, void *data)
++{
++	struct ps3_notification_device *dev = data;
++	int res;
++	u64 tag, status;
++
++	spin_lock(&dev->lock);
++	res = lv1_storage_get_async_status(PS3_NOTIFICATION_DEV_ID, &tag,
++					   &status);
++	if (tag != dev->tag)
++		pr_err("%s:%u: tag mismatch, got %lx, expected %lx\n",
++		       __func__, __LINE__, tag, dev->tag);
++
++	if (res) {
++		pr_err("%s:%u: res %d status 0x%lx\n", __func__, __LINE__, res,
++		       status);
++	} else {
++		pr_debug("%s:%u: completed, status 0x%lx\n", __func__,
++			 __LINE__, status);
++		dev->lv1_status = status;
++		complete(&dev->done);
++	}
++	spin_unlock(&dev->lock);
++	return IRQ_HANDLED;
++}
++
++static int ps3_notification_read_write(struct ps3_notification_device *dev,
++				       u64 lpar, int write)
++{
++	const char *op = write ? "write" : "read";
++	unsigned long flags;
++	int res;
++
++	init_completion(&dev->done);
++	spin_lock_irqsave(&dev->lock, flags);
++	res = write ? lv1_storage_write(dev->sbd.dev_id, 0, 0, 1, 0, lpar,
++					&dev->tag)
++		    : lv1_storage_read(dev->sbd.dev_id, 0, 0, 1, 0, lpar,
++				       &dev->tag);
++	spin_unlock_irqrestore(&dev->lock, flags);
++	if (res) {
++		pr_err("%s:%u: %s failed %d\n", __func__, __LINE__, op, res);
++		return -EPERM;
++	}
++	pr_debug("%s:%u: notification %s issued\n", __func__, __LINE__, op);
++
++	res = wait_event_interruptible(dev->done.wait,
++				       dev->done.done || kthread_should_stop());
++	if (kthread_should_stop())
++		res = -EINTR;
++	if (res) {
++		pr_debug("%s:%u: interrupted %s\n", __func__, __LINE__, op);
++		return res;
++	}
++
++	if (dev->lv1_status) {
++		pr_err("%s:%u: %s not completed, status 0x%lx\n", __func__,
++		       __LINE__, op, dev->lv1_status);
++		return -EIO;
++	}
++	pr_debug("%s:%u: notification %s completed\n", __func__, __LINE__, op);
++
++	return 0;
++}
++
++static struct task_struct *probe_task;
++
+ /**
+  * ps3_probe_thread - Background repository probing at system startup.
+  *
+  * This implementation only supports background probing on a single bus.
++ * It uses the hypervisor's storage device notification mechanism to wait until
++ * a storage device is ready.  The device notification mechanism uses a
++ * pseudo device to asynchronously notify the guest when storage devices become
++ * ready.  The notification device has a block size of 512 bytes.
+  */
+ 
+ static int ps3_probe_thread(void *data)
+ {
+-	struct ps3_repository_device *repo = data;
+-	int result;
+-	unsigned int ms = 250;
++	struct ps3_notification_device dev;
++	struct ps3_repository_device repo;
++	int res;
++	unsigned int irq;
++	u64 lpar;
++	void *buf;
++	struct ps3_notify_cmd *notify_cmd;
++	struct ps3_notify_event *notify_event;
+ 
+ 	pr_debug(" -> %s:%u: kthread started\n", __func__, __LINE__);
+ 
+-	do {
+-		try_to_freeze();
++	buf = kzalloc(512, GFP_KERNEL);
++	if (!buf)
++		return -ENOMEM;
+ 
+-		pr_debug("%s:%u: probing...\n", __func__, __LINE__);
++	lpar = ps3_mm_phys_to_lpar(__pa(buf));
++	notify_cmd = buf;
++	notify_event = buf;
+ 
+-		do {
+-			result = ps3_repository_find_device(repo);
++	/* dummy system bus device */
++	dev.sbd.bus_id = (u64)data;
++	dev.sbd.dev_id = PS3_NOTIFICATION_DEV_ID;
++	dev.sbd.interrupt_id = PS3_NOTIFICATION_INTERRUPT_ID;
++
++	res = lv1_open_device(dev.sbd.bus_id, dev.sbd.dev_id, 0);
++	if (res) {
++		pr_err("%s:%u: lv1_open_device failed %s\n", __func__,
++		       __LINE__, ps3_result(res));
++		goto fail_free;
++	}
+ 
+-			if (result == -ENODEV)
+-				pr_debug("%s:%u: nothing new\n", __func__,
+-					__LINE__);
+-			else if (result)
+-				pr_debug("%s:%u: find device error.\n",
+-					__func__, __LINE__);
+-			else {
+-				pr_debug("%s:%u: found device (%u:%u:%u)\n",
+-					 __func__, __LINE__, repo->bus_index,
+-					 repo->dev_index, repo->dev_type);
+-				ps3_register_repository_device(repo);
+-				ps3_repository_bump_device(repo);
+-				ms = 250;
+-			}
+-		} while (!result);
++	res = ps3_sb_event_receive_port_setup(&dev.sbd, PS3_BINDING_CPU_ANY,
++					      &irq);
++	if (res) {
++		pr_err("%s:%u: ps3_sb_event_receive_port_setup failed %d\n",
++		       __func__, __LINE__, res);
++	       goto fail_close_device;
++	}
++
++	spin_lock_init(&dev.lock);
++
++	res = request_irq(irq, ps3_notification_interrupt, IRQF_DISABLED,
++			  "ps3_notification", &dev);
++	if (res) {
++		pr_err("%s:%u: request_irq failed %d\n", __func__, __LINE__,
++		       res);
++		goto fail_sb_event_receive_port_destroy;
++	}
+ 
+-		pr_debug("%s:%u: ms %u\n", __func__, __LINE__, ms);
++	/* Setup and write the request for device notification. */
++	notify_cmd->operation_code = 0; /* must be zero */
++	notify_cmd->event_mask = 1UL << notify_region_probe;
+ 
+-		if ( ms > 60000)
++	res = ps3_notification_read_write(&dev, lpar, 1);
++	if (res)
++		goto fail_free_irq;
++
++	/* Loop here processing the requested notification events. */
++	do {
++		try_to_freeze();
++
++		memset(notify_event, 0, sizeof(*notify_event));
++
++		res = ps3_notification_read_write(&dev, lpar, 0);
++		if (res)
+ 			break;
+ 
+-		msleep_interruptible(ms);
++		pr_debug("%s:%u: notify event type 0x%lx bus id %lu dev id %lu"
++			 " type %lu port %lu\n", __func__, __LINE__,
++			 notify_event->event_type, notify_event->bus_id,
++			 notify_event->dev_id, notify_event->dev_type,
++			 notify_event->dev_port);
+ 
+-		/* An exponential backoff. */
+-		ms <<= 1;
++		if (notify_event->event_type != notify_region_probe ||
++		    notify_event->bus_id != dev.sbd.bus_id) {
++			pr_warning("%s:%u: bad notify_event: event %lu, "
++				   "dev_id %lu, dev_type %lu\n",
++				   __func__, __LINE__, notify_event->event_type,
++				   notify_event->dev_id,
++				   notify_event->dev_type);
++			continue;
++		}
++
++		res = ps3_repository_find_device_by_id(&repo, dev.sbd.bus_id,
++						       notify_event->dev_id);
++		if (res) {
++			pr_warning("%s:%u: device %lu:%lu not found\n",
++				   __func__, __LINE__, dev.sbd.bus_id,
++				   notify_event->dev_id);
++			continue;
++		}
++
++		pr_debug("%s:%u: device %lu:%lu found\n", __func__, __LINE__,
++			 dev.sbd.bus_id, notify_event->dev_id);
++		ps3_register_repository_device(&repo);
+ 
+ 	} while (!kthread_should_stop());
+ 
++fail_free_irq:
++	free_irq(irq, &dev);
++fail_sb_event_receive_port_destroy:
++	ps3_sb_event_receive_port_destroy(&dev.sbd, irq);
++fail_close_device:
++	lv1_close_device(dev.sbd.bus_id, dev.sbd.dev_id);
++fail_free:
++	kfree(buf);
++
++	probe_task = NULL;
++
+ 	pr_debug(" <- %s:%u: kthread finished\n", __func__, __LINE__);
+ 
+ 	return 0;
+ }
+ 
+ /**
++ * ps3_stop_probe_thread - Stops the background probe thread.
++ *
++ */
++
++static int ps3_stop_probe_thread(struct notifier_block *nb, unsigned long code,
++				 void *data)
++{
++	if (probe_task)
++		kthread_stop(probe_task);
++	return 0;
++}
++
++static struct notifier_block nb = {
++	.notifier_call = ps3_stop_probe_thread
++};
++
++/**
+  * ps3_start_probe_thread - Starts the background probe thread.
+  *
+  */
+@@ -723,7 +739,7 @@ static int __init ps3_start_probe_thread
+ {
+ 	int result;
+ 	struct task_struct *task;
+-	static struct ps3_repository_device repo; /* must be static */
++	struct ps3_repository_device repo;
+ 
+ 	pr_debug(" -> %s:%d\n", __func__, __LINE__);
+ 
+@@ -746,7 +762,8 @@ static int __init ps3_start_probe_thread
+ 		return -ENODEV;
+ 	}
+ 
+-	task = kthread_run(ps3_probe_thread, &repo, "ps3-probe-%u", bus_type);
++	task = kthread_run(ps3_probe_thread, (void *)repo.bus_id,
++			   "ps3-probe-%u", bus_type);
+ 
+ 	if (IS_ERR(task)) {
+ 		result = PTR_ERR(task);
+@@ -755,6 +772,9 @@ static int __init ps3_start_probe_thread
+ 		return result;
+ 	}
+ 
++	probe_task = task;
++	register_reboot_notifier(&nb);
++
+ 	pr_debug(" <- %s:%d\n", __func__, __LINE__);
+ 	return 0;
+ }
+--- a/arch/powerpc/platforms/ps3/platform.h
++++ b/arch/powerpc/platforms/ps3/platform.h
+@@ -89,8 +89,6 @@ enum ps3_dev_type {
+ 	PS3_DEV_TYPE_STOR_ROM = TYPE_ROM,	/* 5 */
+ 	PS3_DEV_TYPE_SB_GPIO = 6,
+ 	PS3_DEV_TYPE_STOR_FLASH = TYPE_RBC,	/* 14 */
+-	PS3_DEV_TYPE_STOR_DUMMY = 32,
+-	PS3_DEV_TYPE_NOACCESS = 255,
+ };
+ 
+ int ps3_repository_read_bus_str(unsigned int bus_index, const char *bus_str,
+--- a/arch/powerpc/platforms/ps3/repository.c
++++ b/arch/powerpc/platforms/ps3/repository.c
+@@ -344,35 +344,6 @@ int ps3_repository_find_device(struct ps
+ 		return result;
+ 	}
+ 
+-	if (tmp.bus_type == PS3_BUS_TYPE_STORAGE) {
+-		/*
+-		 * A storage device may show up in the repository before the
+-		 * hypervisor has finished probing its type and regions
+-		 */
+-		unsigned int num_regions;
+-
+-		if (tmp.dev_type == PS3_DEV_TYPE_STOR_DUMMY) {
+-			pr_debug("%s:%u storage device not ready\n", __func__,
+-				 __LINE__);
+-			return -ENODEV;
+-		}
+-
+-		result = ps3_repository_read_stor_dev_num_regions(tmp.bus_index,
+-								  tmp.dev_index,
+-								  &num_regions);
+-		if (result) {
+-			pr_debug("%s:%d read_stor_dev_num_regions failed\n",
+-				 __func__, __LINE__);
+-			return result;
+-		}
+-
+-		if (!num_regions) {
+-			pr_debug("%s:%u storage device has no regions yet\n",
+-				 __func__, __LINE__);
+-			return -ENODEV;
+-		}
+-	}
+-
+ 	result = ps3_repository_read_dev_id(tmp.bus_index, tmp.dev_index,
+ 		&tmp.dev_id);
+ 
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-storage-fw190-workaround.diff linux-2.6.25-id/patches/ps3-stable/ps3-storage-fw190-workaround.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3-storage-fw190-workaround.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-storage-fw190-workaround.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,85 @@
+Subject: PS3: Add repository polling loop to work around timing bug
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+PS3: Add repository polling loop to work around timing bug
+
+On some firmware versions (e.g. 1.90), the storage device may not show up
+in the repository immediately after receiving the notification message.
+Add a small polling loop to make sure we don't miss it.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/device-init.c |   46 ++++++++++++++++++++++---------
+ 1 file changed, 33 insertions(+), 13 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/device-init.c
++++ b/arch/powerpc/platforms/ps3/device-init.c
+@@ -489,6 +489,38 @@ static int ps3_register_repository_devic
+ 	return result;
+ }
+ 
++static void ps3_find_and_add_device(u64 bus_id, u64 dev_id)
++{
++	struct ps3_repository_device repo;
++	int res;
++	unsigned int retries;
++	unsigned long rem;
++
++	/*
++	 * On some firmware versions (e.g. 1.90), the device may not show up
++	 * in the repository immediately
++	 */
++	for (retries = 0; retries < 10; retries++) {
++		res = ps3_repository_find_device_by_id(&repo, bus_id, dev_id);
++		if (!res)
++			goto found;
++
++		rem = msleep_interruptible(100);
++		if (rem)
++			break;
++	}
++	pr_warning("%s:%u: device %lu:%lu not found\n", __func__, __LINE__,
++		   bus_id, dev_id);
++	return;
++
++found:
++	if (retries)
++		pr_debug("%s:%u: device %lu:%lu found after %u retries\n",
++			 __func__, __LINE__, bus_id, dev_id, retries);
++
++	ps3_register_repository_device(&repo);
++	return;
++}
+ 
+ #define PS3_NOTIFICATION_DEV_ID		ULONG_MAX
+ #define PS3_NOTIFICATION_INTERRUPT_ID	0
+@@ -600,7 +632,6 @@ static struct task_struct *probe_task;
+ static int ps3_probe_thread(void *data)
+ {
+ 	struct ps3_notification_device dev;
+-	struct ps3_repository_device repo;
+ 	int res;
+ 	unsigned int irq;
+ 	u64 lpar;
+@@ -682,18 +713,7 @@ static int ps3_probe_thread(void *data)
+ 			continue;
+ 		}
+ 
+-		res = ps3_repository_find_device_by_id(&repo, dev.sbd.bus_id,
+-						       notify_event->dev_id);
+-		if (res) {
+-			pr_warning("%s:%u: device %lu:%lu not found\n",
+-				   __func__, __LINE__, dev.sbd.bus_id,
+-				   notify_event->dev_id);
+-			continue;
+-		}
+-
+-		pr_debug("%s:%u: device %lu:%lu found\n", __func__, __LINE__,
+-			 dev.sbd.bus_id, notify_event->dev_id);
+-		ps3_register_repository_device(&repo);
++		ps3_find_and_add_device(dev.sbd.bus_id, notify_event->dev_id);
+ 
+ 	} while (!kthread_should_stop());
+ 
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3-vuart-change-sem-to-mutex.patch linux-2.6.25-id/patches/ps3-stable/ps3-vuart-change-sem-to-mutex.patch
--- linux-2.6.25-org/patches/ps3-stable/ps3-vuart-change-sem-to-mutex.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3-vuart-change-sem-to-mutex.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,111 @@
+Subject: PS3: Vuart change semaphore to mutex
+
+A general housekeeping change of the PS3 vuart variable
+vuart_bus_priv.probe_mutex from semaphore to mutex.
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ drivers/ps3/ps3-vuart.c |   22 +++++++++++-----------
+ 1 file changed, 11 insertions(+), 11 deletions(-)
+
+--- a/drivers/ps3/ps3-vuart.c
++++ b/drivers/ps3/ps3-vuart.c
+@@ -877,7 +877,7 @@ static int ps3_vuart_handle_port_interru
+ struct vuart_bus_priv {
+ 	struct ports_bmp *bmp;
+ 	unsigned int virq;
+-	struct semaphore probe_mutex;
++	struct mutex probe_mutex;
+ 	int use_count;
+ 	struct ps3_system_bus_device *devices[PORT_COUNT];
+ } static vuart_bus_priv;
+@@ -1015,7 +1015,7 @@ static int ps3_vuart_probe(struct ps3_sy
+ 		return -EINVAL;
+ 	}
+ 
+-	down(&vuart_bus_priv.probe_mutex);
++	mutex_lock(&vuart_bus_priv.probe_mutex);
+ 
+ 	result = ps3_vuart_bus_interrupt_get();
+ 
+@@ -1076,7 +1076,7 @@ static int ps3_vuart_probe(struct ps3_sy
+ 		goto fail_probe;
+ 	}
+ 
+-	up(&vuart_bus_priv.probe_mutex);
++	mutex_unlock(&vuart_bus_priv.probe_mutex);
+ 
+ 	return result;
+ 
+@@ -1089,7 +1089,7 @@ fail_dev_malloc:
+ fail_busy:
+ 	ps3_vuart_bus_interrupt_put();
+ fail_setup_interrupt:
+-	up(&vuart_bus_priv.probe_mutex);
++	mutex_unlock(&vuart_bus_priv.probe_mutex);
+ 	dev_dbg(&dev->core, "%s:%d: failed\n", __func__, __LINE__);
+ 	return result;
+ }
+@@ -1128,7 +1128,7 @@ static int ps3_vuart_remove(struct ps3_s
+ 
+ 	BUG_ON(!dev);
+ 
+-	down(&vuart_bus_priv.probe_mutex);
++	mutex_lock(&vuart_bus_priv.probe_mutex);
+ 
+ 	dev_dbg(&dev->core, " -> %s:%d: match_id %d\n", __func__, __LINE__,
+ 		dev->match_id);
+@@ -1136,7 +1136,7 @@ static int ps3_vuart_remove(struct ps3_s
+ 	if (!dev->core.driver) {
+ 		dev_dbg(&dev->core, "%s:%d: no driver bound\n", __func__,
+ 			__LINE__);
+-		up(&vuart_bus_priv.probe_mutex);
++		mutex_unlock(&vuart_bus_priv.probe_mutex);
+ 		return 0;
+ 	}
+ 
+@@ -1159,7 +1159,7 @@ static int ps3_vuart_remove(struct ps3_s
+ 	priv = NULL;
+ 
+ 	dev_dbg(&dev->core, " <- %s:%d\n", __func__, __LINE__);
+-	up(&vuart_bus_priv.probe_mutex);
++	mutex_unlock(&vuart_bus_priv.probe_mutex);
+ 	return 0;
+ }
+ 
+@@ -1179,7 +1179,7 @@ static int ps3_vuart_shutdown(struct ps3
+ 
+ 	BUG_ON(!dev);
+ 
+-	down(&vuart_bus_priv.probe_mutex);
++	mutex_lock(&vuart_bus_priv.probe_mutex);
+ 
+ 	dev_dbg(&dev->core, " -> %s:%d: match_id %d\n", __func__, __LINE__,
+ 		dev->match_id);
+@@ -1187,7 +1187,7 @@ static int ps3_vuart_shutdown(struct ps3
+ 	if (!dev->core.driver) {
+ 		dev_dbg(&dev->core, "%s:%d: no driver bound\n", __func__,
+ 			__LINE__);
+-		up(&vuart_bus_priv.probe_mutex);
++		mutex_unlock(&vuart_bus_priv.probe_mutex);
+ 		return 0;
+ 	}
+ 
+@@ -1211,7 +1211,7 @@ static int ps3_vuart_shutdown(struct ps3
+ 
+ 	dev_dbg(&dev->core, " <- %s:%d\n", __func__, __LINE__);
+ 
+-	up(&vuart_bus_priv.probe_mutex);
++	mutex_unlock(&vuart_bus_priv.probe_mutex);
+ 	return 0;
+ }
+ 
+@@ -1222,7 +1222,7 @@ static int __init ps3_vuart_bus_init(voi
+ 	if (!firmware_has_feature(FW_FEATURE_PS3_LV1))
+ 		return -ENODEV;
+ 
+-	init_MUTEX(&vuart_bus_priv.probe_mutex);
++	mutex_init(&vuart_bus_priv.probe_mutex);
+ 
+ 	return 0;
+ }
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3_repository_find_device_by_id.diff linux-2.6.25-id/patches/ps3-stable/ps3_repository_find_device_by_id.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3_repository_find_device_by_id.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3_repository_find_device_by_id.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,112 @@
+Subject: PS3: Add ps3_repository_find_device_by_id()
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+The storage probe feature of the PS3 hypervisor returns device IDs. Add
+the corresponding repository routine ps3_repository_find_device_by_id()
+which can be used to retrieve the device info from the repository.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/ps3/platform.h   |    2 
+ arch/powerpc/platforms/ps3/repository.c |   77 ++++++++++++++++++++++++++++++++
+ 2 files changed, 79 insertions(+)
+
+--- a/arch/powerpc/platforms/ps3/platform.h
++++ b/arch/powerpc/platforms/ps3/platform.h
+@@ -153,6 +153,8 @@ static inline struct ps3_repository_devi
+ 	return repo;
+ }
+ int ps3_repository_find_device(struct ps3_repository_device *repo);
++int ps3_repository_find_device_by_id(struct ps3_repository_device *repo,
++				     u64 bus_id, u64 dev_id);
+ int ps3_repository_find_devices(enum ps3_bus_type bus_type,
+ 	int (*callback)(const struct ps3_repository_device *repo));
+ int ps3_repository_find_bus(enum ps3_bus_type bus_type, unsigned int from,
+--- a/arch/powerpc/platforms/ps3/repository.c
++++ b/arch/powerpc/platforms/ps3/repository.c
+@@ -389,6 +389,83 @@ int ps3_repository_find_device(struct ps
+ 	return 0;
+ }
+ 
++int ps3_repository_find_device_by_id(struct ps3_repository_device *repo,
++				     u64 bus_id, u64 dev_id)
++{
++	int result = -ENODEV;
++	struct ps3_repository_device tmp;
++	unsigned int num_dev;
++
++	pr_debug(" -> %s:%u: find device by id %lu:%lu\n", __func__, __LINE__,
++		 bus_id, dev_id);
++
++	for (tmp.bus_index = 0; tmp.bus_index < 10; tmp.bus_index++) {
++		result = ps3_repository_read_bus_id(tmp.bus_index,
++						    &tmp.bus_id);
++		if (result) {
++			pr_debug("%s:%u read_bus_id(%u) failed\n", __func__,
++				 __LINE__, tmp.bus_index);
++			return result;
++		}
++
++		if (tmp.bus_id == bus_id)
++			goto found_bus;
++
++		pr_debug("%s:%u: skip, bus_id %lu\n", __func__, __LINE__,
++			 tmp.bus_id);
++	}
++	pr_debug(" <- %s:%u: bus not found\n", __func__, __LINE__);
++	return result;
++
++found_bus:
++	result = ps3_repository_read_bus_type(tmp.bus_index, &tmp.bus_type);
++	if (result) {
++		pr_debug("%s:%u read_bus_type(%u) failed\n", __func__,
++			 __LINE__, tmp.bus_index);
++		return result;
++	}
++
++	result = ps3_repository_read_bus_num_dev(tmp.bus_index, &num_dev);
++	if (result) {
++		pr_debug("%s:%u read_bus_num_dev failed\n", __func__,
++			 __LINE__);
++		return result;
++	}
++
++	for (tmp.dev_index = 0; tmp.dev_index < num_dev; tmp.dev_index++) {
++		result = ps3_repository_read_dev_id(tmp.bus_index,
++						    tmp.dev_index,
++						    &tmp.dev_id);
++		if (result) {
++			pr_debug("%s:%u read_dev_id(%u:%u) failed\n", __func__,
++				 __LINE__, tmp.bus_index, tmp.dev_index);
++			return result;
++		}
++
++		if (tmp.dev_id == dev_id)
++			goto found_dev;
++
++		pr_debug("%s:%u: skip, dev_id %lu\n", __func__, __LINE__,
++			 tmp.dev_id);
++	}
++	pr_debug(" <- %s:%u: dev not found\n", __func__, __LINE__);
++	return result;
++
++found_dev:
++	result = ps3_repository_read_dev_type(tmp.bus_index, tmp.dev_index,
++					      &tmp.dev_type);
++	if (result) {
++		pr_debug("%s:%u read_dev_type failed\n", __func__, __LINE__);
++		return result;
++	}
++
++	pr_debug(" <- %s:%u: found: type (%u:%u) index (%u:%u) id (%lu:%lu)\n",
++		 __func__, __LINE__, tmp.bus_type, tmp.dev_type, tmp.bus_index,
++		 tmp.dev_index, tmp.bus_id, tmp.dev_id);
++	*repo = tmp;
++	return 0;
++}
++
+ int __devinit ps3_repository_find_devices(enum ps3_bus_type bus_type,
+ 	int (*callback)(const struct ps3_repository_device *repo))
+ {
diff -Naur linux-2.6.25-org/patches/ps3-stable/ps3disk-superfluous-cast.diff linux-2.6.25-id/patches/ps3-stable/ps3disk-superfluous-cast.diff
--- linux-2.6.25-org/patches/ps3-stable/ps3disk-superfluous-cast.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/ps3disk-superfluous-cast.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,26 @@
+Subject: ps3disk: Remove superfluous cast
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3disk: Remove a superfluous cast
+
+As ps3disk is a ppc64-only driver, sector_t equals to unsigned long.
+Future {re,ab}use is protected by the safety net called `compiler warning'.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/block/ps3disk.c |    3 +--
+ 1 files changed, 1 insertion(+), 2 deletions(-)
+
+--- a/drivers/block/ps3disk.c
++++ b/drivers/block/ps3disk.c
+@@ -102,8 +102,7 @@ static void ps3disk_scatter_gather(struc
+ 		dev_dbg(&dev->sbd.core,
+ 			"%s:%u: bio %u: %u segs %u sectors from %lu\n",
+ 			__func__, __LINE__, i, bio_segments(iter.bio),
+-			bio_sectors(iter.bio),
+-			(unsigned long)iter.bio->bi_sector);
++			bio_sectors(iter.bio), iter.bio->bi_sector);
+ 
+ 		size = bvec->bv_len;
+ 		buf = bvec_kmap_irq(bvec, &flags);
diff -Naur linux-2.6.25-org/patches/ps3-stable/spufs-wrap-master-run-bit.diff linux-2.6.25-id/patches/ps3-stable/spufs-wrap-master-run-bit.diff
--- linux-2.6.25-org/patches/ps3-stable/spufs-wrap-master-run-bit.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-stable/spufs-wrap-master-run-bit.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,239 @@
+Subject: Cell: Wrap master run control bit
+
+From: Masato Noguchi <Masato.Noguchi@jp.sony.com>
+
+Add platform specific SPU run control routines to the spufs.  The current
+spufs implementation uses the SPU master run control bit (MFC_SR1[S]) to
+control SPE execution, but the PS3 hypervisor does not support the use of
+this feature.
+
+This change adds the run control wrapper routies spu_enable_spu() and
+spu_disable_spu().  The bare metal routines use the master run control
+bit, and the PS3 specific routines use the priv2 run control register.
+
+An outstanding enhancement for the PS3 would be to add a guard to check
+for incorrect access to the spu problem state when the spu context is
+disabled.  This check could be implemented with a flag added to the spu
+context that would inhibit mapping problem state pages, and a routine
+to unmap spu problem state pages.  When the spu is enabled with
+ps3_enable_spu() the flag would be set allowing pages to be mapped,
+and when the spu is disabled with ps3_disable_spu() the flag would be
+cleared and mapped problem state pages would be unmapped.
+
+Signed-off-by: Masato Noguchi <Masato.Noguchi@jp.sony.com>
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/platforms/cell/spu_manage.c        |   13 +++++++++++
+ arch/powerpc/platforms/cell/spufs/backing_ops.c |    6 +++++
+ arch/powerpc/platforms/cell/spufs/hw_ops.c      |   10 ++++++++
+ arch/powerpc/platforms/cell/spufs/run.c         |    4 +--
+ arch/powerpc/platforms/cell/spufs/spufs.h       |    1 
+ arch/powerpc/platforms/ps3/spu.c                |   27 ++++++++++++++++++++++--
+ include/asm-powerpc/spu_priv1.h                 |   15 +++++++++++++
+ 7 files changed, 72 insertions(+), 4 deletions(-)
+
+--- a/arch/powerpc/platforms/cell/spu_manage.c
++++ b/arch/powerpc/platforms/cell/spu_manage.c
+@@ -35,6 +35,7 @@
+ #include <asm/firmware.h>
+ #include <asm/prom.h>
+ 
++#include "spufs/spufs.h"
+ #include "interrupt.h"
+ 
+ struct device_node *spu_devnode(struct spu *spu)
+@@ -369,6 +370,16 @@ static int of_destroy_spu(struct spu *sp
+ 	return 0;
+ }
+ 
++static void enable_spu_by_master_run(struct spu_context *ctx)
++{
++	ctx->ops->master_start(ctx);
++}
++
++static void disable_spu_by_master_run(struct spu_context *ctx)
++{
++	ctx->ops->master_stop(ctx);
++}
++
+ /* Hardcoded affinity idxs for qs20 */
+ #define QS20_SPES_PER_BE 8
+ static int qs20_reg_idxs[QS20_SPES_PER_BE] =   { 0, 2, 4, 6, 7, 5, 3, 1 };
+@@ -535,5 +546,7 @@ const struct spu_management_ops spu_mana
+ 	.enumerate_spus = of_enumerate_spus,
+ 	.create_spu = of_create_spu,
+ 	.destroy_spu = of_destroy_spu,
++	.enable_spu = enable_spu_by_master_run,
++	.disable_spu = disable_spu_by_master_run,
+ 	.init_affinity = init_affinity,
+ };
+--- a/arch/powerpc/platforms/cell/spufs/backing_ops.c
++++ b/arch/powerpc/platforms/cell/spufs/backing_ops.c
+@@ -285,6 +285,11 @@ static void spu_backing_runcntl_write(st
+ 	spin_unlock(&ctx->csa.register_lock);
+ }
+ 
++static void spu_backing_runcntl_stop(struct spu_context *ctx)
++{
++	spu_backing_runcntl_write(ctx, SPU_RUNCNTL_STOP);
++}
++
+ static void spu_backing_master_start(struct spu_context *ctx)
+ {
+ 	struct spu_state *csa = &ctx->csa;
+@@ -381,6 +386,7 @@ struct spu_context_ops spu_backing_ops =
+ 	.get_ls = spu_backing_get_ls,
+ 	.runcntl_read = spu_backing_runcntl_read,
+ 	.runcntl_write = spu_backing_runcntl_write,
++	.runcntl_stop = spu_backing_runcntl_stop,
+ 	.master_start = spu_backing_master_start,
+ 	.master_stop = spu_backing_master_stop,
+ 	.set_mfc_query = spu_backing_set_mfc_query,
+--- a/arch/powerpc/platforms/cell/spufs/hw_ops.c
++++ b/arch/powerpc/platforms/cell/spufs/hw_ops.c
+@@ -220,6 +220,15 @@ static void spu_hw_runcntl_write(struct 
+ 	spin_unlock_irq(&ctx->spu->register_lock);
+ }
+ 
++static void spu_hw_runcntl_stop(struct spu_context *ctx)
++{
++	spin_lock_irq(&ctx->spu->register_lock);
++	out_be32(&ctx->spu->problem->spu_runcntl_RW, SPU_RUNCNTL_STOP);
++	while (in_be32(&ctx->spu->problem->spu_status_R) & SPU_STATUS_RUNNING)
++		cpu_relax();
++	spin_unlock_irq(&ctx->spu->register_lock);
++}
++
+ static void spu_hw_master_start(struct spu_context *ctx)
+ {
+ 	struct spu *spu = ctx->spu;
+@@ -321,6 +330,7 @@ struct spu_context_ops spu_hw_ops = {
+ 	.get_ls = spu_hw_get_ls,
+ 	.runcntl_read = spu_hw_runcntl_read,
+ 	.runcntl_write = spu_hw_runcntl_write,
++	.runcntl_stop = spu_hw_runcntl_stop,
+ 	.master_start = spu_hw_master_start,
+ 	.master_stop = spu_hw_master_stop,
+ 	.set_mfc_query = spu_hw_set_mfc_query,
+--- a/arch/powerpc/platforms/cell/spufs/run.c
++++ b/arch/powerpc/platforms/cell/spufs/run.c
+@@ -302,7 +302,7 @@ long spufs_run_spu(struct spu_context *c
+ 	if (mutex_lock_interruptible(&ctx->run_mutex))
+ 		return -ERESTARTSYS;
+ 
+-	ctx->ops->master_start(ctx);
++	spu_enable_spu(ctx);
+ 	ctx->event_return = 0;
+ 
+ 	spu_acquire(ctx);
+@@ -376,7 +376,7 @@ long spufs_run_spu(struct spu_context *c
+ 		ctx->stats.libassist++;
+ 
+ 
+-	ctx->ops->master_stop(ctx);
++	spu_disable_spu(ctx);
+ 	ret = spu_run_fini(ctx, npc, &status);
+ 	spu_yield(ctx);
+ 
+--- a/arch/powerpc/platforms/cell/spufs/spufs.h
++++ b/arch/powerpc/platforms/cell/spufs/spufs.h
+@@ -170,6 +170,7 @@ struct spu_context_ops {
+ 	char*(*get_ls) (struct spu_context * ctx);
+ 	 u32 (*runcntl_read) (struct spu_context * ctx);
+ 	void (*runcntl_write) (struct spu_context * ctx, u32 data);
++	void (*runcntl_stop) (struct spu_context * ctx);
+ 	void (*master_start) (struct spu_context * ctx);
+ 	void (*master_stop) (struct spu_context * ctx);
+ 	int (*set_mfc_query)(struct spu_context * ctx, u32 mask, u32 mode);
+--- a/arch/powerpc/platforms/ps3/spu.c
++++ b/arch/powerpc/platforms/ps3/spu.c
+@@ -28,6 +28,7 @@
+ #include <asm/spu_priv1.h>
+ #include <asm/lv1call.h>
+ 
++#include "../cell/spufs/spufs.h"
+ #include "platform.h"
+ 
+ /* spu_management_ops */
+@@ -419,10 +420,34 @@ static int ps3_init_affinity(void)
+ 	return 0;
+ }
+ 
++/**
++ * ps3_enable_spu - Enable SPU run control.
++ *
++ * An outstanding enhancement for the PS3 would be to add a guard to check
++ * for incorrect access to the spu problem state when the spu context is
++ * disabled.  This check could be implemented with a flag added to the spu
++ * context that would inhibit mapping problem state pages, and a routine
++ * to unmap spu problem state pages.  When the spu is enabled with
++ * ps3_enable_spu() the flag would be set allowing pages to be mapped,
++ * and when the spu is disabled with ps3_disable_spu() the flag would be
++ * cleared and the mapped problem state pages would be unmapped.
++ */
++
++static void ps3_enable_spu(struct spu_context *ctx)
++{
++}
++
++static void ps3_disable_spu(struct spu_context *ctx)
++{
++	ctx->ops->runcntl_stop(ctx);
++}
++
+ const struct spu_management_ops spu_management_ps3_ops = {
+ 	.enumerate_spus = ps3_enumerate_spus,
+ 	.create_spu = ps3_create_spu,
+ 	.destroy_spu = ps3_destroy_spu,
++	.enable_spu = ps3_enable_spu,
++	.disable_spu = ps3_disable_spu,
+ 	.init_affinity = ps3_init_affinity,
+ };
+ 
+@@ -505,8 +530,6 @@ static void mfc_sr1_set(struct spu *spu,
+ 	static const u64 allowed = ~(MFC_STATE1_LOCAL_STORAGE_DECODE_MASK
+ 		| MFC_STATE1_PROBLEM_STATE_MASK);
+ 
+-	sr1 |= MFC_STATE1_MASTER_RUN_CONTROL_MASK;
+-
+ 	BUG_ON((sr1 & allowed) != (spu_pdata(spu)->cache.sr1 & allowed));
+ 
+ 	spu_pdata(spu)->cache.sr1 = sr1;
+--- a/include/asm-powerpc/spu_priv1.h
++++ b/include/asm-powerpc/spu_priv1.h
+@@ -24,6 +24,7 @@
+ #include <linux/types.h>
+ 
+ struct spu;
++struct spu_context;
+ 
+ /* access to priv1 registers */
+ 
+@@ -178,6 +179,8 @@ struct spu_management_ops {
+ 	int (*enumerate_spus)(int (*fn)(void *data));
+ 	int (*create_spu)(struct spu *spu, void *data);
+ 	int (*destroy_spu)(struct spu *spu);
++	void (*enable_spu)(struct spu_context *ctx);
++	void (*disable_spu)(struct spu_context *ctx);
+ 	int (*init_affinity)(void);
+ };
+ 
+@@ -207,6 +210,18 @@ spu_init_affinity (void)
+ 	return spu_management_ops->init_affinity();
+ }
+ 
++static inline void
++spu_enable_spu (struct spu_context *ctx)
++{
++	spu_management_ops->enable_spu(ctx);
++}
++
++static inline void
++spu_disable_spu (struct spu_context *ctx)
++{
++	spu_management_ops->disable_spu(ctx);
++}
++
+ /*
+  * The declarations folowing are put here for convenience
+  * and only intended to be used by the platform setup code.
diff -Naur linux-2.6.25-org/patches/ps3-wip/perfmon/cell-add-hw-thid-check-for-ppu-event.patch linux-2.6.25-id/patches/ps3-wip/perfmon/cell-add-hw-thid-check-for-ppu-event.patch
--- linux-2.6.25-org/patches/ps3-wip/perfmon/cell-add-hw-thid-check-for-ppu-event.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/perfmon/cell-add-hw-thid-check-for-ppu-event.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,277 @@
+Cell PPU IU(Instruction Unit)/XU(Execution Unit) event support in 
+per-thread(task) mode.
+
+In per-thread mode, the PPU IU/XU events should be measured 
+while the target SW thread(task) runs on the target HW thread. 
+To realize that, the counter enable bit of the pmX_control PMC is
+manipulated in pfm_cell_restore_pmcs().
+
+Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto at jp.sony.com>
+---
+ arch/powerpc/perfmon/perfmon_cell.c |  207 +++++++++++++++++++++++++++++-------
+ 1 file changed, 169 insertions(+), 38 deletions(-)
+
+--- a/arch/powerpc/perfmon/perfmon_cell.c
++++ b/arch/powerpc/perfmon/perfmon_cell.c
+@@ -96,7 +96,9 @@ static struct pfm_regmap_desc pfm_cell_p
+ };
+ #define PFM_PM_NUM_PMCS	ARRAY_SIZE(pfm_cell_pmc_desc)
+ 
+-#define CELL_PMC_PM_STATUS 20
++#define CELL_PMC_GROUP_CONTROL    16
++#define CELL_PMC_PM_STATUS        20
++
+ /*
+  * Mapping from Perfmon logical data counters to Cell hardware counters.
+  */
+@@ -112,6 +114,19 @@ static struct pfm_regmap_desc pfm_cell_p
+ };
+ #define PFM_PM_NUM_PMDS	ARRAY_SIZE(pfm_cell_pmd_desc)
+ 
++#define PFM_EVENT_PMC_BUS_WORD(x)      (((x) >> 48) & 0x00ff)
++#define PFM_EVENT_PMC_FULL_SIGNAL_NUMBER(x) ((x) & 0xffffffff)
++#define PFM_EVENT_PMC_SIGNAL_GROUP(x) (((x) & 0xffffffff) / 100)
++#define PFM_PM_CTR_INPUT_MUX_BIT(pm07_control) (((pm07_control) >> 26) & 0x1f)
++#define PFM_PM_CTR_INPUT_MUX_GROUP_INDEX(pm07_control) ((pm07_control) >> 31)
++#define PFM_GROUP_CONTROL_GROUP0_WORD(grp_ctrl) ((grp_ctrl) >> 30)
++#define PFM_GROUP_CONTROL_GROUP1_WORD(grp_ctrl) (((grp_ctrl) >> 28) & 0x3)
++#define PFM_NUM_OF_GROUPS 2
++#define PFM_PPU_IU1_THREAD1_BASE_BIT 19
++#define PFM_PPU_XU_THREAD1_BASE_BIT  16
++#define PFM_COUNTER_CTRL_PMC_PPU_TH0 0x100000000ULL
++#define PFM_COUNTER_CTRL_PMC_PPU_TH1 0x200000000ULL
++
+ /*
+  * Debug-bus signal handling.
+  *
+@@ -691,6 +706,72 @@ static void pfm_cell_disable_counters(st
+ 		reset_signals(smp_processor_id());
+ }
+ 
++/*
++ * Return the thread id of the specified ppu signal.
++ */
++static inline u32 get_target_ppu_thread_id(u32 group, u32 bit)
++{
++	if ((group == SIG_GROUP_PPU_IU1 &&
++	     bit < PFM_PPU_IU1_THREAD1_BASE_BIT) ||
++	    (group == SIG_GROUP_PPU_XU &&
++	     bit < PFM_PPU_XU_THREAD1_BASE_BIT))
++		return 0;
++	else
++		return 1;
++}
++
++/*
++ * Return whether the specified counter is for PPU signal group.
++ */
++static inline int is_counter_for_ppu_sig_grp(u32 counter_control, u32 sig_grp)
++{
++	if (!(counter_control & CBE_PM_CTR_INPUT_CONTROL) &&
++	    (counter_control & CBE_PM_CTR_ENABLE) &&
++	    ((sig_grp == SIG_GROUP_PPU_IU1) || (sig_grp == SIG_GROUP_PPU_XU)))
++		return 1;
++	else
++		return 0;
++}
++
++/*
++ * Search ppu signal groups.
++ */
++static int get_ppu_signal_groups(struct pfm_event_set *set,
++				 u32 *ppu_sig_grp0, u32 *ppu_sig_grp1)
++{
++	u64 pm_event, *used_pmcs = set->used_pmcs;
++	int i, j;
++	u32 grp0_wd, grp1_wd, wd, sig_grp;
++
++	*ppu_sig_grp0 = 0;
++	*ppu_sig_grp1 = 0;
++	grp0_wd = PFM_GROUP_CONTROL_GROUP0_WORD(
++		set->pmcs[CELL_PMC_GROUP_CONTROL]);
++	grp1_wd = PFM_GROUP_CONTROL_GROUP1_WORD(
++		set->pmcs[CELL_PMC_GROUP_CONTROL]);
++
++	for (i = 0, j = 0; (i < NR_CTRS) && (j < PFM_NUM_OF_GROUPS); i++) {
++		if (test_bit(i + NR_CTRS, used_pmcs)) {
++			pm_event = set->pmcs[i + NR_CTRS];
++			wd = PFM_EVENT_PMC_BUS_WORD(pm_event);
++			sig_grp = PFM_EVENT_PMC_SIGNAL_GROUP(pm_event);
++			if ((sig_grp == SIG_GROUP_PPU_IU1) ||
++			    (sig_grp == SIG_GROUP_PPU_XU)) {
++
++				if (wd == grp0_wd && *ppu_sig_grp0 == 0) {
++					*ppu_sig_grp0 = sig_grp;
++					j++;
++				} else if (wd == grp1_wd &&
++					   *ppu_sig_grp1 == 0) {
++					*ppu_sig_grp1 = sig_grp;
++					j++;
++				}
++			}
++		}
++	}
++	return j;
++}
++
+ /**
+  * pfm_cell_restore_pmcs
+  *
+@@ -699,56 +780,54 @@ static void pfm_cell_disable_counters(st
+  * individually (as is done in other architectures), but that results in
+  * multiple RTAS calls. As an optimization, we will setup the RTAS argument
+  * array so we can do all event-control registers in one RTAS call.
++ *
++ * In per-thread mode,
++ * The counter enable bit of the pmX_control PMC is enabled while the target
++ * task runs on the target HW thread.
+  **/
+ void pfm_cell_restore_pmcs(struct pfm_event_set *set)
+ {
+-	struct cell_rtas_arg signals[NR_CTRS];
+-	u64 value, *used_pmcs = set->used_pmcs;
+-	int i, rc, num_used = 0, cpu = smp_processor_id();
+-	s32 signal_number;
++	u64 ctr_ctrl;
++	u64 *used_pmcs = set->used_pmcs;
++	int i;
++	int cpu = smp_processor_id();
++	u32 current_th_id;
+ 	struct pfm_cell_platform_pmu_info *info =
+ 		((struct pfm_arch_pmu_info *)
+ 		 (pfm_pmu_conf->arch_info))->platform_info;
+ 
+-	memset(signals, 0, sizeof(signals));
+-
+ 	for (i = 0; i < NR_CTRS; i++) {
+-		/* Write the per-counter control register. If the PMC is not
+-		 * in use, then it will simply clear the register, which will
+-		 * disable the associated counter.
+-		 */
+-		info->write_pm07_control(cpu, i, set->pmcs[i]);
++		ctr_ctrl = set->pmcs[i];
+ 
+-		/* Set up the next RTAS array entry for this counter. Only
+-		 * include pm07_event registers that are in use by this set
+-		 * so the RTAS call doesn't have to process blank array entries.
+-		 */
+-		if (!test_bit(i + NR_CTRS, used_pmcs)) {
+-			continue;
+-		}
++		if (ctr_ctrl & PFM_COUNTER_CTRL_PMC_PPU_TH0) {
++			current_th_id = info->get_hw_thread_id(cpu);
+ 
+-		value = set->pmcs[i + NR_CTRS];
+-		signal_number = RTAS_SIGNAL_NUMBER(value);
+-		if (!signal_number) {
+-			/* Don't include counters that are counting cycles. */
+-			continue;
++			/*
++			 * Set the counter enable bit down if the current
++			 * HW thread is NOT 0
++			 **/
++			if (current_th_id)
++				ctr_ctrl = ctr_ctrl & ~CBE_PM_CTR_ENABLE;
++
++		} else if (ctr_ctrl & PFM_COUNTER_CTRL_PMC_PPU_TH1) {
++			current_th_id = info->get_hw_thread_id(cpu);
++
++			/*
++			 * Set the counter enable bit down if the current
++			 * HW thread is 0
++			 **/
++			if (!current_th_id)
++				ctr_ctrl = ctr_ctrl & ~CBE_PM_CTR_ENABLE;
+ 		}
+ 
+-		signals[num_used].cpu = RTAS_CPU(cpu);
+-		signals[num_used].sub_unit = RTAS_SUB_UNIT(value);
+-		signals[num_used].bus_word = 1 << RTAS_BUS_WORD(value);
+-		signals[num_used].signal_group = signal_number / 100;
+-		signals[num_used].bit = signal_number % 100;
+-		num_used++;
+-	}
+-
+-	rc = activate_signals(signals, num_used);
+-	if (rc) {
+-		PFM_WARN("Error calling activate_signal(): %d\n", rc);
+-		/* FIX: We will also need this routine to be able to return
+-		 * an error if Stephane agrees to change pfm_arch_write_pmc
+-		 * to return an error.
++		/* Write the per-counter control register. If the PMC is not
++		 * in use, then it will simply clear the register, which will
++		 * disable the associated counter.
+ 		 */
++		info->write_pm07_control(cpu, i, ctr_ctrl);
++
++		if (test_bit(i + NR_CTRS, used_pmcs))
++			write_pm07_event(cpu, 0, set->pmcs[i + NR_CTRS]);
+ 	}
+ 
+ 	/* Write all the global PMCs. Need to call pfm_cell_write_pmc()
+@@ -760,6 +839,57 @@ void pfm_cell_restore_pmcs(struct pfm_ev
+ }
+ 
+ /**
++ * pfm_cell_load_context
++ *
++ * In per-thread mode,
++ *  The pmX_control PMCs which are used for PPU IU/XU event are marked with
++ *  the thread id(PFM_COUNTER_CTRL_PMC_PPU_TH0/TH1).
++ **/
++static int pfm_cell_load_context(struct pfm_context *ctx,
++				 struct pfm_event_set *set,
++				 struct task_struct *task)
++{
++	int i;
++	u32 ppu_sig_grp[PFM_NUM_OF_GROUPS] = {SIG_GROUP_NONE, SIG_GROUP_NONE};
++	u32 bit;
++	int index;
++	u32 target_th_id;
++	int ppu_sig_num = 0;
++	struct pfm_event_set *s;
++
++	if (ctx->flags.system)
++		return 0;
++
++	list_for_each_entry(s, &ctx->set_list, list) {
++		ppu_sig_num = get_ppu_signal_groups(s, &ppu_sig_grp[0],
++						    &ppu_sig_grp[1]);
++
++		for (i = 0; i < NR_CTRS; i++) {
++			index = PFM_PM_CTR_INPUT_MUX_GROUP_INDEX(s->pmcs[i]);
++			if (ppu_sig_num &&
++			    (ppu_sig_grp[index] != SIG_GROUP_NONE) &&
++			    is_counter_for_ppu_sig_grp(s->pmcs[i],
++						       ppu_sig_grp[index])) {
++
++				bit = PFM_PM_CTR_INPUT_MUX_BIT(s->pmcs[i]);
++				target_th_id = get_target_ppu_thread_id(
++					ppu_sig_grp[index], bit);
++				if (!target_th_id)
++					s->pmcs[i] |=
++						PFM_COUNTER_CTRL_PMC_PPU_TH0;
++				else
++					s->pmcs[i] |=
++						PFM_COUNTER_CTRL_PMC_PPU_TH1;
++				PFM_DBG("set:%d mark ctr:%d target_thread:%d",
++					s->id, i, target_th_id);
++			}
++		}
++	}
++
++	return 0;
++}
++
++/**
+  * pfm_cell_unload_context
+  *
+  * For system-wide contexts and self-monitored contexts, make the RTAS call
+@@ -1026,6 +1156,7 @@ static struct pfm_arch_pmu_info pfm_cell
+ 	.get_ovfl_pmds    = pfm_cell_get_ovfl_pmds,
+ 	.restore_pmcs     = pfm_cell_restore_pmcs,
+ 	.ctxswout_thread  = pfm_cell_ctxswout_thread,
++	.load_context     = pfm_cell_load_context,
+ 	.unload_context   = pfm_cell_unload_context,
+ };
+ 
diff -Naur linux-2.6.25-org/patches/ps3-wip/perfmon/ps3-rename-lpm-open-close.patch linux-2.6.25-id/patches/ps3-wip/perfmon/ps3-rename-lpm-open-close.patch
--- linux-2.6.25-org/patches/ps3-wip/perfmon/ps3-rename-lpm-open-close.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/perfmon/ps3-rename-lpm-open-close.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,31 @@
+---
+ arch/powerpc/perfmon/perfmon_cell.c |   10 ++++------
+ 1 file changed, 4 insertions(+), 6 deletions(-)
+
+--- a/arch/powerpc/perfmon/perfmon_cell.c
++++ b/arch/powerpc/perfmon/perfmon_cell.c
+@@ -985,9 +985,9 @@ int pfm_cell_acquire_pmu(void)
+ 
+ 	if (machine_is(ps3)) {
+ 		PFM_DBG("");
+-		ret = ps3_create_lpm(1, 0, 0, 1);
++		ret = ps3_lpm_open(PS3_LPM_TB_TYPE_INTERNAL, NULL, 0);
+ 		if (ret) {
+-			PFM_ERR("Can't create PS3 lpm. error:%d", ret);
++			PFM_ERR("Can't open PS3 lpm. error:%d", ret);
+ 			return -EFAULT;
+ 		}
+ 	}
+@@ -1004,10 +1004,8 @@ int pfm_cell_acquire_pmu(void)
+ void pfm_cell_release_pmu(void)
+ {
+ #ifdef CONFIG_PPC_PS3
+-	if (machine_is(ps3)) {
+-		if (ps3_delete_lpm())
+-			PFM_ERR("Can't delete PS3 lpm.");
+-	}
++	if (machine_is(ps3))
++		ps3_lpm_close();
+ #endif
+ }
+ 
diff -Naur linux-2.6.25-org/patches/ps3-wip/powerpc-verbose-bootwrapper.diff linux-2.6.25-id/patches/ps3-wip/powerpc-verbose-bootwrapper.diff
--- linux-2.6.25-org/patches/ps3-wip/powerpc-verbose-bootwrapper.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/powerpc-verbose-bootwrapper.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,32 @@
+Subject: powerpc: Make bootwrapper verbose when KBUILD_VERBOSE.
+
+Add '-x' to the envocation of the bootwrapper build script when
+KBUILD_VERBOSE is not zero.
+
+???Will this only work for bash???
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/boot/Makefile |    9 +++++++--
+ 1 file changed, 7 insertions(+), 2 deletions(-)
+
+--- a/arch/powerpc/boot/Makefile
++++ b/arch/powerpc/boot/Makefile
+@@ -121,10 +121,15 @@ CROSSWRAP := -C "$(CROSS_COMPILE)"
+ endif
+ endif
+ 
++ifneq ($(KBUILD_VERBOSE),0)
++verbose_shell := -x
++endif
++
+ # args (to if_changed): 1 = (this rule), 2 = platform, 3 = dts 4=dtb 5=initrd
+ quiet_cmd_wrap	= WRAP    $@
+-      cmd_wrap	=$(CONFIG_SHELL) $(wrapper) -c -o $@ -p $2 $(CROSSWRAP) \
+-		$(if $3, -s $3)$(if $4, -d $4)$(if $5, -i $5) vmlinux
++      cmd_wrap	=$(CONFIG_SHELL) $(verbose_shell) $(wrapper) -c -o $@ -p $2 \
++		$(CROSSWRAP) $(if $3, -s $3)$(if $4, -d $4)$(if $5, -i $5) \
++		vmlinux
+ 
+ image-$(CONFIG_PPC_PSERIES)		+= zImage.pseries
+ image-$(CONFIG_PPC_MAPLE)		+= zImage.pseries
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-cleanup.patch linux-2.6.25-id/patches/ps3-wip/ps3-gelic-cleanup.patch
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-cleanup.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-cleanup.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,1555 @@
+Subject: PS3: gelic: code cleanup
+
+Code cleanup:
+ - Use appropriate prefixes for names instead of fixed 'gelic_net'
+   so that objects of the functions, variables and constants can be estimated.
+ - Remove definitions for IPSec offload to the gelic hardware.  This
+   functionality is never supported on PS3.
+ - Group constants with enum.
+ - Use bitwise constants for interrupt status, instead of bit numbers to
+   eliminate shift operations.
+ - Style fixes.
+Signed-off-by: Masakazu Mokuno <mokuno@sm.sony.co.jp>
+---
+ drivers/net/ps3_gelic_net.c |  464 +++++++++++++++++++++-----------------------
+ drivers/net/ps3_gelic_net.h |  283 +++++++++++++++-----------
+ 2 files changed, 389 insertions(+), 358 deletions(-)
+
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -54,21 +54,21 @@ MODULE_AUTHOR("SCE Inc.");
+ MODULE_DESCRIPTION("Gelic Network driver");
+ MODULE_LICENSE("GPL");
+ 
+-static inline struct device *ctodev(struct gelic_net_card *card)
++static inline struct device *ctodev(struct gelic_card *card)
+ {
+ 	return &card->dev->core;
+ }
+-static inline u64 bus_id(struct gelic_net_card *card)
++static inline u64 bus_id(struct gelic_card *card)
+ {
+ 	return card->dev->bus_id;
+ }
+-static inline u64 dev_id(struct gelic_net_card *card)
++static inline u64 dev_id(struct gelic_card *card)
+ {
+ 	return card->dev->dev_id;
+ }
+ 
+ /* set irq_mask */
+-static int gelic_net_set_irq_mask(struct gelic_net_card *card, u64 mask)
++static int gelic_card_set_irq_mask(struct gelic_card *card, u64 mask)
+ {
+ 	int status;
+ 
+@@ -79,51 +79,40 @@ static int gelic_net_set_irq_mask(struct
+ 			 "lv1_net_set_interrupt_mask failed %d\n", status);
+ 	return status;
+ }
+-static inline void gelic_net_rx_irq_on(struct gelic_net_card *card)
++static inline void gelic_card_rx_irq_on(struct gelic_card *card)
+ {
+-	gelic_net_set_irq_mask(card, card->ghiintmask | GELIC_NET_RXINT);
++	gelic_card_set_irq_mask(card, card->ghiintmask | GELIC_CARD_RXINT);
+ }
+-static inline void gelic_net_rx_irq_off(struct gelic_net_card *card)
++static inline void gelic_card_rx_irq_off(struct gelic_card *card)
+ {
+-	gelic_net_set_irq_mask(card, card->ghiintmask & ~GELIC_NET_RXINT);
++	gelic_card_set_irq_mask(card, card->ghiintmask & ~GELIC_CARD_RXINT);
+ }
+ /**
+- * gelic_net_get_descr_status -- returns the status of a descriptor
++ * gelic_descr_get_status -- returns the status of a descriptor
+  * @descr: descriptor to look at
+  *
+  * returns the status as in the dmac_cmd_status field of the descriptor
+  */
+-static enum gelic_net_descr_status
+-gelic_net_get_descr_status(struct gelic_net_descr *descr)
++static enum gelic_descr_dma_status
++gelic_descr_get_status(struct gelic_descr *descr)
+ {
+-	u32 cmd_status;
+-
+-	cmd_status = be32_to_cpu(descr->dmac_cmd_status);
+-	cmd_status >>= GELIC_NET_DESCR_IND_PROC_SHIFT;
+-	return cmd_status;
++	return be32_to_cpu(descr->dmac_cmd_status) & GELIC_DESCR_DMA_STAT_MASK;
+ }
+ 
+ /**
+- * gelic_net_set_descr_status -- sets the status of a descriptor
++ * gelic_descr_set_status -- sets the status of a descriptor
+  * @descr: descriptor to change
+  * @status: status to set in the descriptor
+  *
+  * changes the status to the specified value. Doesn't change other bits
+  * in the status
+  */
+-static void gelic_net_set_descr_status(struct gelic_net_descr *descr,
+-				       enum gelic_net_descr_status status)
++static void gelic_descr_set_status(struct gelic_descr *descr,
++				   enum gelic_descr_dma_status status)
+ {
+-	u32 cmd_status;
+-
+-	/* read the status */
+-	cmd_status = be32_to_cpu(descr->dmac_cmd_status);
+-	/* clean the upper 4 bits */
+-	cmd_status &= GELIC_NET_DESCR_IND_PROC_MASKO;
+-	/* add the status to it */
+-	cmd_status |= ((u32)status) << GELIC_NET_DESCR_IND_PROC_SHIFT;
+-	/* and write it back */
+-	descr->dmac_cmd_status = cpu_to_be32(cmd_status);
++	descr->dmac_cmd_status = cpu_to_be32(status |
++		(be32_to_cpu(descr->dmac_cmd_status) &
++		 ~GELIC_DESCR_DMA_STAT_MASK));
+ 	/*
+ 	 * dma_cmd_status field is used to indicate whether the descriptor
+ 	 * is valid or not.
+@@ -134,24 +123,24 @@ static void gelic_net_set_descr_status(s
+ }
+ 
+ /**
+- * gelic_net_free_chain - free descriptor chain
++ * gelic_card_free_chain - free descriptor chain
+  * @card: card structure
+  * @descr_in: address of desc
+  */
+-static void gelic_net_free_chain(struct gelic_net_card *card,
+-				 struct gelic_net_descr *descr_in)
++static void gelic_card_free_chain(struct gelic_card *card,
++				  struct gelic_descr *descr_in)
+ {
+-	struct gelic_net_descr *descr;
++	struct gelic_descr *descr;
+ 
+ 	for (descr = descr_in; descr && descr->bus_addr; descr = descr->next) {
+ 		dma_unmap_single(ctodev(card), descr->bus_addr,
+-				 GELIC_NET_DESCR_SIZE, DMA_BIDIRECTIONAL);
++				 GELIC_DESCR_SIZE, DMA_BIDIRECTIONAL);
+ 		descr->bus_addr = 0;
+ 	}
+ }
+ 
+ /**
+- * gelic_net_init_chain - links descriptor chain
++ * gelic_card_init_chain - links descriptor chain
+  * @card: card structure
+  * @chain: address of chain
+  * @start_descr: address of descriptor array
+@@ -162,22 +151,22 @@ static void gelic_net_free_chain(struct 
+  *
+  * returns 0 on success, <0 on failure
+  */
+-static int gelic_net_init_chain(struct gelic_net_card *card,
+-				struct gelic_net_descr_chain *chain,
+-				struct gelic_net_descr *start_descr, int no)
++static int gelic_card_init_chain(struct gelic_card *card,
++				 struct gelic_descr_chain *chain,
++				 struct gelic_descr *start_descr, int no)
+ {
+ 	int i;
+-	struct gelic_net_descr *descr;
++	struct gelic_descr *descr;
+ 
+ 	descr = start_descr;
+ 	memset(descr, 0, sizeof(*descr) * no);
+ 
+ 	/* set up the hardware pointers in each descriptor */
+ 	for (i = 0; i < no; i++, descr++) {
+-		gelic_net_set_descr_status(descr, GELIC_NET_DESCR_NOT_IN_USE);
++		gelic_descr_set_status(descr, GELIC_DESCR_DMA_NOT_IN_USE);
+ 		descr->bus_addr =
+ 			dma_map_single(ctodev(card), descr,
+-				       GELIC_NET_DESCR_SIZE,
++				       GELIC_DESCR_SIZE,
+ 				       DMA_BIDIRECTIONAL);
+ 
+ 		if (!descr->bus_addr)
+@@ -208,13 +197,13 @@ iommu_error:
+ 	for (i--, descr--; 0 <= i; i--, descr--)
+ 		if (descr->bus_addr)
+ 			dma_unmap_single(ctodev(card), descr->bus_addr,
+-					 GELIC_NET_DESCR_SIZE,
++					 GELIC_DESCR_SIZE,
+ 					 DMA_BIDIRECTIONAL);
+ 	return -ENOMEM;
+ }
+ 
+ /**
+- * gelic_net_prepare_rx_descr - reinitializes a rx descriptor
++ * gelic_descr_prepare_rx - reinitializes a rx descriptor
+  * @card: card structure
+  * @descr: descriptor to re-init
+  *
+@@ -223,15 +212,15 @@ iommu_error:
+  * allocates a new rx skb, iommu-maps it and attaches it to the descriptor.
+  * Activate the descriptor state-wise
+  */
+-static int gelic_net_prepare_rx_descr(struct gelic_net_card *card,
+-				      struct gelic_net_descr *descr)
++static int gelic_descr_prepare_rx(struct gelic_card *card,
++				      struct gelic_descr *descr)
+ {
+ 	int offset;
+ 	unsigned int bufsize;
+ 
+-	if (gelic_net_get_descr_status(descr) !=  GELIC_NET_DESCR_NOT_IN_USE) {
++	if (gelic_descr_get_status(descr) !=  GELIC_DESCR_DMA_NOT_IN_USE)
+ 		dev_info(ctodev(card), "%s: ERROR status \n", __func__);
+-	}
++
+ 	/* we need to round up the buffer size to a multiple of 128 */
+ 	bufsize = ALIGN(GELIC_NET_MAX_MTU, GELIC_NET_RXBUF_ALIGN);
+ 
+@@ -265,22 +254,22 @@ static int gelic_net_prepare_rx_descr(st
+ 		descr->skb = NULL;
+ 		dev_info(ctodev(card),
+ 			 "%s:Could not iommu-map rx buffer\n", __func__);
+-		gelic_net_set_descr_status(descr, GELIC_NET_DESCR_NOT_IN_USE);
++		gelic_descr_set_status(descr, GELIC_DESCR_DMA_NOT_IN_USE);
+ 		return -ENOMEM;
+ 	} else {
+-		gelic_net_set_descr_status(descr, GELIC_NET_DESCR_CARDOWNED);
++		gelic_descr_set_status(descr, GELIC_DESCR_DMA_CARDOWNED);
+ 		return 0;
+ 	}
+ }
+ 
+ /**
+- * gelic_net_release_rx_chain - free all skb of rx descr
++ * gelic_card_release_rx_chain - free all skb of rx descr
+  * @card: card structure
+  *
+  */
+-static void gelic_net_release_rx_chain(struct gelic_net_card *card)
++static void gelic_card_release_rx_chain(struct gelic_card *card)
+ {
+-	struct gelic_net_descr *descr = card->rx_chain.head;
++	struct gelic_descr *descr = card->rx_chain.head;
+ 
+ 	do {
+ 		if (descr->skb) {
+@@ -291,29 +280,29 @@ static void gelic_net_release_rx_chain(s
+ 			descr->buf_addr = 0;
+ 			dev_kfree_skb_any(descr->skb);
+ 			descr->skb = NULL;
+-			gelic_net_set_descr_status(descr,
+-						   GELIC_NET_DESCR_NOT_IN_USE);
++			gelic_descr_set_status(descr,
++					       GELIC_DESCR_DMA_NOT_IN_USE);
+ 		}
+ 		descr = descr->next;
+ 	} while (descr != card->rx_chain.head);
+ }
+ 
+ /**
+- * gelic_net_fill_rx_chain - fills descriptors/skbs in the rx chains
++ * gelic_card_fill_rx_chain - fills descriptors/skbs in the rx chains
+  * @card: card structure
+  *
+  * fills all descriptors in the rx chain: allocates skbs
+  * and iommu-maps them.
+- * returns 0 on success, <0 on failure
++ * returns 0 on success, < 0 on failure
+  */
+-static int gelic_net_fill_rx_chain(struct gelic_net_card *card)
++static int gelic_card_fill_rx_chain(struct gelic_card *card)
+ {
+-	struct gelic_net_descr *descr = card->rx_chain.head;
++	struct gelic_descr *descr = card->rx_chain.head;
+ 	int ret;
+ 
+ 	do {
+ 		if (!descr->skb) {
+-			ret = gelic_net_prepare_rx_descr(card, descr);
++			ret = gelic_descr_prepare_rx(card, descr);
+ 			if (ret)
+ 				goto rewind;
+ 		}
+@@ -322,41 +311,42 @@ static int gelic_net_fill_rx_chain(struc
+ 
+ 	return 0;
+ rewind:
+-	gelic_net_release_rx_chain(card);
++	gelic_card_release_rx_chain(card);
+ 	return ret;
+ }
+ 
+ /**
+- * gelic_net_alloc_rx_skbs - allocates rx skbs in rx descriptor chains
++ * gelic_card_alloc_rx_skbs - allocates rx skbs in rx descriptor chains
+  * @card: card structure
+  *
+- * returns 0 on success, <0 on failure
++ * returns 0 on success, < 0 on failure
+  */
+-static int gelic_net_alloc_rx_skbs(struct gelic_net_card *card)
++static int gelic_card_alloc_rx_skbs(struct gelic_card *card)
+ {
+-	struct gelic_net_descr_chain *chain;
++	struct gelic_descr_chain *chain;
+ 	int ret;
+ 	chain = &card->rx_chain;
+-	ret = gelic_net_fill_rx_chain(card);
++	ret = gelic_card_fill_rx_chain(card);
+ 	chain->head = card->rx_top->prev; /* point to the last */
+ 	return ret;
+ }
+ 
+ /**
+- * gelic_net_release_tx_descr - processes a used tx descriptor
++ * gelic_descr_release_tx - processes a used tx descriptor
+  * @card: card structure
+  * @descr: descriptor to release
+  *
+  * releases a used tx descriptor (unmapping, freeing of skb)
+  */
+-static void gelic_net_release_tx_descr(struct gelic_net_card *card,
+-			    struct gelic_net_descr *descr)
++static void gelic_descr_release_tx(struct gelic_card *card,
++			    struct gelic_descr *descr)
+ {
+ 	struct sk_buff *skb = descr->skb;
+ 
++#ifdef DEBUG
+ 	BUG_ON(!(be32_to_cpu(descr->data_status) &
+-		 (1 << GELIC_NET_TXDESC_TAIL)));
+-
++		 (1 << GELIC_DESCR_TX_DMA_FRAME_TAIL)));
++#endif
+ 	dma_unmap_single(ctodev(card),
+ 			 be32_to_cpu(descr->buf_addr), skb->len, DMA_TO_DEVICE);
+ 	dev_kfree_skb_any(skb);
+@@ -371,30 +361,30 @@ static void gelic_net_release_tx_descr(s
+ 	descr->skb = NULL;
+ 
+ 	/* set descr status */
+-	gelic_net_set_descr_status(descr, GELIC_NET_DESCR_NOT_IN_USE);
++	gelic_descr_set_status(descr, GELIC_DESCR_DMA_NOT_IN_USE);
+ }
+ 
+ /**
+- * gelic_net_release_tx_chain - processes sent tx descriptors
++ * gelic_card_release_tx_chain - processes sent tx descriptors
+  * @card: adapter structure
+  * @stop: net_stop sequence
+  *
+  * releases the tx descriptors that gelic has finished with
+  */
+-static void gelic_net_release_tx_chain(struct gelic_net_card *card, int stop)
++static void gelic_card_release_tx_chain(struct gelic_card *card, int stop)
+ {
+-	struct gelic_net_descr_chain *tx_chain;
+-	enum gelic_net_descr_status status;
++	struct gelic_descr_chain *tx_chain;
++	enum gelic_descr_dma_status status;
+ 	int release = 0;
+ 
+ 	for (tx_chain = &card->tx_chain;
+ 	     tx_chain->head != tx_chain->tail && tx_chain->tail;
+ 	     tx_chain->tail = tx_chain->tail->next) {
+-		status = gelic_net_get_descr_status(tx_chain->tail);
++		status = gelic_descr_get_status(tx_chain->tail);
+ 		switch (status) {
+-		case GELIC_NET_DESCR_RESPONSE_ERROR:
+-		case GELIC_NET_DESCR_PROTECTION_ERROR:
+-		case GELIC_NET_DESCR_FORCE_END:
++		case GELIC_DESCR_DMA_RESPONSE_ERROR:
++		case GELIC_DESCR_DMA_PROTECTION_ERROR:
++		case GELIC_DESCR_DMA_FORCE_END:
+ 			if (printk_ratelimit())
+ 				dev_info(ctodev(card),
+ 					 "%s: forcing end of tx descriptor " \
+@@ -403,7 +393,7 @@ static void gelic_net_release_tx_chain(s
+ 			card->netdev->stats.tx_dropped++;
+ 			break;
+ 
+-		case GELIC_NET_DESCR_COMPLETE:
++		case GELIC_DESCR_DMA_COMPLETE:
+ 			if (tx_chain->tail->skb) {
+ 				card->netdev->stats.tx_packets++;
+ 				card->netdev->stats.tx_bytes +=
+@@ -411,14 +401,14 @@ static void gelic_net_release_tx_chain(s
+ 			}
+ 			break;
+ 
+-		case GELIC_NET_DESCR_CARDOWNED:
++		case GELIC_DESCR_DMA_CARDOWNED:
+ 			/* pending tx request */
+ 		default:
+-			/* any other value (== GELIC_NET_DESCR_NOT_IN_USE) */
++			/* any other value (== GELIC_DESCR_DMA_NOT_IN_USE) */
+ 			if (!stop)
+ 				goto out;
+ 		}
+-		gelic_net_release_tx_descr(card, tx_chain->tail);
++		gelic_descr_release_tx(card, tx_chain->tail);
+ 		release ++;
+ 	}
+ out:
+@@ -436,7 +426,7 @@ out:
+  */
+ static void gelic_net_set_multi(struct net_device *netdev)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 	struct dev_mc_list *mc;
+ 	unsigned int i;
+ 	uint8_t *p;
+@@ -489,13 +479,13 @@ static void gelic_net_set_multi(struct n
+ }
+ 
+ /**
+- * gelic_net_enable_rxdmac - enables the receive DMA controller
++ * gelic_card_enable_rxdmac - enables the receive DMA controller
+  * @card: card structure
+  *
+- * gelic_net_enable_rxdmac enables the DMA controller by setting RX_DMA_EN
++ * gelic_card_enable_rxdmac enables the DMA controller by setting RX_DMA_EN
+  * in the GDADMACCNTR register
+  */
+-static inline void gelic_net_enable_rxdmac(struct gelic_net_card *card)
++static inline void gelic_card_enable_rxdmac(struct gelic_card *card)
+ {
+ 	int status;
+ 
+@@ -507,13 +497,13 @@ static inline void gelic_net_enable_rxdm
+ }
+ 
+ /**
+- * gelic_net_disable_rxdmac - disables the receive DMA controller
++ * gelic_card_disable_rxdmac - disables the receive DMA controller
+  * @card: card structure
+  *
+- * gelic_net_disable_rxdmac terminates processing on the DMA controller by
++ * gelic_card_disable_rxdmac terminates processing on the DMA controller by
+  * turing off DMA and issueing a force end
+  */
+-static inline void gelic_net_disable_rxdmac(struct gelic_net_card *card)
++static inline void gelic_card_disable_rxdmac(struct gelic_card *card)
+ {
+ 	int status;
+ 
+@@ -525,13 +515,13 @@ static inline void gelic_net_disable_rxd
+ }
+ 
+ /**
+- * gelic_net_disable_txdmac - disables the transmit DMA controller
++ * gelic_card_disable_txdmac - disables the transmit DMA controller
+  * @card: card structure
+  *
+- * gelic_net_disable_txdmac terminates processing on the DMA controller by
++ * gelic_card_disable_txdmac terminates processing on the DMA controller by
+  * turing off DMA and issueing a force end
+  */
+-static inline void gelic_net_disable_txdmac(struct gelic_net_card *card)
++static inline void gelic_card_disable_txdmac(struct gelic_card *card)
+ {
+ 	int status;
+ 
+@@ -550,16 +540,16 @@ static inline void gelic_net_disable_txd
+  */
+ static int gelic_net_stop(struct net_device *netdev)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 
+ 	napi_disable(&card->napi);
+ 	netif_stop_queue(netdev);
+ 
+ 	/* turn off DMA, force end */
+-	gelic_net_disable_rxdmac(card);
+-	gelic_net_disable_txdmac(card);
++	gelic_card_disable_rxdmac(card);
++	gelic_card_disable_txdmac(card);
+ 
+-	gelic_net_set_irq_mask(card, 0);
++	gelic_card_set_irq_mask(card, 0);
+ 
+ 	/* disconnect event port */
+ 	free_irq(card->netdev->irq, card->netdev);
+@@ -569,30 +559,30 @@ static int gelic_net_stop(struct net_dev
+ 	netif_carrier_off(netdev);
+ 
+ 	/* release chains */
+-	gelic_net_release_tx_chain(card, 1);
+-	gelic_net_release_rx_chain(card);
++	gelic_card_release_tx_chain(card, 1);
++	gelic_card_release_rx_chain(card);
+ 
+-	gelic_net_free_chain(card, card->tx_top);
+-	gelic_net_free_chain(card, card->rx_top);
++	gelic_card_free_chain(card, card->tx_top);
++	gelic_card_free_chain(card, card->rx_top);
+ 
+ 	return 0;
+ }
+ 
+ /**
+- * gelic_net_get_next_tx_descr - returns the next available tx descriptor
++ * gelic_card_get_next_tx_descr - returns the next available tx descriptor
+  * @card: device structure to get descriptor from
+  *
+  * returns the address of the next descriptor, or NULL if not available.
+  */
+-static struct gelic_net_descr *
+-gelic_net_get_next_tx_descr(struct gelic_net_card *card)
++static struct gelic_descr *
++gelic_card_get_next_tx_descr(struct gelic_card *card)
+ {
+ 	if (!card->tx_chain.head)
+ 		return NULL;
+ 	/*  see if the next descriptor is free */
+ 	if (card->tx_chain.tail != card->tx_chain.head->next &&
+-	    gelic_net_get_descr_status(card->tx_chain.head) ==
+-	    GELIC_NET_DESCR_NOT_IN_USE)
++	    gelic_descr_get_status(card->tx_chain.head) ==
++	    GELIC_DESCR_DMA_NOT_IN_USE)
+ 		return card->tx_chain.head;
+ 	else
+ 		return NULL;
+@@ -600,7 +590,7 @@ gelic_net_get_next_tx_descr(struct gelic
+ }
+ 
+ /**
+- * gelic_net_set_txdescr_cmdstat - sets the tx descriptor command field
++ * gelic_descr_set_tx_cmdstat - sets the tx descriptor command field
+  * @descr: descriptor structure to fill out
+  * @skb: packet to consider
+  *
+@@ -608,33 +598,33 @@ gelic_net_get_next_tx_descr(struct gelic
+  * depending on hardware checksum settings. This function assumes a wmb()
+  * has executed before.
+  */
+-static void gelic_net_set_txdescr_cmdstat(struct gelic_net_descr *descr,
+-					  struct sk_buff *skb)
++static void gelic_descr_set_tx_cmdstat(struct gelic_descr *descr,
++				       struct sk_buff *skb)
+ {
+ 	if (skb->ip_summed != CHECKSUM_PARTIAL)
+ 		descr->dmac_cmd_status =
+-			cpu_to_be32(GELIC_NET_DMAC_CMDSTAT_NOCS |
+-				    GELIC_NET_DMAC_CMDSTAT_END_FRAME);
++			cpu_to_be32(GELIC_DESCR_DMA_CMD_NO_CHKSUM |
++				    GELIC_DESCR_TX_DMA_FRAME_TAIL);
+ 	else {
+ 		/* is packet ip?
+ 		 * if yes: tcp? udp? */
+ 		if (skb->protocol == htons(ETH_P_IP)) {
+ 			if (ip_hdr(skb)->protocol == IPPROTO_TCP)
+ 				descr->dmac_cmd_status =
+-				cpu_to_be32(GELIC_NET_DMAC_CMDSTAT_TCPCS |
+-					    GELIC_NET_DMAC_CMDSTAT_END_FRAME);
++				cpu_to_be32(GELIC_DESCR_DMA_CMD_TCP_CHKSUM |
++					    GELIC_DESCR_TX_DMA_FRAME_TAIL);
+ 
+ 			else if (ip_hdr(skb)->protocol == IPPROTO_UDP)
+ 				descr->dmac_cmd_status =
+-				cpu_to_be32(GELIC_NET_DMAC_CMDSTAT_UDPCS |
+-					    GELIC_NET_DMAC_CMDSTAT_END_FRAME);
++				cpu_to_be32(GELIC_DESCR_DMA_CMD_UDP_CHKSUM |
++					    GELIC_DESCR_TX_DMA_FRAME_TAIL);
+ 			else	/*
+ 				 * the stack should checksum non-tcp and non-udp
+ 				 * packets on his own: NETIF_F_IP_CSUM
+ 				 */
+ 				descr->dmac_cmd_status =
+-				cpu_to_be32(GELIC_NET_DMAC_CMDSTAT_NOCS |
+-					    GELIC_NET_DMAC_CMDSTAT_END_FRAME);
++				cpu_to_be32(GELIC_DESCR_DMA_CMD_NO_CHKSUM |
++					    GELIC_DESCR_TX_DMA_FRAME_TAIL);
+ 		}
+ 	}
+ }
+@@ -665,7 +655,7 @@ static inline struct sk_buff *gelic_put_
+ }
+ 
+ /**
+- * gelic_net_prepare_tx_descr_v - get dma address of skb_data
++ * gelic_descr_prepare_tx - get dma address of skb_data
+  * @card: card structure
+  * @descr: descriptor structure
+  * @skb: packet to use
+@@ -673,9 +663,9 @@ static inline struct sk_buff *gelic_put_
+  * returns 0 on success, <0 on failure.
+  *
+  */
+-static int gelic_net_prepare_tx_descr_v(struct gelic_net_card *card,
+-					struct gelic_net_descr *descr,
+-					struct sk_buff *skb)
++static int gelic_descr_prepare_tx(struct gelic_card *card,
++				  struct gelic_descr *descr,
++				  struct sk_buff *skb)
+ {
+ 	dma_addr_t buf;
+ 
+@@ -702,7 +692,7 @@ static int gelic_net_prepare_tx_descr_v(
+ 	descr->skb = skb;
+ 	descr->data_status = 0;
+ 	descr->next_descr_addr = 0; /* terminate hw descr */
+-	gelic_net_set_txdescr_cmdstat(descr, skb);
++	gelic_descr_set_tx_cmdstat(descr, skb);
+ 
+ 	/* bump free descriptor pointer */
+ 	card->tx_chain.head = descr->next;
+@@ -710,20 +700,20 @@ static int gelic_net_prepare_tx_descr_v(
+ }
+ 
+ /**
+- * gelic_net_kick_txdma - enables TX DMA processing
++ * gelic_card_kick_txdma - enables TX DMA processing
+  * @card: card structure
+  * @descr: descriptor address to enable TX processing at
+  *
+  */
+-static int gelic_net_kick_txdma(struct gelic_net_card *card,
+-				struct gelic_net_descr *descr)
++static int gelic_card_kick_txdma(struct gelic_card *card,
++				 struct gelic_descr *descr)
+ {
+ 	int status = 0;
+ 
+ 	if (card->tx_dma_progress)
+ 		return 0;
+ 
+-	if (gelic_net_get_descr_status(descr) == GELIC_NET_DESCR_CARDOWNED) {
++	if (gelic_descr_get_status(descr) == GELIC_DESCR_DMA_CARDOWNED) {
+ 		card->tx_dma_progress = 1;
+ 		status = lv1_net_start_tx_dma(bus_id(card), dev_id(card),
+ 					      descr->bus_addr, 0);
+@@ -743,16 +733,16 @@ static int gelic_net_kick_txdma(struct g
+  */
+ static int gelic_net_xmit(struct sk_buff *skb, struct net_device *netdev)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
+-	struct gelic_net_descr *descr;
++	struct gelic_card *card = netdev_priv(netdev);
++	struct gelic_descr *descr;
+ 	int result;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&card->tx_dma_lock, flags);
+ 
+-	gelic_net_release_tx_chain(card, 0);
++	gelic_card_release_tx_chain(card, 0);
+ 
+-	descr = gelic_net_get_next_tx_descr(card);
++	descr = gelic_card_get_next_tx_descr(card);
+ 	if (!descr) {
+ 		/*
+ 		 * no more descriptors free
+@@ -762,7 +752,7 @@ static int gelic_net_xmit(struct sk_buff
+ 		return NETDEV_TX_BUSY;
+ 	}
+ 
+-	result = gelic_net_prepare_tx_descr_v(card, descr, skb);
++	result = gelic_descr_prepare_tx(card, descr, skb);
+ 	if (result) {
+ 		/*
+ 		 * DMA map failed.  As chanses are that failure
+@@ -783,14 +773,14 @@ static int gelic_net_xmit(struct sk_buff
+ 	 * ensure that the hardware sees it
+ 	 */
+ 	wmb();
+-	if (gelic_net_kick_txdma(card, descr)) {
++	if (gelic_card_kick_txdma(card, descr)) {
+ 		/*
+ 		 * kick failed.
+ 		 * release descriptors which were just prepared
+ 		 */
+ 		card->netdev->stats.tx_dropped++;
+-		gelic_net_release_tx_descr(card, descr);
+-		gelic_net_release_tx_descr(card, descr->next);
++		gelic_descr_release_tx(card, descr);
++		gelic_descr_release_tx(card, descr->next);
+ 		card->tx_chain.tail = descr->next->next;
+ 		dev_info(ctodev(card), "%s: kick failure\n", __func__);
+ 	} else {
+@@ -810,8 +800,8 @@ static int gelic_net_xmit(struct sk_buff
+  * iommu-unmaps the skb, fills out skb structure and passes the data to the
+  * stack. The descriptor state is not changed.
+  */
+-static void gelic_net_pass_skb_up(struct gelic_net_descr *descr,
+-				 struct gelic_net_card *card)
++static void gelic_net_pass_skb_up(struct gelic_descr *descr,
++				 struct gelic_card *card)
+ {
+ 	struct sk_buff *skb;
+ 	struct net_device *netdev;
+@@ -845,8 +835,8 @@ static void gelic_net_pass_skb_up(struct
+ 
+ 	/* checksum offload */
+ 	if (card->rx_csum) {
+-		if ((data_status & GELIC_NET_DATA_STATUS_CHK_MASK) &&
+-		    (!(data_error & GELIC_NET_DATA_ERROR_CHK_MASK)))
++		if ((data_status & GELIC_DESCR_DATA_STATUS_CHK_MASK) &&
++		    (!(data_error & GELIC_DESCR_DATA_ERROR_CHK_MASK)))
+ 			skb->ip_summed = CHECKSUM_UNNECESSARY;
+ 		else
+ 			skb->ip_summed = CHECKSUM_NONE;
+@@ -862,7 +852,7 @@ static void gelic_net_pass_skb_up(struct
+ }
+ 
+ /**
+- * gelic_net_decode_one_descr - processes an rx descriptor
++ * gelic_card_decode_one_descr - processes an rx descriptor
+  * @card: card structure
+  *
+  * returns 1 if a packet has been sent to the stack, otherwise 0
+@@ -870,37 +860,37 @@ static void gelic_net_pass_skb_up(struct
+  * processes an rx descriptor by iommu-unmapping the data buffer and passing
+  * the packet up to the stack
+  */
+-static int gelic_net_decode_one_descr(struct gelic_net_card *card)
++static int gelic_card_decode_one_descr(struct gelic_card *card)
+ {
+-	enum gelic_net_descr_status status;
+-	struct gelic_net_descr_chain *chain = &card->rx_chain;
+-	struct gelic_net_descr *descr = chain->tail;
++	enum gelic_descr_dma_status status;
++	struct gelic_descr_chain *chain = &card->rx_chain;
++	struct gelic_descr *descr = chain->tail;
+ 	int dmac_chain_ended;
+ 
+-	status = gelic_net_get_descr_status(descr);
++	status = gelic_descr_get_status(descr);
+ 	/* is this descriptor terminated with next_descr == NULL? */
+ 	dmac_chain_ended =
+ 		be32_to_cpu(descr->dmac_cmd_status) &
+-		GELIC_NET_DMAC_CMDSTAT_RXDCEIS;
++		GELIC_DESCR_RX_DMA_CHAIN_END;
+ 
+-	if (status == GELIC_NET_DESCR_CARDOWNED)
++	if (status == GELIC_DESCR_DMA_CARDOWNED)
+ 		return 0;
+ 
+-	if (status == GELIC_NET_DESCR_NOT_IN_USE) {
++	if (status == GELIC_DESCR_DMA_NOT_IN_USE) {
+ 		dev_dbg(ctodev(card), "dormant descr? %p\n", descr);
+ 		return 0;
+ 	}
+ 
+-	if ((status == GELIC_NET_DESCR_RESPONSE_ERROR) ||
+-	    (status == GELIC_NET_DESCR_PROTECTION_ERROR) ||
+-	    (status == GELIC_NET_DESCR_FORCE_END)) {
++	if ((status == GELIC_DESCR_DMA_RESPONSE_ERROR) ||
++	    (status == GELIC_DESCR_DMA_PROTECTION_ERROR) ||
++	    (status == GELIC_DESCR_DMA_FORCE_END)) {
+ 		dev_info(ctodev(card), "dropping RX descriptor with state %x\n",
+ 			 status);
+ 		card->netdev->stats.rx_dropped++;
+ 		goto refill;
+ 	}
+ 
+-	if (status == GELIC_NET_DESCR_BUFFER_FULL) {
++	if (status == GELIC_DESCR_DMA_BUFFER_FULL) {
+ 		/*
+ 		 * Buffer full would occur if and only if
+ 		 * the frame length was longer than the size of this
+@@ -917,7 +907,7 @@ static int gelic_net_decode_one_descr(st
+ 	 * descriptoers any other than FRAME_END here should
+ 	 * be treated as error.
+ 	 */
+-	if (status != GELIC_NET_DESCR_FRAME_END) {
++	if (status != GELIC_DESCR_DMA_FRAME_END) {
+ 		dev_dbg(ctodev(card), "RX descriptor with state %x\n",
+ 			status);
+ 		goto refill;
+@@ -934,13 +924,13 @@ refill:
+ 	descr->next_descr_addr = 0;
+ 
+ 	/* change the descriptor state: */
+-	gelic_net_set_descr_status(descr, GELIC_NET_DESCR_NOT_IN_USE);
++	gelic_descr_set_status(descr, GELIC_DESCR_DMA_NOT_IN_USE);
+ 
+ 	/*
+ 	 * this call can fail, but for now, just leave this
+ 	 * decriptor without skb
+ 	 */
+-	gelic_net_prepare_rx_descr(card, descr);
++	gelic_descr_prepare_rx(card, descr);
+ 
+ 	chain->head = descr;
+ 	chain->tail = descr->next;
+@@ -973,12 +963,12 @@ refill:
+  */
+ static int gelic_net_poll(struct napi_struct *napi, int budget)
+ {
+-	struct gelic_net_card *card = container_of(napi, struct gelic_net_card, napi);
++	struct gelic_card *card = container_of(napi, struct gelic_card, napi);
+ 	struct net_device *netdev = card->netdev;
+ 	int packets_done = 0;
+ 
+ 	while (packets_done < budget) {
+-		if (!gelic_net_decode_one_descr(card))
++		if (!gelic_card_decode_one_descr(card))
+ 			break;
+ 
+ 		packets_done++;
+@@ -986,7 +976,7 @@ static int gelic_net_poll(struct napi_st
+ 
+ 	if (packets_done < budget) {
+ 		netif_rx_complete(netdev, napi);
+-		gelic_net_rx_irq_on(card);
++		gelic_card_rx_irq_on(card);
+ 	}
+ 	return packets_done;
+ }
+@@ -1010,13 +1000,13 @@ static int gelic_net_change_mtu(struct n
+ }
+ 
+ /**
+- * gelic_net_interrupt - event handler for gelic_net
++ * gelic_card_interrupt - event handler for gelic_net
+  */
+-static irqreturn_t gelic_net_interrupt(int irq, void *ptr)
++static irqreturn_t gelic_card_interrupt(int irq, void *ptr)
+ {
+ 	unsigned long flags;
+ 	struct net_device *netdev = ptr;
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 	u64 status;
+ 
+ 	status = card->irq_status;
+@@ -1026,20 +1016,20 @@ static irqreturn_t gelic_net_interrupt(i
+ 
+ 	if (card->rx_dma_restart_required) {
+ 		card->rx_dma_restart_required = 0;
+-		gelic_net_enable_rxdmac(card);
++		gelic_card_enable_rxdmac(card);
+ 	}
+ 
+-	if (status & GELIC_NET_RXINT) {
+-		gelic_net_rx_irq_off(card);
++	if (status & GELIC_CARD_RXINT) {
++		gelic_card_rx_irq_off(card);
+ 		netif_rx_schedule(netdev, &card->napi);
+ 	}
+ 
+-	if (status & GELIC_NET_TXINT) {
++	if (status & GELIC_CARD_TXINT) {
+ 		spin_lock_irqsave(&card->tx_dma_lock, flags);
+ 		card->tx_dma_progress = 0;
+-		gelic_net_release_tx_chain(card, 0);
++		gelic_card_release_tx_chain(card, 0);
+ 		/* kick outstanding tx descriptor if any */
+-		gelic_net_kick_txdma(card, card->tx_chain.tail);
++		gelic_card_kick_txdma(card, card->tx_chain.tail);
+ 		spin_unlock_irqrestore(&card->tx_dma_lock, flags);
+ 	}
+ 	return IRQ_HANDLED;
+@@ -1054,19 +1044,19 @@ static irqreturn_t gelic_net_interrupt(i
+  */
+ static void gelic_net_poll_controller(struct net_device *netdev)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 
+-	gelic_net_set_irq_mask(card, 0);
+-	gelic_net_interrupt(netdev->irq, netdev);
+-	gelic_net_set_irq_mask(card, card->ghiintmask);
++	gelic_card_set_irq_mask(card, 0);
++	gelic_card_interrupt(netdev->irq, netdev);
++	gelic_card_set_irq_mask(card, card->ghiintmask);
+ }
+ #endif /* CONFIG_NET_POLL_CONTROLLER */
+ 
+ /**
+- * gelic_net_open_device - open device and map dma region
++ * gelic_card_open - open device and map dma region
+  * @card: card structure
+  */
+-static int gelic_net_open_device(struct gelic_net_card *card)
++static int gelic_card_open(struct gelic_card *card)
+ {
+ 	int result;
+ 
+@@ -1075,13 +1065,13 @@ static int gelic_net_open_device(struct 
+ 
+ 	if (result) {
+ 		dev_info(ctodev(card),
+-			 "%s:%d: gelic_net_open_device failed (%d)\n",
++			 "%s:%d: recieve_port_setup failed (%d)\n",
+ 			 __func__, __LINE__, result);
+ 		result = -EPERM;
+ 		goto fail_alloc_irq;
+ 	}
+ 
+-	result = request_irq(card->netdev->irq, gelic_net_interrupt,
++	result = request_irq(card->netdev->irq, gelic_card_interrupt,
+ 			     IRQF_DISABLED, card->netdev->name, card->netdev);
+ 
+ 	if (result) {
+@@ -1111,37 +1101,37 @@ fail_alloc_irq:
+  */
+ static int gelic_net_open(struct net_device *netdev)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 
+ 	dev_dbg(ctodev(card), " -> %s:%d\n", __func__, __LINE__);
+ 
+-	gelic_net_open_device(card);
++	gelic_card_open(card);
+ 
+-	if (gelic_net_init_chain(card, &card->tx_chain,
+-			card->descr, GELIC_NET_TX_DESCRIPTORS))
++	if (gelic_card_init_chain(card, &card->tx_chain,
++				  card->descr, GELIC_NET_TX_DESCRIPTORS))
+ 		goto alloc_tx_failed;
+-	if (gelic_net_init_chain(card, &card->rx_chain,
+-				 card->descr + GELIC_NET_TX_DESCRIPTORS,
+-				 GELIC_NET_RX_DESCRIPTORS))
++	if (gelic_card_init_chain(card, &card->rx_chain,
++				  card->descr + GELIC_NET_TX_DESCRIPTORS,
++				  GELIC_NET_RX_DESCRIPTORS))
+ 		goto alloc_rx_failed;
+ 
+ 	/* head of chain */
+ 	card->tx_top = card->tx_chain.head;
+ 	card->rx_top = card->rx_chain.head;
+ 	dev_dbg(ctodev(card), "descr rx %p, tx %p, size %#lx, num %#x\n",
+-		card->rx_top, card->tx_top, sizeof(struct gelic_net_descr),
++		card->rx_top, card->tx_top, sizeof(struct gelic_descr),
+ 		GELIC_NET_RX_DESCRIPTORS);
+ 	/* allocate rx skbs */
+-	if (gelic_net_alloc_rx_skbs(card))
++	if (gelic_card_alloc_rx_skbs(card))
+ 		goto alloc_skbs_failed;
+ 
+ 	napi_enable(&card->napi);
+ 
+ 	card->tx_dma_progress = 0;
+-	card->ghiintmask = GELIC_NET_RXINT | GELIC_NET_TXINT;
++	card->ghiintmask = GELIC_CARD_RXINT | GELIC_CARD_TXINT;
+ 
+-	gelic_net_set_irq_mask(card, card->ghiintmask);
+-	gelic_net_enable_rxdmac(card);
++	gelic_card_set_irq_mask(card, card->ghiintmask);
++	gelic_card_enable_rxdmac(card);
+ 
+ 	netif_start_queue(netdev);
+ 	netif_carrier_on(netdev);
+@@ -1149,46 +1139,47 @@ static int gelic_net_open(struct net_dev
+ 	return 0;
+ 
+ alloc_skbs_failed:
+-	gelic_net_free_chain(card, card->rx_top);
++	gelic_card_free_chain(card, card->rx_top);
+ alloc_rx_failed:
+-	gelic_net_free_chain(card, card->tx_top);
++	gelic_card_free_chain(card, card->tx_top);
+ alloc_tx_failed:
+ 	return -ENOMEM;
+ }
+ 
+-static void gelic_net_get_drvinfo (struct net_device *netdev,
+-				   struct ethtool_drvinfo *info)
++static void gelic_net_get_drvinfo(struct net_device *netdev,
++				  struct ethtool_drvinfo *info)
+ {
+ 	strncpy(info->driver, DRV_NAME, sizeof(info->driver) - 1);
+ 	strncpy(info->version, DRV_VERSION, sizeof(info->version) - 1);
+ }
+ 
+-static int gelic_net_get_settings(struct net_device *netdev,
+-				  struct ethtool_cmd *cmd)
++static int gelic_ether_get_settings(struct net_device *netdev,
++				    struct ethtool_cmd *cmd)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 	int status;
+ 	u64 v1, v2;
+ 	int speed, duplex;
+ 
+ 	speed = duplex = -1;
+ 	status = lv1_net_control(bus_id(card), dev_id(card),
+-			GELIC_NET_GET_ETH_PORT_STATUS, GELIC_NET_PORT, 0, 0,
+-			&v1, &v2);
++				 GELIC_LV1_GET_ETH_PORT_STATUS,
++				 GELIC_LV1_VLAN_TX_ETHERNET, 0, 0,
++				 &v1, &v2);
+ 	if (status) {
+ 		/* link down */
+ 	} else {
+-		if (v1 & GELIC_NET_FULL_DUPLEX) {
++		if (v1 & GELIC_LV1_ETHER_FULL_DUPLEX) {
+ 			duplex = DUPLEX_FULL;
+ 		} else {
+ 			duplex = DUPLEX_HALF;
+ 		}
+ 
+-		if (v1 & GELIC_NET_SPEED_10 ) {
++		if (v1 & GELIC_LV1_ETHER_SPEED_10) {
+ 			speed = SPEED_10;
+-		} else if (v1 & GELIC_NET_SPEED_100) {
++		} else if (v1 & GELIC_LV1_ETHER_SPEED_100) {
+ 			speed = SPEED_100;
+-		} else if (v1 & GELIC_NET_SPEED_1000) {
++		} else if (v1 & GELIC_LV1_ETHER_SPEED_1000) {
+ 			speed = SPEED_1000;
+ 		}
+ 	}
+@@ -1205,20 +1196,21 @@ static int gelic_net_get_settings(struct
+ 	return 0;
+ }
+ 
+-static u32 gelic_net_get_link(struct net_device *netdev)
++static u32 gelic_ether_get_link(struct net_device *netdev)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 	int status;
+ 	u64 v1, v2;
+ 	int link;
+ 
+ 	status = lv1_net_control(bus_id(card), dev_id(card),
+-			GELIC_NET_GET_ETH_PORT_STATUS, GELIC_NET_PORT, 0, 0,
+-			&v1, &v2);
++				 GELIC_LV1_GET_ETH_PORT_STATUS,
++				 GELIC_LV1_VLAN_TX_ETHERNET, 0, 0,
++				 &v1, &v2);
+ 	if (status)
+ 		return 0; /* link down */
+ 
+-	if (v1 & GELIC_NET_LINK_UP)
++	if (v1 & GELIC_LV1_ETHER_LINK_UP)
+ 		link = 1;
+ 	else
+ 		link = 0;
+@@ -1252,14 +1244,14 @@ static int gelic_net_set_tx_csum(struct 
+ 
+ static u32 gelic_net_get_rx_csum(struct net_device *netdev)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 
+ 	return card->rx_csum;
+ }
+ 
+ static int gelic_net_set_rx_csum(struct net_device *netdev, u32 data)
+ {
+-	struct gelic_net_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_priv(netdev);
+ 
+ 	card->rx_csum = data;
+ 	return 0;
+@@ -1267,8 +1259,8 @@ static int gelic_net_set_rx_csum(struct 
+ 
+ static struct ethtool_ops gelic_net_ethtool_ops = {
+ 	.get_drvinfo	= gelic_net_get_drvinfo,
+-	.get_settings	= gelic_net_get_settings,
+-	.get_link	= gelic_net_get_link,
++	.get_settings	= gelic_ether_get_settings,
++	.get_link	= gelic_ether_get_link,
+ 	.nway_reset	= gelic_net_nway_reset,
+ 	.get_tx_csum	= gelic_net_get_tx_csum,
+ 	.set_tx_csum	= gelic_net_set_tx_csum,
+@@ -1285,8 +1277,8 @@ static struct ethtool_ops gelic_net_etht
+  */
+ static void gelic_net_tx_timeout_task(struct work_struct *work)
+ {
+-	struct gelic_net_card *card =
+-		container_of(work, struct gelic_net_card, tx_timeout_task);
++	struct gelic_card *card =
++		container_of(work, struct gelic_card, tx_timeout_task);
+ 	struct net_device *netdev = card->netdev;
+ 
+ 	dev_info(ctodev(card), "%s:Timed out. Restarting... \n", __func__);
+@@ -1312,7 +1304,7 @@ out:
+  */
+ static void gelic_net_tx_timeout(struct net_device *netdev)
+ {
+-	struct gelic_net_card *card;
++	struct gelic_card *card;
+ 
+ 	card = netdev_priv(netdev);
+ 	atomic_inc(&card->tx_timeout_task_counter);
+@@ -1323,12 +1315,12 @@ static void gelic_net_tx_timeout(struct 
+ }
+ 
+ /**
+- * gelic_net_setup_netdev_ops - initialization of net_device operations
++ * gelic_ether_setup_netdev_ops - initialization of net_device operations
+  * @netdev: net_device structure
+  *
+  * fills out function pointers in the net_device structure
+  */
+-static void gelic_net_setup_netdev_ops(struct net_device *netdev)
++static void gelic_ether_setup_netdev_ops(struct net_device *netdev)
+ {
+ 	netdev->open = &gelic_net_open;
+ 	netdev->stop = &gelic_net_stop;
+@@ -1349,7 +1341,7 @@ static void gelic_net_setup_netdev_ops(s
+  *
+  * gelic_net_setup_netdev initializes the net_device structure
+  **/
+-static int gelic_net_setup_netdev(struct gelic_net_card *card)
++static int gelic_net_setup_netdev(struct gelic_card *card)
+ {
+ 	struct net_device *netdev = card->netdev;
+ 	struct sockaddr addr;
+@@ -1363,7 +1355,7 @@ static int gelic_net_setup_netdev(struct
+ 
+ 	card->rx_csum = GELIC_NET_RX_CSUM_DEFAULT;
+ 
+-	gelic_net_setup_netdev_ops(netdev);
++	gelic_ether_setup_netdev_ops(netdev);
+ 
+ 	netif_napi_add(netdev, &card->napi,
+ 		       gelic_net_poll, GELIC_NET_NAPI_WEIGHT);
+@@ -1371,7 +1363,7 @@ static int gelic_net_setup_netdev(struct
+ 	netdev->features = NETIF_F_IP_CSUM;
+ 
+ 	status = lv1_net_control(bus_id(card), dev_id(card),
+-				 GELIC_NET_GET_MAC_ADDRESS,
++				 GELIC_LV1_GET_MAC_ADDRESS,
+ 				 0, 0, 0, &v1, &v2);
+ 	if (status || !is_valid_ether_addr((u8 *)&v1)) {
+ 		dev_info(ctodev(card),
+@@ -1388,17 +1380,17 @@ static int gelic_net_setup_netdev(struct
+ 	card->vlan_index = -1;	/* no vlan */
+ 	for (i = 0; i < GELIC_NET_VLAN_MAX; i++) {
+ 		status = lv1_net_control(bus_id(card), dev_id(card),
+-					GELIC_NET_GET_VLAN_ID,
++					GELIC_LV1_GET_VLAN_ID,
+ 					i + 1, /* index; one based */
+ 					0, 0, &v1, &v2);
+-		if (status == GELIC_NET_VLAN_NO_ENTRY) {
++		if (status == LV1_NO_ENTRY) {
+ 			dev_dbg(ctodev(card),
+ 				"GELIC_VLAN_ID no entry:%d, VLAN disabled\n",
+ 				status);
+ 			card->vlan_id[i] = 0;
+ 		} else if (status) {
+ 			dev_dbg(ctodev(card),
+-				"%s:GELIC_NET_VLAN_ID faild, status=%d\n",
++				"%s:get vlan id faild, status=%d\n",
+ 				__func__, status);
+ 			card->vlan_id[i] = 0;
+ 		} else {
+@@ -1407,8 +1399,8 @@ static int gelic_net_setup_netdev(struct
+ 		}
+ 	}
+ 
+-	if (card->vlan_id[GELIC_NET_VLAN_WIRED - 1]) {
+-		card->vlan_index = GELIC_NET_VLAN_WIRED - 1;
++	if (card->vlan_id[GELIC_LV1_VLAN_TX_ETHERNET - 1]) {
++		card->vlan_index = GELIC_LV1_VLAN_TX_ETHERNET - 1;
+ 		netdev->hard_header_len += VLAN_HLEN;
+ 	}
+ 
+@@ -1423,31 +1415,31 @@ static int gelic_net_setup_netdev(struct
+ }
+ 
+ /**
+- * gelic_net_alloc_card - allocates net_device and card structure
++ * gelic_alloc_card_net - allocates net_device and card structure
+  *
+  * returns the card structure or NULL in case of errors
+  *
+  * the card and net_device structures are linked to each other
+  */
+-static struct gelic_net_card *gelic_net_alloc_card(void)
++static struct gelic_card *gelic_alloc_card_net(void)
+ {
+ 	struct net_device *netdev;
+-	struct gelic_net_card *card;
++	struct gelic_card *card;
+ 	size_t alloc_size;
+ 
+-	alloc_size = sizeof (*card) +
+-		sizeof (struct gelic_net_descr) * GELIC_NET_RX_DESCRIPTORS +
+-		sizeof (struct gelic_net_descr) * GELIC_NET_TX_DESCRIPTORS;
++	alloc_size = sizeof(*card) +
++		sizeof(struct gelic_descr) * GELIC_NET_RX_DESCRIPTORS +
++		sizeof(struct gelic_descr) * GELIC_NET_TX_DESCRIPTORS;
+ 	/*
+ 	 * we assume private data is allocated 32 bytes (or more) aligned
+-	 * so that gelic_net_descr should be 32 bytes aligned.
++	 * so that gelic_descr should be 32 bytes aligned.
+ 	 * Current alloc_etherdev() does do it because NETDEV_ALIGN
+ 	 * is 32.
+ 	 * check this assumption here.
+ 	 */
+ 	BUILD_BUG_ON(NETDEV_ALIGN < 32);
+-	BUILD_BUG_ON(offsetof(struct gelic_net_card, irq_status) % 8);
+-	BUILD_BUG_ON(offsetof(struct gelic_net_card, descr) % 32);
++	BUILD_BUG_ON(offsetof(struct gelic_card, irq_status) % 8);
++	BUILD_BUG_ON(offsetof(struct gelic_card, descr) % 32);
+ 
+ 	netdev = alloc_etherdev(alloc_size);
+ 	if (!netdev)
+@@ -1465,9 +1457,9 @@ static struct gelic_net_card *gelic_net_
+ /**
+  * ps3_gelic_driver_probe - add a device to the control of this driver
+  */
+-static int ps3_gelic_driver_probe (struct ps3_system_bus_device *dev)
++static int ps3_gelic_driver_probe(struct ps3_system_bus_device *dev)
+ {
+-	struct gelic_net_card *card = gelic_net_alloc_card();
++	struct gelic_card *card = gelic_alloc_card_net();
+ 	int result;
+ 
+ 	if (!card) {
+@@ -1537,9 +1529,9 @@ fail_alloc_card:
+  * ps3_gelic_driver_remove - remove a device from the control of this driver
+  */
+ 
+-static int ps3_gelic_driver_remove (struct ps3_system_bus_device *dev)
++static int ps3_gelic_driver_remove(struct ps3_system_bus_device *dev)
+ {
+-	struct gelic_net_card *card = ps3_system_bus_get_driver_data(dev);
++	struct gelic_card *card = ps3_system_bus_get_driver_data(dev);
+ 
+ 	wait_event(card->waitq,
+ 		   atomic_read(&card->tx_timeout_task_counter) == 0);
+@@ -1580,8 +1572,8 @@ static void __exit ps3_gelic_driver_exit
+ 	ps3_system_bus_driver_unregister(&ps3_gelic_driver);
+ }
+ 
+-module_init (ps3_gelic_driver_init);
+-module_exit (ps3_gelic_driver_exit);
++module_init(ps3_gelic_driver_init);
++module_exit(ps3_gelic_driver_exit);
+ 
+ MODULE_ALIAS(PS3_MODULE_ALIAS_GELIC);
+ 
+--- a/drivers/net/ps3_gelic_net.h
++++ b/drivers/net/ps3_gelic_net.h
+@@ -43,131 +43,170 @@
+ #define GELIC_NET_VLAN_MAX              4
+ #define GELIC_NET_MC_COUNT_MAX          32 /* multicast address list */
+ 
+-enum gelic_net_int0_status {
+-	GELIC_NET_GDTDCEINT  = 24,
+-	GELIC_NET_GRFANMINT  = 28,
+-};
++/* virtual interrupt status register bits */
++	/* INT1 */
++#define GELIC_CARD_TX_RAM_FULL_ERR           0x0000000000000001L
++#define GELIC_CARD_RX_RAM_FULL_ERR           0x0000000000000002L
++#define GELIC_CARD_TX_SHORT_FRAME_ERR        0x0000000000000004L
++#define GELIC_CARD_TX_INVALID_DESCR_ERR      0x0000000000000008L
++#define GELIC_CARD_RX_FIFO_FULL_ERR          0x0000000000002000L
++#define GELIC_CARD_RX_DESCR_CHAIN_END        0x0000000000004000L
++#define GELIC_CARD_RX_INVALID_DESCR_ERR      0x0000000000008000L
++#define GELIC_CARD_TX_RESPONCE_ERR           0x0000000000010000L
++#define GELIC_CARD_RX_RESPONCE_ERR           0x0000000000100000L
++#define GELIC_CARD_TX_PROTECTION_ERR         0x0000000000400000L
++#define GELIC_CARD_RX_PROTECTION_ERR         0x0000000004000000L
++#define GELIC_CARD_TX_TCP_UDP_CHECKSUM_ERR   0x0000000008000000L
++#define GELIC_CARD_PORT_STATUS_CHANGED       0x0000000020000000L
++	/* INT 0 */
++#define GELIC_CARD_TX_FLAGGED_DESCR          0x0004000000000000L
++#define GELIC_CARD_RX_FLAGGED_DESCR          0x0040000000000000L
++#define GELIC_CARD_TX_TRANSFER_END           0x0080000000000000L
++#define GELIC_CARD_TX_DESCR_CHAIN_END        0x0100000000000000L
++#define GELIC_CARD_NUMBER_OF_RX_FRAME        0x1000000000000000L
++#define GELIC_CARD_ONE_TIME_COUNT_TIMER      0x4000000000000000L
++#define GELIC_CARD_FREE_RUN_COUNT_TIMER      0x8000000000000000L
++
++/* initial interrupt mask */
++#define GELIC_CARD_TXINT	GELIC_CARD_TX_DESCR_CHAIN_END
+ 
+-/* GHIINT1STS bits */
+-enum gelic_net_int1_status {
+-	GELIC_NET_GDADCEINT = 14,
++#define GELIC_CARD_RXINT	(GELIC_CARD_RX_DESCR_CHAIN_END | \
++				 GELIC_CARD_NUMBER_OF_RX_FRAME)
++
++ /* RX descriptor data_status bits */
++enum gelic_descr_rx_status {
++	GELIC_DESCR_RXDMADU	= 0x80000000, /* destination MAC addr unknown */
++	GELIC_DESCR_RXLSTFBF	= 0x40000000, /* last frame buffer            */
++	GELIC_DESCR_RXIPCHK	= 0x20000000, /* IP checksum performed        */
++	GELIC_DESCR_RXTCPCHK	= 0x10000000, /* TCP/UDP checksup performed   */
++	GELIC_DESCR_RXWTPKT	= 0x00C00000, /*
++					       * wakeup trigger packet
++					       * 01: Magic Packet (TM)
++					       * 10: ARP packet
++					       * 11: Multicast MAC addr
++					       */
++	GELIC_DESCR_RXVLNPKT	= 0x00200000, /* VLAN packet */
++	/* bit 20..16 reserved */
++	GELIC_DESCR_RXRRECNUM	= 0x0000ff00, /* reception receipt number */
++	/* bit 7..0 reserved */
+ };
+ 
+-/* interrupt mask */
+-#define GELIC_NET_TXINT                   (1L << (GELIC_NET_GDTDCEINT + 32))
++#define GELIC_DESCR_DATA_STATUS_CHK_MASK	\
++	(GELIC_DESCR_RXIPCHK | GELIC_DESCR_RXTCPCHK)
+ 
+-#define GELIC_NET_RXINT0                  (1L << (GELIC_NET_GRFANMINT + 32))
+-#define GELIC_NET_RXINT1                  (1L << GELIC_NET_GDADCEINT)
+-#define GELIC_NET_RXINT                   (GELIC_NET_RXINT0 | GELIC_NET_RXINT1)
++ /* TX descriptor data_status bits */
++enum gelic_descr_tx_status {
++	GELIC_DESCR_TX_TAIL	= 0x00000001, /* gelic treated this
++					       * descriptor was end of
++					       * a tx frame
++					       */
++};
+ 
+- /* RX descriptor data_status bits */
+-#define GELIC_NET_RXDMADU	0x80000000 /* destination MAC addr unknown */
+-#define GELIC_NET_RXLSTFBF	0x40000000 /* last frame buffer            */
+-#define GELIC_NET_RXIPCHK	0x20000000 /* IP checksum performed        */
+-#define GELIC_NET_RXTCPCHK	0x10000000 /* TCP/UDP checksup performed   */
+-#define GELIC_NET_RXIPSPKT	0x08000000 /* IPsec packet   */
+-#define GELIC_NET_RXIPSAHPRT	0x04000000 /* IPsec AH protocol performed */
+-#define GELIC_NET_RXIPSESPPRT	0x02000000 /* IPsec ESP protocol performed */
+-#define GELIC_NET_RXSESPAH	0x01000000 /*
+-					    * IPsec ESP protocol auth
+-					    * performed
+-					    */
+-
+-#define GELIC_NET_RXWTPKT	0x00C00000 /*
+-					    * wakeup trigger packet
+-					    * 01: Magic Packet (TM)
+-					    * 10: ARP packet
+-					    * 11: Multicast MAC addr
+-					    */
+-#define GELIC_NET_RXVLNPKT	0x00200000 /* VLAN packet */
+-/* bit 20..16 reserved */
+-#define GELIC_NET_RXRRECNUM	0x0000ff00 /* reception receipt number */
+-#define GELIC_NET_RXRRECNUM_SHIFT	8
+-/* bit 7..0 reserved */
+-
+-#define GELIC_NET_TXDESC_TAIL		0
+-#define GELIC_NET_DATA_STATUS_CHK_MASK	(GELIC_NET_RXIPCHK | GELIC_NET_RXTCPCHK)
+-
+-/* RX descriptor data_error bits */
+-/* bit 31 reserved */
+-#define GELIC_NET_RXALNERR	0x40000000 /* alignement error 10/100M */
+-#define GELIC_NET_RXOVERERR	0x20000000 /* oversize error */
+-#define GELIC_NET_RXRNTERR	0x10000000 /* Runt error */
+-#define GELIC_NET_RXIPCHKERR	0x08000000 /* IP checksum  error */
+-#define GELIC_NET_RXTCPCHKERR	0x04000000 /* TCP/UDP checksum  error */
+-#define GELIC_NET_RXUMCHSP	0x02000000 /* unmatched sp on sp */
+-#define GELIC_NET_RXUMCHSPI	0x01000000 /* unmatched SPI on SAD */
+-#define GELIC_NET_RXUMCHSAD	0x00800000 /* unmatched SAD */
+-#define GELIC_NET_RXIPSAHERR	0x00400000 /* auth error on AH protocol
+-					    * processing */
+-#define GELIC_NET_RXIPSESPAHERR	0x00200000 /* auth error on ESP protocol
+-					    * processing */
+-#define GELIC_NET_RXDRPPKT	0x00100000 /* drop packet */
+-#define GELIC_NET_RXIPFMTERR	0x00080000 /* IP packet format error */
+-/* bit 18 reserved */
+-#define GELIC_NET_RXDATAERR	0x00020000 /* IP packet format error */
+-#define GELIC_NET_RXCALERR	0x00010000 /* cariier extension length
+-					    * error */
+-#define GELIC_NET_RXCREXERR	0x00008000 /* carrier extention error */
+-#define GELIC_NET_RXMLTCST	0x00004000 /* multicast address frame */
+-/* bit 13..0 reserved */
+-#define GELIC_NET_DATA_ERROR_CHK_MASK		\
+-	(GELIC_NET_RXIPCHKERR | GELIC_NET_RXTCPCHKERR)
++/* RX descriptor data error bits */
++enum gelic_descr_rx_error {
++	/* bit 31 reserved */
++	GELIC_DESCR_RXALNERR	= 0x40000000, /* alignement error 10/100M */
++	GELIC_DESCR_RXOVERERR	= 0x20000000, /* oversize error */
++	GELIC_DESCR_RXRNTERR	= 0x10000000, /* Runt error */
++	GELIC_DESCR_RXIPCHKERR	= 0x08000000, /* IP checksum  error */
++	GELIC_DESCR_RXTCPCHKERR	= 0x04000000, /* TCP/UDP checksum  error */
++	GELIC_DESCR_RXDRPPKT	= 0x00100000, /* drop packet */
++	GELIC_DESCR_RXIPFMTERR	= 0x00080000, /* IP packet format error */
++	/* bit 18 reserved */
++	GELIC_DESCR_RXDATAERR	= 0x00020000, /* IP packet format error */
++	GELIC_DESCR_RXCALERR	= 0x00010000, /* cariier extension length
++					      * error */
++	GELIC_DESCR_RXCREXERR	= 0x00008000, /* carrier extention error */
++	GELIC_DESCR_RXMLTCST	= 0x00004000, /* multicast address frame */
++	/* bit 13..0 reserved */
++};
++#define GELIC_DESCR_DATA_ERROR_CHK_MASK		\
++	(GELIC_DESCR_RXIPCHKERR | GELIC_DESCR_RXTCPCHKERR)
++
++/* DMA command and status (RX and TX)*/
++enum gelic_descr_dma_status {
++	GELIC_DESCR_DMA_COMPLETE            = 0x00000000, /* used in tx */
++	GELIC_DESCR_DMA_BUFFER_FULL         = 0x00000000, /* used in rx */
++	GELIC_DESCR_DMA_RESPONSE_ERROR      = 0x10000000, /* used in rx, tx */
++	GELIC_DESCR_DMA_PROTECTION_ERROR    = 0x20000000, /* used in rx, tx */
++	GELIC_DESCR_DMA_FRAME_END           = 0x40000000, /* used in rx */
++	GELIC_DESCR_DMA_FORCE_END           = 0x50000000, /* used in rx, tx */
++	GELIC_DESCR_DMA_CARDOWNED           = 0xa0000000, /* used in rx, tx */
++	GELIC_DESCR_DMA_NOT_IN_USE          = 0xb0000000, /* any other value */
++};
+ 
++#define GELIC_DESCR_DMA_STAT_MASK	(0xf0000000)
+ 
+ /* tx descriptor command and status */
+-#define GELIC_NET_DMAC_CMDSTAT_NOCS       0xa0080000 /* middle of frame */
+-#define GELIC_NET_DMAC_CMDSTAT_TCPCS      0xa00a0000
+-#define GELIC_NET_DMAC_CMDSTAT_UDPCS      0xa00b0000
+-#define GELIC_NET_DMAC_CMDSTAT_END_FRAME  0x00040000 /* end of frame */
+-
+-#define GELIC_NET_DMAC_CMDSTAT_RXDCEIS	  0x00000002 /* descriptor chain end
+-						      * interrupt status */
+-
+-#define GELIC_NET_DMAC_CMDSTAT_CHAIN_END  0x00000002 /* RXDCEIS:DMA stopped */
+-#define GELIC_NET_DESCR_IND_PROC_SHIFT    28
+-#define GELIC_NET_DESCR_IND_PROC_MASKO    0x0fffffff
+-
+-
+-enum gelic_net_descr_status {
+-	GELIC_NET_DESCR_COMPLETE            = 0x00, /* used in tx */
+-	GELIC_NET_DESCR_BUFFER_FULL         = 0x00, /* used in rx */
+-	GELIC_NET_DESCR_RESPONSE_ERROR      = 0x01, /* used in rx and tx */
+-	GELIC_NET_DESCR_PROTECTION_ERROR    = 0x02, /* used in rx and tx */
+-	GELIC_NET_DESCR_FRAME_END           = 0x04, /* used in rx */
+-	GELIC_NET_DESCR_FORCE_END           = 0x05, /* used in rx and tx */
+-	GELIC_NET_DESCR_CARDOWNED           = 0x0a, /* used in rx and tx */
+-	GELIC_NET_DESCR_NOT_IN_USE          = 0x0b  /* any other value */
++enum gelic_descr_tx_dma_status {
++	/* [19] */
++	GELIC_DESCR_TX_DMA_IKE		= 0x00080000, /* IPSEC off */
++	/* [18] */
++	GELIC_DESCR_TX_DMA_FRAME_TAIL	= 0x00040000, /* last descriptor of
++						       * the packet
++						       */
++	/* [17..16] */
++	GELIC_DESCR_TX_DMA_TCP_CHKSUM	= 0x00020000, /* TCP packet */
++	GELIC_DESCR_TX_DMA_UDP_CHKSUM	= 0x00030000, /* UDP packet */
++	GELIC_DESCR_TX_DMA_NO_CHKSUM	= 0x00000000, /* no checksum */
++
++	/* [1] */
++	GELIC_DESCR_TX_DMA_CHAIN_END	= 0x00000002, /* DMA terminated
++						       * due to chain end
++						       */
++};
++
++#define GELIC_DESCR_DMA_CMD_NO_CHKSUM	\
++	(GELIC_DESCR_DMA_CARDOWNED | GELIC_DESCR_TX_DMA_IKE | \
++	GELIC_DESCR_TX_DMA_NO_CHKSUM)
++
++#define GELIC_DESCR_DMA_CMD_TCP_CHKSUM	\
++	(GELIC_DESCR_DMA_CARDOWNED | GELIC_DESCR_TX_DMA_IKE | \
++	GELIC_DESCR_TX_DMA_TCP_CHKSUM)
++
++#define GELIC_DESCR_DMA_CMD_UDP_CHKSUM	\
++	(GELIC_DESCR_DMA_CARDOWNED | GELIC_DESCR_TX_DMA_IKE | \
++	GELIC_DESCR_TX_DMA_UDP_CHKSUM)
++
++enum gelic_descr_rx_dma_status {
++	/* [ 1 ] */
++	GELIC_DESCR_RX_DMA_CHAIN_END	= 0x00000002, /* DMA terminated
++						       * due to chain end
++						       */
+ };
++
+ /* for lv1_net_control */
+-#define GELIC_NET_GET_MAC_ADDRESS               0x0000000000000001
+-#define GELIC_NET_GET_ETH_PORT_STATUS           0x0000000000000002
+-#define GELIC_NET_SET_NEGOTIATION_MODE          0x0000000000000003
+-#define GELIC_NET_GET_VLAN_ID                   0x0000000000000004
+-
+-#define GELIC_NET_LINK_UP                       0x0000000000000001
+-#define GELIC_NET_FULL_DUPLEX                   0x0000000000000002
+-#define GELIC_NET_AUTO_NEG                      0x0000000000000004
+-#define GELIC_NET_SPEED_10                      0x0000000000000010
+-#define GELIC_NET_SPEED_100                     0x0000000000000020
+-#define GELIC_NET_SPEED_1000                    0x0000000000000040
+-
+-#define GELIC_NET_VLAN_ALL                      0x0000000000000001
+-#define GELIC_NET_VLAN_WIRED                    0x0000000000000002
+-#define GELIC_NET_VLAN_WIRELESS                 0x0000000000000003
+-#define GELIC_NET_VLAN_PSP                      0x0000000000000004
+-#define GELIC_NET_VLAN_PORT0                    0x0000000000000010
+-#define GELIC_NET_VLAN_PORT1                    0x0000000000000011
+-#define GELIC_NET_VLAN_PORT2                    0x0000000000000012
+-#define GELIC_NET_VLAN_DAEMON_CLIENT_BSS        0x0000000000000013
+-#define GELIC_NET_VLAN_LIBERO_CLIENT_BSS        0x0000000000000014
+-#define GELIC_NET_VLAN_NO_ENTRY                 -6
++enum gelic_lv1_net_control_code {
++	GELIC_LV1_GET_MAC_ADDRESS	= 1,
++	GELIC_LV1_GET_ETH_PORT_STATUS	= 2,
++	GELIC_LV1_SET_NEGOTIATION_MODE	= 3,
++	GELIC_LV1_GET_VLAN_ID		= 4,
++};
++
++/* status returened from GET_ETH_PORT_STATUS */
++enum gelic_lv1_ether_port_status {
++	GELIC_LV1_ETHER_LINK_UP		= 0x0000000000000001L,
++	GELIC_LV1_ETHER_FULL_DUPLEX	= 0x0000000000000002L,
++	GELIC_LV1_ETHER_AUTO_NEG	= 0x0000000000000004L,
++
++	GELIC_LV1_ETHER_SPEED_10	= 0x0000000000000010L,
++	GELIC_LV1_ETHER_SPEED_100	= 0x0000000000000020L,
++	GELIC_LV1_ETHER_SPEED_1000	= 0x0000000000000040L,
++	GELIC_LV1_ETHER_SPEED_MASK	= 0x0000000000000070L
++};
+ 
+-#define GELIC_NET_PORT                          2 /* for port status */
++enum gelic_lv1_vlan_index {
++	/* for outgoing packets */
++	GELIC_LV1_VLAN_TX_ETHERNET	= 0x0000000000000002L,
++	GELIC_LV1_VLAN_TX_WIRELESS	= 0x0000000000000003L,
++	/* for incoming packets */
++	GELIC_LV1_VLAN_RX_ETHERNET	= 0x0000000000000012L,
++	GELIC_LV1_VLAN_RX_WIRELESS	= 0x0000000000000013L
++};
+ 
+ /* size of hardware part of gelic descriptor */
+-#define GELIC_NET_DESCR_SIZE	(32)
+-struct gelic_net_descr {
++#define GELIC_DESCR_SIZE	(32)
++struct gelic_descr {
+ 	/* as defined by the hardware */
+ 	__be32 buf_addr;
+ 	__be32 buf_size;
+@@ -181,18 +220,18 @@ struct gelic_net_descr {
+ 	/* used in the driver */
+ 	struct sk_buff *skb;
+ 	dma_addr_t bus_addr;
+-	struct gelic_net_descr *next;
+-	struct gelic_net_descr *prev;
++	struct gelic_descr *next;
++	struct gelic_descr *prev;
+ 	struct vlan_ethhdr vlan;
+ } __attribute__((aligned(32)));
+ 
+-struct gelic_net_descr_chain {
++struct gelic_descr_chain {
+ 	/* we walk from tail to head */
+-	struct gelic_net_descr *head;
+-	struct gelic_net_descr *tail;
++	struct gelic_descr *head;
++	struct gelic_descr *tail;
+ };
+ 
+-struct gelic_net_card {
++struct gelic_card {
+ 	struct net_device *netdev;
+ 	struct napi_struct napi;
+ 	/*
+@@ -207,8 +246,8 @@ struct gelic_net_card {
+ 	u32 vlan_id[GELIC_NET_VLAN_MAX];
+ 	int vlan_index;
+ 
+-	struct gelic_net_descr_chain tx_chain;
+-	struct gelic_net_descr_chain rx_chain;
++	struct gelic_descr_chain tx_chain;
++	struct gelic_descr_chain rx_chain;
+ 	int rx_dma_restart_required;
+ 	/* gurad dmac descriptor chain*/
+ 	spinlock_t chain_lock;
+@@ -222,8 +261,8 @@ struct gelic_net_card {
+ 	atomic_t tx_timeout_task_counter;
+ 	wait_queue_head_t waitq;
+ 
+-	struct gelic_net_descr *tx_top, *rx_top;
+-	struct gelic_net_descr descr[0];
++	struct gelic_descr *tx_top, *rx_top;
++	struct gelic_descr descr[0];
+ };
+ 
+ 
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-endianness.patch linux-2.6.25-id/patches/ps3-wip/ps3-gelic-endianness.patch
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-endianness.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-endianness.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,228 @@
+Subject: PS3: gelic: Add endianness macros
+
+Mark the members of the structure for DMA descriptors with proper endian
+annotations and use the appropriate accessor macros.
+As the gelic driver works only on PS3, all these macros will be
+expanded to null.
+
+Signed-off-by: Masakazu Mokuno <mokuno@sm.sony.co.jp>
+---
+ drivers/net/ps3_gelic_net.c |   70 ++++++++++++++++++++++++--------------------
+ drivers/net/ps3_gelic_net.h |   16 +++++-----
+ 2 files changed, 47 insertions(+), 39 deletions(-)
+
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -98,7 +98,7 @@ gelic_net_get_descr_status(struct gelic_
+ {
+ 	u32 cmd_status;
+ 
+-	cmd_status = descr->dmac_cmd_status;
++	cmd_status = be32_to_cpu(descr->dmac_cmd_status);
+ 	cmd_status >>= GELIC_NET_DESCR_IND_PROC_SHIFT;
+ 	return cmd_status;
+ }
+@@ -117,13 +117,13 @@ static void gelic_net_set_descr_status(s
+ 	u32 cmd_status;
+ 
+ 	/* read the status */
+-	cmd_status = descr->dmac_cmd_status;
++	cmd_status = be32_to_cpu(descr->dmac_cmd_status);
+ 	/* clean the upper 4 bits */
+ 	cmd_status &= GELIC_NET_DESCR_IND_PROC_MASKO;
+ 	/* add the status to it */
+ 	cmd_status |= ((u32)status) << GELIC_NET_DESCR_IND_PROC_SHIFT;
+ 	/* and write it back */
+-	descr->dmac_cmd_status = cmd_status;
++	descr->dmac_cmd_status = cpu_to_be32(cmd_status);
+ 	/*
+ 	 * dma_cmd_status field is used to indicate whether the descriptor
+ 	 * is valid or not.
+@@ -193,7 +193,7 @@ static int gelic_net_init_chain(struct g
+ 	/* chain bus addr of hw descriptor */
+ 	descr = start_descr;
+ 	for (i = 0; i < no; i++, descr++) {
+-		descr->next_descr_addr = descr->next->bus_addr;
++		descr->next_descr_addr = cpu_to_be32(descr->next->bus_addr);
+ 	}
+ 
+ 	chain->head = start_descr;
+@@ -245,7 +245,7 @@ static int gelic_net_prepare_rx_descr(st
+ 			 "%s:allocate skb failed !!\n", __func__);
+ 		return -ENOMEM;
+ 	}
+-	descr->buf_size = bufsize;
++	descr->buf_size = cpu_to_be32(bufsize);
+ 	descr->dmac_cmd_status = 0;
+ 	descr->result_size = 0;
+ 	descr->valid_size = 0;
+@@ -256,9 +256,10 @@ static int gelic_net_prepare_rx_descr(st
+ 	if (offset)
+ 		skb_reserve(descr->skb, GELIC_NET_RXBUF_ALIGN - offset);
+ 	/* io-mmu-map the skb */
+-	descr->buf_addr = dma_map_single(ctodev(card), descr->skb->data,
+-					 GELIC_NET_MAX_MTU,
+-					 DMA_FROM_DEVICE);
++	descr->buf_addr = cpu_to_be32(dma_map_single(ctodev(card),
++						     descr->skb->data,
++						     GELIC_NET_MAX_MTU,
++						     DMA_FROM_DEVICE));
+ 	if (!descr->buf_addr) {
+ 		dev_kfree_skb_any(descr->skb);
+ 		descr->skb = NULL;
+@@ -284,7 +285,7 @@ static void gelic_net_release_rx_chain(s
+ 	do {
+ 		if (descr->skb) {
+ 			dma_unmap_single(ctodev(card),
+-					 descr->buf_addr,
++					 be32_to_cpu(descr->buf_addr),
+ 					 descr->skb->len,
+ 					 DMA_FROM_DEVICE);
+ 			descr->buf_addr = 0;
+@@ -353,10 +354,11 @@ static void gelic_net_release_tx_descr(s
+ {
+ 	struct sk_buff *skb = descr->skb;
+ 
+-	BUG_ON(!(descr->data_status & (1 << GELIC_NET_TXDESC_TAIL)));
++	BUG_ON(!(be32_to_cpu(descr->data_status) &
++		 (1 << GELIC_NET_TXDESC_TAIL)));
+ 
+-	dma_unmap_single(ctodev(card), descr->buf_addr, skb->len,
+-			 DMA_TO_DEVICE);
++	dma_unmap_single(ctodev(card),
++			 be32_to_cpu(descr->buf_addr), skb->len, DMA_TO_DEVICE);
+ 	dev_kfree_skb_any(skb);
+ 
+ 	descr->buf_addr = 0;
+@@ -610,28 +612,29 @@ static void gelic_net_set_txdescr_cmdsta
+ 					  struct sk_buff *skb)
+ {
+ 	if (skb->ip_summed != CHECKSUM_PARTIAL)
+-		descr->dmac_cmd_status = GELIC_NET_DMAC_CMDSTAT_NOCS |
+-			GELIC_NET_DMAC_CMDSTAT_END_FRAME;
++		descr->dmac_cmd_status =
++			cpu_to_be32(GELIC_NET_DMAC_CMDSTAT_NOCS |
++				    GELIC_NET_DMAC_CMDSTAT_END_FRAME);
+ 	else {
+ 		/* is packet ip?
+ 		 * if yes: tcp? udp? */
+ 		if (skb->protocol == htons(ETH_P_IP)) {
+ 			if (ip_hdr(skb)->protocol == IPPROTO_TCP)
+ 				descr->dmac_cmd_status =
+-					GELIC_NET_DMAC_CMDSTAT_TCPCS |
+-					GELIC_NET_DMAC_CMDSTAT_END_FRAME;
++				cpu_to_be32(GELIC_NET_DMAC_CMDSTAT_TCPCS |
++					    GELIC_NET_DMAC_CMDSTAT_END_FRAME);
+ 
+ 			else if (ip_hdr(skb)->protocol == IPPROTO_UDP)
+ 				descr->dmac_cmd_status =
+-					GELIC_NET_DMAC_CMDSTAT_UDPCS |
+-					GELIC_NET_DMAC_CMDSTAT_END_FRAME;
++				cpu_to_be32(GELIC_NET_DMAC_CMDSTAT_UDPCS |
++					    GELIC_NET_DMAC_CMDSTAT_END_FRAME);
+ 			else	/*
+ 				 * the stack should checksum non-tcp and non-udp
+ 				 * packets on his own: NETIF_F_IP_CSUM
+ 				 */
+ 				descr->dmac_cmd_status =
+-					GELIC_NET_DMAC_CMDSTAT_NOCS |
+-					GELIC_NET_DMAC_CMDSTAT_END_FRAME;
++				cpu_to_be32(GELIC_NET_DMAC_CMDSTAT_NOCS |
++					    GELIC_NET_DMAC_CMDSTAT_END_FRAME);
+ 		}
+ 	}
+ }
+@@ -694,8 +697,8 @@ static int gelic_net_prepare_tx_descr_v(
+ 		return -ENOMEM;
+ 	}
+ 
+-	descr->buf_addr = buf;
+-	descr->buf_size = skb->len;
++	descr->buf_addr = cpu_to_be32(buf);
++	descr->buf_size = cpu_to_be32(skb->len);
+ 	descr->skb = skb;
+ 	descr->data_status = 0;
+ 	descr->next_descr_addr = 0; /* terminate hw descr */
+@@ -774,7 +777,7 @@ static int gelic_net_xmit(struct sk_buff
+ 	 * link this prepared descriptor to previous one
+ 	 * to achieve high performance
+ 	 */
+-	descr->prev->next_descr_addr = descr->bus_addr;
++	descr->prev->next_descr_addr = cpu_to_be32(descr->bus_addr);
+ 	/*
+ 	 * as hardware descriptor is modified in the above lines,
+ 	 * ensure that the hardware sees it
+@@ -814,19 +817,23 @@ static void gelic_net_pass_skb_up(struct
+ 	struct net_device *netdev;
+ 	u32 data_status, data_error;
+ 
+-	data_status = descr->data_status;
+-	data_error = descr->data_error;
++	data_status = be32_to_cpu(descr->data_status);
++	data_error = be32_to_cpu(descr->data_error);
+ 	netdev = card->netdev;
+ 	/* unmap skb buffer */
+ 	skb = descr->skb;
+-	dma_unmap_single(ctodev(card), descr->buf_addr, GELIC_NET_MAX_MTU,
++	dma_unmap_single(ctodev(card),
++			 be32_to_cpu(descr->buf_addr), GELIC_NET_MAX_MTU,
+ 			 DMA_FROM_DEVICE);
+ 
+-	skb_put(skb, descr->valid_size? descr->valid_size : descr->result_size);
++	skb_put(skb, descr->valid_size ?
++		be32_to_cpu(descr->valid_size) :
++		be32_to_cpu(descr->result_size));
+ 	if (!descr->valid_size)
+ 		dev_info(ctodev(card), "buffer full %x %x %x\n",
+-			 descr->result_size, descr->buf_size,
+-			 descr->dmac_cmd_status);
++			 be32_to_cpu(descr->result_size),
++			 be32_to_cpu(descr->buf_size),
++			 be32_to_cpu(descr->dmac_cmd_status));
+ 
+ 	descr->skb = NULL;
+ 	/*
+@@ -873,7 +880,8 @@ static int gelic_net_decode_one_descr(st
+ 	status = gelic_net_get_descr_status(descr);
+ 	/* is this descriptor terminated with next_descr == NULL? */
+ 	dmac_chain_ended =
+-		descr->dmac_cmd_status & GELIC_NET_DMAC_CMDSTAT_RXDCEIS;
++		be32_to_cpu(descr->dmac_cmd_status) &
++		GELIC_NET_DMAC_CMDSTAT_RXDCEIS;
+ 
+ 	if (status == GELIC_NET_DESCR_CARDOWNED)
+ 		return 0;
+@@ -940,7 +948,7 @@ refill:
+ 	/*
+ 	 * Set this descriptor the end of the chain.
+ 	 */
+-	descr->prev->next_descr_addr = descr->bus_addr;
++	descr->prev->next_descr_addr = cpu_to_be32(descr->bus_addr);
+ 
+ 	/*
+ 	 * If dmac chain was met, DMAC stopped.
+--- a/drivers/net/ps3_gelic_net.h
++++ b/drivers/net/ps3_gelic_net.h
+@@ -169,14 +169,14 @@ enum gelic_net_descr_status {
+ #define GELIC_NET_DESCR_SIZE	(32)
+ struct gelic_net_descr {
+ 	/* as defined by the hardware */
+-	u32 buf_addr;
+-	u32 buf_size;
+-	u32 next_descr_addr;
+-	u32 dmac_cmd_status;
+-	u32 result_size;
+-	u32 valid_size;	/* all zeroes for tx */
+-	u32 data_status;
+-	u32 data_error;	/* all zeroes for tx */
++	__be32 buf_addr;
++	__be32 buf_size;
++	__be32 next_descr_addr;
++	__be32 dmac_cmd_status;
++	__be32 result_size;
++	__be32 valid_size;	/* all zeroes for tx */
++	__be32 data_status;
++	__be32 data_error;	/* all zeroes for tx */
+ 
+ 	/* used in the driver */
+ 	struct sk_buff *skb;
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-ethernet-linkstatus.patch linux-2.6.25-id/patches/ps3-wip/ps3-gelic-ethernet-linkstatus.patch
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-ethernet-linkstatus.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-ethernet-linkstatus.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,141 @@
+Subject: PS3: gelic: add support for port link status
+
+Add support for interrupt driven port link status detection.
+
+Signed-off-by: Masakazu Mokuno <mokuno@sm.sony.co.jp>
+---
+ drivers/net/ps3_gelic_net.c |   77 ++++++++++++++++++++++++++++----------------
+ drivers/net/ps3_gelic_net.h |    2 +
+ 2 files changed, 52 insertions(+), 27 deletions(-)
+
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -87,6 +87,28 @@ static inline void gelic_card_rx_irq_off
+ {
+ 	gelic_card_set_irq_mask(card, card->ghiintmask & ~GELIC_CARD_RXINT);
+ }
++
++static void
++gelic_card_get_ether_port_status(struct gelic_card *card, int inform)
++{
++	u64 v2;
++	struct net_device *ether_netdev;
++
++	lv1_net_control(bus_id(card), dev_id(card),
++			GELIC_LV1_GET_ETH_PORT_STATUS,
++			GELIC_LV1_VLAN_TX_ETHERNET, 0, 0,
++			&card->ether_port_status, &v2);
++
++	if (inform) {
++		ether_netdev = card->netdev;
++		if (card->ether_port_status & GELIC_LV1_ETHER_LINK_UP)
++			netif_carrier_on(ether_netdev);
++		else
++			netif_carrier_off(ether_netdev);
++	}
++}
++
++
+ /**
+  * gelic_descr_get_status -- returns the status of a descriptor
+  * @descr: descriptor to look at
+@@ -1032,6 +1054,10 @@ static irqreturn_t gelic_card_interrupt(
+ 		gelic_card_kick_txdma(card, card->tx_chain.tail);
+ 		spin_unlock_irqrestore(&card->tx_dma_lock, flags);
+ 	}
++
++	/* ether port status changed */
++	if (status & GELIC_CARD_PORT_STATUS_CHANGED)
++		gelic_card_get_ether_port_status(card, 1);
+ 	return IRQ_HANDLED;
+ }
+ 
+@@ -1128,13 +1154,14 @@ static int gelic_net_open(struct net_dev
+ 	napi_enable(&card->napi);
+ 
+ 	card->tx_dma_progress = 0;
+-	card->ghiintmask = GELIC_CARD_RXINT | GELIC_CARD_TXINT;
++	card->ghiintmask = GELIC_CARD_RXINT | GELIC_CARD_TXINT |
++		GELIC_CARD_PORT_STATUS_CHANGED;
+ 
+ 	gelic_card_set_irq_mask(card, card->ghiintmask);
+ 	gelic_card_enable_rxdmac(card);
+ 
+ 	netif_start_queue(netdev);
+-	netif_carrier_on(netdev);
++	gelic_card_get_ether_port_status(card, 1);
+ 
+ 	return 0;
+ 
+@@ -1157,39 +1184,35 @@ static int gelic_ether_get_settings(stru
+ 				    struct ethtool_cmd *cmd)
+ {
+ 	struct gelic_card *card = netdev_priv(netdev);
+-	int status;
+-	u64 v1, v2;
+-	int speed, duplex;
+ 
+-	speed = duplex = -1;
+-	status = lv1_net_control(bus_id(card), dev_id(card),
+-				 GELIC_LV1_GET_ETH_PORT_STATUS,
+-				 GELIC_LV1_VLAN_TX_ETHERNET, 0, 0,
+-				 &v1, &v2);
+-	if (status) {
+-		/* link down */
+-	} else {
+-		if (v1 & GELIC_LV1_ETHER_FULL_DUPLEX) {
+-			duplex = DUPLEX_FULL;
+-		} else {
+-			duplex = DUPLEX_HALF;
+-		}
++	gelic_card_get_ether_port_status(card, 0);
+ 
+-		if (v1 & GELIC_LV1_ETHER_SPEED_10) {
+-			speed = SPEED_10;
+-		} else if (v1 & GELIC_LV1_ETHER_SPEED_100) {
+-			speed = SPEED_100;
+-		} else if (v1 & GELIC_LV1_ETHER_SPEED_1000) {
+-			speed = SPEED_1000;
+-		}
++	if (card->ether_port_status & GELIC_LV1_ETHER_FULL_DUPLEX)
++		cmd->duplex = DUPLEX_FULL;
++	else
++		cmd->duplex = DUPLEX_HALF;
++
++	switch (card->ether_port_status & GELIC_LV1_ETHER_SPEED_MASK) {
++	case GELIC_LV1_ETHER_SPEED_10:
++		cmd->speed = SPEED_10;
++		break;
++	case GELIC_LV1_ETHER_SPEED_100:
++		cmd->speed = SPEED_100;
++		break;
++	case GELIC_LV1_ETHER_SPEED_1000:
++		cmd->speed = SPEED_1000;
++		break;
++	default:
++		pr_info("%s: speed unknown\n", __func__);
++		cmd->speed = SPEED_10;
++		break;
+ 	}
++
+ 	cmd->supported = SUPPORTED_TP | SUPPORTED_Autoneg |
+ 			SUPPORTED_10baseT_Half | SUPPORTED_10baseT_Full |
+ 			SUPPORTED_100baseT_Half | SUPPORTED_100baseT_Full |
+ 			SUPPORTED_1000baseT_Half | SUPPORTED_1000baseT_Full;
+ 	cmd->advertising = cmd->supported;
+-	cmd->speed = speed;
+-	cmd->duplex = duplex;
+ 	cmd->autoneg = AUTONEG_ENABLE; /* always enabled */
+ 	cmd->port = PORT_TP;
+ 
+--- a/drivers/net/ps3_gelic_net.h
++++ b/drivers/net/ps3_gelic_net.h
+@@ -261,6 +261,8 @@ struct gelic_card {
+ 	atomic_t tx_timeout_task_counter;
+ 	wait_queue_head_t waitq;
+ 
++	u64 ether_port_status;
++
+ 	struct gelic_descr *tx_top, *rx_top;
+ 	struct gelic_descr descr[0];
+ };
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-fix-fallback.diff linux-2.6.25-id/patches/ps3-wip/ps3-gelic-fix-fallback.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-fix-fallback.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-fix-fallback.diff	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,21 @@
+Subject: PS3: gelic: Fix the wrong dev_id passed
+
+The device id for lv1_net_set_interrupt_status_indicator() is wrong.
+This path would be invoked only in the case of an initialization failure.
+
+Signed-off-by: Masakazu Mokuno <mokuno@sm.sony.co.jp>
+---
+ drivers/net/ps3_gelic_net.c |    2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -1512,7 +1512,7 @@ static int ps3_gelic_driver_probe (struc
+ 
+ fail_setup_netdev:
+ 	lv1_net_set_interrupt_status_indicator(bus_id(card),
+-					       bus_id(card),
++					       dev_id(card),
+ 					       0 , 0);
+ fail_status_indicator:
+ 	ps3_dma_region_free(dev->d_region);
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-multiple-interface.patch linux-2.6.25-id/patches/ps3-wip/ps3-gelic-multiple-interface.patch
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-multiple-interface.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-multiple-interface.patch	2008-04-23 11:22:11.000000000 +0200
@@ -0,0 +1,1535 @@
+Subject: PS3: gelic: Add support for dual network interface
+
+Add support for dual network (net_device) interface so that ethernet
+and wireless can own separate ethX interfaces.
+
+V2
+  - Fix the bug that bringing down and up the interface keeps rx
+    disabled.
+  - Make 'gelic_net_poll_controller()' extern , as David Woodhouse
+    pointed out at the previous submission.
+  - Fix weird usage of member names for the rx descriptor chain
+V1
+  - Export functions which are convenient for both interfaces
+  - Move irq allocation/release code to driver probe/remove handlers
+    because interfaces share interrupts.
+  - Allocate skbs by using dev_alloc_skb() instead of netdev_alloc_skb()
+    as the interfaces share the hardware rx queue.
+  - Add gelic_port struct in order to abstract dual interface handling
+  - Change handlers for hardware queues so that they can handle dual
+    {source,destination} interfaces.
+  - Use new NAPI functions
+This is a prerequisite for the new PS3 wireless support.
+
+Signed-off-by: Masakazu Mokuno <mokuno@sm.sony.co.jp>
+---
+ drivers/net/ps3_gelic_net.c |  765 +++++++++++++++++++++++++++-----------------
+ drivers/net/ps3_gelic_net.h |  108 +++++-
+ 2 files changed, 564 insertions(+), 309 deletions(-)
+
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -48,27 +48,22 @@
+ #include "ps3_gelic_net.h"
+ 
+ #define DRV_NAME "Gelic Network Driver"
+-#define DRV_VERSION "1.0"
++#define DRV_VERSION "1.1"
+ 
+ MODULE_AUTHOR("SCE Inc.");
+ MODULE_DESCRIPTION("Gelic Network driver");
+ MODULE_LICENSE("GPL");
+ 
+-static inline struct device *ctodev(struct gelic_card *card)
+-{
+-	return &card->dev->core;
+-}
+-static inline u64 bus_id(struct gelic_card *card)
+-{
+-	return card->dev->bus_id;
+-}
+-static inline u64 dev_id(struct gelic_card *card)
+-{
+-	return card->dev->dev_id;
+-}
++
++static inline void gelic_card_enable_rxdmac(struct gelic_card *card);
++static inline void gelic_card_disable_rxdmac(struct gelic_card *card);
++static inline void gelic_card_disable_txdmac(struct gelic_card *card);
++static inline void gelic_card_reset_chain(struct gelic_card *card,
++					  struct gelic_descr_chain *chain,
++					  struct gelic_descr *start_descr);
+ 
+ /* set irq_mask */
+-static int gelic_card_set_irq_mask(struct gelic_card *card, u64 mask)
++int gelic_card_set_irq_mask(struct gelic_card *card, u64 mask)
+ {
+ 	int status;
+ 
+@@ -76,20 +71,23 @@ static int gelic_card_set_irq_mask(struc
+ 					    mask, 0);
+ 	if (status)
+ 		dev_info(ctodev(card),
+-			 "lv1_net_set_interrupt_mask failed %d\n", status);
++			 "%s failed %d\n", __func__, status);
+ 	return status;
+ }
++
+ static inline void gelic_card_rx_irq_on(struct gelic_card *card)
+ {
+-	gelic_card_set_irq_mask(card, card->ghiintmask | GELIC_CARD_RXINT);
++	card->irq_mask |= GELIC_CARD_RXINT;
++	gelic_card_set_irq_mask(card, card->irq_mask);
+ }
+ static inline void gelic_card_rx_irq_off(struct gelic_card *card)
+ {
+-	gelic_card_set_irq_mask(card, card->ghiintmask & ~GELIC_CARD_RXINT);
++	card->irq_mask &= ~GELIC_CARD_RXINT;
++	gelic_card_set_irq_mask(card, card->irq_mask);
+ }
+ 
+-static void
+-gelic_card_get_ether_port_status(struct gelic_card *card, int inform)
++static void gelic_card_get_ether_port_status(struct gelic_card *card,
++					     int inform)
+ {
+ 	u64 v2;
+ 	struct net_device *ether_netdev;
+@@ -100,7 +98,7 @@ gelic_card_get_ether_port_status(struct 
+ 			&card->ether_port_status, &v2);
+ 
+ 	if (inform) {
+-		ether_netdev = card->netdev;
++		ether_netdev = card->netdev[GELIC_PORT_ETHERNET];
+ 		if (card->ether_port_status & GELIC_LV1_ETHER_LINK_UP)
+ 			netif_carrier_on(ether_netdev);
+ 		else
+@@ -108,6 +106,48 @@ gelic_card_get_ether_port_status(struct 
+ 	}
+ }
+ 
++void gelic_card_up(struct gelic_card *card)
++{
++	pr_debug("%s: called\n", __func__);
++	down(&card->updown_lock);
++	if (atomic_inc_return(&card->users) == 1) {
++		pr_debug("%s: real do\n", __func__);
++		/* enable irq */
++		gelic_card_set_irq_mask(card, card->irq_mask);
++		/* start rx */
++		gelic_card_enable_rxdmac(card);
++
++		napi_enable(&card->napi);
++	}
++	up(&card->updown_lock);
++	pr_debug("%s: done\n", __func__);
++}
++
++void gelic_card_down(struct gelic_card *card)
++{
++	u64 mask;
++	pr_debug("%s: called\n", __func__);
++	down(&card->updown_lock);
++	if (atomic_dec_if_positive(&card->users) == 0) {
++		pr_debug("%s: real do\n", __func__);
++		napi_disable(&card->napi);
++		/*
++		 * Disable irq. Wireless interrupts will
++		 * be disabled later if any
++		 */
++		mask = card->irq_mask & (GELIC_CARD_WLAN_EVENT_RECEIVED |
++					 GELIC_CARD_WLAN_COMMAND_COMPLETED);
++		gelic_card_set_irq_mask(card, mask);
++		/* stop rx */
++		gelic_card_disable_rxdmac(card);
++		gelic_card_reset_chain(card, &card->rx_chain,
++				       card->descr + GELIC_NET_TX_DESCRIPTORS);
++		/* stop tx */
++		gelic_card_disable_txdmac(card);
++	}
++	up(&card->updown_lock);
++	pr_debug("%s: done\n", __func__);
++}
+ 
+ /**
+  * gelic_descr_get_status -- returns the status of a descriptor
+@@ -133,8 +173,8 @@ static void gelic_descr_set_status(struc
+ 				   enum gelic_descr_dma_status status)
+ {
+ 	descr->dmac_cmd_status = cpu_to_be32(status |
+-		(be32_to_cpu(descr->dmac_cmd_status) &
+-		 ~GELIC_DESCR_DMA_STAT_MASK));
++			(be32_to_cpu(descr->dmac_cmd_status) &
++			 ~GELIC_DESCR_DMA_STAT_MASK));
+ 	/*
+ 	 * dma_cmd_status field is used to indicate whether the descriptor
+ 	 * is valid or not.
+@@ -225,6 +265,31 @@ iommu_error:
+ }
+ 
+ /**
++ * gelic_card_reset_chain - reset status of a descriptor chain
++ * @card: card structure
++ * @chain: address of chain
++ * @start_descr: address of descriptor array
++ *
++ * Reset the status of dma descriptors to ready state
++ * and re-initialize the hardware chain for later use
++ */
++static void gelic_card_reset_chain(struct gelic_card *card,
++				   struct gelic_descr_chain *chain,
++				   struct gelic_descr *start_descr)
++{
++	struct gelic_descr *descr;
++
++	for (descr = start_descr; start_descr != descr->next; descr++) {
++		gelic_descr_set_status(descr, GELIC_DESCR_DMA_CARDOWNED);
++		descr->next_descr_addr = cpu_to_be32(descr->next->bus_addr);
++	}
++
++	chain->head = start_descr;
++	chain->tail = (descr - 1);
++
++	(descr - 1)->next_descr_addr = 0;
++}
++/**
+  * gelic_descr_prepare_rx - reinitializes a rx descriptor
+  * @card: card structure
+  * @descr: descriptor to re-init
+@@ -235,21 +300,19 @@ iommu_error:
+  * Activate the descriptor state-wise
+  */
+ static int gelic_descr_prepare_rx(struct gelic_card *card,
+-				      struct gelic_descr *descr)
++				  struct gelic_descr *descr)
+ {
+ 	int offset;
+ 	unsigned int bufsize;
+ 
+ 	if (gelic_descr_get_status(descr) !=  GELIC_DESCR_DMA_NOT_IN_USE)
+ 		dev_info(ctodev(card), "%s: ERROR status \n", __func__);
+-
+ 	/* we need to round up the buffer size to a multiple of 128 */
+ 	bufsize = ALIGN(GELIC_NET_MAX_MTU, GELIC_NET_RXBUF_ALIGN);
+ 
+ 	/* and we need to have it 128 byte aligned, therefore we allocate a
+ 	 * bit more */
+-	descr->skb = netdev_alloc_skb(card->netdev,
+-		bufsize + GELIC_NET_RXBUF_ALIGN - 1);
++	descr->skb = dev_alloc_skb(bufsize + GELIC_NET_RXBUF_ALIGN - 1);
+ 	if (!descr->skb) {
+ 		descr->buf_addr = 0; /* tell DMAC don't touch memory */
+ 		dev_info(ctodev(card),
+@@ -349,7 +412,7 @@ static int gelic_card_alloc_rx_skbs(stru
+ 	int ret;
+ 	chain = &card->rx_chain;
+ 	ret = gelic_card_fill_rx_chain(card);
+-	chain->head = card->rx_top->prev; /* point to the last */
++	chain->tail = card->rx_top->prev; /* point to the last */
+ 	return ret;
+ }
+ 
+@@ -361,16 +424,14 @@ static int gelic_card_alloc_rx_skbs(stru
+  * releases a used tx descriptor (unmapping, freeing of skb)
+  */
+ static void gelic_descr_release_tx(struct gelic_card *card,
+-			    struct gelic_descr *descr)
++				       struct gelic_descr *descr)
+ {
+ 	struct sk_buff *skb = descr->skb;
+ 
+-#ifdef DEBUG
+-	BUG_ON(!(be32_to_cpu(descr->data_status) &
+-		 (1 << GELIC_DESCR_TX_DMA_FRAME_TAIL)));
+-#endif
+-	dma_unmap_single(ctodev(card),
+-			 be32_to_cpu(descr->buf_addr), skb->len, DMA_TO_DEVICE);
++	BUG_ON(!(be32_to_cpu(descr->data_status) & GELIC_DESCR_TX_TAIL));
++
++	dma_unmap_single(ctodev(card), be32_to_cpu(descr->buf_addr), skb->len,
++			 DMA_TO_DEVICE);
+ 	dev_kfree_skb_any(skb);
+ 
+ 	descr->buf_addr = 0;
+@@ -386,6 +447,20 @@ static void gelic_descr_release_tx(struc
+ 	gelic_descr_set_status(descr, GELIC_DESCR_DMA_NOT_IN_USE);
+ }
+ 
++static void gelic_card_stop_queues(struct gelic_card *card)
++{
++	netif_stop_queue(card->netdev[GELIC_PORT_ETHERNET]);
++
++	if (card->netdev[GELIC_PORT_WIRELESS])
++		netif_stop_queue(card->netdev[GELIC_PORT_WIRELESS]);
++}
++static void gelic_card_wake_queues(struct gelic_card *card)
++{
++	netif_wake_queue(card->netdev[GELIC_PORT_ETHERNET]);
++
++	if (card->netdev[GELIC_PORT_WIRELESS])
++		netif_wake_queue(card->netdev[GELIC_PORT_WIRELESS]);
++}
+ /**
+  * gelic_card_release_tx_chain - processes sent tx descriptors
+  * @card: adapter structure
+@@ -397,12 +472,14 @@ static void gelic_card_release_tx_chain(
+ {
+ 	struct gelic_descr_chain *tx_chain;
+ 	enum gelic_descr_dma_status status;
++	struct net_device *netdev;
+ 	int release = 0;
+ 
+ 	for (tx_chain = &card->tx_chain;
+ 	     tx_chain->head != tx_chain->tail && tx_chain->tail;
+ 	     tx_chain->tail = tx_chain->tail->next) {
+ 		status = gelic_descr_get_status(tx_chain->tail);
++		netdev = tx_chain->tail->skb->dev;
+ 		switch (status) {
+ 		case GELIC_DESCR_DMA_RESPONSE_ERROR:
+ 		case GELIC_DESCR_DMA_PROTECTION_ERROR:
+@@ -412,13 +489,13 @@ static void gelic_card_release_tx_chain(
+ 					 "%s: forcing end of tx descriptor " \
+ 					 "with status %x\n",
+ 					 __func__, status);
+-			card->netdev->stats.tx_dropped++;
++			netdev->stats.tx_dropped++;
+ 			break;
+ 
+ 		case GELIC_DESCR_DMA_COMPLETE:
+ 			if (tx_chain->tail->skb) {
+-				card->netdev->stats.tx_packets++;
+-				card->netdev->stats.tx_bytes +=
++				netdev->stats.tx_packets++;
++				netdev->stats.tx_bytes +=
+ 					tx_chain->tail->skb->len;
+ 			}
+ 			break;
+@@ -435,7 +512,7 @@ static void gelic_card_release_tx_chain(
+ 	}
+ out:
+ 	if (!stop && release)
+-		netif_wake_queue(card->netdev);
++		gelic_card_wake_queues(card);
+ }
+ 
+ /**
+@@ -446,9 +523,9 @@ out:
+  * netdev interface. It also sets up multicast, allmulti and promisc
+  * flags appropriately
+  */
+-static void gelic_net_set_multi(struct net_device *netdev)
++void gelic_net_set_multi(struct net_device *netdev)
+ {
+-	struct gelic_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_card(netdev);
+ 	struct dev_mc_list *mc;
+ 	unsigned int i;
+ 	uint8_t *p;
+@@ -470,8 +547,8 @@ static void gelic_net_set_multi(struct n
+ 			"lv1_net_add_multicast_address failed, %d\n",
+ 			status);
+ 
+-	if (netdev->flags & IFF_ALLMULTI
+-		|| netdev->mc_count > GELIC_NET_MC_COUNT_MAX) { /* list max */
++	if ((netdev->flags & IFF_ALLMULTI) ||
++	    (netdev->mc_count > GELIC_NET_MC_COUNT_MAX)) {
+ 		status = lv1_net_add_multicast_address(bus_id(card),
+ 						       dev_id(card),
+ 						       0, 1);
+@@ -482,7 +559,7 @@ static void gelic_net_set_multi(struct n
+ 		return;
+ 	}
+ 
+-	/* set multicast address */
++	/* set multicast addresses */
+ 	for (mc = netdev->mc_list; mc; mc = mc->next) {
+ 		addr = 0;
+ 		p = mc->dmi_addr;
+@@ -511,8 +588,19 @@ static inline void gelic_card_enable_rxd
+ {
+ 	int status;
+ 
++#ifdef DEBUG
++	if (gelic_descr_get_status(card->rx_chain.head) !=
++	    GELIC_DESCR_DMA_CARDOWNED) {
++		printk(KERN_ERR "%s: status=%x\n", __func__,
++		       be32_to_cpu(card->rx_chain.head->dmac_cmd_status));
++		printk(KERN_ERR "%s: nextphy=%x\n", __func__,
++		       be32_to_cpu(card->rx_chain.head->next_descr_addr));
++		printk(KERN_ERR "%s: head=%p\n", __func__,
++		       card->rx_chain.head);
++	}
++#endif
+ 	status = lv1_net_start_rx_dma(bus_id(card), dev_id(card),
+-				card->rx_chain.tail->bus_addr, 0);
++				card->rx_chain.head->bus_addr, 0);
+ 	if (status)
+ 		dev_info(ctodev(card),
+ 			 "lv1_net_start_rx_dma failed, status=%d\n", status);
+@@ -560,33 +648,19 @@ static inline void gelic_card_disable_tx
+  *
+  * always returns 0
+  */
+-static int gelic_net_stop(struct net_device *netdev)
++int gelic_net_stop(struct net_device *netdev)
+ {
+-	struct gelic_card *card = netdev_priv(netdev);
+-
+-	napi_disable(&card->napi);
+-	netif_stop_queue(netdev);
+-
+-	/* turn off DMA, force end */
+-	gelic_card_disable_rxdmac(card);
+-	gelic_card_disable_txdmac(card);
+-
+-	gelic_card_set_irq_mask(card, 0);
++	struct gelic_card *card;
+ 
+-	/* disconnect event port */
+-	free_irq(card->netdev->irq, card->netdev);
+-	ps3_sb_event_receive_port_destroy(card->dev, card->netdev->irq);
+-	card->netdev->irq = NO_IRQ;
++	pr_debug("%s: start\n", __func__);
+ 
++	netif_stop_queue(netdev);
+ 	netif_carrier_off(netdev);
+ 
+-	/* release chains */
+-	gelic_card_release_tx_chain(card, 1);
+-	gelic_card_release_rx_chain(card);
+-
+-	gelic_card_free_chain(card, card->tx_top);
+-	gelic_card_free_chain(card, card->rx_top);
++	card = netdev_card(netdev);
++	gelic_card_down(card);
+ 
++	pr_debug("%s: done\n", __func__);
+ 	return 0;
+ }
+ 
+@@ -612,7 +686,7 @@ gelic_card_get_next_tx_descr(struct geli
+ }
+ 
+ /**
+- * gelic_descr_set_tx_cmdstat - sets the tx descriptor command field
++ * gelic_net_set_txdescr_cmdstat - sets the tx descriptor command field
+  * @descr: descriptor structure to fill out
+  * @skb: packet to consider
+  *
+@@ -677,7 +751,7 @@ static inline struct sk_buff *gelic_put_
+ }
+ 
+ /**
+- * gelic_descr_prepare_tx - get dma address of skb_data
++ * gelic_descr_prepare_tx - setup a descriptor for sending packets
+  * @card: card structure
+  * @descr: descriptor structure
+  * @skb: packet to use
+@@ -691,10 +765,13 @@ static int gelic_descr_prepare_tx(struct
+ {
+ 	dma_addr_t buf;
+ 
+-	if (card->vlan_index != -1) {
++	if (card->vlan_required) {
+ 		struct sk_buff *skb_tmp;
++		enum gelic_port_type type;
++
++		type = netdev_port(skb->dev)->type;
+ 		skb_tmp = gelic_put_vlan_tag(skb,
+-					     card->vlan_id[card->vlan_index]);
++					     card->vlan[type].tx);
+ 		if (!skb_tmp)
+ 			return -ENOMEM;
+ 		skb = skb_tmp;
+@@ -753,14 +830,14 @@ static int gelic_card_kick_txdma(struct 
+  *
+  * returns 0 on success, <0 on failure
+  */
+-static int gelic_net_xmit(struct sk_buff *skb, struct net_device *netdev)
++int gelic_net_xmit(struct sk_buff *skb, struct net_device *netdev)
+ {
+-	struct gelic_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_card(netdev);
+ 	struct gelic_descr *descr;
+ 	int result;
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&card->tx_dma_lock, flags);
++	spin_lock_irqsave(&card->tx_lock, flags);
+ 
+ 	gelic_card_release_tx_chain(card, 0);
+ 
+@@ -769,8 +846,8 @@ static int gelic_net_xmit(struct sk_buff
+ 		/*
+ 		 * no more descriptors free
+ 		 */
+-		netif_stop_queue(netdev);
+-		spin_unlock_irqrestore(&card->tx_dma_lock, flags);
++		gelic_card_stop_queues(card);
++		spin_unlock_irqrestore(&card->tx_lock, flags);
+ 		return NETDEV_TX_BUSY;
+ 	}
+ 
+@@ -780,9 +857,9 @@ static int gelic_net_xmit(struct sk_buff
+ 		 * DMA map failed.  As chanses are that failure
+ 		 * would continue, just release skb and return
+ 		 */
+-		card->netdev->stats.tx_dropped++;
++		netdev->stats.tx_dropped++;
+ 		dev_kfree_skb_any(skb);
+-		spin_unlock_irqrestore(&card->tx_dma_lock, flags);
++		spin_unlock_irqrestore(&card->tx_lock, flags);
+ 		return NETDEV_TX_OK;
+ 	}
+ 	/*
+@@ -800,7 +877,7 @@ static int gelic_net_xmit(struct sk_buff
+ 		 * kick failed.
+ 		 * release descriptors which were just prepared
+ 		 */
+-		card->netdev->stats.tx_dropped++;
++		netdev->stats.tx_dropped++;
+ 		gelic_descr_release_tx(card, descr);
+ 		gelic_descr_release_tx(card, descr->next);
+ 		card->tx_chain.tail = descr->next->next;
+@@ -810,7 +887,7 @@ static int gelic_net_xmit(struct sk_buff
+ 		netdev->trans_start = jiffies;
+ 	}
+ 
+-	spin_unlock_irqrestore(&card->tx_dma_lock, flags);
++	spin_unlock_irqrestore(&card->tx_lock, flags);
+ 	return NETDEV_TX_OK;
+ }
+ 
+@@ -818,27 +895,27 @@ static int gelic_net_xmit(struct sk_buff
+  * gelic_net_pass_skb_up - takes an skb from a descriptor and passes it on
+  * @descr: descriptor to process
+  * @card: card structure
++ * @netdev: net_device structure to be passed packet
+  *
+  * iommu-unmaps the skb, fills out skb structure and passes the data to the
+  * stack. The descriptor state is not changed.
+  */
+ static void gelic_net_pass_skb_up(struct gelic_descr *descr,
+-				 struct gelic_card *card)
++				  struct gelic_card *card,
++				  struct net_device *netdev)
++
+ {
+-	struct sk_buff *skb;
+-	struct net_device *netdev;
++	struct sk_buff *skb = descr->skb;
+ 	u32 data_status, data_error;
+ 
+ 	data_status = be32_to_cpu(descr->data_status);
+ 	data_error = be32_to_cpu(descr->data_error);
+-	netdev = card->netdev;
+ 	/* unmap skb buffer */
+-	skb = descr->skb;
+-	dma_unmap_single(ctodev(card),
+-			 be32_to_cpu(descr->buf_addr), GELIC_NET_MAX_MTU,
++	dma_unmap_single(ctodev(card), be32_to_cpu(descr->buf_addr),
++			 GELIC_NET_MAX_MTU,
+ 			 DMA_FROM_DEVICE);
+ 
+-	skb_put(skb, descr->valid_size ?
++	skb_put(skb, be32_to_cpu(descr->valid_size)?
+ 		be32_to_cpu(descr->valid_size) :
+ 		be32_to_cpu(descr->result_size));
+ 	if (!descr->valid_size)
+@@ -866,8 +943,8 @@ static void gelic_net_pass_skb_up(struct
+ 		skb->ip_summed = CHECKSUM_NONE;
+ 
+ 	/* update netdevice statistics */
+-	card->netdev->stats.rx_packets++;
+-	card->netdev->stats.rx_bytes += skb->len;
++	netdev->stats.rx_packets++;
++	netdev->stats.rx_bytes += skb->len;
+ 
+ 	/* pass skb up to stack */
+ 	netif_receive_skb(skb);
+@@ -886,7 +963,8 @@ static int gelic_card_decode_one_descr(s
+ {
+ 	enum gelic_descr_dma_status status;
+ 	struct gelic_descr_chain *chain = &card->rx_chain;
+-	struct gelic_descr *descr = chain->tail;
++	struct gelic_descr *descr = chain->head;
++	struct net_device *netdev = NULL;
+ 	int dmac_chain_ended;
+ 
+ 	status = gelic_descr_get_status(descr);
+@@ -903,12 +981,30 @@ static int gelic_card_decode_one_descr(s
+ 		return 0;
+ 	}
+ 
++	/* netdevice select */
++	if (card->vlan_required) {
++		unsigned int i;
++		u16 vid;
++		vid = *(u16 *)(descr->skb->data) & VLAN_VID_MASK;
++		for (i = 0; i < GELIC_PORT_MAX; i++) {
++			if (card->vlan[i].rx == vid) {
++				netdev = card->netdev[i];
++				break;
++			}
++		};
++		if (GELIC_PORT_MAX <= i) {
++			pr_info("%s: unknown packet vid=%x\n", __func__, vid);
++			goto refill;
++		}
++	} else
++		netdev = card->netdev[GELIC_PORT_ETHERNET];
++
+ 	if ((status == GELIC_DESCR_DMA_RESPONSE_ERROR) ||
+ 	    (status == GELIC_DESCR_DMA_PROTECTION_ERROR) ||
+ 	    (status == GELIC_DESCR_DMA_FORCE_END)) {
+ 		dev_info(ctodev(card), "dropping RX descriptor with state %x\n",
+ 			 status);
+-		card->netdev->stats.rx_dropped++;
++		netdev->stats.rx_dropped++;
+ 		goto refill;
+ 	}
+ 
+@@ -936,7 +1032,7 @@ static int gelic_card_decode_one_descr(s
+ 	}
+ 
+ 	/* ok, we've got a packet in descr */
+-	gelic_net_pass_skb_up(descr, card);
++	gelic_net_pass_skb_up(descr, card, netdev);
+ refill:
+ 	/*
+ 	 * So that always DMAC can see the end
+@@ -954,8 +1050,8 @@ refill:
+ 	 */
+ 	gelic_descr_prepare_rx(card, descr);
+ 
+-	chain->head = descr;
+-	chain->tail = descr->next;
++	chain->tail = descr;
++	chain->head = descr->next;
+ 
+ 	/*
+ 	 * Set this descriptor the end of the chain.
+@@ -976,17 +1072,15 @@ refill:
+ 
+ /**
+  * gelic_net_poll - NAPI poll function called by the stack to return packets
+- * @netdev: interface device structure
++ * @napi: napi structure
+  * @budget: number of packets we can pass to the stack at most
+  *
+- * returns 0 if no more packets available to the driver/stack. Returns 1,
+- * if the quota is exceeded, but the driver has still packets.
++ * returns the number of the processed packets
+  *
+  */
+ static int gelic_net_poll(struct napi_struct *napi, int budget)
+ {
+ 	struct gelic_card *card = container_of(napi, struct gelic_card, napi);
+-	struct net_device *netdev = card->netdev;
+ 	int packets_done = 0;
+ 
+ 	while (packets_done < budget) {
+@@ -997,7 +1091,7 @@ static int gelic_net_poll(struct napi_st
+ 	}
+ 
+ 	if (packets_done < budget) {
+-		netif_rx_complete(netdev, napi);
++		napi_complete(napi);
+ 		gelic_card_rx_irq_on(card);
+ 	}
+ 	return packets_done;
+@@ -1009,7 +1103,7 @@ static int gelic_net_poll(struct napi_st
+  *
+  * returns 0 on success, <0 on failure
+  */
+-static int gelic_net_change_mtu(struct net_device *netdev, int new_mtu)
++int gelic_net_change_mtu(struct net_device *netdev, int new_mtu)
+ {
+ 	/* no need to re-alloc skbs or so -- the max mtu is about 2.3k
+ 	 * and mtu is outbound only anyway */
+@@ -1027,8 +1121,7 @@ static int gelic_net_change_mtu(struct n
+ static irqreturn_t gelic_card_interrupt(int irq, void *ptr)
+ {
+ 	unsigned long flags;
+-	struct net_device *netdev = ptr;
+-	struct gelic_card *card = netdev_priv(netdev);
++	struct gelic_card *card = ptr;
+ 	u64 status;
+ 
+ 	status = card->irq_status;
+@@ -1036,6 +1129,8 @@ static irqreturn_t gelic_card_interrupt(
+ 	if (!status)
+ 		return IRQ_NONE;
+ 
++	status &= card->irq_mask;
++
+ 	if (card->rx_dma_restart_required) {
+ 		card->rx_dma_restart_required = 0;
+ 		gelic_card_enable_rxdmac(card);
+@@ -1043,21 +1138,22 @@ static irqreturn_t gelic_card_interrupt(
+ 
+ 	if (status & GELIC_CARD_RXINT) {
+ 		gelic_card_rx_irq_off(card);
+-		netif_rx_schedule(netdev, &card->napi);
++		napi_schedule(&card->napi);
+ 	}
+ 
+ 	if (status & GELIC_CARD_TXINT) {
+-		spin_lock_irqsave(&card->tx_dma_lock, flags);
++		spin_lock_irqsave(&card->tx_lock, flags);
+ 		card->tx_dma_progress = 0;
+ 		gelic_card_release_tx_chain(card, 0);
+ 		/* kick outstanding tx descriptor if any */
+ 		gelic_card_kick_txdma(card, card->tx_chain.tail);
+-		spin_unlock_irqrestore(&card->tx_dma_lock, flags);
++		spin_unlock_irqrestore(&card->tx_lock, flags);
+ 	}
+ 
+ 	/* ether port status changed */
+ 	if (status & GELIC_CARD_PORT_STATUS_CHANGED)
+ 		gelic_card_get_ether_port_status(card, 1);
++
+ 	return IRQ_HANDLED;
+ }
+ 
+@@ -1068,55 +1164,17 @@ static irqreturn_t gelic_card_interrupt(
+  *
+  * see Documentation/networking/netconsole.txt
+  */
+-static void gelic_net_poll_controller(struct net_device *netdev)
++void gelic_net_poll_controller(struct net_device *netdev)
+ {
+-	struct gelic_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_card(netdev);
+ 
+ 	gelic_card_set_irq_mask(card, 0);
+ 	gelic_card_interrupt(netdev->irq, netdev);
+-	gelic_card_set_irq_mask(card, card->ghiintmask);
++	gelic_card_set_irq_mask(card, card->irq_mask);
+ }
+ #endif /* CONFIG_NET_POLL_CONTROLLER */
+ 
+ /**
+- * gelic_card_open - open device and map dma region
+- * @card: card structure
+- */
+-static int gelic_card_open(struct gelic_card *card)
+-{
+-	int result;
+-
+-	result = ps3_sb_event_receive_port_setup(card->dev, PS3_BINDING_CPU_ANY,
+-		&card->netdev->irq);
+-
+-	if (result) {
+-		dev_info(ctodev(card),
+-			 "%s:%d: recieve_port_setup failed (%d)\n",
+-			 __func__, __LINE__, result);
+-		result = -EPERM;
+-		goto fail_alloc_irq;
+-	}
+-
+-	result = request_irq(card->netdev->irq, gelic_card_interrupt,
+-			     IRQF_DISABLED, card->netdev->name, card->netdev);
+-
+-	if (result) {
+-		dev_info(ctodev(card), "%s:%d: request_irq failed (%d)\n",
+-			__func__, __LINE__, result);
+-		goto fail_request_irq;
+-	}
+-
+-	return 0;
+-
+-fail_request_irq:
+-	ps3_sb_event_receive_port_destroy(card->dev, card->netdev->irq);
+-	card->netdev->irq = NO_IRQ;
+-fail_alloc_irq:
+-	return result;
+-}
+-
+-
+-/**
+  * gelic_net_open - called upon ifonfig up
+  * @netdev: interface device structure
+  *
+@@ -1125,56 +1183,23 @@ fail_alloc_irq:
+  * gelic_net_open allocates all the descriptors and memory needed for
+  * operation, sets up multicast list and enables interrupts
+  */
+-static int gelic_net_open(struct net_device *netdev)
++int gelic_net_open(struct net_device *netdev)
+ {
+-	struct gelic_card *card = netdev_priv(netdev);
+-
+-	dev_dbg(ctodev(card), " -> %s:%d\n", __func__, __LINE__);
++	struct gelic_card *card = netdev_card(netdev);
+ 
+-	gelic_card_open(card);
++	dev_dbg(ctodev(card), " -> %s %p\n", __func__, netdev);
+ 
+-	if (gelic_card_init_chain(card, &card->tx_chain,
+-				  card->descr, GELIC_NET_TX_DESCRIPTORS))
+-		goto alloc_tx_failed;
+-	if (gelic_card_init_chain(card, &card->rx_chain,
+-				  card->descr + GELIC_NET_TX_DESCRIPTORS,
+-				  GELIC_NET_RX_DESCRIPTORS))
+-		goto alloc_rx_failed;
+-
+-	/* head of chain */
+-	card->tx_top = card->tx_chain.head;
+-	card->rx_top = card->rx_chain.head;
+-	dev_dbg(ctodev(card), "descr rx %p, tx %p, size %#lx, num %#x\n",
+-		card->rx_top, card->tx_top, sizeof(struct gelic_descr),
+-		GELIC_NET_RX_DESCRIPTORS);
+-	/* allocate rx skbs */
+-	if (gelic_card_alloc_rx_skbs(card))
+-		goto alloc_skbs_failed;
+-
+-	napi_enable(&card->napi);
+-
+-	card->tx_dma_progress = 0;
+-	card->ghiintmask = GELIC_CARD_RXINT | GELIC_CARD_TXINT |
+-		GELIC_CARD_PORT_STATUS_CHANGED;
+-
+-	gelic_card_set_irq_mask(card, card->ghiintmask);
+-	gelic_card_enable_rxdmac(card);
++	gelic_card_up(card);
+ 
+ 	netif_start_queue(netdev);
+ 	gelic_card_get_ether_port_status(card, 1);
+ 
++	dev_dbg(ctodev(card), " <- %s\n", __func__);
+ 	return 0;
+-
+-alloc_skbs_failed:
+-	gelic_card_free_chain(card, card->rx_top);
+-alloc_rx_failed:
+-	gelic_card_free_chain(card, card->tx_top);
+-alloc_tx_failed:
+-	return -ENOMEM;
+ }
+ 
+-static void gelic_net_get_drvinfo(struct net_device *netdev,
+-				  struct ethtool_drvinfo *info)
++void gelic_net_get_drvinfo(struct net_device *netdev,
++			   struct ethtool_drvinfo *info)
+ {
+ 	strncpy(info->driver, DRV_NAME, sizeof(info->driver) - 1);
+ 	strncpy(info->version, DRV_VERSION, sizeof(info->version) - 1);
+@@ -1183,7 +1208,7 @@ static void gelic_net_get_drvinfo(struct
+ static int gelic_ether_get_settings(struct net_device *netdev,
+ 				    struct ethtool_cmd *cmd)
+ {
+-	struct gelic_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_card(netdev);
+ 
+ 	gelic_card_get_ether_port_status(card, 0);
+ 
+@@ -1219,35 +1244,25 @@ static int gelic_ether_get_settings(stru
+ 	return 0;
+ }
+ 
+-static int gelic_net_nway_reset(struct net_device *netdev)
++u32 gelic_net_get_rx_csum(struct net_device *netdev)
+ {
+-	if (netif_running(netdev)) {
+-		gelic_net_stop(netdev);
+-		gelic_net_open(netdev);
+-	}
+-	return 0;
+-}
+-
+-static u32 gelic_net_get_rx_csum(struct net_device *netdev)
+-{
+-	struct gelic_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_card(netdev);
+ 
+ 	return card->rx_csum;
+ }
+ 
+-static int gelic_net_set_rx_csum(struct net_device *netdev, u32 data)
++int gelic_net_set_rx_csum(struct net_device *netdev, u32 data)
+ {
+-	struct gelic_card *card = netdev_priv(netdev);
++	struct gelic_card *card = netdev_card(netdev);
+ 
+ 	card->rx_csum = data;
+ 	return 0;
+ }
+ 
+-static struct ethtool_ops gelic_net_ethtool_ops = {
++static struct ethtool_ops gelic_ether_ethtool_ops = {
+ 	.get_drvinfo	= gelic_net_get_drvinfo,
+ 	.get_settings	= gelic_ether_get_settings,
+ 	.get_link	= ethtool_op_get_link,
+-	.nway_reset	= gelic_net_nway_reset,
+ 	.get_tx_csum	= ethtool_op_get_tx_csum,
+ 	.set_tx_csum	= ethtool_op_set_tx_csum,
+ 	.get_rx_csum	= gelic_net_get_rx_csum,
+@@ -1265,7 +1280,7 @@ static void gelic_net_tx_timeout_task(st
+ {
+ 	struct gelic_card *card =
+ 		container_of(work, struct gelic_card, tx_timeout_task);
+-	struct net_device *netdev = card->netdev;
++	struct net_device *netdev = card->netdev[GELIC_PORT_ETHERNET];
+ 
+ 	dev_info(ctodev(card), "%s:Timed out. Restarting... \n", __func__);
+ 
+@@ -1288,11 +1303,11 @@ out:
+  *
+  * called, if tx hangs. Schedules a task that resets the interface
+  */
+-static void gelic_net_tx_timeout(struct net_device *netdev)
++void gelic_net_tx_timeout(struct net_device *netdev)
+ {
+ 	struct gelic_card *card;
+ 
+-	card = netdev_priv(netdev);
++	card = netdev_card(netdev);
+ 	atomic_inc(&card->tx_timeout_task_counter);
+ 	if (netdev->flags & IFF_UP)
+ 		schedule_work(&card->tx_timeout_task);
+@@ -1306,7 +1321,8 @@ static void gelic_net_tx_timeout(struct 
+  *
+  * fills out function pointers in the net_device structure
+  */
+-static void gelic_ether_setup_netdev_ops(struct net_device *netdev)
++static void gelic_ether_setup_netdev_ops(struct net_device *netdev,
++					 struct napi_struct *napi)
+ {
+ 	netdev->open = &gelic_net_open;
+ 	netdev->stop = &gelic_net_stop;
+@@ -1316,86 +1332,63 @@ static void gelic_ether_setup_netdev_ops
+ 	/* tx watchdog */
+ 	netdev->tx_timeout = &gelic_net_tx_timeout;
+ 	netdev->watchdog_timeo = GELIC_NET_WATCHDOG_TIMEOUT;
+-	netdev->ethtool_ops = &gelic_net_ethtool_ops;
++	/* NAPI */
++	netif_napi_add(netdev, napi,
++		       gelic_net_poll, GELIC_NET_NAPI_WEIGHT);
++	netdev->ethtool_ops = &gelic_ether_ethtool_ops;
++#ifdef CONFIG_NET_POLL_CONTROLLER
++	netdev->poll_controller = gelic_net_poll_controller;
++#endif
+ }
+ 
+ /**
+- * gelic_net_setup_netdev - initialization of net_device
++ * gelic_ether_setup_netdev - initialization of net_device
++ * @netdev: net_device structure
+  * @card: card structure
+  *
+  * Returns 0 on success or <0 on failure
+  *
+- * gelic_net_setup_netdev initializes the net_device structure
++ * gelic_ether_setup_netdev initializes the net_device structure
++ * and register it.
+  **/
+-static int gelic_net_setup_netdev(struct gelic_card *card)
++int gelic_net_setup_netdev(struct net_device *netdev, struct gelic_card *card)
+ {
+-	struct net_device *netdev = card->netdev;
+-	struct sockaddr addr;
+-	unsigned int i;
+ 	int status;
+ 	u64 v1, v2;
+ 	DECLARE_MAC_BUF(mac);
+ 
+-	SET_NETDEV_DEV(netdev, &card->dev->core);
+-	spin_lock_init(&card->tx_dma_lock);
+-
+-	card->rx_csum = GELIC_NET_RX_CSUM_DEFAULT;
+-
+-	gelic_ether_setup_netdev_ops(netdev);
+-
+-	netif_napi_add(netdev, &card->napi,
+-		       gelic_net_poll, GELIC_NET_NAPI_WEIGHT);
+-
+ 	netdev->features = NETIF_F_IP_CSUM;
+ 
+ 	status = lv1_net_control(bus_id(card), dev_id(card),
+ 				 GELIC_LV1_GET_MAC_ADDRESS,
+ 				 0, 0, 0, &v1, &v2);
++	v1 <<= 16;
+ 	if (status || !is_valid_ether_addr((u8 *)&v1)) {
+ 		dev_info(ctodev(card),
+ 			 "%s:lv1_net_control GET_MAC_ADDR failed %d\n",
+ 			 __func__, status);
+ 		return -EINVAL;
+ 	}
+-	v1 <<= 16;
+-	memcpy(addr.sa_data, &v1, ETH_ALEN);
+-	memcpy(netdev->dev_addr, addr.sa_data, ETH_ALEN);
+-	dev_info(ctodev(card), "MAC addr %s\n",
+-		 print_mac(mac, netdev->dev_addr));
+-
+-	card->vlan_index = -1;	/* no vlan */
+-	for (i = 0; i < GELIC_NET_VLAN_MAX; i++) {
+-		status = lv1_net_control(bus_id(card), dev_id(card),
+-					GELIC_LV1_GET_VLAN_ID,
+-					i + 1, /* index; one based */
+-					0, 0, &v1, &v2);
+-		if (status == LV1_NO_ENTRY) {
+-			dev_dbg(ctodev(card),
+-				"GELIC_VLAN_ID no entry:%d, VLAN disabled\n",
+-				status);
+-			card->vlan_id[i] = 0;
+-		} else if (status) {
+-			dev_dbg(ctodev(card),
+-				"%s:get vlan id faild, status=%d\n",
+-				__func__, status);
+-			card->vlan_id[i] = 0;
+-		} else {
+-			card->vlan_id[i] = (u32)v1;
+-			dev_dbg(ctodev(card), "vlan_id:%d, %lx\n", i, v1);
+-		}
+-	}
++	memcpy(netdev->dev_addr, &v1, ETH_ALEN);
+ 
+-	if (card->vlan_id[GELIC_LV1_VLAN_TX_ETHERNET - 1]) {
+-		card->vlan_index = GELIC_LV1_VLAN_TX_ETHERNET - 1;
++	if (card->vlan_required) {
+ 		netdev->hard_header_len += VLAN_HLEN;
++		/*
++		 * As vlan is internally used,
++		 * we can not receive vlan packets
++		 */
++		netdev->features |= NETIF_F_VLAN_CHALLENGED;
+ 	}
+ 
+ 	status = register_netdev(netdev);
+ 	if (status) {
+-		dev_err(ctodev(card), "%s:Couldn't register net_device: %d\n",
+-			__func__, status);
++		dev_err(ctodev(card), "%s:Couldn't register %s %d\n",
++			__func__, netdev->name, status);
+ 		return status;
+ 	}
++	dev_info(ctodev(card), "%s: MAC addr %s\n",
++		 netdev->name,
++		 print_mac(mac, netdev->dev_addr));
+ 
+ 	return 0;
+ }
+@@ -1407,72 +1400,171 @@ static int gelic_net_setup_netdev(struct
+  *
+  * the card and net_device structures are linked to each other
+  */
+-static struct gelic_card *gelic_alloc_card_net(void)
++#define GELIC_ALIGN (32)
++static struct gelic_card *gelic_alloc_card_net(struct net_device **netdev)
+ {
+-	struct net_device *netdev;
+ 	struct gelic_card *card;
++	struct gelic_port *port;
++	void *p;
+ 	size_t alloc_size;
+-
+-	alloc_size = sizeof(*card) +
+-		sizeof(struct gelic_descr) * GELIC_NET_RX_DESCRIPTORS +
+-		sizeof(struct gelic_descr) * GELIC_NET_TX_DESCRIPTORS;
+ 	/*
+-	 * we assume private data is allocated 32 bytes (or more) aligned
+-	 * so that gelic_descr should be 32 bytes aligned.
+-	 * Current alloc_etherdev() does do it because NETDEV_ALIGN
+-	 * is 32.
+-	 * check this assumption here.
++	 * gelic requires dma descriptor is 32 bytes aligned and
++	 * the hypervisor requires irq_status is 8 bytes aligned.
+ 	 */
+-	BUILD_BUG_ON(NETDEV_ALIGN < 32);
+ 	BUILD_BUG_ON(offsetof(struct gelic_card, irq_status) % 8);
+ 	BUILD_BUG_ON(offsetof(struct gelic_card, descr) % 32);
++	alloc_size =
++		sizeof(struct gelic_card) +
++		sizeof(struct gelic_descr) * GELIC_NET_RX_DESCRIPTORS +
++		sizeof(struct gelic_descr) * GELIC_NET_TX_DESCRIPTORS +
++		GELIC_ALIGN - 1;
+ 
+-	netdev = alloc_etherdev(alloc_size);
+-	if (!netdev)
++	p  = kzalloc(alloc_size, GFP_KERNEL);
++	if (!p)
+ 		return NULL;
++	card = PTR_ALIGN(p, GELIC_ALIGN);
++	card->unalign = p;
++
++	/*
++	 * alloc netdev
++	 */
++	*netdev = alloc_etherdev(sizeof(struct gelic_port));
++	if (!netdev) {
++		kfree(card->unalign);
++		return NULL;
++	}
++	port = netdev_priv(*netdev);
++
++	/* gelic_port */
++	port->netdev = *netdev;
++	port->card = card;
++	port->type = GELIC_PORT_ETHERNET;
++
++	/* gelic_card */
++	card->netdev[GELIC_PORT_ETHERNET] = *netdev;
+ 
+-	card = netdev_priv(netdev);
+-	card->netdev = netdev;
+ 	INIT_WORK(&card->tx_timeout_task, gelic_net_tx_timeout_task);
+ 	init_waitqueue_head(&card->waitq);
+ 	atomic_set(&card->tx_timeout_task_counter, 0);
++	init_MUTEX(&card->updown_lock);
++	atomic_set(&card->users, 0);
+ 
+ 	return card;
+ }
+ 
++static void gelic_card_get_vlan_info(struct gelic_card *card)
++{
++	u64 v1, v2;
++	int status;
++	unsigned int i;
++	struct {
++		int tx;
++		int rx;
++	} vlan_id_ix[2] = {
++		[GELIC_PORT_ETHERNET] = {
++			.tx = GELIC_LV1_VLAN_TX_ETHERNET,
++			.rx = GELIC_LV1_VLAN_RX_ETHERNET
++		},
++		[GELIC_PORT_WIRELESS] = {
++			.tx = GELIC_LV1_VLAN_TX_WIRELESS,
++			.rx = GELIC_LV1_VLAN_RX_WIRELESS
++		}
++	};
++
++	for (i = 0; i < ARRAY_SIZE(vlan_id_ix); i++) {
++		/* tx tag */
++		status = lv1_net_control(bus_id(card), dev_id(card),
++					 GELIC_LV1_GET_VLAN_ID,
++					 vlan_id_ix[i].tx,
++					 0, 0, &v1, &v2);
++		if (status || !v1) {
++			if (status != LV1_NO_ENTRY)
++				dev_dbg(ctodev(card),
++					"get vlan id for tx(%d) failed(%d)\n",
++					vlan_id_ix[i].tx, status);
++			card->vlan[i].tx = 0;
++			card->vlan[i].rx = 0;
++			continue;
++		}
++		card->vlan[i].tx = (u16)v1;
++
++		/* rx tag */
++		status = lv1_net_control(bus_id(card), dev_id(card),
++					 GELIC_LV1_GET_VLAN_ID,
++					 vlan_id_ix[i].rx,
++					 0, 0, &v1, &v2);
++		if (status || !v1) {
++			if (status != LV1_NO_ENTRY)
++				dev_info(ctodev(card),
++					 "get vlan id for rx(%d) failed(%d)\n",
++					 vlan_id_ix[i].rx, status);
++			card->vlan[i].tx = 0;
++			card->vlan[i].rx = 0;
++			continue;
++		}
++		card->vlan[i].rx = (u16)v1;
++
++		dev_dbg(ctodev(card), "vlan_id[%d] tx=%02x rx=%02x\n",
++			i, card->vlan[i].tx, card->vlan[i].rx);
++	}
++
++	if (card->vlan[GELIC_PORT_ETHERNET].tx) {
++		BUG_ON(!card->vlan[GELIC_PORT_WIRELESS].tx);
++		card->vlan_required = 1;
++	} else
++		card->vlan_required = 0;
++
++	/* check wirelss capable firmware */
++	if (ps3_compare_firmware_version(1, 6, 0) < 0) {
++		card->vlan[GELIC_PORT_WIRELESS].tx = 0;
++		card->vlan[GELIC_PORT_WIRELESS].rx = 0;
++	}
++
++	dev_info(ctodev(card), "internal vlan %s\n",
++		 card->vlan_required? "enabled" : "disabled");
++}
+ /**
+  * ps3_gelic_driver_probe - add a device to the control of this driver
+  */
+ static int ps3_gelic_driver_probe(struct ps3_system_bus_device *dev)
+ {
+-	struct gelic_card *card = gelic_alloc_card_net();
++	struct gelic_card *card;
++	struct net_device *netdev;
+ 	int result;
+ 
+-	if (!card) {
+-		dev_info(&dev->core, "gelic_net_alloc_card failed\n");
+-		result = -ENOMEM;
+-		goto fail_alloc_card;
+-	}
+-
+-	ps3_system_bus_set_driver_data(dev, card);
+-	card->dev = dev;
+-
++	pr_debug("%s: called\n", __func__);
+ 	result = ps3_open_hv_device(dev);
+ 
+ 	if (result) {
+-		dev_dbg(&dev->core, "ps3_open_hv_device failed\n");
++		dev_dbg(&dev->core, "%s:ps3_open_hv_device failed\n",
++			__func__);
+ 		goto fail_open;
+ 	}
+ 
+ 	result = ps3_dma_region_create(dev->d_region);
+ 
+ 	if (result) {
+-		dev_dbg(&dev->core, "ps3_dma_region_create failed(%d)\n",
+-			result);
++		dev_dbg(&dev->core, "%s:ps3_dma_region_create failed(%d)\n",
++			__func__, result);
+ 		BUG_ON("check region type");
+ 		goto fail_dma_region;
+ 	}
+ 
++	/* alloc card/netdevice */
++	card = gelic_alloc_card_net(&netdev);
++	if (!card) {
++		dev_info(&dev->core, "%s:gelic_net_alloc_card failed\n",
++			 __func__);
++		result = -ENOMEM;
++		goto fail_alloc_card;
++	}
++	ps3_system_bus_set_driver_data(dev, card);
++	card->dev = dev;
++
++	/* get internal vlan info */
++	gelic_card_get_vlan_info(card);
++
++	/* setup interrupt */
+ 	result = lv1_net_set_interrupt_status_indicator(bus_id(card),
+ 							dev_id(card),
+ 		ps3_mm_phys_to_lpar(__pa(&card->irq_status)),
+@@ -1480,34 +1572,95 @@ static int ps3_gelic_driver_probe(struct
+ 
+ 	if (result) {
+ 		dev_dbg(&dev->core,
+-			"lv1_net_set_interrupt_status_indicator failed: %s\n",
+-			ps3_result(result));
++			"%s:set_interrupt_status_indicator failed: %s\n",
++			__func__, ps3_result(result));
+ 		result = -EIO;
+ 		goto fail_status_indicator;
+ 	}
+ 
+-	result = gelic_net_setup_netdev(card);
++	result = ps3_sb_event_receive_port_setup(dev, PS3_BINDING_CPU_ANY,
++		&card->irq);
++
++	if (result) {
++		dev_info(ctodev(card),
++			 "%s:gelic_net_open_device failed (%d)\n",
++			 __func__, result);
++		result = -EPERM;
++		goto fail_alloc_irq;
++	}
++	result = request_irq(card->irq, gelic_card_interrupt,
++			     IRQF_DISABLED, netdev->name, card);
++
++	if (result) {
++		dev_info(ctodev(card), "%s:request_irq failed (%d)\n",
++			__func__, result);
++		goto fail_request_irq;
++	}
++
++	/* setup card structure */
++	card->irq_mask = GELIC_CARD_RXINT | GELIC_CARD_TXINT |
++		GELIC_CARD_PORT_STATUS_CHANGED;
++	card->rx_csum = GELIC_CARD_RX_CSUM_DEFAULT;
++
++
++	if (gelic_card_init_chain(card, &card->tx_chain,
++			card->descr, GELIC_NET_TX_DESCRIPTORS))
++		goto fail_alloc_tx;
++	if (gelic_card_init_chain(card, &card->rx_chain,
++				 card->descr + GELIC_NET_TX_DESCRIPTORS,
++				 GELIC_NET_RX_DESCRIPTORS))
++		goto fail_alloc_rx;
++
++	/* head of chain */
++	card->tx_top = card->tx_chain.head;
++	card->rx_top = card->rx_chain.head;
++	dev_dbg(ctodev(card), "descr rx %p, tx %p, size %#lx, num %#x\n",
++		card->rx_top, card->tx_top, sizeof(struct gelic_descr),
++		GELIC_NET_RX_DESCRIPTORS);
++	/* allocate rx skbs */
++	if (gelic_card_alloc_rx_skbs(card))
++		goto fail_alloc_skbs;
++
++	spin_lock_init(&card->tx_lock);
++	card->tx_dma_progress = 0;
+ 
++	/* setup net_device structure */
++	netdev->irq = card->irq;
++	SET_NETDEV_DEV(netdev, &card->dev->core);
++	gelic_ether_setup_netdev_ops(netdev, &card->napi);
++	result = gelic_net_setup_netdev(netdev, card);
+ 	if (result) {
+-		dev_dbg(&dev->core, "%s:%d: ps3_dma_region_create failed: "
+-			"(%d)\n", __func__, __LINE__, result);
++		dev_dbg(&dev->core, "%s: setup_netdev failed %d",
++			__func__, result);
+ 		goto fail_setup_netdev;
+ 	}
+ 
++	pr_debug("%s: done\n", __func__);
+ 	return 0;
+ 
+ fail_setup_netdev:
++fail_alloc_skbs:
++	gelic_card_free_chain(card, card->rx_chain.head);
++fail_alloc_rx:
++	gelic_card_free_chain(card, card->tx_chain.head);
++fail_alloc_tx:
++	free_irq(card->irq, card);
++	netdev->irq = NO_IRQ;
++fail_request_irq:
++	ps3_sb_event_receive_port_destroy(dev, card->irq);
++fail_alloc_irq:
+ 	lv1_net_set_interrupt_status_indicator(bus_id(card),
+-					       dev_id(card),
+-					       0 , 0);
++					       bus_id(card),
++					       0, 0);
+ fail_status_indicator:
++	ps3_system_bus_set_driver_data(dev, NULL);
++	kfree(netdev_card(netdev)->unalign);
++	free_netdev(netdev);
++fail_alloc_card:
+ 	ps3_dma_region_free(dev->d_region);
+ fail_dma_region:
+ 	ps3_close_hv_device(dev);
+ fail_open:
+-	ps3_system_bus_set_driver_data(dev, NULL);
+-	free_netdev(card->netdev);
+-fail_alloc_card:
+ 	return result;
+ }
+ 
+@@ -1518,6 +1671,28 @@ fail_alloc_card:
+ static int ps3_gelic_driver_remove(struct ps3_system_bus_device *dev)
+ {
+ 	struct gelic_card *card = ps3_system_bus_get_driver_data(dev);
++	struct net_device *netdev0;
++	pr_debug("%s: called\n", __func__);
++
++	/* stop interrupt */
++	gelic_card_set_irq_mask(card, 0);
++
++	/* turn off DMA, force end */
++	gelic_card_disable_rxdmac(card);
++	gelic_card_disable_txdmac(card);
++
++	/* release chains */
++	gelic_card_release_tx_chain(card, 1);
++	gelic_card_release_rx_chain(card);
++
++	gelic_card_free_chain(card, card->tx_top);
++	gelic_card_free_chain(card, card->rx_top);
++
++	netdev0 = card->netdev[GELIC_PORT_ETHERNET];
++	/* disconnect event port */
++	free_irq(card->irq, card);
++	netdev0->irq = NO_IRQ;
++	ps3_sb_event_receive_port_destroy(card->dev, card->irq);
+ 
+ 	wait_event(card->waitq,
+ 		   atomic_read(&card->tx_timeout_task_counter) == 0);
+@@ -1525,8 +1700,9 @@ static int ps3_gelic_driver_remove(struc
+ 	lv1_net_set_interrupt_status_indicator(bus_id(card), dev_id(card),
+ 					       0 , 0);
+ 
+-	unregister_netdev(card->netdev);
+-	free_netdev(card->netdev);
++	unregister_netdev(netdev0);
++	kfree(netdev_card(netdev0)->unalign);
++	free_netdev(netdev0);
+ 
+ 	ps3_system_bus_set_driver_data(dev, NULL);
+ 
+@@ -1534,6 +1710,7 @@ static int ps3_gelic_driver_remove(struc
+ 
+ 	ps3_close_hv_device(dev);
+ 
++	pr_debug("%s: done\n", __func__);
+ 	return 0;
+ }
+ 
+--- a/drivers/net/ps3_gelic_net.h
++++ b/drivers/net/ps3_gelic_net.h
+@@ -35,12 +35,11 @@
+ #define GELIC_NET_MAX_MTU               VLAN_ETH_FRAME_LEN
+ #define GELIC_NET_MIN_MTU               VLAN_ETH_ZLEN
+ #define GELIC_NET_RXBUF_ALIGN           128
+-#define GELIC_NET_RX_CSUM_DEFAULT       1 /* hw chksum */
++#define GELIC_CARD_RX_CSUM_DEFAULT      1 /* hw chksum */
+ #define GELIC_NET_WATCHDOG_TIMEOUT      5*HZ
+ #define GELIC_NET_NAPI_WEIGHT           (GELIC_NET_RX_DESCRIPTORS)
+ #define GELIC_NET_BROADCAST_ADDR        0xffffffffffffL
+-#define GELIC_NET_VLAN_POS              (VLAN_ETH_ALEN * 2)
+-#define GELIC_NET_VLAN_MAX              4
++
+ #define GELIC_NET_MC_COUNT_MAX          32 /* multicast address list */
+ 
+ /* virtual interrupt status register bits */
+@@ -206,6 +205,13 @@ enum gelic_lv1_vlan_index {
+ 
+ /* size of hardware part of gelic descriptor */
+ #define GELIC_DESCR_SIZE	(32)
++
++enum gelic_port_type {
++	GELIC_PORT_ETHERNET = 0,
++	GELIC_PORT_WIRELESS = 1,
++	GELIC_PORT_MAX
++};
++
+ struct gelic_descr {
+ 	/* as defined by the hardware */
+ 	__be32 buf_addr;
+@@ -222,7 +228,6 @@ struct gelic_descr {
+ 	dma_addr_t bus_addr;
+ 	struct gelic_descr *next;
+ 	struct gelic_descr *prev;
+-	struct vlan_ethhdr vlan;
+ } __attribute__((aligned(32)));
+ 
+ struct gelic_descr_chain {
+@@ -231,43 +236,116 @@ struct gelic_descr_chain {
+ 	struct gelic_descr *tail;
+ };
+ 
++struct gelic_vlan_id {
++	u16 tx;
++	u16 rx;
++};
++
+ struct gelic_card {
+-	struct net_device *netdev;
+ 	struct napi_struct napi;
++	struct net_device *netdev[GELIC_PORT_MAX];
+ 	/*
+ 	 * hypervisor requires irq_status should be
+ 	 * 8 bytes aligned, but u64 member is
+ 	 * always disposed in that manner
+ 	 */
+ 	u64 irq_status;
+-	u64 ghiintmask;
++	u64 irq_mask;
+ 
+ 	struct ps3_system_bus_device *dev;
+-	u32 vlan_id[GELIC_NET_VLAN_MAX];
+-	int vlan_index;
++	struct gelic_vlan_id vlan[GELIC_PORT_MAX];
++	int vlan_required;
+ 
+ 	struct gelic_descr_chain tx_chain;
+ 	struct gelic_descr_chain rx_chain;
+ 	int rx_dma_restart_required;
+-	/* gurad dmac descriptor chain*/
+-	spinlock_t chain_lock;
+-
+ 	int rx_csum;
+-	/* guard tx_dma_progress */
+-	spinlock_t tx_dma_lock;
++	/*
++	 * tx_lock guards tx descriptor list and
++	 * tx_dma_progress.
++	 */
++	spinlock_t tx_lock;
+ 	int tx_dma_progress;
+ 
+ 	struct work_struct tx_timeout_task;
+ 	atomic_t tx_timeout_task_counter;
+ 	wait_queue_head_t waitq;
+ 
++	/* only first user should up the card */
++	struct semaphore updown_lock;
++	atomic_t users;
++
+ 	u64 ether_port_status;
++	/* original address returned by kzalloc */
++	void *unalign;
+ 
++	/*
++	 * each netdevice has copy of irq
++	 */
++	unsigned int irq;
+ 	struct gelic_descr *tx_top, *rx_top;
+-	struct gelic_descr descr[0];
++	struct gelic_descr descr[0]; /* must be the last */
+ };
+ 
++struct gelic_port {
++	struct gelic_card *card;
++	struct net_device *netdev;
++	enum gelic_port_type type;
++	long priv[0]; /* long for alignment */
++};
+ 
+-extern unsigned long p_to_lp(long pa);
++static inline struct gelic_card *port_to_card(struct gelic_port *p)
++{
++	return p->card;
++}
++static inline struct net_device *port_to_netdev(struct gelic_port *p)
++{
++	return p->netdev;
++}
++static inline struct gelic_card *netdev_card(struct net_device *d)
++{
++	return ((struct gelic_port *)netdev_priv(d))->card;
++}
++static inline struct gelic_port *netdev_port(struct net_device *d)
++{
++	return (struct gelic_port *)netdev_priv(d);
++}
++static inline struct device *ctodev(struct gelic_card *card)
++{
++	return &card->dev->core;
++}
++static inline u64 bus_id(struct gelic_card *card)
++{
++	return card->dev->bus_id;
++}
++static inline u64 dev_id(struct gelic_card *card)
++{
++	return card->dev->dev_id;
++}
++
++static inline void *port_priv(struct gelic_port *port)
++{
++	return port->priv;
++}
++
++extern int gelic_card_set_irq_mask(struct gelic_card *card, u64 mask);
++/* shared netdev ops */
++extern void gelic_card_up(struct gelic_card *card);
++extern void gelic_card_down(struct gelic_card *card);
++extern int gelic_net_open(struct net_device *netdev);
++extern int gelic_net_stop(struct net_device *netdev);
++extern int gelic_net_xmit(struct sk_buff *skb, struct net_device *netdev);
++extern void gelic_net_set_multi(struct net_device *netdev);
++extern void gelic_net_tx_timeout(struct net_device *netdev);
++extern int gelic_net_change_mtu(struct net_device *netdev, int new_mtu);
++extern int gelic_net_setup_netdev(struct net_device *netdev,
++				  struct gelic_card *card);
++
++/* shared ethtool ops */
++extern void gelic_net_get_drvinfo(struct net_device *netdev,
++				  struct ethtool_drvinfo *info);
++extern u32 gelic_net_get_rx_csum(struct net_device *netdev);
++extern int gelic_net_set_rx_csum(struct net_device *netdev, u32 data);
++extern void gelic_net_poll_controller(struct net_device *netdev);
+ 
+ #endif /* _GELIC_NET_H */
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-remove-duplicate-ethtool-handlers.patch linux-2.6.25-id/patches/ps3-wip/ps3-gelic-remove-duplicate-ethtool-handlers.patch
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-remove-duplicate-ethtool-handlers.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-remove-duplicate-ethtool-handlers.patch	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,77 @@
+Subject: PS3: gelic: remove duplicated ethtool handlers
+
+Remove some ethtool handlers, which duplicate functionality that was already
+provided by the common ethtool handlers.
+
+Signed-off-by: Masakazu Mokuno <mokuno@sm.sony.co.jp>
+---
+ drivers/net/ps3_gelic_net.c |   43 +++----------------------------------------
+ 1 file changed, 3 insertions(+), 40 deletions(-)
+
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -1196,28 +1196,6 @@ static int gelic_ether_get_settings(stru
+ 	return 0;
+ }
+ 
+-static u32 gelic_ether_get_link(struct net_device *netdev)
+-{
+-	struct gelic_card *card = netdev_priv(netdev);
+-	int status;
+-	u64 v1, v2;
+-	int link;
+-
+-	status = lv1_net_control(bus_id(card), dev_id(card),
+-				 GELIC_LV1_GET_ETH_PORT_STATUS,
+-				 GELIC_LV1_VLAN_TX_ETHERNET, 0, 0,
+-				 &v1, &v2);
+-	if (status)
+-		return 0; /* link down */
+-
+-	if (v1 & GELIC_LV1_ETHER_LINK_UP)
+-		link = 1;
+-	else
+-		link = 0;
+-
+-	return link;
+-}
+-
+ static int gelic_net_nway_reset(struct net_device *netdev)
+ {
+ 	if (netif_running(netdev)) {
+@@ -1227,21 +1205,6 @@ static int gelic_net_nway_reset(struct n
+ 	return 0;
+ }
+ 
+-static u32 gelic_net_get_tx_csum(struct net_device *netdev)
+-{
+-	return (netdev->features & NETIF_F_IP_CSUM) != 0;
+-}
+-
+-static int gelic_net_set_tx_csum(struct net_device *netdev, u32 data)
+-{
+-	if (data)
+-		netdev->features |= NETIF_F_IP_CSUM;
+-	else
+-		netdev->features &= ~NETIF_F_IP_CSUM;
+-
+-	return 0;
+-}
+-
+ static u32 gelic_net_get_rx_csum(struct net_device *netdev)
+ {
+ 	struct gelic_card *card = netdev_priv(netdev);
+@@ -1260,10 +1223,10 @@ static int gelic_net_set_rx_csum(struct 
+ static struct ethtool_ops gelic_net_ethtool_ops = {
+ 	.get_drvinfo	= gelic_net_get_drvinfo,
+ 	.get_settings	= gelic_ether_get_settings,
+-	.get_link	= gelic_ether_get_link,
++	.get_link	= ethtool_op_get_link,
+ 	.nway_reset	= gelic_net_nway_reset,
+-	.get_tx_csum	= gelic_net_get_tx_csum,
+-	.set_tx_csum	= gelic_net_set_tx_csum,
++	.get_tx_csum	= ethtool_op_get_tx_csum,
++	.set_tx_csum	= ethtool_op_set_tx_csum,
+ 	.get_rx_csum	= gelic_net_get_rx_csum,
+ 	.set_rx_csum	= gelic_net_set_rx_csum,
+ };
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-wireless-v2.patch linux-2.6.25-id/patches/ps3-wip/ps3-gelic-wireless-v2.patch
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-wireless-v2.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-wireless-v2.patch	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,3225 @@
+Subject: PS3: gelic: Add wireless support for PS3
+
+PS3: gelic: Add wireless support for PS3
+
+Signed-off-by: Masakazu Mokuno <mokuno@sm.sony.co.jp>
+---
+This is the version 2 of the re-worked (rewritten) version of the wireless
+support driver for PS3.  The version 1 of the new driver was submitted on 13th
+Dec. 2007 (and the old one was submitted to net-dev ML in June 2007).
+
+Major differences with the old driver are:
+  - The new driver has a separate ethX interface from ethernet.
+    They share the same MAC address.
+  - Thus we can use both ethernet and wireless simultaenously.
+  - The new driver returns AP's cipher information by the common IE
+    format in the scan information.
+  - Cipher selection is done via the common wireless extension way
+
+V2 changes:
+  - Emit more null IWAP events at appropriate timings, as Dan Williams
+    pointed out at the V1 submission.
+  - Fix sparse warnings
+    o make some functions as static
+    o use NULL for pointer instead of 0
+  - Remove extra space
+
+ drivers/net/Kconfig              |   10 
+ drivers/net/Makefile             |    3 
+ drivers/net/ps3_gelic_net.c      |   18 
+ drivers/net/ps3_gelic_net.h      |    6 
+ drivers/net/ps3_gelic_wireless.c | 2753 +++++++++++++++++++++++++++++++++++++++
+ drivers/net/ps3_gelic_wireless.h |  329 ++++
+ 6 files changed, 3117 insertions(+), 2 deletions(-)
+
+--- a/drivers/net/Kconfig
++++ b/drivers/net/Kconfig
+@@ -2302,6 +2302,16 @@ config GELIC_NET
+ 	  To compile this driver as a module, choose M here: the
+ 	  module will be called ps3_gelic.
+ 
++config GELIC_WIRELESS
++       bool "PS3 Wireless support"
++       depends on GELIC_NET
++       help
++        This option adds the support for the wireless feature of PS3.
++        If you have the wireless-less model of PS3 or have no plan to
++        use wireless feature, disabling this option saves memory.  As
++        the driver automatically distinguishes the models, you can
++        safely enable this option even if you have a wireless-less model.
++
+ config GIANFAR
+ 	tristate "Gianfar Ethernet"
+ 	depends on 85xx || 83xx || PPC_86xx
+--- a/drivers/net/Makefile
++++ b/drivers/net/Makefile
+@@ -66,7 +66,8 @@ obj-$(CONFIG_BNX2) += bnx2.o
+ spidernet-y += spider_net.o spider_net_ethtool.o
+ obj-$(CONFIG_SPIDER_NET) += spidernet.o sungem_phy.o
+ obj-$(CONFIG_GELIC_NET) += ps3_gelic.o
+-ps3_gelic-objs += ps3_gelic_net.o
++gelic_wireless-$(CONFIG_GELIC_WIRELESS) += ps3_gelic_wireless.o
++ps3_gelic-objs += ps3_gelic_net.o $(gelic_wireless-y)
+ obj-$(CONFIG_TC35815) += tc35815.o
+ obj-$(CONFIG_SKGE) += skge.o
+ obj-$(CONFIG_SKY2) += sky2.o
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -46,9 +46,10 @@
+ #include <asm/lv1call.h>
+ 
+ #include "ps3_gelic_net.h"
++#include "ps3_gelic_wireless.h"
+ 
+ #define DRV_NAME "Gelic Network Driver"
+-#define DRV_VERSION "1.1"
++#define DRV_VERSION "2.0"
+ 
+ MODULE_AUTHOR("SCE Inc.");
+ MODULE_DESCRIPTION("Gelic Network driver");
+@@ -1154,6 +1155,12 @@ static irqreturn_t gelic_card_interrupt(
+ 	if (status & GELIC_CARD_PORT_STATUS_CHANGED)
+ 		gelic_card_get_ether_port_status(card, 1);
+ 
++#ifdef CONFIG_GELIC_WIRELESS
++	if (status & (GELIC_CARD_WLAN_EVENT_RECEIVED |
++		      GELIC_CARD_WLAN_COMMAND_COMPLETED))
++		gelic_wl_interrupt(card->netdev[GELIC_PORT_WIRELESS], status);
++#endif
++
+ 	return IRQ_HANDLED;
+ }
+ 
+@@ -1635,6 +1642,12 @@ static int ps3_gelic_driver_probe(struct
+ 		goto fail_setup_netdev;
+ 	}
+ 
++#ifdef CONFIG_GELIC_WIRELESS
++	if (gelic_wl_driver_probe(card)) {
++		dev_dbg(&dev->core, "%s: WL init failed\n", __func__);
++		goto fail_setup_netdev;
++	}
++#endif
+ 	pr_debug("%s: done\n", __func__);
+ 	return 0;
+ 
+@@ -1674,6 +1687,9 @@ static int ps3_gelic_driver_remove(struc
+ 	struct net_device *netdev0;
+ 	pr_debug("%s: called\n", __func__);
+ 
++#ifdef CONFIG_GELIC_WIRELESS
++	gelic_wl_driver_remove(card);
++#endif
+ 	/* stop interrupt */
+ 	gelic_card_set_irq_mask(card, 0);
+ 
+--- a/drivers/net/ps3_gelic_net.h
++++ b/drivers/net/ps3_gelic_net.h
+@@ -57,6 +57,8 @@
+ #define GELIC_CARD_RX_PROTECTION_ERR         0x0000000004000000L
+ #define GELIC_CARD_TX_TCP_UDP_CHECKSUM_ERR   0x0000000008000000L
+ #define GELIC_CARD_PORT_STATUS_CHANGED       0x0000000020000000L
++#define GELIC_CARD_WLAN_EVENT_RECEIVED       0x0000000040000000L
++#define GELIC_CARD_WLAN_COMMAND_COMPLETED    0x0000000080000000L
+ 	/* INT 0 */
+ #define GELIC_CARD_TX_FLAGGED_DESCR          0x0004000000000000L
+ #define GELIC_CARD_RX_FLAGGED_DESCR          0x0040000000000000L
+@@ -180,6 +182,10 @@ enum gelic_lv1_net_control_code {
+ 	GELIC_LV1_GET_ETH_PORT_STATUS	= 2,
+ 	GELIC_LV1_SET_NEGOTIATION_MODE	= 3,
+ 	GELIC_LV1_GET_VLAN_ID		= 4,
++	GELIC_LV1_GET_CHANNEL           = 6,
++	GELIC_LV1_POST_WLAN_CMD		= 9,
++	GELIC_LV1_GET_WLAN_CMD_RESULT	= 10,
++	GELIC_LV1_GET_WLAN_EVENT	= 11
+ };
+ 
+ /* status returened from GET_ETH_PORT_STATUS */
+--- /dev/null
++++ b/drivers/net/ps3_gelic_wireless.c
+@@ -0,0 +1,2753 @@
++/*
++ *  PS3 gelic network driver.
++ *
++ * Copyright (C) 2007 Sony Computer Entertainment Inc.
++ * Copyright 2007 Sony Corporation
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2
++ * as published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
++ */
++#undef DEBUG
++
++#include <linux/kernel.h>
++#include <linux/module.h>
++
++#include <linux/etherdevice.h>
++#include <linux/ethtool.h>
++#include <linux/if_vlan.h>
++
++#include <linux/in.h>
++#include <linux/ip.h>
++#include <linux/tcp.h>
++#include <linux/wireless.h>
++#include <linux/ctype.h>
++#include <linux/string.h>
++#include <net/iw_handler.h>
++#include <net/ieee80211.h>
++
++#include <linux/dma-mapping.h>
++#include <net/checksum.h>
++#include <asm/firmware.h>
++#include <asm/ps3.h>
++#include <asm/lv1call.h>
++
++#include "ps3_gelic_net.h"
++#include "ps3_gelic_wireless.h"
++
++
++static int gelic_wl_start_scan(struct gelic_wl_info *wl, int always_scan);
++static int gelic_wl_try_associate(struct net_device *netdev);
++
++/*
++ * tables
++ */
++
++/* 802.11b/g channel to freq in MHz */
++static const int channel_freq[] = {
++	2412, 2417, 2422, 2427, 2432,
++	2437, 2442, 2447, 2452, 2457,
++	2462, 2467, 2472, 2484
++};
++#define NUM_CHANNELS ARRAY_SIZE(channel_freq)
++
++/* in bps */
++static const int bitrate_list[] = {
++	  1000000,
++	  2000000,
++	  5500000,
++	 11000000,
++	  6000000,
++	  9000000,
++	 12000000,
++	 18000000,
++	 24000000,
++	 36000000,
++	 48000000,
++	 54000000
++};
++#define NUM_BITRATES ARRAY_SIZE(bitrate_list)
++
++/*
++ * wpa2 support requires the hypervisor version 2.0 or later
++ */
++static inline int wpa2_capable(void)
++{
++	return (0 <= ps3_compare_firmware_version(2, 0, 0));
++}
++
++static inline int precise_ie(void)
++{
++	return 0; /* FIXME */
++}
++/*
++ * post_eurus_cmd helpers
++ */
++struct eurus_cmd_arg_info {
++	int pre_arg; /* command requres arg1, arg2 at POST COMMAND */
++	int post_arg; /* command requires arg1, arg2 at GET_RESULT */
++};
++
++static const struct eurus_cmd_arg_info cmd_info[GELIC_EURUS_CMD_MAX_INDEX] = {
++	[GELIC_EURUS_CMD_SET_COMMON_CFG] = { .pre_arg = 1},
++	[GELIC_EURUS_CMD_SET_WEP_CFG]    = { .pre_arg = 1},
++	[GELIC_EURUS_CMD_SET_WPA_CFG]    = { .pre_arg = 1},
++	[GELIC_EURUS_CMD_GET_COMMON_CFG] = { .post_arg = 1},
++	[GELIC_EURUS_CMD_GET_WEP_CFG]    = { .post_arg = 1},
++	[GELIC_EURUS_CMD_GET_WPA_CFG]    = { .post_arg = 1},
++	[GELIC_EURUS_CMD_GET_RSSI_CFG]   = { .post_arg = 1},
++	[GELIC_EURUS_CMD_GET_SCAN]       = { .post_arg = 1},
++};
++
++#ifdef DEBUG
++static const char *cmdstr(enum gelic_eurus_command ix)
++{
++	switch (ix) {
++	case GELIC_EURUS_CMD_ASSOC:
++		return "ASSOC";
++	case GELIC_EURUS_CMD_DISASSOC:
++		return "DISASSOC";
++	case GELIC_EURUS_CMD_START_SCAN:
++		return "SCAN";
++	case GELIC_EURUS_CMD_GET_SCAN:
++		return "GET SCAN";
++	case GELIC_EURUS_CMD_SET_COMMON_CFG:
++		return "SET_COMMON_CFG";
++	case GELIC_EURUS_CMD_GET_COMMON_CFG:
++		return "GET_COMMON_CFG";
++	case GELIC_EURUS_CMD_SET_WEP_CFG:
++		return "SET_WEP_CFG";
++	case GELIC_EURUS_CMD_GET_WEP_CFG:
++		return "GET_WEP_CFG";
++	case GELIC_EURUS_CMD_SET_WPA_CFG:
++		return "SET_WPA_CFG";
++	case GELIC_EURUS_CMD_GET_WPA_CFG:
++		return "GET_WPA_CFG";
++	case GELIC_EURUS_CMD_GET_RSSI_CFG:
++		return "GET_RSSI";
++	default:
++		break;
++	}
++	return "";
++};
++#else
++static inline const char *cmdstr(enum gelic_eurus_command ix)
++{
++	return "";
++}
++#endif
++
++/* synchronously do eurus commands */
++static void gelic_eurus_sync_cmd_worker(struct work_struct *work)
++{
++	struct gelic_eurus_cmd *cmd;
++	struct gelic_card *card;
++	struct gelic_wl_info *wl;
++
++	u64 arg1, arg2;
++
++	pr_debug("%s: <-\n", __func__);
++	cmd = container_of(work, struct gelic_eurus_cmd, work);
++	BUG_ON(cmd_info[cmd->cmd].pre_arg &&
++	       cmd_info[cmd->cmd].post_arg);
++	wl = cmd->wl;
++	card = port_to_card(wl_port(wl));
++
++	if (cmd_info[cmd->cmd].pre_arg) {
++		arg1 = ps3_mm_phys_to_lpar(__pa(cmd->buffer));
++		arg2 = cmd->buf_size;
++	} else {
++		arg1 = 0;
++		arg2 = 0;
++	}
++	init_completion(&wl->cmd_done_intr);
++	pr_debug("%s: cmd='%s' start\n", __func__, cmdstr(cmd->cmd));
++	cmd->status = lv1_net_control(bus_id(card), dev_id(card),
++				      GELIC_LV1_POST_WLAN_CMD,
++				      cmd->cmd, arg1, arg2,
++				      &cmd->tag, &cmd->size);
++	if (cmd->status) {
++		complete(&cmd->done);
++		pr_info("%s: cmd issue failed\n", __func__);
++		return;
++	}
++
++	wait_for_completion(&wl->cmd_done_intr);
++
++	if (cmd_info[cmd->cmd].post_arg) {
++		arg1 = ps3_mm_phys_to_lpar(__pa(cmd->buffer));
++		arg2 = cmd->buf_size;
++	} else {
++		arg1 = 0;
++		arg2 = 0;
++	}
++
++	cmd->status = lv1_net_control(bus_id(card), dev_id(card),
++				      GELIC_LV1_GET_WLAN_CMD_RESULT,
++				      cmd->tag, arg1, arg2,
++				      &cmd->cmd_status, &cmd->size);
++#ifdef DEBUG
++	if (cmd->status || cmd->cmd_status) {
++	pr_debug("%s: cmd done tag=%#lx arg1=%#lx, arg2=%#lx\n", __func__,
++		 cmd->tag, arg1, arg2);
++	pr_debug("%s: cmd done status=%#x cmd_status=%#lx size=%#lx\n",
++		 __func__, cmd->status, cmd->cmd_status, cmd->size);
++	}
++#endif
++	complete(&cmd->done);
++	pr_debug("%s: cmd='%s' done\n", __func__, cmdstr(cmd->cmd));
++}
++
++static struct gelic_eurus_cmd *gelic_eurus_sync_cmd(struct gelic_wl_info *wl,
++						    unsigned int eurus_cmd,
++						    void *buffer,
++						    unsigned int buf_size)
++{
++	struct gelic_eurus_cmd *cmd;
++
++	/* allocate cmd */
++	cmd = kzalloc(sizeof(*cmd), GFP_KERNEL);
++	if (!cmd)
++		return NULL;
++
++	/* initialize members */
++	cmd->cmd = eurus_cmd;
++	cmd->buffer = buffer;
++	cmd->buf_size = buf_size;
++	cmd->wl = wl;
++	INIT_WORK(&cmd->work, gelic_eurus_sync_cmd_worker);
++	init_completion(&cmd->done);
++	queue_work(wl->eurus_cmd_queue, &cmd->work);
++
++	/* wait for command completion */
++	wait_for_completion(&cmd->done);
++
++	return cmd;
++}
++
++static u32 gelic_wl_get_link(struct net_device *netdev)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_port(netdev));
++	u32 ret;
++
++	pr_debug("%s: <-\n", __func__);
++	down(&wl->assoc_stat_lock);
++	if (wl->assoc_stat == GELIC_WL_ASSOC_STAT_ASSOCIATED)
++		ret = 1;
++	else
++		ret = 0;
++	up(&wl->assoc_stat_lock);
++	pr_debug("%s: ->\n", __func__);
++	return ret;
++}
++
++static void gelic_wl_send_iwap_event(struct gelic_wl_info *wl, u8 *bssid)
++{
++	union iwreq_data data;
++
++	memset(&data, 0, sizeof(data));
++	if (bssid)
++		memcpy(data.ap_addr.sa_data, bssid, ETH_ALEN);
++	data.ap_addr.sa_family = ARPHRD_ETHER;
++	wireless_send_event(port_to_netdev(wl_port(wl)), SIOCGIWAP,
++			    &data, NULL);
++}
++
++/*
++ * wireless extension handlers and helpers
++ */
++
++/* SIOGIWNAME */
++static int gelic_wl_get_name(struct net_device *dev,
++			     struct iw_request_info *info,
++			     union iwreq_data *iwreq, char *extra)
++{
++	strcpy(iwreq->name, "IEEE 802.11bg");
++	return 0;
++}
++
++static void gelic_wl_get_ch_info(struct gelic_wl_info *wl)
++{
++	struct gelic_card *card = port_to_card(wl_port(wl));
++	u64 ch_info_raw, tmp;
++	int status;
++
++	if (!test_and_set_bit(GELIC_WL_STAT_CH_INFO, &wl->stat)) {
++		status = lv1_net_control(bus_id(card), dev_id(card),
++					 GELIC_LV1_GET_CHANNEL, 0, 0, 0,
++					 &ch_info_raw,
++					 &tmp);
++		/* some fw versions may return error */
++		if (status) {
++			if (status != LV1_NO_ENTRY)
++				pr_info("%s: available ch unknown\n", __func__);
++			wl->ch_info = 0x07ff;/* 11 ch */
++		} else
++			/* 16 bits of MSB has available channels */
++			wl->ch_info = ch_info_raw >> 48;
++	}
++	return;
++}
++
++/* SIOGIWRANGE */
++static int gelic_wl_get_range(struct net_device *netdev,
++			      struct iw_request_info *info,
++			      union iwreq_data *iwreq, char *extra)
++{
++	struct iw_point *point = &iwreq->data;
++	struct iw_range *range = (struct iw_range *)extra;
++	struct gelic_wl_info *wl = port_wl(netdev_port(netdev));
++	unsigned int i, chs;
++
++	pr_debug("%s: <-\n", __func__);
++	point->length = sizeof(struct iw_range);
++	memset(range, 0, sizeof(struct iw_range));
++
++	range->we_version_compiled = WIRELESS_EXT;
++	range->we_version_source = 22;
++
++	/* available channels and frequencies */
++	gelic_wl_get_ch_info(wl);
++
++	for (i = 0, chs = 0;
++	     i < NUM_CHANNELS && chs < IW_MAX_FREQUENCIES; i++)
++		if (wl->ch_info & (1 << i)) {
++			range->freq[chs].i = i + 1;
++			range->freq[chs].m = channel_freq[i];
++			range->freq[chs].e = 6;
++			chs++;
++		}
++	range->num_frequency = chs;
++	range->old_num_frequency = chs;
++	range->num_channels = chs;
++	range->old_num_channels = chs;
++
++	/* bitrates */
++	for (i = 0; i < NUM_BITRATES; i++)
++		range->bitrate[i] = bitrate_list[i];
++	range->num_bitrates = i;
++
++	/* signal levels */
++	range->max_qual.qual = 100; /* relative value */
++	range->max_qual.level = 100;
++	range->avg_qual.qual = 50;
++	range->avg_qual.level = 50;
++	range->sensitivity = 0;
++
++	/* Event capability */
++	IW_EVENT_CAPA_SET_KERNEL(range->event_capa);
++	IW_EVENT_CAPA_SET(range->event_capa, SIOCGIWAP);
++	IW_EVENT_CAPA_SET(range->event_capa, SIOCGIWSCAN);
++
++	/* encryption capability */
++	range->enc_capa = IW_ENC_CAPA_WPA |
++		IW_ENC_CAPA_CIPHER_TKIP | IW_ENC_CAPA_CIPHER_CCMP;
++	if (wpa2_capable())
++		range->enc_capa |= IW_ENC_CAPA_WPA2;
++	range->encoding_size[0] = 5;	/* 40bit WEP */
++	range->encoding_size[1] = 13;	/* 104bit WEP */
++	range->encoding_size[2] = 32;	/* WPA-PSK */
++	range->num_encoding_sizes = 3;
++	range->max_encoding_tokens = GELIC_WEP_KEYS;
++
++	pr_debug("%s: ->\n", __func__);
++	return 0;
++
++}
++
++/* SIOC{G,S}IWSCAN */
++static int gelic_wl_set_scan(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++
++	return gelic_wl_start_scan(wl, 1);
++}
++
++#define OUI_LEN 3
++static const u8 rsn_oui[OUI_LEN] = { 0x00, 0x0f, 0xac };
++static const u8 wpa_oui[OUI_LEN] = { 0x00, 0x50, 0xf2 };
++
++/*
++ * synthesize WPA/RSN IE data
++ * See WiFi WPA specification and IEEE 802.11-2007 7.3.2.25
++ * for the format
++ */
++static size_t gelic_wl_synthesize_ie(u8 *buf,
++				     struct gelic_eurus_scan_info *scan)
++{
++
++	const u8 *oui_header;
++	u8 *start = buf;
++	int rsn;
++	int ccmp;
++
++	pr_debug("%s: <- sec=%16x\n", __func__, scan->security);
++	switch (be16_to_cpu(scan->security) & GELIC_EURUS_SCAN_SEC_MASK) {
++	case GELIC_EURUS_SCAN_SEC_WPA:
++		rsn = 0;
++		break;
++	case GELIC_EURUS_SCAN_SEC_WPA2:
++		rsn = 1;
++		break;
++	default:
++		/* WEP or none.  No IE returned */
++		return 0;
++	}
++
++	switch (be16_to_cpu(scan->security) & GELIC_EURUS_SCAN_SEC_WPA_MASK) {
++	case GELIC_EURUS_SCAN_SEC_WPA_TKIP:
++		ccmp = 0;
++		break;
++	case GELIC_EURUS_SCAN_SEC_WPA_AES:
++		ccmp = 1;
++		break;
++	default:
++		if (rsn) {
++			ccmp = 1;
++			pr_info("%s: no cipher info. defaulted to CCMP\n",
++				__func__);
++		} else {
++			ccmp = 0;
++			pr_info("%s: no cipher info. defaulted to TKIP\n",
++				__func__);
++		}
++	}
++
++	if (rsn)
++		oui_header = rsn_oui;
++	else
++		oui_header = wpa_oui;
++
++	/* element id */
++	if (rsn)
++		*buf++ = MFIE_TYPE_RSN;
++	else
++		*buf++ = MFIE_TYPE_GENERIC;
++
++	/* length filed; set later */
++	buf++;
++
++	/* wpa special header */
++	if (!rsn) {
++		memcpy(buf, wpa_oui, OUI_LEN);
++		buf += OUI_LEN;
++		*buf++ = 0x01;
++	}
++
++	/* version */
++	*buf++ = 0x01; /* version 1.0 */
++	*buf++ = 0x00;
++
++	/* group cipher */
++	memcpy(buf, oui_header, OUI_LEN);
++	buf += OUI_LEN;
++
++	if (ccmp)
++		*buf++ = 0x04; /* CCMP */
++	else
++		*buf++ = 0x02; /* TKIP */
++
++	/* pairwise key count always 1 */
++	*buf++ = 0x01;
++	*buf++ = 0x00;
++
++	/* pairwise key suit */
++	memcpy(buf, oui_header, OUI_LEN);
++	buf += OUI_LEN;
++	if (ccmp)
++		*buf++ = 0x04; /* CCMP */
++	else
++		*buf++ = 0x02; /* TKIP */
++
++	/* AKM count is 1 */
++	*buf++ = 0x01;
++	*buf++ = 0x00;
++
++	/* AKM suite is assumed as PSK*/
++	memcpy(buf, oui_header, OUI_LEN);
++	buf += OUI_LEN;
++	*buf++ = 0x02; /* PSK */
++
++	/* RSN capabilities is 0 */
++	*buf++ = 0x00;
++	*buf++ = 0x00;
++
++	/* set length field */
++	start[1] = (buf - start - 2);
++
++	pr_debug("%s: ->\n", __func__);
++	return (buf - start);
++}
++
++struct ie_item {
++	u8 *data;
++	u8 len;
++};
++
++struct ie_info {
++	struct ie_item wpa;
++	struct ie_item rsn;
++};
++
++static void gelic_wl_parse_ie(u8 *data, size_t len,
++			      struct ie_info *ie_info)
++{
++	size_t data_left = len;
++	u8 *pos = data;
++	u8 item_len;
++	u8 item_id;
++
++	pr_debug("%s: data=%p len=%ld \n", __func__,
++		 data, len);
++	memset(ie_info, 0, sizeof(struct ie_info));
++
++	while (0 < data_left) {
++		item_id = *pos++;
++		item_len = *pos++;
++
++		switch (item_id) {
++		case MFIE_TYPE_GENERIC:
++			if (!memcmp(pos, wpa_oui, OUI_LEN) &&
++			    pos[OUI_LEN] == 0x01) {
++				ie_info->wpa.data = pos - 2;
++				ie_info->wpa.len = item_len + 2;
++			}
++			break;
++		case MFIE_TYPE_RSN:
++			ie_info->rsn.data = pos - 2;
++			/* length includes the header */
++			ie_info->rsn.len = item_len + 2;
++			break;
++		default:
++			pr_debug("%s: ignore %#x,%d\n", __func__,
++				 item_id, item_len);
++			break;
++		}
++		pos += item_len;
++		data_left -= item_len + 2;
++	}
++	pr_debug("%s: wpa=%p,%d wpa2=%p,%d\n", __func__,
++		 ie_info->wpa.data, ie_info->wpa.len,
++		 ie_info->rsn.data, ie_info->rsn.len);
++}
++
++
++/*
++ * translate the scan informations from hypervisor to a
++ * independent format
++ */
++static char *gelic_wl_translate_scan(struct net_device *netdev,
++				     char *ev,
++				     char *stop,
++				     struct gelic_wl_scan_info *network)
++{
++	struct iw_event iwe;
++	struct gelic_eurus_scan_info *scan = network->hwinfo;
++	char *tmp;
++	u8 rate;
++	unsigned int i, j, len;
++	u8 buf[MAX_WPA_IE_LEN];
++
++	pr_debug("%s: <-\n", __func__);
++
++	/* first entry should be AP's mac address */
++	iwe.cmd = SIOCGIWAP;
++	iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
++	memcpy(iwe.u.ap_addr.sa_data, &scan->bssid[2], ETH_ALEN);
++	ev = iwe_stream_add_event(ev, stop, &iwe, IW_EV_ADDR_LEN);
++
++	/* ESSID */
++	iwe.cmd = SIOCGIWESSID;
++	iwe.u.data.flags = 1;
++	iwe.u.data.length = strnlen(scan->essid, 32);
++	ev = iwe_stream_add_point(ev, stop, &iwe, scan->essid);
++
++	/* FREQUENCY */
++	iwe.cmd = SIOCGIWFREQ;
++	iwe.u.freq.m = be16_to_cpu(scan->channel);
++	iwe.u.freq.e = 0; /* table value in MHz */
++	iwe.u.freq.i = 0;
++	ev = iwe_stream_add_event(ev, stop, &iwe, IW_EV_FREQ_LEN);
++
++	/* RATES */
++	iwe.cmd = SIOCGIWRATE;
++	iwe.u.bitrate.fixed = iwe.u.bitrate.disabled = 0;
++	/* to stuff multiple values in one event */
++	tmp = ev + IW_EV_LCP_LEN;
++	/* put them in ascendant order (older is first) */
++	i = 0;
++	j = 0;
++	pr_debug("%s: rates=%d rate=%d\n", __func__,
++		 network->rate_len, network->rate_ext_len);
++	while (i < network->rate_len) {
++		if (j < network->rate_ext_len &&
++		    ((scan->ext_rate[j] & 0x7f) < (scan->rate[i] & 0x7f)))
++		    rate = scan->ext_rate[j++] & 0x7f;
++		else
++		    rate = scan->rate[i++] & 0x7f;
++		iwe.u.bitrate.value = rate * 500000; /* 500kbps unit */
++		tmp = iwe_stream_add_value(ev, tmp, stop, &iwe,
++					   IW_EV_PARAM_LEN);
++	}
++	while (j < network->rate_ext_len) {
++		iwe.u.bitrate.value = (scan->ext_rate[j++] & 0x7f) * 500000;
++		tmp = iwe_stream_add_value(ev, tmp, stop, &iwe,
++					   IW_EV_PARAM_LEN);
++	}
++	/* Check if we added any rate */
++	if (IW_EV_LCP_LEN < (tmp - ev))
++		ev = tmp;
++
++	/* ENCODE */
++	iwe.cmd = SIOCGIWENCODE;
++	if (be16_to_cpu(scan->capability) & WLAN_CAPABILITY_PRIVACY)
++		iwe.u.data.flags = IW_ENCODE_ENABLED | IW_ENCODE_NOKEY;
++	else
++		iwe.u.data.flags = IW_ENCODE_DISABLED;
++	iwe.u.data.length = 0;
++	ev = iwe_stream_add_point(ev, stop, &iwe, scan->essid);
++
++	/* MODE */
++	iwe.cmd = SIOCGIWMODE;
++	if (be16_to_cpu(scan->capability) &
++	    (WLAN_CAPABILITY_ESS | WLAN_CAPABILITY_IBSS)) {
++		if (be16_to_cpu(scan->capability) & WLAN_CAPABILITY_ESS)
++			iwe.u.mode = IW_MODE_MASTER;
++		else
++			iwe.u.mode = IW_MODE_ADHOC;
++		ev = iwe_stream_add_event(ev, stop, &iwe, IW_EV_UINT_LEN);
++	}
++
++	/* QUAL */
++	iwe.cmd = IWEVQUAL;
++	iwe.u.qual.updated  = IW_QUAL_ALL_UPDATED |
++			IW_QUAL_QUAL_INVALID | IW_QUAL_NOISE_INVALID;
++	iwe.u.qual.level = be16_to_cpu(scan->rssi);
++	iwe.u.qual.qual = be16_to_cpu(scan->rssi);
++	iwe.u.qual.noise = 0;
++	ev  = iwe_stream_add_event(ev, stop, &iwe, IW_EV_QUAL_LEN);
++
++	/* RSN */
++	memset(&iwe, 0, sizeof(iwe));
++	if (be16_to_cpu(scan->size) <= sizeof(*scan)) {
++		/* If wpa[2] capable station, synthesize IE and put it */
++		len = gelic_wl_synthesize_ie(buf, scan);
++		if (len) {
++			iwe.cmd = IWEVGENIE;
++			iwe.u.data.length = len;
++			ev = iwe_stream_add_point(ev, stop, &iwe, buf);
++		}
++	} else {
++		/* this scan info has IE data */
++		struct ie_info ie_info;
++		size_t data_len;
++
++		data_len = be16_to_cpu(scan->size) - sizeof(*scan);
++
++		gelic_wl_parse_ie(scan->elements, data_len, &ie_info);
++
++		if (ie_info.wpa.len && (ie_info.wpa.len <= sizeof(buf))) {
++			memcpy(buf, ie_info.wpa.data, ie_info.wpa.len);
++			iwe.cmd = IWEVGENIE;
++			iwe.u.data.length = ie_info.wpa.len;
++			ev = iwe_stream_add_point(ev, stop, &iwe, buf);
++		}
++
++		if (ie_info.rsn.len && (ie_info.rsn.len <= sizeof(buf))) {
++			memset(&iwe, 0, sizeof(iwe));
++			memcpy(buf, ie_info.rsn.data, ie_info.rsn.len);
++			iwe.cmd = IWEVGENIE;
++			iwe.u.data.length = ie_info.rsn.len;
++			ev = iwe_stream_add_point(ev, stop, &iwe, buf);
++		}
++	}
++
++	pr_debug("%s: ->\n", __func__);
++	return ev;
++}
++
++
++static int gelic_wl_get_scan(struct net_device *netdev,
++			     struct iw_request_info *info,
++			     union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	struct gelic_wl_scan_info *scan_info;
++	char *ev = extra;
++	char *stop = ev + wrqu->data.length;
++	int ret = 0;
++	unsigned long this_time = jiffies;
++
++	pr_debug("%s: <-\n", __func__);
++	if (down_interruptible(&wl->scan_lock))
++		return -EAGAIN;
++
++	switch (wl->scan_stat) {
++	case GELIC_WL_SCAN_STAT_SCANNING:
++		/* If a scan in progress, caller should call me again */
++		ret = -EAGAIN;
++		goto out;
++		break;
++
++	case GELIC_WL_SCAN_STAT_INIT:
++		/* last scan request failed or never issued */
++		ret = -ENODEV;
++		goto out;
++		break;
++	case GELIC_WL_SCAN_STAT_GOT_LIST:
++		/* ok, use current list */
++		break;
++	}
++
++	list_for_each_entry(scan_info, &wl->network_list, list) {
++		if (wl->scan_age == 0 ||
++		    time_after(scan_info->last_scanned + wl->scan_age,
++			       this_time))
++			ev = gelic_wl_translate_scan(netdev, ev, stop,
++						     scan_info);
++		else
++			pr_debug("%s:entry too old\n", __func__);
++
++		if (stop - ev <= IW_EV_ADDR_LEN) {
++			ret = -E2BIG;
++			goto out;
++		}
++	}
++
++	wrqu->data.length = ev - extra;
++	wrqu->data.flags = 0;
++out:
++	up(&wl->scan_lock);
++	pr_debug("%s: -> %d %d\n", __func__, ret, wrqu->data.length);
++	return ret;
++}
++
++#ifdef DEBUG
++static void scan_list_dump(struct gelic_wl_info *wl)
++{
++	struct gelic_wl_scan_info *scan_info;
++	int i;
++	DECLARE_MAC_BUF(mac);
++
++	i = 0;
++	list_for_each_entry(scan_info, &wl->network_list, list) {
++		pr_debug("%s: item %d\n", __func__, i++);
++		pr_debug("valid=%d eurusindex=%d last=%lx\n",
++			 scan_info->valid, scan_info->eurus_index,
++			 scan_info->last_scanned);
++		pr_debug("r_len=%d r_ext_len=%d essid_len=%d\n",
++			 scan_info->rate_len, scan_info->rate_ext_len,
++			 scan_info->essid_len);
++		/* -- */
++		pr_debug("bssid=%s\n",
++			 print_mac(mac, &scan_info->hwinfo->bssid[2]));
++		pr_debug("essid=%s\n", scan_info->hwinfo->essid);
++	}
++}
++#endif
++
++static int gelic_wl_set_auth(struct net_device *netdev,
++			     struct iw_request_info *info,
++			     union iwreq_data *data, char *extra)
++{
++	struct iw_param *param = &data->param;
++	struct gelic_wl_info *wl = port_wl(netdev_port(netdev));
++	unsigned long irqflag;
++	int ret = 0;
++
++	pr_debug("%s: <- %d\n", __func__, param->flags & IW_AUTH_INDEX);
++	spin_lock_irqsave(&wl->lock, irqflag);
++	switch (param->flags & IW_AUTH_INDEX) {
++	case IW_AUTH_WPA_VERSION:
++		if (param->value & IW_AUTH_WPA_VERSION_DISABLED) {
++			pr_debug("%s: NO WPA selected\n", __func__);
++			wl->wpa_level = GELIC_WL_WPA_LEVEL_NONE;
++			wl->group_cipher_method = GELIC_WL_CIPHER_WEP;
++			wl->pairwise_cipher_method = GELIC_WL_CIPHER_WEP;
++		}
++		if (param->value & IW_AUTH_WPA_VERSION_WPA) {
++			pr_debug("%s: WPA version 1 selected\n", __func__);
++			wl->wpa_level = GELIC_WL_WPA_LEVEL_WPA;
++			wl->group_cipher_method = GELIC_WL_CIPHER_TKIP;
++			wl->pairwise_cipher_method = GELIC_WL_CIPHER_TKIP;
++			wl->auth_method = GELIC_EURUS_AUTH_OPEN;
++		}
++		if (param->value & IW_AUTH_WPA_VERSION_WPA2) {
++			/*
++			 * As the hypervisor may not tell the cipher
++			 * information of the AP if it is WPA2,
++			 * you will not decide suitable cipher from
++			 * its beacon.
++			 * You should have knowledge about the AP's
++			 * cipher infomation in other method prior to
++			 * the association.
++			 */
++			if (!precise_ie())
++				pr_info("%s: WPA2 may not work\n", __func__);
++			if (wpa2_capable()) {
++				wl->wpa_level = GELIC_WL_WPA_LEVEL_WPA2;
++				wl->group_cipher_method = GELIC_WL_CIPHER_AES;
++				wl->pairwise_cipher_method =
++					GELIC_WL_CIPHER_AES;
++				wl->auth_method = GELIC_EURUS_AUTH_OPEN;
++			} else
++				ret = -EINVAL;
++		}
++		break;
++
++	case IW_AUTH_CIPHER_PAIRWISE:
++		if (param->value &
++		    (IW_AUTH_CIPHER_WEP104 | IW_AUTH_CIPHER_WEP40)) {
++			pr_debug("%s: WEP selected\n", __func__);
++			wl->pairwise_cipher_method = GELIC_WL_CIPHER_WEP;
++		}
++		if (param->value & IW_AUTH_CIPHER_TKIP) {
++			pr_debug("%s: TKIP selected\n", __func__);
++			wl->pairwise_cipher_method = GELIC_WL_CIPHER_TKIP;
++		}
++		if (param->value & IW_AUTH_CIPHER_CCMP) {
++			pr_debug("%s: CCMP selected\n", __func__);
++			wl->pairwise_cipher_method = GELIC_WL_CIPHER_AES;
++		}
++		if (param->value & IW_AUTH_CIPHER_NONE) {
++			pr_debug("%s: no auth selected\n", __func__);
++			wl->pairwise_cipher_method = GELIC_WL_CIPHER_NONE;
++		}
++		break;
++	case IW_AUTH_CIPHER_GROUP:
++		if (param->value &
++		    (IW_AUTH_CIPHER_WEP104 | IW_AUTH_CIPHER_WEP40)) {
++			pr_debug("%s: WEP selected\n", __func__);
++			wl->group_cipher_method = GELIC_WL_CIPHER_WEP;
++		}
++		if (param->value & IW_AUTH_CIPHER_TKIP) {
++			pr_debug("%s: TKIP selected\n", __func__);
++			wl->group_cipher_method = GELIC_WL_CIPHER_TKIP;
++		}
++		if (param->value & IW_AUTH_CIPHER_CCMP) {
++			pr_debug("%s: CCMP selected\n", __func__);
++			wl->group_cipher_method = GELIC_WL_CIPHER_AES;
++		}
++		if (param->value & IW_AUTH_CIPHER_NONE) {
++			pr_debug("%s: no auth selected\n", __func__);
++			wl->group_cipher_method = GELIC_WL_CIPHER_NONE;
++		}
++		break;
++	case IW_AUTH_80211_AUTH_ALG:
++		if (param->value & IW_AUTH_ALG_SHARED_KEY) {
++			pr_debug("%s: shared key specified\n", __func__);
++			wl->auth_method = GELIC_EURUS_AUTH_SHARED;
++		} else if (param->value & IW_AUTH_ALG_OPEN_SYSTEM) {
++			pr_debug("%s: open system specified\n", __func__);
++			wl->auth_method = GELIC_EURUS_AUTH_OPEN;
++		} else
++			ret = -EINVAL;
++		break;
++
++	case IW_AUTH_WPA_ENABLED:
++		if (param->value) {
++			pr_debug("%s: WPA enabled\n", __func__);
++			wl->wpa_level = GELIC_WL_WPA_LEVEL_WPA;
++		} else {
++			pr_debug("%s: WPA disabled\n", __func__);
++			wl->wpa_level = GELIC_WL_WPA_LEVEL_NONE;
++		}
++		break;
++
++	case IW_AUTH_KEY_MGMT:
++		if (param->value & IW_AUTH_KEY_MGMT_PSK)
++			break;
++		/* intentionally fall through */
++	default:
++		ret = -EOPNOTSUPP;
++		break;
++	};
++
++	if (!ret)
++		set_bit(GELIC_WL_STAT_CONFIGURED, &wl->stat);
++
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s: -> %d\n", __func__, ret);
++	return ret;
++}
++
++static int gelic_wl_get_auth(struct net_device *netdev,
++			     struct iw_request_info *info,
++			     union iwreq_data *iwreq, char *extra)
++{
++	struct iw_param *param = &iwreq->param;
++	struct gelic_wl_info *wl = port_wl(netdev_port(netdev));
++	unsigned long irqflag;
++	int ret = 0;
++
++	pr_debug("%s: <- %d\n", __func__, param->flags & IW_AUTH_INDEX);
++	spin_lock_irqsave(&wl->lock, irqflag);
++	switch (param->flags & IW_AUTH_INDEX) {
++	case IW_AUTH_WPA_VERSION:
++		switch (wl->wpa_level) {
++		case GELIC_WL_WPA_LEVEL_WPA:
++			param->value |= IW_AUTH_WPA_VERSION_WPA;
++			break;
++		case GELIC_WL_WPA_LEVEL_WPA2:
++			param->value |= IW_AUTH_WPA_VERSION_WPA2;
++			break;
++		default:
++			param->value |= IW_AUTH_WPA_VERSION_DISABLED;
++		}
++		break;
++
++	case IW_AUTH_80211_AUTH_ALG:
++		if (wl->auth_method == GELIC_EURUS_AUTH_SHARED)
++			param->value = IW_AUTH_ALG_SHARED_KEY;
++		else if (wl->auth_method == GELIC_EURUS_AUTH_OPEN)
++			param->value = IW_AUTH_ALG_OPEN_SYSTEM;
++		break;
++
++	case IW_AUTH_WPA_ENABLED:
++		switch (wl->wpa_level) {
++		case GELIC_WL_WPA_LEVEL_WPA:
++		case GELIC_WL_WPA_LEVEL_WPA2:
++			param->value = 1;
++			break;
++		default:
++			param->value = 0;
++			break;
++		}
++		break;
++	default:
++		ret = -EOPNOTSUPP;
++	}
++
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s: -> %d\n", __func__, ret);
++	return ret;
++}
++
++/* SIOC{S,G}IWESSID */
++static int gelic_wl_set_essid(struct net_device *netdev,
++			      struct iw_request_info *info,
++			      union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	unsigned long irqflag;
++
++	pr_debug("%s: <- l=%d f=%d\n", __func__,
++		 data->essid.length, data->essid.flags);
++	if (IW_ESSID_MAX_SIZE < data->essid.length)
++		return -EINVAL;
++
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (data->essid.flags) {
++		wl->essid_len = data->essid.length;
++		memcpy(wl->essid, extra, wl->essid_len);
++		pr_debug("%s: essid = '%s'\n", __func__, extra);
++		set_bit(GELIC_WL_STAT_ESSID_SET, &wl->stat);
++	} else {
++		pr_debug("%s: ESSID any \n", __func__);
++		clear_bit(GELIC_WL_STAT_ESSID_SET, &wl->stat);
++	}
++	set_bit(GELIC_WL_STAT_CONFIGURED, &wl->stat);
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++
++
++	gelic_wl_try_associate(netdev); /* FIXME */
++	pr_debug("%s: -> \n", __func__);
++	return 0;
++}
++
++static int gelic_wl_get_essid(struct net_device *netdev,
++			      struct iw_request_info *info,
++			      union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	unsigned long irqflag;
++
++	pr_debug("%s: <- \n", __func__);
++	down(&wl->assoc_stat_lock);
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (test_bit(GELIC_WL_STAT_ESSID_SET, &wl->stat) ||
++	    wl->assoc_stat == GELIC_WL_ASSOC_STAT_ASSOCIATED) {
++		memcpy(extra, wl->essid, wl->essid_len);
++		data->essid.length = wl->essid_len;
++		data->essid.flags = 1;
++	} else
++		data->essid.flags = 0;
++
++	up(&wl->assoc_stat_lock);
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s: -> len=%d \n", __func__, data->essid.length);
++
++	return 0;
++}
++
++/* SIO{S,G}IWENCODE */
++static int gelic_wl_set_encode(struct net_device *netdev,
++			       struct iw_request_info *info,
++			       union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	struct iw_point *enc = &data->encoding;
++	__u16 flags;
++	unsigned int irqflag;
++	int key_index, index_specified;
++	int ret = 0;
++
++	pr_debug("%s: <- \n", __func__);
++	flags = enc->flags & IW_ENCODE_FLAGS;
++	key_index = enc->flags & IW_ENCODE_INDEX;
++
++	pr_debug("%s: key_index = %d\n", __func__, key_index);
++	pr_debug("%s: key_len = %d\n", __func__, enc->length);
++	pr_debug("%s: flag=%x\n", __func__, enc->flags & IW_ENCODE_FLAGS);
++
++	if (GELIC_WEP_KEYS < key_index)
++		return -EINVAL;
++
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (key_index) {
++		index_specified = 1;
++		key_index--;
++	} else {
++		index_specified = 0;
++		key_index = wl->current_key;
++	}
++
++	if (flags & IW_ENCODE_NOKEY) {
++		/* if just IW_ENCODE_NOKEY, change current key index */
++		if (!flags && index_specified) {
++			wl->current_key = key_index;
++			goto done;
++		}
++
++		if (flags & IW_ENCODE_DISABLED) {
++			if (!index_specified) {
++				/* disable encryption */
++				wl->group_cipher_method = GELIC_WL_CIPHER_NONE;
++				wl->pairwise_cipher_method =
++					GELIC_WL_CIPHER_NONE;
++				/* invalidate all key */
++				wl->key_enabled = 0;
++			} else
++				clear_bit(key_index, &wl->key_enabled);
++		}
++
++		if (flags & IW_ENCODE_OPEN)
++			wl->auth_method = GELIC_EURUS_AUTH_OPEN;
++		if (flags & IW_ENCODE_RESTRICTED) {
++			pr_info("%s: shared key mode enabled\n", __func__);
++			wl->auth_method = GELIC_EURUS_AUTH_SHARED;
++		}
++	} else {
++		if (IW_ENCODING_TOKEN_MAX < enc->length) {
++			ret = -EINVAL;
++			goto done;
++		}
++		wl->key_len[key_index] = enc->length;
++		memcpy(wl->key[key_index], extra, enc->length);
++		set_bit(key_index, &wl->key_enabled);
++		wl->pairwise_cipher_method = GELIC_WL_CIPHER_WEP;
++		wl->group_cipher_method = GELIC_WL_CIPHER_WEP;
++	}
++	set_bit(GELIC_WL_STAT_CONFIGURED, &wl->stat);
++done:
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s: -> \n", __func__);
++	return ret;
++}
++
++static int gelic_wl_get_encode(struct net_device *netdev,
++			       struct iw_request_info *info,
++			       union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	struct iw_point *enc = &data->encoding;
++	unsigned int irqflag;
++	unsigned int key_index, index_specified;
++	int ret = 0;
++
++	pr_debug("%s: <- \n", __func__);
++	key_index = enc->flags & IW_ENCODE_INDEX;
++	pr_debug("%s: flag=%#x point=%p len=%d extra=%p\n", __func__,
++		 enc->flags, enc->pointer, enc->length, extra);
++	if (GELIC_WEP_KEYS < key_index)
++		return -EINVAL;
++
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (key_index) {
++		index_specified = 1;
++		key_index--;
++	} else {
++		index_specified = 0;
++		key_index = wl->current_key;
++	}
++
++	if (wl->group_cipher_method == GELIC_WL_CIPHER_WEP) {
++		switch (wl->auth_method) {
++		case GELIC_EURUS_AUTH_OPEN:
++			enc->flags = IW_ENCODE_OPEN;
++			break;
++		case GELIC_EURUS_AUTH_SHARED:
++			enc->flags = IW_ENCODE_RESTRICTED;
++			break;
++		}
++	} else
++		enc->flags = IW_ENCODE_DISABLED;
++
++	if (test_bit(key_index, &wl->key_enabled)) {
++		if (enc->length < wl->key_len[key_index]) {
++			ret = -EINVAL;
++			goto done;
++		}
++		enc->length = wl->key_len[key_index];
++		memcpy(extra, wl->key[key_index], wl->key_len[key_index]);
++	} else {
++		enc->length = 0;
++		enc->flags |= IW_ENCODE_NOKEY;
++	}
++	enc->flags |= key_index + 1;
++	pr_debug("%s: -> flag=%x len=%d\n", __func__,
++		 enc->flags, enc->length);
++
++done:
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	return ret;
++}
++
++/* SIOC{S,G}IWAP */
++static int gelic_wl_set_ap(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	unsigned long irqflag;
++
++	pr_debug("%s: <-\n", __func__);
++	if (data->ap_addr.sa_family != ARPHRD_ETHER)
++		return -EINVAL;
++
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (is_valid_ether_addr(data->ap_addr.sa_data)) {
++		memcpy(wl->bssid, data->ap_addr.sa_data,
++		       ETH_ALEN);
++		set_bit(GELIC_WL_STAT_BSSID_SET, &wl->stat);
++		set_bit(GELIC_WL_STAT_CONFIGURED, &wl->stat);
++		pr_debug("%s: bss=%02x:%02x:%02x:%02x:%02x:%02x\n",
++			 __func__,
++			 wl->bssid[0], wl->bssid[1],
++			 wl->bssid[2], wl->bssid[3],
++			 wl->bssid[4], wl->bssid[5]);
++	} else {
++		pr_debug("%s: clear bssid\n", __func__);
++		clear_bit(GELIC_WL_STAT_BSSID_SET, &wl->stat);
++		memset(wl->bssid, 0, ETH_ALEN);
++	}
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s: ->\n", __func__);
++	return 0;
++}
++
++static int gelic_wl_get_ap(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	unsigned long irqflag;
++
++	pr_debug("%s: <-\n", __func__);
++	down(&wl->assoc_stat_lock);
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (wl->assoc_stat == GELIC_WL_ASSOC_STAT_ASSOCIATED) {
++		data->ap_addr.sa_family = ARPHRD_ETHER;
++		memcpy(data->ap_addr.sa_data, wl->active_bssid,
++		       ETH_ALEN);
++	} else
++		memset(data->ap_addr.sa_data, 0, ETH_ALEN);
++
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	up(&wl->assoc_stat_lock);
++	pr_debug("%s: ->\n", __func__);
++	return 0;
++}
++
++/* SIOC{S,G}IWENCODEEXT */
++static int gelic_wl_set_encodeext(struct net_device *netdev,
++				  struct iw_request_info *info,
++				  union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	struct iw_point *enc = &data->encoding;
++	struct iw_encode_ext *ext = (struct iw_encode_ext *)extra;
++	__u16 alg;
++	__u16 flags;
++	unsigned int irqflag;
++	int key_index;
++	int ret = 0;
++
++	pr_debug("%s: <- \n", __func__);
++	flags = enc->flags & IW_ENCODE_FLAGS;
++	alg = ext->alg;
++	key_index = enc->flags & IW_ENCODE_INDEX;
++
++	pr_debug("%s: key_index = %d\n", __func__, key_index);
++	pr_debug("%s: key_len = %d\n", __func__, enc->length);
++	pr_debug("%s: flag=%x\n", __func__, enc->flags & IW_ENCODE_FLAGS);
++	pr_debug("%s: ext_flag=%x\n", __func__, ext->ext_flags);
++	pr_debug("%s: ext_key_len=%x\n", __func__, ext->key_len);
++
++	if (GELIC_WEP_KEYS < key_index)
++		return -EINVAL;
++
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (key_index)
++		key_index--;
++	else
++		key_index = wl->current_key;
++
++	if (!enc->length && (ext->ext_flags & IW_ENCODE_EXT_SET_TX_KEY)) {
++		/* reques to change default key index */
++		pr_debug("%s: request to change default key to %d\n",
++			 __func__, key_index);
++		wl->current_key = key_index;
++		goto done;
++	}
++
++	if (alg == IW_ENCODE_ALG_NONE || (flags & IW_ENCODE_DISABLED)) {
++		pr_debug("%s: alg disabled\n", __func__);
++		wl->wpa_level = GELIC_WL_WPA_LEVEL_NONE;
++		wl->group_cipher_method = GELIC_WL_CIPHER_NONE;
++		wl->pairwise_cipher_method = GELIC_WL_CIPHER_NONE;
++		wl->auth_method = GELIC_EURUS_AUTH_OPEN; /* should be open */
++	} else if (alg == IW_ENCODE_ALG_WEP) {
++		pr_debug("%s: WEP requested\n", __func__);
++		if (flags & IW_ENCODE_OPEN) {
++			pr_debug("%s: open key mode\n", __func__);
++			wl->auth_method = GELIC_EURUS_AUTH_OPEN;
++		}
++		if (flags & IW_ENCODE_RESTRICTED) {
++			pr_debug("%s: shared key mode\n", __func__);
++			wl->auth_method = GELIC_EURUS_AUTH_SHARED;
++		}
++		if (IW_ENCODING_TOKEN_MAX < ext->key_len) {
++			pr_info("%s: key is too long %d\n", __func__,
++				ext->key_len);
++			ret = -EINVAL;
++			goto done;
++		}
++		/* OK, update the key */
++		wl->key_len[key_index] = ext->key_len;
++		memset(wl->key[key_index], 0, IW_ENCODING_TOKEN_MAX);
++		memcpy(wl->key[key_index], ext->key, ext->key_len);
++		set_bit(key_index, &wl->key_enabled);
++		/* remember wep info changed */
++		set_bit(GELIC_WL_STAT_CONFIGURED, &wl->stat);
++	} else if ((alg == IW_ENCODE_ALG_TKIP) || (alg == IW_ENCODE_ALG_CCMP)) {
++		pr_debug("%s: TKIP/CCMP requested alg=%d\n", __func__, alg);
++		/* check key length */
++		if (IW_ENCODING_TOKEN_MAX < ext->key_len) {
++			pr_info("%s: key is too long %d\n", __func__,
++				ext->key_len);
++			ret = -EINVAL;
++			goto done;
++		}
++		if (alg == IW_ENCODE_ALG_CCMP) {
++			pr_debug("%s: AES selected\n", __func__);
++			wl->group_cipher_method = GELIC_WL_CIPHER_AES;
++			wl->pairwise_cipher_method = GELIC_WL_CIPHER_AES;
++			wl->wpa_level = GELIC_WL_WPA_LEVEL_WPA2;
++		} else {
++			pr_debug("%s: TKIP selected, WPA forced\n", __func__);
++			wl->group_cipher_method = GELIC_WL_CIPHER_TKIP;
++			wl->pairwise_cipher_method = GELIC_WL_CIPHER_TKIP;
++			/* FIXME: how do we do if WPA2 + TKIP? */
++			wl->wpa_level = GELIC_WL_WPA_LEVEL_WPA;
++		}
++		if (flags & IW_ENCODE_RESTRICTED)
++			BUG();
++		wl->auth_method = GELIC_EURUS_AUTH_OPEN;
++		/* We should use same key for both and unicast */
++		if (ext->ext_flags & IW_ENCODE_EXT_GROUP_KEY)
++			pr_debug("%s: group key \n", __func__);
++		else
++			pr_debug("%s: unicast key \n", __func__);
++		/* OK, update the key */
++		wl->key_len[key_index] = ext->key_len;
++		memset(wl->key[key_index], 0, IW_ENCODING_TOKEN_MAX);
++		memcpy(wl->key[key_index], ext->key, ext->key_len);
++		set_bit(key_index, &wl->key_enabled);
++		/* remember info changed */
++		set_bit(GELIC_WL_STAT_CONFIGURED, &wl->stat);
++	}
++done:
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s: -> \n", __func__);
++	return ret;
++}
++
++static int gelic_wl_get_encodeext(struct net_device *netdev,
++				  struct iw_request_info *info,
++				  union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	struct iw_point *enc = &data->encoding;
++	struct iw_encode_ext *ext = (struct iw_encode_ext *)extra;
++	unsigned int irqflag;
++	int key_index;
++	int ret = 0;
++	int max_key_len;
++
++	pr_debug("%s: <- \n", __func__);
++
++	max_key_len = enc->length - sizeof(struct iw_encode_ext);
++	if (max_key_len < 0)
++		return -EINVAL;
++	key_index = enc->flags & IW_ENCODE_INDEX;
++
++	pr_debug("%s: key_index = %d\n", __func__, key_index);
++	pr_debug("%s: key_len = %d\n", __func__, enc->length);
++	pr_debug("%s: flag=%x\n", __func__, enc->flags & IW_ENCODE_FLAGS);
++
++	if (GELIC_WEP_KEYS < key_index)
++		return -EINVAL;
++
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (key_index)
++		key_index--;
++	else
++		key_index = wl->current_key;
++
++	memset(ext, 0, sizeof(struct iw_encode_ext));
++	switch (wl->group_cipher_method) {
++	case GELIC_WL_CIPHER_WEP:
++		ext->alg = IW_ENCODE_ALG_WEP;
++		enc->flags |= IW_ENCODE_ENABLED;
++		break;
++	case GELIC_WL_CIPHER_TKIP:
++		ext->alg = IW_ENCODE_ALG_TKIP;
++		enc->flags |= IW_ENCODE_ENABLED;
++		break;
++	case GELIC_WL_CIPHER_AES:
++		ext->alg = IW_ENCODE_ALG_CCMP;
++		enc->flags |= IW_ENCODE_ENABLED;
++		break;
++	case GELIC_WL_CIPHER_NONE:
++	default:
++		ext->alg = IW_ENCODE_ALG_NONE;
++		enc->flags |= IW_ENCODE_NOKEY;
++		break;
++	}
++
++	if (!(enc->flags & IW_ENCODE_NOKEY)) {
++		if (max_key_len < wl->key_len[key_index]) {
++			ret = -E2BIG;
++			goto out;
++		}
++		if (test_bit(key_index, &wl->key_enabled))
++			memcpy(ext->key, wl->key[key_index],
++			       wl->key_len[key_index]);
++		else
++			pr_debug("%s: disabled key requested ix=%d\n",
++				 __func__, key_index);
++	}
++out:
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s: -> \n", __func__);
++	return ret;
++}
++/* SIOC{S,G}IWMODE */
++static int gelic_wl_set_mode(struct net_device *netdev,
++			     struct iw_request_info *info,
++			     union iwreq_data *data, char *extra)
++{
++	__u32 mode = data->mode;
++	int ret;
++
++	pr_debug("%s: <- \n", __func__);
++	if (mode == IW_MODE_INFRA)
++		ret = 0;
++	else
++		ret = -EOPNOTSUPP;
++	pr_debug("%s: -> %d\n", __func__, ret);
++	return ret;
++}
++
++static int gelic_wl_get_mode(struct net_device *netdev,
++			     struct iw_request_info *info,
++			     union iwreq_data *data, char *extra)
++{
++	__u32 *mode = &data->mode;
++	pr_debug("%s: <- \n", __func__);
++	*mode = IW_MODE_INFRA;
++	pr_debug("%s: ->\n", __func__);
++	return 0;
++}
++
++/* SIOCIWFIRSTPRIV */
++static int hex2bin(u8 *str, u8 *bin, unsigned int len)
++{
++	unsigned int i;
++	static unsigned char *hex = "0123456789ABCDEF";
++	unsigned char *p, *q;
++	u8 tmp;
++
++	if (len != WPA_PSK_LEN * 2)
++		return -EINVAL;
++
++	for (i = 0; i < WPA_PSK_LEN * 2; i += 2) {
++		p = strchr(hex, toupper(str[i]));
++		q = strchr(hex, toupper(str[i + 1]));
++		if (!p || !q) {
++			pr_info("%s: unconvertible PSK digit=%d\n",
++				__func__, i);
++			return -EINVAL;
++		}
++		tmp = ((p - hex) << 4) + (q - hex);
++		*bin++ = tmp;
++	}
++	return 0;
++};
++
++static int gelic_wl_priv_set_psk(struct net_device *net_dev,
++				 struct iw_request_info *info,
++				 union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(net_dev));
++	unsigned int len;
++	unsigned int irqflag;
++	int ret = 0;
++
++	pr_debug("%s:<- len=%d\n", __func__, data->data.length);
++	len = data->data.length - 1;
++	if (len <= 2)
++		return -EINVAL;
++
++	spin_lock_irqsave(&wl->lock, irqflag);
++	if (extra[0] == '"' && extra[len - 1] == '"') {
++		pr_debug("%s: passphrase mode\n", __func__);
++		/* pass phrase */
++		if (GELIC_WL_EURUS_PSK_MAX_LEN < (len - 2)) {
++			pr_info("%s: passphrase too long\n", __func__);
++			ret = -E2BIG;
++			goto out;
++		}
++		memset(wl->psk, 0, sizeof(wl->psk));
++		wl->psk_len = len - 2;
++		memcpy(wl->psk, &(extra[1]), wl->psk_len);
++		wl->psk_type = GELIC_EURUS_WPA_PSK_PASSPHRASE;
++	} else {
++		ret = hex2bin(extra, wl->psk, len);
++		if (ret)
++			goto out;
++		wl->psk_len = WPA_PSK_LEN;
++		wl->psk_type = GELIC_EURUS_WPA_PSK_BIN;
++	}
++	set_bit(GELIC_WL_STAT_WPA_PSK_SET, &wl->stat);
++out:
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s:->\n", __func__);
++	return ret;
++}
++
++static int gelic_wl_priv_get_psk(struct net_device *net_dev,
++				 struct iw_request_info *info,
++				 union iwreq_data *data, char *extra)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(net_dev));
++	char *p;
++	unsigned int irqflag;
++	unsigned int i;
++
++	pr_debug("%s:<-\n", __func__);
++	if (!capable(CAP_NET_ADMIN))
++		return -EPERM;
++
++	spin_lock_irqsave(&wl->lock, irqflag);
++	p = extra;
++	if (test_bit(GELIC_WL_STAT_WPA_PSK_SET, &wl->stat)) {
++		if (wl->psk_type == GELIC_EURUS_WPA_PSK_BIN) {
++			for (i = 0; i < wl->psk_len; i++) {
++				sprintf(p, "%02xu", wl->psk[i]);
++				p += 2;
++			}
++			*p = '\0';
++			data->data.length = wl->psk_len * 2;
++		} else {
++			*p++ = '"';
++			memcpy(p, wl->psk, wl->psk_len);
++			p += wl->psk_len;
++			*p++ = '"';
++			*p = '\0';
++			data->data.length = wl->psk_len + 2;
++		}
++	} else
++		/* no psk set */
++		data->data.length = 0;
++	spin_unlock_irqrestore(&wl->lock, irqflag);
++	pr_debug("%s:-> %d\n", __func__, data->data.length);
++	return 0;
++}
++
++/* SIOCGIWNICKN */
++static int gelic_wl_get_nick(struct net_device *net_dev,
++				  struct iw_request_info *info,
++				  union iwreq_data *data, char *extra)
++{
++	strcpy(extra, "gelic_wl");
++	data->data.length = strlen(extra);
++	data->data.flags = 1;
++	return 0;
++}
++
++
++/* --- */
++
++static struct iw_statistics *gelic_wl_get_wireless_stats(
++	struct net_device *netdev)
++{
++
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	struct gelic_eurus_cmd *cmd;
++	struct iw_statistics *is;
++	struct gelic_eurus_rssi_info *rssi;
++
++	pr_debug("%s: <-\n", __func__);
++
++	is = &wl->iwstat;
++	memset(is, 0, sizeof(*is));
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_GET_RSSI_CFG,
++				   wl->buf, sizeof(*rssi));
++	if (cmd && !cmd->status && !cmd->cmd_status) {
++		rssi = wl->buf;
++		is->qual.level = be16_to_cpu(rssi->rssi);
++		is->qual.updated = IW_QUAL_LEVEL_UPDATED |
++			IW_QUAL_QUAL_INVALID | IW_QUAL_NOISE_INVALID;
++	} else
++		/* not associated */
++		is->qual.updated = IW_QUAL_ALL_INVALID;
++
++	kfree(cmd);
++	pr_debug("%s: ->\n", __func__);
++	return is;
++}
++
++/*
++ *  scanning helpers
++ */
++static int gelic_wl_start_scan(struct gelic_wl_info *wl, int always_scan)
++{
++	struct gelic_eurus_cmd *cmd;
++	int ret = 0;
++
++	pr_debug("%s: <- always=%d\n", __func__, always_scan);
++	if (down_interruptible(&wl->scan_lock))
++		return -ERESTARTSYS;
++
++	/*
++	 * If already a scan in progress, do not trigger more
++	 */
++	if (wl->scan_stat == GELIC_WL_SCAN_STAT_SCANNING) {
++		pr_debug("%s: scanning now\n", __func__);
++		goto out;
++	}
++
++	init_completion(&wl->scan_done);
++	/*
++	 * If we have already a bss list, don't try to get new
++	 */
++	if (!always_scan && wl->scan_stat == GELIC_WL_SCAN_STAT_GOT_LIST) {
++		pr_debug("%s: already has the list\n", __func__);
++		complete(&wl->scan_done);
++		goto out;
++	}
++	/*
++	 * issue start scan request
++	 */
++	wl->scan_stat = GELIC_WL_SCAN_STAT_SCANNING;
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_START_SCAN,
++				   NULL, 0);
++	if (!cmd || cmd->status || cmd->cmd_status) {
++		wl->scan_stat = GELIC_WL_SCAN_STAT_INIT;
++		complete(&wl->scan_done);
++		ret = -ENOMEM;
++		goto out;
++	}
++	kfree(cmd);
++out:
++	up(&wl->scan_lock);
++	pr_debug("%s: ->\n", __func__);
++	return ret;
++}
++
++/*
++ * retrieve scan result from the chip (hypervisor)
++ * this function is invoked by schedule work.
++ */
++static void gelic_wl_scan_complete_event(struct gelic_wl_info *wl)
++{
++	struct gelic_eurus_cmd *cmd = NULL;
++	struct gelic_wl_scan_info *target, *tmp;
++	struct gelic_wl_scan_info *oldest = NULL;
++	struct gelic_eurus_scan_info *scan_info;
++	unsigned int scan_info_size;
++	union iwreq_data data;
++	unsigned long this_time = jiffies;
++	unsigned int data_len, i, found, r;
++	DECLARE_MAC_BUF(mac);
++
++	pr_debug("%s:start\n", __func__);
++	down(&wl->scan_lock);
++
++	if (wl->scan_stat != GELIC_WL_SCAN_STAT_SCANNING) {
++		/*
++		 * stop() may be called while scanning, ignore result
++		 */
++		pr_debug("%s: scan complete when stat != scanning(%d)\n",
++			 __func__, wl->scan_stat);
++		goto out;
++	}
++
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_GET_SCAN,
++				   wl->buf, PAGE_SIZE);
++	if (!cmd || cmd->status || cmd->cmd_status) {
++		wl->scan_stat = GELIC_WL_SCAN_STAT_INIT;
++		pr_info("%s:cmd failed\n", __func__);
++		kfree(cmd);
++		goto out;
++	}
++	data_len = cmd->size;
++	pr_debug("%s: data_len = %d\n", __func__, data_len);
++	kfree(cmd);
++
++	/* OK, bss list retrieved */
++	wl->scan_stat = GELIC_WL_SCAN_STAT_GOT_LIST;
++
++	/* mark all entries are old */
++	list_for_each_entry_safe(target, tmp, &wl->network_list, list) {
++		target->valid = 0;
++		/* expire too old entries */
++		if (time_before(target->last_scanned + wl->scan_age,
++				this_time)) {
++			kfree(target->hwinfo);
++			target->hwinfo = NULL;
++			list_move_tail(&target->list, &wl->network_free_list);
++		}
++	}
++
++	/* put them in the newtork_list */
++	scan_info = wl->buf;
++	scan_info_size = 0;
++	i = 0;
++	while (scan_info_size < data_len) {
++		pr_debug("%s:size=%d bssid=%s scan_info=%p\n", __func__,
++			 be16_to_cpu(scan_info->size),
++			 print_mac(mac, &scan_info->bssid[2]), scan_info);
++		found = 0;
++		oldest = NULL;
++		list_for_each_entry(target, &wl->network_list, list) {
++			if (!compare_ether_addr(&target->hwinfo->bssid[2],
++						&scan_info->bssid[2])) {
++				found = 1;
++				pr_debug("%s: same BBS found scanned list\n",
++					 __func__);
++				break;
++			}
++			if (!oldest ||
++			    (target->last_scanned < oldest->last_scanned))
++				oldest = target;
++		}
++
++		if (!found) {
++			/* not found in the list */
++			if (list_empty(&wl->network_free_list)) {
++				/* expire oldest */
++				target = oldest;
++			} else {
++				target = list_entry(wl->network_free_list.next,
++						    struct gelic_wl_scan_info,
++						    list);
++			}
++		}
++
++		/* update the item */
++		target->last_scanned = this_time;
++		target->valid = 1;
++		target->eurus_index = i;
++		kfree(target->hwinfo);
++		target->hwinfo = kzalloc(be16_to_cpu(scan_info->size),
++					 GFP_KERNEL);
++		if (!target->hwinfo) {
++			pr_info("%s: kzalloc failed\n", __func__);
++			i++;
++			scan_info_size += be16_to_cpu(scan_info->size);
++			scan_info = (void *)scan_info +
++				be16_to_cpu(scan_info->size);
++			continue;
++		}
++		/* copy hw scan info */
++		memcpy(target->hwinfo, scan_info, scan_info->size);
++		target->essid_len = strnlen(scan_info->essid,
++					    sizeof(scan_info->essid));
++		target->rate_len = 0;
++		for (r = 0; r < MAX_RATES_LENGTH; r++)
++			if (scan_info->rate[r])
++				target->rate_len++;
++		if (8 < target->rate_len)
++			pr_info("%s: AP returns %d rates\n", __func__,
++				target->rate_len);
++		target->rate_ext_len = 0;
++		for (r = 0; r < MAX_RATES_EX_LENGTH; r++)
++			if (scan_info->ext_rate[r])
++				target->rate_ext_len++;
++		list_move_tail(&target->list, &wl->network_list);
++		/* bump pointer */
++		i++;
++		scan_info_size += be16_to_cpu(scan_info->size);
++		scan_info = (void *)scan_info + be16_to_cpu(scan_info->size);
++	}
++	memset(&data, 0, sizeof(data));
++	wireless_send_event(port_to_netdev(wl_port(wl)), SIOCGIWSCAN, &data,
++			    NULL);
++out:
++	complete(&wl->scan_done);
++	up(&wl->scan_lock);
++	pr_debug("%s:end\n", __func__);
++}
++
++/*
++ * Select an appropriate bss from current scan list regarding
++ * current settings from userspace.
++ * The caller must hold wl->scan_lock,
++ * and on the state of wl->scan_state == GELIC_WL_SCAN_GOT_LIST
++ */
++static void update_best(struct gelic_wl_scan_info **best,
++			struct gelic_wl_scan_info *candid,
++			int *best_weight,
++			int *weight)
++{
++	if (*best_weight < ++(*weight)) {
++		*best_weight = *weight;
++		*best = candid;
++	}
++}
++
++static
++struct gelic_wl_scan_info *gelic_wl_find_best_bss(struct gelic_wl_info *wl)
++{
++	struct gelic_wl_scan_info *scan_info;
++	struct gelic_wl_scan_info *best_bss;
++	int weight, best_weight;
++	u16 security;
++	DECLARE_MAC_BUF(mac);
++
++	pr_debug("%s: <-\n", __func__);
++
++	best_bss = NULL;
++	best_weight = 0;
++
++	list_for_each_entry(scan_info, &wl->network_list, list) {
++		pr_debug("%s: station %p\n", __func__, scan_info);
++
++		if (!scan_info->valid) {
++			pr_debug("%s: station invalid\n", __func__);
++			continue;
++		}
++
++		/* If bss specified, check it only */
++		if (test_bit(GELIC_WL_STAT_BSSID_SET, &wl->stat)) {
++			if (!compare_ether_addr(&scan_info->hwinfo->bssid[2],
++						wl->bssid)) {
++				best_bss = scan_info;
++				pr_debug("%s: bssid matched\n", __func__);
++				break;
++			} else {
++				pr_debug("%s: bssid unmached\n", __func__);
++				continue;
++			}
++		}
++
++		weight = 0;
++
++		/* security */
++		security = be16_to_cpu(scan_info->hwinfo->security) &
++			GELIC_EURUS_SCAN_SEC_MASK;
++		if (wl->wpa_level == GELIC_WL_WPA_LEVEL_WPA2) {
++			if (security == GELIC_EURUS_SCAN_SEC_WPA2)
++				update_best(&best_bss, scan_info,
++					    &best_weight, &weight);
++			else
++				continue;
++		} else if (wl->wpa_level == GELIC_WL_WPA_LEVEL_WPA) {
++			if (security == GELIC_EURUS_SCAN_SEC_WPA)
++				update_best(&best_bss, scan_info,
++					    &best_weight, &weight);
++			else
++				continue;
++		} else if (wl->wpa_level == GELIC_WL_WPA_LEVEL_NONE &&
++			   wl->group_cipher_method == GELIC_WL_CIPHER_WEP) {
++			if (security == GELIC_EURUS_SCAN_SEC_WEP)
++				update_best(&best_bss, scan_info,
++					    &best_weight, &weight);
++			else
++				continue;
++		}
++
++		/* If ESSID is set, check it */
++		if (test_bit(GELIC_WL_STAT_ESSID_SET, &wl->stat)) {
++			if ((scan_info->essid_len == wl->essid_len) &&
++			    !strncmp(wl->essid,
++				     scan_info->hwinfo->essid,
++				     scan_info->essid_len))
++				update_best(&best_bss, scan_info,
++					    &best_weight, &weight);
++			else
++				continue;
++		}
++	}
++
++#ifdef DEBUG
++	pr_debug("%s: -> bss=%p\n", __func__, best_bss);
++	if (best_bss) {
++		pr_debug("%s:addr=%s\n", __func__,
++			 print_mac(mac, &best_bss->hwinfo->bssid[2]));
++	}
++#endif
++	return best_bss;
++}
++
++/*
++ * Setup WEP configuration to the chip
++ * The caller must hold wl->scan_lock,
++ * and on the state of wl->scan_state == GELIC_WL_SCAN_GOT_LIST
++ */
++static int gelic_wl_do_wep_setup(struct gelic_wl_info *wl)
++{
++	unsigned int i;
++	struct gelic_eurus_wep_cfg *wep;
++	struct gelic_eurus_cmd *cmd;
++	int wep104 = 0;
++	int have_key = 0;
++	int ret = 0;
++
++	pr_debug("%s: <-\n", __func__);
++	/* we can assume no one should uses the buffer */
++	wep = wl->buf;
++	memset(wep, 0, sizeof(*wep));
++
++	if (wl->group_cipher_method == GELIC_WL_CIPHER_WEP) {
++		pr_debug("%s: WEP mode\n", __func__);
++		for (i = 0; i < GELIC_WEP_KEYS; i++) {
++			if (!test_bit(i, &wl->key_enabled))
++				continue;
++
++			pr_debug("%s: key#%d enabled\n", __func__, i);
++			have_key = 1;
++			if (wl->key_len[i] == 13)
++				wep104 = 1;
++			else if (wl->key_len[i] != 5) {
++				pr_info("%s: wrong wep key[%d]=%d\n",
++					__func__, i, wl->key_len[i]);
++				ret = -EINVAL;
++				goto out;
++			}
++			memcpy(wep->key[i], wl->key[i], wl->key_len[i]);
++		}
++
++		if (!have_key) {
++			pr_info("%s: all wep key disabled\n", __func__);
++			ret = -EINVAL;
++			goto out;
++		}
++
++		if (wep104) {
++			pr_debug("%s: 104bit key\n", __func__);
++			wep->security = cpu_to_be16(GELIC_EURUS_WEP_SEC_104BIT);
++		} else {
++			pr_debug("%s: 40bit key\n", __func__);
++			wep->security = cpu_to_be16(GELIC_EURUS_WEP_SEC_40BIT);
++		}
++	} else {
++		pr_debug("%s: NO encryption\n", __func__);
++		wep->security = cpu_to_be16(GELIC_EURUS_WEP_SEC_NONE);
++	}
++
++	/* issue wep setup */
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_SET_WEP_CFG,
++				   wep, sizeof(*wep));
++	if (!cmd)
++		ret = -ENOMEM;
++	else if (cmd->status || cmd->cmd_status)
++		ret = -ENXIO;
++
++	kfree(cmd);
++out:
++	pr_debug("%s: ->\n", __func__);
++	return ret;
++}
++
++#ifdef DEBUG
++static const char *wpasecstr(enum gelic_eurus_wpa_security sec)
++{
++	switch (sec) {
++	case GELIC_EURUS_WPA_SEC_NONE:
++		return "NONE";
++		break;
++	case GELIC_EURUS_WPA_SEC_WPA_TKIP_TKIP:
++		return "WPA_TKIP_TKIP";
++		break;
++	case GELIC_EURUS_WPA_SEC_WPA_TKIP_AES:
++		return "WPA_TKIP_AES";
++		break;
++	case GELIC_EURUS_WPA_SEC_WPA_AES_AES:
++		return "WPA_AES_AES";
++		break;
++	case GELIC_EURUS_WPA_SEC_WPA2_TKIP_TKIP:
++		return "WPA2_TKIP_TKIP";
++		break;
++	case GELIC_EURUS_WPA_SEC_WPA2_TKIP_AES:
++		return "WPA2_TKIP_AES";
++		break;
++	case GELIC_EURUS_WPA_SEC_WPA2_AES_AES:
++		return "WPA2_AES_AES";
++		break;
++	}
++	return "";
++};
++#endif
++
++static int gelic_wl_do_wpa_setup(struct gelic_wl_info *wl)
++{
++	struct gelic_eurus_wpa_cfg *wpa;
++	struct gelic_eurus_cmd *cmd;
++	u16 security;
++	int ret = 0;
++
++	pr_debug("%s: <-\n", __func__);
++	/* we can assume no one should uses the buffer */
++	wpa = wl->buf;
++	memset(wpa, 0, sizeof(*wpa));
++
++	if (!test_bit(GELIC_WL_STAT_WPA_PSK_SET, &wl->stat))
++		pr_info("%s: PSK not configured yet\n", __func__);
++
++	/* copy key */
++	memcpy(wpa->psk, wl->psk, wl->psk_len);
++
++	/* set security level */
++	if (wl->wpa_level == GELIC_WL_WPA_LEVEL_WPA2) {
++		if (wl->group_cipher_method == GELIC_WL_CIPHER_AES) {
++			security = GELIC_EURUS_WPA_SEC_WPA2_AES_AES;
++		} else {
++			if (wl->pairwise_cipher_method == GELIC_WL_CIPHER_AES &&
++			    precise_ie())
++				security = GELIC_EURUS_WPA_SEC_WPA2_TKIP_AES;
++			else
++				security = GELIC_EURUS_WPA_SEC_WPA2_TKIP_TKIP;
++		}
++	} else {
++		if (wl->group_cipher_method == GELIC_WL_CIPHER_AES) {
++			security = GELIC_EURUS_WPA_SEC_WPA_AES_AES;
++		} else {
++			if (wl->pairwise_cipher_method == GELIC_WL_CIPHER_AES &&
++			    precise_ie())
++				security = GELIC_EURUS_WPA_SEC_WPA_TKIP_AES;
++			else
++				security = GELIC_EURUS_WPA_SEC_WPA_TKIP_TKIP;
++		}
++	}
++	wpa->security = cpu_to_be16(security);
++
++	/* PSK type */
++	wpa->psk_type = cpu_to_be16(wl->psk_type);
++#ifdef DEBUG
++	pr_debug("%s: sec=%s psktype=%s\nn", __func__,
++		 wpasecstr(wpa->security),
++		 (wpa->psk_type == GELIC_EURUS_WPA_PSK_BIN) ?
++		 "BIN" : "passphrase");
++#if 0
++	/*
++	 * don't enable here if you plan to submit
++	 * the debug log because this dumps your precious
++	 * passphrase/key.
++	 */
++	pr_debug("%s: psk=%s\n",
++		 (wpa->psk_type == GELIC_EURUS_WPA_PSK_BIN) ?
++		 (char *)"N/A" : (char *)wpa->psk);
++#endif
++#endif
++	/* issue wpa setup */
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_SET_WPA_CFG,
++				   wpa, sizeof(*wpa));
++	if (!cmd)
++		ret = -ENOMEM;
++	else if (cmd->status || cmd->cmd_status)
++		ret = -ENXIO;
++	kfree(cmd);
++	pr_debug("%s: --> %d\n", __func__, ret);
++	return ret;
++}
++
++/*
++ * Start association. caller must hold assoc_stat_lock
++ */
++static int gelic_wl_associate_bss(struct gelic_wl_info *wl,
++				  struct gelic_wl_scan_info *bss)
++{
++	struct gelic_eurus_cmd *cmd;
++	struct gelic_eurus_common_cfg *common;
++	int ret = 0;
++	unsigned long rc;
++
++	pr_debug("%s: <-\n", __func__);
++
++	/* do common config */
++	common = wl->buf;
++	memset(common, 0, sizeof(*common));
++	common->bss_type = cpu_to_be16(GELIC_EURUS_BSS_INFRA);
++	common->op_mode = cpu_to_be16(GELIC_EURUS_OPMODE_11BG);
++
++	common->scan_index = cpu_to_be16(bss->eurus_index);
++	switch (wl->auth_method) {
++	case GELIC_EURUS_AUTH_OPEN:
++		common->auth_method = cpu_to_be16(GELIC_EURUS_AUTH_OPEN);
++		break;
++	case GELIC_EURUS_AUTH_SHARED:
++		common->auth_method = cpu_to_be16(GELIC_EURUS_AUTH_SHARED);
++		break;
++	}
++
++#ifdef DEBUG
++	scan_list_dump(wl);
++#endif
++	pr_debug("%s: common cfg index=%d bsstype=%d auth=%d\n", __func__,
++		 be16_to_cpu(common->scan_index),
++		 be16_to_cpu(common->bss_type),
++		 be16_to_cpu(common->auth_method));
++
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_SET_COMMON_CFG,
++				   common, sizeof(*common));
++	if (!cmd || cmd->status || cmd->cmd_status) {
++		ret = -ENOMEM;
++		kfree(cmd);
++		goto out;
++	}
++	kfree(cmd);
++
++	/* WEP/WPA */
++	switch (wl->wpa_level) {
++	case GELIC_WL_WPA_LEVEL_NONE:
++		/* If WEP or no security, setup WEP config */
++		ret = gelic_wl_do_wep_setup(wl);
++		break;
++	case GELIC_WL_WPA_LEVEL_WPA:
++	case GELIC_WL_WPA_LEVEL_WPA2:
++		ret = gelic_wl_do_wpa_setup(wl);
++		break;
++	};
++
++	if (ret) {
++		pr_debug("%s: WEP/WPA setup failed %d\n", __func__,
++			 ret);
++	}
++
++	/* start association */
++	init_completion(&wl->assoc_done);
++	wl->assoc_stat = GELIC_WL_ASSOC_STAT_ASSOCIATING;
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_ASSOC,
++				   NULL, 0);
++	if (!cmd || cmd->status || cmd->cmd_status) {
++		pr_debug("%s: assoc request failed\n", __func__);
++		wl->assoc_stat = GELIC_WL_ASSOC_STAT_DISCONN;
++		kfree(cmd);
++		ret = -ENOMEM;
++		gelic_wl_send_iwap_event(wl, NULL);
++		goto out;
++	}
++	kfree(cmd);
++
++	/* wait for connected event */
++	rc = wait_for_completion_timeout(&wl->assoc_done, HZ * 4);/*FIXME*/
++
++	if (!rc) {
++		/* timeouted.  Maybe key or cyrpt mode is wrong */
++		pr_info("%s: connect timeout \n", __func__);
++		cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_DISASSOC,
++					   NULL, 0);
++		kfree(cmd);
++		wl->assoc_stat = GELIC_WL_ASSOC_STAT_DISCONN;
++		gelic_wl_send_iwap_event(wl, NULL);
++		ret = -ENXIO;
++	} else {
++		wl->assoc_stat = GELIC_WL_ASSOC_STAT_ASSOCIATED;
++		/* copy bssid */
++		memcpy(wl->active_bssid, &bss->hwinfo->bssid[2], ETH_ALEN);
++
++		/* send connect event */
++		gelic_wl_send_iwap_event(wl, wl->active_bssid);
++		pr_info("%s: connected\n", __func__);
++	}
++out:
++	pr_debug("%s: ->\n", __func__);
++	return ret;
++}
++
++/*
++ * connected event
++ */
++static void gelic_wl_connected_event(struct gelic_wl_info *wl,
++				     u64 event)
++{
++	u64 desired_event = 0;
++
++	switch (wl->wpa_level) {
++	case GELIC_WL_WPA_LEVEL_NONE:
++		desired_event = GELIC_LV1_WL_EVENT_CONNECTED;
++		break;
++	case GELIC_WL_WPA_LEVEL_WPA:
++	case GELIC_WL_WPA_LEVEL_WPA2:
++		desired_event = GELIC_LV1_WL_EVENT_WPA_CONNECTED;
++		break;
++	}
++
++	if (desired_event == event) {
++		pr_debug("%s: completed \n", __func__);
++		complete(&wl->assoc_done);
++		netif_carrier_on(port_to_netdev(wl_port(wl)));
++	} else
++		pr_debug("%s: event %#lx under wpa\n",
++				 __func__, event);
++}
++
++/*
++ * disconnect event
++ */
++static void gelic_wl_disconnect_event(struct gelic_wl_info *wl,
++				      u64 event)
++{
++	struct gelic_eurus_cmd *cmd;
++	int lock;
++
++	/*
++	 * If we fall here in the middle of association,
++	 * associate_bss() should be waiting for complation of
++	 * wl->assoc_done.
++	 * As it waits with timeout, just leave assoc_done
++	 * uncompleted, then it terminates with timeout
++	 */
++	if (down_trylock(&wl->assoc_stat_lock)) {
++		pr_debug("%s: already locked\n", __func__);
++		lock = 0;
++	} else {
++		pr_debug("%s: obtain lock\n", __func__);
++		lock = 1;
++	}
++
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_DISASSOC, NULL, 0);
++	kfree(cmd);
++
++	/* send disconnected event to the supplicant */
++	if (wl->assoc_stat == GELIC_WL_ASSOC_STAT_ASSOCIATED)
++		gelic_wl_send_iwap_event(wl, NULL);
++
++	wl->assoc_stat = GELIC_WL_ASSOC_STAT_DISCONN;
++	netif_carrier_off(port_to_netdev(wl_port(wl)));
++
++	if (lock)
++		up(&wl->assoc_stat_lock);
++}
++/*
++ * event worker
++ */
++#ifdef DEBUG
++static const char *eventstr(enum gelic_lv1_wl_event event)
++{
++	static char buf[32];
++	char *ret;
++	if (event & GELIC_LV1_WL_EVENT_DEVICE_READY)
++		ret = "EURUS_READY";
++	else if (event & GELIC_LV1_WL_EVENT_SCAN_COMPLETED)
++		ret = "SCAN_COMPLETED";
++	else if (event & GELIC_LV1_WL_EVENT_DEAUTH)
++		ret = "DEAUTH";
++	else if (event & GELIC_LV1_WL_EVENT_BEACON_LOST)
++		ret = "BEACON_LOST";
++	else if (event & GELIC_LV1_WL_EVENT_CONNECTED)
++		ret = "CONNECTED";
++	else if (event & GELIC_LV1_WL_EVENT_WPA_CONNECTED)
++		ret = "WPA_CONNECTED";
++	else if (event & GELIC_LV1_WL_EVENT_WPA_ERROR)
++		ret = "WPA_ERROR";
++	else {
++		sprintf(buf, "Unknown(%#x)", event);
++		ret = buf;
++	}
++	return ret;
++}
++#else
++static const char *eventstr(enum gelic_lv1_wl_event event)
++{
++	return NULL;
++}
++#endif
++static void gelic_wl_event_worker(struct work_struct *work)
++{
++	struct gelic_wl_info *wl;
++	struct gelic_port *port;
++	u64 event, tmp;
++	int status;
++
++	pr_debug("%s:start\n", __func__);
++	wl = container_of(work, struct gelic_wl_info, event_work.work);
++	port = wl_port(wl);
++	while (1) {
++		status = lv1_net_control(bus_id(port->card), dev_id(port->card),
++					 GELIC_LV1_GET_WLAN_EVENT, 0, 0, 0,
++					 &event, &tmp);
++		if (status) {
++			if (status != LV1_NO_ENTRY)
++				pr_debug("%s:wlan event failed %d\n",
++					 __func__, status);
++			/* got all events */
++			pr_debug("%s:end\n", __func__);
++			return;
++		}
++		pr_debug("%s: event=%s\n", __func__, eventstr(event));
++		switch (event) {
++		case GELIC_LV1_WL_EVENT_SCAN_COMPLETED:
++			gelic_wl_scan_complete_event(wl);
++			break;
++		case GELIC_LV1_WL_EVENT_BEACON_LOST:
++		case GELIC_LV1_WL_EVENT_DEAUTH:
++			gelic_wl_disconnect_event(wl, event);
++			break;
++		case GELIC_LV1_WL_EVENT_CONNECTED:
++		case GELIC_LV1_WL_EVENT_WPA_CONNECTED:
++			gelic_wl_connected_event(wl, event);
++			break;
++		default:
++			break;
++		}
++	} /* while */
++}
++/*
++ * association worker
++ */
++static void gelic_wl_assoc_worker(struct work_struct *work)
++{
++	struct gelic_wl_info *wl;
++
++	struct gelic_wl_scan_info *best_bss;
++	int ret;
++
++	wl = container_of(work, struct gelic_wl_info, assoc_work.work);
++
++	down(&wl->assoc_stat_lock);
++
++	if (wl->assoc_stat != GELIC_WL_ASSOC_STAT_DISCONN)
++		goto out;
++
++	ret = gelic_wl_start_scan(wl, 0);
++	if (ret == -ERESTARTSYS) {
++		pr_debug("%s: scan start failed association\n", __func__);
++		schedule_delayed_work(&wl->assoc_work, HZ/10); /*FIXME*/
++		goto out;
++	} else if (ret) {
++		pr_info("%s: scan prerequisite failed\n", __func__);
++		goto out;
++	}
++
++	/*
++	 * Wait for bss scan completion
++	 * If we have scan list already, gelic_wl_start_scan()
++	 * returns OK and raises the complete.  Thus,
++	 * it's ok to wait unconditionally here
++	 */
++	wait_for_completion(&wl->scan_done);
++
++	pr_debug("%s: scan done\n", __func__);
++	down(&wl->scan_lock);
++	if (wl->scan_stat != GELIC_WL_SCAN_STAT_GOT_LIST) {
++		gelic_wl_send_iwap_event(wl, NULL);
++		pr_info("%s: no scan list. association failed\n", __func__);
++		goto scan_lock_out;
++	}
++
++	/* find best matching bss */
++	best_bss = gelic_wl_find_best_bss(wl);
++	if (!best_bss) {
++		gelic_wl_send_iwap_event(wl, NULL);
++		pr_info("%s: no bss matched. association failed\n", __func__);
++		goto scan_lock_out;
++	}
++
++	/* ok, do association */
++	ret = gelic_wl_associate_bss(wl, best_bss);
++	if (ret)
++		pr_info("%s: association failed %d\n", __func__, ret);
++scan_lock_out:
++	up(&wl->scan_lock);
++out:
++	up(&wl->assoc_stat_lock);
++}
++/*
++ * Interrupt handler
++ * Called from the ethernet interrupt handler
++ * Processes wireless specific virtual interrupts only
++ */
++void gelic_wl_interrupt(struct net_device *netdev, u64 status)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++
++	if (status & GELIC_CARD_WLAN_COMMAND_COMPLETED) {
++		pr_debug("%s:cmd complete\n", __func__);
++		complete(&wl->cmd_done_intr);
++	}
++
++	if (status & GELIC_CARD_WLAN_EVENT_RECEIVED) {
++		pr_debug("%s:event received\n", __func__);
++		queue_delayed_work(wl->event_queue, &wl->event_work, 0);
++	}
++}
++
++/*
++ * driver helpers
++ */
++#define IW_IOCTL(n) [(n) - SIOCSIWCOMMIT]
++static const iw_handler gelic_wl_wext_handler[] =
++{
++	IW_IOCTL(SIOCGIWNAME)		= gelic_wl_get_name,
++	IW_IOCTL(SIOCGIWRANGE)		= gelic_wl_get_range,
++	IW_IOCTL(SIOCSIWSCAN)		= gelic_wl_set_scan,
++	IW_IOCTL(SIOCGIWSCAN)		= gelic_wl_get_scan,
++	IW_IOCTL(SIOCSIWAUTH)		= gelic_wl_set_auth,
++	IW_IOCTL(SIOCGIWAUTH)		= gelic_wl_get_auth,
++	IW_IOCTL(SIOCSIWESSID)		= gelic_wl_set_essid,
++	IW_IOCTL(SIOCGIWESSID)		= gelic_wl_get_essid,
++	IW_IOCTL(SIOCSIWENCODE)		= gelic_wl_set_encode,
++	IW_IOCTL(SIOCGIWENCODE)		= gelic_wl_get_encode,
++	IW_IOCTL(SIOCSIWAP)		= gelic_wl_set_ap,
++	IW_IOCTL(SIOCGIWAP)		= gelic_wl_get_ap,
++	IW_IOCTL(SIOCSIWENCODEEXT)	= gelic_wl_set_encodeext,
++	IW_IOCTL(SIOCGIWENCODEEXT)	= gelic_wl_get_encodeext,
++	IW_IOCTL(SIOCSIWMODE)		= gelic_wl_set_mode,
++	IW_IOCTL(SIOCGIWMODE)		= gelic_wl_get_mode,
++	IW_IOCTL(SIOCGIWNICKN)		= gelic_wl_get_nick,
++};
++
++static struct iw_priv_args gelic_wl_private_args[] =
++{
++	{
++		.cmd = GELIC_WL_PRIV_SET_PSK,
++		.set_args = IW_PRIV_TYPE_CHAR |
++		(GELIC_WL_EURUS_PSK_MAX_LEN + 2),
++		.name = "set_psk"
++	},
++	{
++		.cmd = GELIC_WL_PRIV_GET_PSK,
++		.get_args = IW_PRIV_TYPE_CHAR |
++		(GELIC_WL_EURUS_PSK_MAX_LEN + 2),
++		.name = "get_psk"
++	}
++};
++
++static const iw_handler gelic_wl_private_handler[] =
++{
++	gelic_wl_priv_set_psk,
++	gelic_wl_priv_get_psk,
++};
++
++static const struct iw_handler_def gelic_wl_wext_handler_def = {
++	.num_standard		= ARRAY_SIZE(gelic_wl_wext_handler),
++	.standard		= gelic_wl_wext_handler,
++	.get_wireless_stats	= gelic_wl_get_wireless_stats,
++	.num_private		= ARRAY_SIZE(gelic_wl_private_handler),
++	.num_private_args	= ARRAY_SIZE(gelic_wl_private_args),
++	.private		= gelic_wl_private_handler,
++	.private_args		= gelic_wl_private_args,
++};
++
++static struct net_device *gelic_wl_alloc(struct gelic_card *card)
++{
++	struct net_device *netdev;
++	struct gelic_port *port;
++	struct gelic_wl_info *wl;
++	unsigned int i;
++
++	pr_debug("%s:start\n", __func__);
++	netdev = alloc_etherdev(sizeof(struct gelic_port) +
++				sizeof(struct gelic_wl_info));
++	pr_debug("%s: netdev =%p card=%p \np", __func__, netdev, card);
++	if (!netdev)
++		return NULL;
++
++	port = netdev_priv(netdev);
++	port->netdev = netdev;
++	port->card = card;
++	port->type = GELIC_PORT_WIRELESS;
++
++	wl = port_wl(port);
++	pr_debug("%s: wl=%p port=%p\n", __func__, wl, port);
++
++	/* allocate scan list */
++	wl->networks = kzalloc(sizeof(struct gelic_wl_scan_info) *
++			       GELIC_WL_BSS_MAX_ENT, GFP_KERNEL);
++
++	if (!wl->networks)
++		goto fail_bss;
++
++	wl->eurus_cmd_queue = create_singlethread_workqueue("gelic_cmd");
++	if (!wl->eurus_cmd_queue)
++		goto fail_cmd_workqueue;
++
++	wl->event_queue = create_singlethread_workqueue("gelic_event");
++	if (!wl->event_queue)
++		goto fail_event_workqueue;
++
++	INIT_LIST_HEAD(&wl->network_free_list);
++	INIT_LIST_HEAD(&wl->network_list);
++	for (i = 0; i < GELIC_WL_BSS_MAX_ENT; i++)
++		list_add_tail(&wl->networks[i].list,
++			      &wl->network_free_list);
++	init_completion(&wl->cmd_done_intr);
++
++	INIT_DELAYED_WORK(&wl->event_work, gelic_wl_event_worker);
++	INIT_DELAYED_WORK(&wl->assoc_work, gelic_wl_assoc_worker);
++	init_MUTEX(&wl->scan_lock);
++	init_MUTEX(&wl->assoc_stat_lock);
++
++	init_completion(&wl->scan_done);
++	/* for the case that no scan request is issued and stop() is called */
++	complete(&wl->scan_done);
++
++	spin_lock_init(&wl->lock);
++
++	wl->scan_age = 5*HZ; /* FIXME */
++
++	/* buffer for receiving scanned list etc */
++	BUILD_BUG_ON(PAGE_SIZE <
++		     sizeof(struct gelic_eurus_scan_info) *
++		     GELIC_EURUS_MAX_SCAN);
++	wl->buf = (void *)get_zeroed_page(GFP_KERNEL);
++	if (!wl->buf) {
++		pr_info("%s:buffer allocation failed\n", __func__);
++		goto fail_getpage;
++	}
++	pr_debug("%s:end\n", __func__);
++	return netdev;
++
++fail_getpage:
++	destroy_workqueue(wl->event_queue);
++fail_event_workqueue:
++	destroy_workqueue(wl->eurus_cmd_queue);
++fail_cmd_workqueue:
++	kfree(wl->networks);
++fail_bss:
++	free_netdev(netdev);
++	pr_debug("%s:end error\n", __func__);
++	return NULL;
++
++}
++
++static void gelic_wl_free(struct gelic_wl_info *wl)
++{
++	struct gelic_wl_scan_info *scan_info;
++	unsigned int i;
++
++	pr_debug("%s: <-\n", __func__);
++
++	pr_debug("%s: destroy queues\n", __func__);
++	destroy_workqueue(wl->eurus_cmd_queue);
++	destroy_workqueue(wl->event_queue);
++
++	scan_info = wl->networks;
++	for (i = 0; i < GELIC_WL_BSS_MAX_ENT; i++, scan_info++)
++		kfree(scan_info->hwinfo);
++	kfree(wl->networks);
++
++	free_netdev(port_to_netdev(wl_port(wl)));
++
++	pr_debug("%s: ->\n", __func__);
++}
++
++static int gelic_wl_try_associate(struct net_device *netdev)
++{
++	struct gelic_wl_info *wl = port_wl(netdev_priv(netdev));
++	int ret = -1;
++	unsigned int i;
++
++	pr_debug("%s: <-\n", __func__);
++
++	/* check constraits for start association */
++	/* for no access restriction AP */
++	if (wl->group_cipher_method == GELIC_WL_CIPHER_NONE) {
++		if (test_bit(GELIC_WL_STAT_CONFIGURED,
++			     &wl->stat))
++			goto do_associate;
++		else {
++			pr_debug("%s: no wep, not configured\n", __func__);
++			return ret;
++		}
++	}
++
++	/* for WEP, one of four keys should be set */
++	if (wl->group_cipher_method == GELIC_WL_CIPHER_WEP) {
++		/* one of keys set */
++		for (i = 0; i < GELIC_WEP_KEYS; i++) {
++			if (test_bit(i, &wl->key_enabled))
++			    goto do_associate;
++		}
++		pr_debug("%s: WEP, but no key specified\n", __func__);
++		return ret;
++	}
++
++	/* for WPA[2], psk should be set */
++	if ((wl->group_cipher_method == GELIC_WL_CIPHER_TKIP) ||
++	    (wl->group_cipher_method == GELIC_WL_CIPHER_AES)) {
++		if (test_bit(GELIC_WL_STAT_WPA_PSK_SET,
++			     &wl->stat))
++			goto do_associate;
++		else {
++			pr_debug("%s: AES/TKIP, but PSK not configured\n",
++				 __func__);
++			return ret;
++		}
++	}
++
++do_associate:
++	ret = schedule_delayed_work(&wl->assoc_work, 0);
++	pr_debug("%s: start association work %d\n", __func__, ret);
++	return ret;
++}
++
++/*
++ * netdev handlers
++ */
++static int gelic_wl_open(struct net_device *netdev)
++{
++	struct gelic_card *card = netdev_card(netdev);
++
++	pr_debug("%s:->%p\n", __func__, netdev);
++
++	gelic_card_up(card);
++
++	/* try to associate */
++	gelic_wl_try_associate(netdev);
++
++	netif_start_queue(netdev);
++
++	pr_debug("%s:<-\n", __func__);
++	return 0;
++}
++
++/*
++ * reset state machine
++ */
++static int gelic_wl_reset_state(struct gelic_wl_info *wl)
++{
++	struct gelic_wl_scan_info *target;
++	struct gelic_wl_scan_info *tmp;
++
++	/* empty scan list */
++	list_for_each_entry_safe(target, tmp, &wl->network_list, list) {
++		list_move_tail(&target->list, &wl->network_free_list);
++	}
++	wl->scan_stat = GELIC_WL_SCAN_STAT_INIT;
++
++	/* clear configuration */
++	wl->auth_method = GELIC_EURUS_AUTH_OPEN;
++	wl->group_cipher_method = GELIC_WL_CIPHER_NONE;
++	wl->pairwise_cipher_method = GELIC_WL_CIPHER_NONE;
++	wl->wpa_level = GELIC_WL_WPA_LEVEL_NONE;
++
++	wl->key_enabled = 0;
++	wl->current_key = 0;
++
++	wl->psk_type = GELIC_EURUS_WPA_PSK_PASSPHRASE;
++	wl->psk_len = 0;
++
++	wl->essid_len = 0;
++	memset(wl->essid, 0, sizeof(wl->essid));
++	memset(wl->bssid, 0, sizeof(wl->bssid));
++	memset(wl->active_bssid, 0, sizeof(wl->active_bssid));
++
++	wl->assoc_stat = GELIC_WL_ASSOC_STAT_DISCONN;
++
++	memset(&wl->iwstat, 0, sizeof(wl->iwstat));
++	/* all status bit clear */
++	wl->stat = 0;
++	return 0;
++}
++
++/*
++ * Tell eurus to terminate association
++ */
++static void gelic_wl_disconnect(struct net_device *netdev)
++{
++	struct gelic_port *port = netdev_priv(netdev);
++	struct gelic_wl_info *wl = port_wl(port);
++	struct gelic_eurus_cmd *cmd;
++
++	/*
++	 * If scann process is running on chip,
++	 * further requests will be rejected
++	 */
++	if (wl->scan_stat == GELIC_WL_SCAN_STAT_SCANNING)
++		wait_for_completion_timeout(&wl->scan_done, HZ);
++
++	cmd = gelic_eurus_sync_cmd(wl, GELIC_EURUS_CMD_DISASSOC, NULL, 0);
++	kfree(cmd);
++	gelic_wl_send_iwap_event(wl, NULL);
++};
++
++static int gelic_wl_stop(struct net_device *netdev)
++{
++	struct gelic_port *port = netdev_priv(netdev);
++	struct gelic_wl_info *wl = port_wl(port);
++	struct gelic_card *card = netdev_card(netdev);
++
++	pr_debug("%s:<-\n", __func__);
++
++	/*
++	 * Cancel pending association work.
++	 * event work can run after netdev down
++	 */
++	cancel_delayed_work(&wl->assoc_work);
++
++	if (wl->assoc_stat == GELIC_WL_ASSOC_STAT_ASSOCIATED)
++		gelic_wl_disconnect(netdev);
++
++	/* reset our state machine */
++	gelic_wl_reset_state(wl);
++
++	netif_stop_queue(netdev);
++
++	gelic_card_down(card);
++
++	pr_debug("%s:->\n", __func__);
++	return 0;
++}
++
++/* -- */
++
++static struct ethtool_ops gelic_wl_ethtool_ops = {
++	.get_drvinfo	= gelic_net_get_drvinfo,
++	.get_link	= gelic_wl_get_link,
++	.get_tx_csum	= ethtool_op_get_tx_csum,
++	.set_tx_csum	= ethtool_op_set_tx_csum,
++	.get_rx_csum	= gelic_net_get_rx_csum,
++	.set_rx_csum	= gelic_net_set_rx_csum,
++};
++
++static void gelic_wl_setup_netdev_ops(struct net_device *netdev)
++{
++	struct gelic_wl_info *wl;
++	wl = port_wl(netdev_priv(netdev));
++	BUG_ON(!wl);
++	netdev->open = &gelic_wl_open;
++	netdev->stop = &gelic_wl_stop;
++	netdev->hard_start_xmit = &gelic_net_xmit;
++	netdev->set_multicast_list = &gelic_net_set_multi;
++	netdev->change_mtu = &gelic_net_change_mtu;
++	netdev->wireless_data = &wl->wireless_data;
++	netdev->wireless_handlers = &gelic_wl_wext_handler_def;
++	/* tx watchdog */
++	netdev->tx_timeout = &gelic_net_tx_timeout;
++	netdev->watchdog_timeo = GELIC_NET_WATCHDOG_TIMEOUT;
++
++	netdev->ethtool_ops = &gelic_wl_ethtool_ops;
++#ifdef CONFIG_NET_POLL_CONTROLLER
++	netdev->poll_controller = gelic_net_poll_controller;
++#endif
++}
++
++/*
++ * driver probe/remove
++ */
++int gelic_wl_driver_probe(struct gelic_card *card)
++{
++	int ret;
++	struct net_device *netdev;
++
++	pr_debug("%s:start\n", __func__);
++
++	if (ps3_compare_firmware_version(1, 6, 0) < 0)
++		return 0;
++	if (!card->vlan[GELIC_PORT_WIRELESS].tx)
++		return 0;
++
++	/* alloc netdevice for wireless */
++	netdev = gelic_wl_alloc(card);
++	if (!netdev)
++		return -ENOMEM;
++
++	/* setup net_device structure */
++	gelic_wl_setup_netdev_ops(netdev);
++
++	/* setup some of net_device and register it */
++	ret = gelic_net_setup_netdev(netdev, card);
++	if (ret)
++		goto fail_setup;
++	card->netdev[GELIC_PORT_WIRELESS] = netdev;
++
++	/* add enable wireless interrupt */
++	card->irq_mask |= GELIC_CARD_WLAN_EVENT_RECEIVED |
++		GELIC_CARD_WLAN_COMMAND_COMPLETED;
++	/* to allow wireless commands while both interfaces are down */
++	gelic_card_set_irq_mask(card, GELIC_CARD_WLAN_EVENT_RECEIVED |
++				GELIC_CARD_WLAN_COMMAND_COMPLETED);
++	pr_debug("%s:end\n", __func__);
++	return 0;
++
++fail_setup:
++	gelic_wl_free(port_wl(netdev_port(netdev)));
++
++	return ret;
++}
++
++int gelic_wl_driver_remove(struct gelic_card *card)
++{
++	struct gelic_wl_info *wl;
++	struct net_device *netdev;
++
++	pr_debug("%s:start\n", __func__);
++
++	if (ps3_compare_firmware_version(1, 6, 0) < 0)
++		return 0;
++	if (!card->vlan[GELIC_PORT_WIRELESS].tx)
++		return 0;
++
++	netdev = card->netdev[GELIC_PORT_WIRELESS];
++	wl = port_wl(netdev_priv(netdev));
++
++	/* if the interface was not up, but associated */
++	if (wl->assoc_stat == GELIC_WL_ASSOC_STAT_ASSOCIATED)
++		gelic_wl_disconnect(netdev);
++
++	complete(&wl->cmd_done_intr);
++
++	/* cancel all work queue */
++	cancel_delayed_work(&wl->assoc_work);
++	cancel_delayed_work(&wl->event_work);
++	flush_workqueue(wl->eurus_cmd_queue);
++	flush_workqueue(wl->event_queue);
++
++	unregister_netdev(netdev);
++
++	/* disable wireless interrupt */
++	pr_debug("%s: disable intr\n", __func__);
++	card->irq_mask &= ~(GELIC_CARD_WLAN_EVENT_RECEIVED |
++			    GELIC_CARD_WLAN_COMMAND_COMPLETED);
++	/* free bss list, netdev*/
++	gelic_wl_free(wl);
++	pr_debug("%s:end\n", __func__);
++	return 0;
++}
+--- /dev/null
++++ b/drivers/net/ps3_gelic_wireless.h
+@@ -0,0 +1,329 @@
++/*
++ *  PS3 gelic network driver.
++ *
++ * Copyright (C) 2007 Sony Computer Entertainment Inc.
++ * Copyright 2007 Sony Corporation
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License as published by
++ * the Free Software Foundation version 2.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
++ */
++#ifndef _GELIC_WIRELESS_H
++#define _GELIC_WIRELESS_H
++
++#include <linux/wireless.h>
++#include <net/iw_handler.h>
++
++
++/* return value from  GELIC_LV1_GET_WLAN_EVENT netcontrol */
++enum gelic_lv1_wl_event {
++	GELIC_LV1_WL_EVENT_DEVICE_READY   = 0x01, /* Eurus ready */
++	GELIC_LV1_WL_EVENT_SCAN_COMPLETED = 0x02, /* Scan has completed */
++	GELIC_LV1_WL_EVENT_DEAUTH         = 0x04, /* Deauthed by the AP */
++	GELIC_LV1_WL_EVENT_BEACON_LOST    = 0x08, /* Beacon lost detected */
++	GELIC_LV1_WL_EVENT_CONNECTED      = 0x10, /* Connected to AP */
++	GELIC_LV1_WL_EVENT_WPA_CONNECTED  = 0x20, /* WPA connection */
++	GELIC_LV1_WL_EVENT_WPA_ERROR      = 0x40, /* MIC error */
++};
++
++/* arguments for GELIC_LV1_POST_WLAN_COMMAND netcontrol */
++enum gelic_eurus_command {
++	GELIC_EURUS_CMD_ASSOC		=  1, /* association start */
++	GELIC_EURUS_CMD_DISASSOC	=  2, /* disassociate      */
++	GELIC_EURUS_CMD_START_SCAN	=  3, /* scan start        */
++	GELIC_EURUS_CMD_GET_SCAN	=  4, /* get scan result   */
++	GELIC_EURUS_CMD_SET_COMMON_CFG	=  5, /* set common config */
++	GELIC_EURUS_CMD_GET_COMMON_CFG	=  6, /* set common config */
++	GELIC_EURUS_CMD_SET_WEP_CFG	=  7, /* set WEP config    */
++	GELIC_EURUS_CMD_GET_WEP_CFG	=  8, /* get WEP config    */
++	GELIC_EURUS_CMD_SET_WPA_CFG	=  9, /* set WPA config    */
++	GELIC_EURUS_CMD_GET_WPA_CFG	= 10, /* get WPA config    */
++	GELIC_EURUS_CMD_GET_RSSI_CFG	= 11, /* get RSSI info.    */
++	GELIC_EURUS_CMD_MAX_INDEX
++};
++
++/* for GELIC_EURUS_CMD_COMMON_CFG */
++enum gelic_eurus_bss_type {
++	GELIC_EURUS_BSS_INFRA = 0,
++	GELIC_EURUS_BSS_ADHOC = 1, /* not supported */
++};
++
++enum gelic_eurus_auth_method {
++	GELIC_EURUS_AUTH_OPEN = 0, /* FIXME: WLAN_AUTH_OPEN */
++	GELIC_EURUS_AUTH_SHARED = 1, /* not supported */
++};
++
++enum gelic_eurus_opmode {
++	GELIC_EURUS_OPMODE_11BG = 0, /* 802.11b/g */
++	GELIC_EURUS_OPMODE_11B = 1, /* 802.11b only */
++	GELIC_EURUS_OPMODE_11G = 2, /* 802.11g only */
++};
++
++struct gelic_eurus_common_cfg {
++	/* all fields are big endian */
++	u16 scan_index;
++	u16 bss_type;    /* infra or adhoc */
++	u16 auth_method; /* shared key or open */
++	u16 op_mode; /* B/G */
++} __attribute__((packed));
++
++
++/* for GELIC_EURUS_CMD_WEP_CFG */
++enum gelic_eurus_wep_security {
++	GELIC_EURUS_WEP_SEC_NONE	= 0,
++	GELIC_EURUS_WEP_SEC_40BIT	= 1,
++	GELIC_EURUS_WEP_SEC_104BIT	= 2,
++};
++
++struct gelic_eurus_wep_cfg {
++	/* all fields are big endian */
++	u16 security;
++	u8 key[4][16];
++} __attribute__((packed));
++
++/* for GELIC_EURUS_CMD_WPA_CFG */
++enum gelic_eurus_wpa_security {
++	GELIC_EURUS_WPA_SEC_NONE		= 0x0000,
++	/* group=TKIP, pairwise=TKIP */
++	GELIC_EURUS_WPA_SEC_WPA_TKIP_TKIP	= 0x0001,
++	/* group=AES, pairwise=AES */
++	GELIC_EURUS_WPA_SEC_WPA_AES_AES		= 0x0002,
++	/* group=TKIP, pairwise=TKIP */
++	GELIC_EURUS_WPA_SEC_WPA2_TKIP_TKIP	= 0x0004,
++	/* group=AES, pairwise=AES */
++	GELIC_EURUS_WPA_SEC_WPA2_AES_AES	= 0x0008,
++	/* group=TKIP, pairwise=AES */
++	GELIC_EURUS_WPA_SEC_WPA_TKIP_AES	= 0x0010,
++	/* group=TKIP, pairwise=AES */
++	GELIC_EURUS_WPA_SEC_WPA2_TKIP_AES	= 0x0020,
++};
++
++enum gelic_eurus_wpa_psk_type {
++	GELIC_EURUS_WPA_PSK_PASSPHRASE	= 0, /* passphrase string   */
++	GELIC_EURUS_WPA_PSK_BIN		= 1, /* 32 bytes binary key */
++};
++
++#define GELIC_WL_EURUS_PSK_MAX_LEN	64
++#define WPA_PSK_LEN			32 /* WPA spec says 256bit */
++
++struct gelic_eurus_wpa_cfg {
++	/* all fields are big endian */
++	u16 security;
++	u16 psk_type; /* psk key encoding type */
++	u8 psk[GELIC_WL_EURUS_PSK_MAX_LEN]; /* psk key; hex or passphrase */
++} __attribute__((packed));
++
++/* for GELIC_EURUS_CMD_{START,GET}_SCAN */
++enum gelic_eurus_scan_capability {
++	GELIC_EURUS_SCAN_CAP_ADHOC	= 0x0000,
++	GELIC_EURUS_SCAN_CAP_INFRA	= 0x0001,
++	GELIC_EURUS_SCAN_CAP_MASK	= 0x0001,
++};
++
++enum gelic_eurus_scan_sec_type {
++	GELIC_EURUS_SCAN_SEC_NONE	= 0x0000,
++	GELIC_EURUS_SCAN_SEC_WEP	= 0x0100,
++	GELIC_EURUS_SCAN_SEC_WPA	= 0x0200,
++	GELIC_EURUS_SCAN_SEC_WPA2	= 0x0400,
++	GELIC_EURUS_SCAN_SEC_MASK	= 0x0f00,
++};
++
++enum gelic_eurus_scan_sec_wep_type {
++	GELIC_EURUS_SCAN_SEC_WEP_UNKNOWN	= 0x0000,
++	GELIC_EURUS_SCAN_SEC_WEP_40		= 0x0001,
++	GELIC_EURUS_SCAN_SEC_WEP_104		= 0x0002,
++	GELIC_EURUS_SCAN_SEC_WEP_MASK		= 0x0003,
++};
++
++enum gelic_eurus_scan_sec_wpa_type {
++	GELIC_EURUS_SCAN_SEC_WPA_UNKNOWN	= 0x0000,
++	GELIC_EURUS_SCAN_SEC_WPA_TKIP		= 0x0001,
++	GELIC_EURUS_SCAN_SEC_WPA_AES		= 0x0002,
++	GELIC_EURUS_SCAN_SEC_WPA_MASK		= 0x0003,
++};
++
++/*
++ * hw BSS information structure returned from GELIC_EURUS_CMD_GET_SCAN
++ */
++struct gelic_eurus_scan_info {
++	/* all fields are big endian */
++	__be16 size;
++	__be16 rssi; /* percentage */
++	__be16 channel; /* channel number */
++	__be16 beacon_period; /* FIXME: in msec unit */
++	__be16 capability;
++	__be16 security;
++	u8  bssid[8]; /* last ETH_ALEN are valid. bssid[0],[1] are unused */
++	u8  essid[32]; /* IW_ESSID_MAX_SIZE */
++	u8  rate[16]; /* first MAX_RATES_LENGTH(12) are valid */
++	u8  ext_rate[16]; /* first MAX_RATES_EX_LENGTH(16) are valid */
++	__be32 reserved1;
++	__be32 reserved2;
++	__be32 reserved3;
++	__be32 reserved4;
++	u8 elements[0]; /* ie */
++} __attribute__ ((packed));
++
++/* the hypervisor returns bbs up to 16 */
++#define GELIC_EURUS_MAX_SCAN  (16)
++struct gelic_wl_scan_info {
++	struct list_head list;
++	struct gelic_eurus_scan_info *hwinfo;
++
++	int valid; /* set 1 if this entry was in latest scanned list
++		     * from Eurus */
++	unsigned int eurus_index; /* index in the Eurus list */
++	unsigned long last_scanned; /* acquired time */
++
++	unsigned int rate_len;
++	unsigned int rate_ext_len;
++	unsigned int essid_len;
++};
++
++/* for GELIC_EURUS_CMD_GET_RSSI */
++struct gelic_eurus_rssi_info {
++	/* big endian */
++	__be16 rssi;
++} __attribute__ ((packed));
++
++
++/* for 'stat' member of gelic_wl_info */
++enum gelic_wl_info_status_bit {
++	GELIC_WL_STAT_CONFIGURED,
++	GELIC_WL_STAT_CH_INFO,   /* ch info aquired */
++	GELIC_WL_STAT_ESSID_SET, /* ESSID specified by userspace */
++	GELIC_WL_STAT_BSSID_SET, /* BSSID specified by userspace */
++	GELIC_WL_STAT_WPA_PSK_SET, /* PMK specified by userspace */
++	GELIC_WL_STAT_WPA_LEVEL_SET, /* WEP or WPA[2] selected */
++};
++
++/* for 'scan_stat' member of gelic_wl_info */
++enum gelic_wl_scan_state {
++	/* just initialized or get last scan result failed */
++	GELIC_WL_SCAN_STAT_INIT,
++	/* scan request issued, accepted or chip is scanning */
++	GELIC_WL_SCAN_STAT_SCANNING,
++	/* scan results retrieved */
++	GELIC_WL_SCAN_STAT_GOT_LIST,
++};
++
++/* for 'cipher_method' */
++enum gelic_wl_cipher_method {
++	GELIC_WL_CIPHER_NONE,
++	GELIC_WL_CIPHER_WEP,
++	GELIC_WL_CIPHER_TKIP,
++	GELIC_WL_CIPHER_AES,
++};
++
++/* for 'wpa_level' */
++enum gelic_wl_wpa_level {
++	GELIC_WL_WPA_LEVEL_NONE,
++	GELIC_WL_WPA_LEVEL_WPA,
++	GELIC_WL_WPA_LEVEL_WPA2,
++};
++
++/* for 'assoc_stat' */
++enum gelic_wl_assoc_state {
++	GELIC_WL_ASSOC_STAT_DISCONN,
++	GELIC_WL_ASSOC_STAT_ASSOCIATING,
++	GELIC_WL_ASSOC_STAT_ASSOCIATED,
++};
++/* part of private data alloc_etherdev() allocated */
++#define GELIC_WEP_KEYS 4
++struct gelic_wl_info {
++	/* bss list */
++	struct semaphore scan_lock;
++	struct list_head network_list;
++	struct list_head network_free_list;
++	struct gelic_wl_scan_info *networks;
++
++	unsigned long scan_age; /* last scanned time */
++	enum gelic_wl_scan_state scan_stat;
++	struct completion scan_done;
++
++	/* eurus command queue */
++	struct workqueue_struct *eurus_cmd_queue;
++	struct completion cmd_done_intr;
++
++	/* eurus event handling */
++	struct workqueue_struct *event_queue;
++	struct delayed_work event_work;
++
++	/* wl status bits */
++	unsigned long stat;
++	enum gelic_eurus_auth_method auth_method; /* open/shared */
++	enum gelic_wl_cipher_method group_cipher_method;
++	enum gelic_wl_cipher_method pairwise_cipher_method;
++	enum gelic_wl_wpa_level wpa_level; /* wpa/wpa2 */
++
++	/* association handling */
++	struct semaphore assoc_stat_lock;
++	struct delayed_work assoc_work;
++	enum gelic_wl_assoc_state assoc_stat;
++	struct completion assoc_done;
++
++	spinlock_t lock;
++	u16 ch_info; /* available channels. bit0 = ch1 */
++	/* WEP keys */
++	u8 key[GELIC_WEP_KEYS][IW_ENCODING_TOKEN_MAX];
++	unsigned long key_enabled;
++	unsigned int key_len[GELIC_WEP_KEYS];
++	unsigned int current_key;
++	/* WWPA PSK */
++	u8 psk[GELIC_WL_EURUS_PSK_MAX_LEN];
++	enum gelic_eurus_wpa_psk_type psk_type;
++	unsigned int psk_len;
++
++	u8 essid[IW_ESSID_MAX_SIZE];
++	u8 bssid[ETH_ALEN]; /* userland requested */
++	u8 active_bssid[ETH_ALEN]; /* associated bssid */
++	unsigned int essid_len;
++
++	/* buffer for hypervisor IO */
++	void *buf;
++
++	struct iw_public_data wireless_data;
++	struct iw_statistics iwstat;
++};
++
++#define GELIC_WL_BSS_MAX_ENT 32
++#define GELIC_WL_ASSOC_RETRY 50
++static inline struct gelic_port *wl_port(struct gelic_wl_info *wl)
++{
++	return container_of((void *)wl, struct gelic_port, priv);
++}
++static inline struct gelic_wl_info *port_wl(struct gelic_port *port)
++{
++	return port_priv(port);
++}
++
++struct gelic_eurus_cmd {
++	struct work_struct work;
++	struct gelic_wl_info *wl;
++	unsigned int cmd; /* command code */
++	u64 tag;
++	u64 size;
++	void *buffer;
++	unsigned int buf_size;
++	struct completion done;
++	int status;
++	u64 cmd_status;
++};
++
++/* private ioctls to pass PSK */
++#define GELIC_WL_PRIV_SET_PSK		(SIOCIWFIRSTPRIV + 0)
++#define GELIC_WL_PRIV_GET_PSK		(SIOCIWFIRSTPRIV + 1)
++
++extern int gelic_wl_driver_probe(struct gelic_card *card);
++extern int gelic_wl_driver_remove(struct gelic_card *card);
++extern void gelic_wl_interrupt(struct net_device *netdev, u64 status);
++#endif /* _GELIC_WIRELESS_H */
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-wireless.diff linux-2.6.25-id/patches/ps3-wip/ps3-gelic-wireless.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-wireless.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-wireless.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,2520 @@
+Subject: ps3: wireless LAN driver
+
+Add support of the internal wireless LAN of PS3.
+ 
+Signed-off-by: Masakazu Mokuno <mokuno@sm.sony.co.jp>
+---
+ drivers/net/Kconfig              |    8 
+ drivers/net/Makefile             |    3 
+ drivers/net/ps3_gelic_net.c      |   28 
+ drivers/net/ps3_gelic_net.h      |    5 
+ drivers/net/ps3_gelic_wireless.c | 2126 +++++++++++++++++++++++++++++++++++++++
+ drivers/net/ps3_gelic_wireless.h |  246 ++++
+ 6 files changed, 2414 insertions(+), 2 deletions(-)
+
+--- a/drivers/net/Kconfig
++++ b/drivers/net/Kconfig
+@@ -2302,6 +2302,14 @@ config GELIC_NET
+ 	  To compile this driver as a module, choose M here: the
+ 	  module will be called ps3_gelic.
+ 
++config GELIC_WIRELESS
++	bool "PS3 Wireless support"
++	depends on GELIC_NET
++	select WIRELESS_EXT
++	help
++	  This option enables the wireless networking support of
++	  the PS3 network driver.
++
+ config GIANFAR
+ 	tristate "Gianfar Ethernet"
+ 	depends on 85xx || 83xx || PPC_86xx
+--- a/drivers/net/Makefile
++++ b/drivers/net/Makefile
+@@ -66,7 +66,8 @@ obj-$(CONFIG_BNX2) += bnx2.o
+ spidernet-y += spider_net.o spider_net_ethtool.o
+ obj-$(CONFIG_SPIDER_NET) += spidernet.o sungem_phy.o
+ obj-$(CONFIG_GELIC_NET) += ps3_gelic.o
+-ps3_gelic-objs += ps3_gelic_net.o
++gelic_wireless-$(CONFIG_GELIC_WIRELESS) += ps3_gelic_wireless.o
++ps3_gelic-objs += ps3_gelic_net.o $(gelic_wireless-y)
+ obj-$(CONFIG_TC35815) += tc35815.o
+ obj-$(CONFIG_SKGE) += skge.o
+ obj-$(CONFIG_SKY2) += sky2.o
+--- a/drivers/net/ps3_gelic_net.c
++++ b/drivers/net/ps3_gelic_net.c
+@@ -551,6 +551,9 @@ static int gelic_net_stop(struct net_dev
+ 	struct gelic_net_card *card = netdev_priv(netdev);
+ 
+ 	napi_disable(&card->napi);
++#ifdef CONFIG_GELIC_WIRELESS
++	gelicw_down(netdev);
++#endif
+ 	netif_stop_queue(netdev);
+ 
+ 	/* turn off DMA, force end */
+@@ -1034,6 +1037,9 @@ static irqreturn_t gelic_net_interrupt(i
+ 		gelic_net_kick_txdma(card, card->tx_chain.tail);
+ 		spin_unlock_irqrestore(&card->tx_dma_lock, flags);
+ 	}
++#ifdef CONFIG_GELIC_WIRELESS
++	gelicw_interrupt(netdev, status);
++#endif
+ 	return IRQ_HANDLED;
+ }
+ 
+@@ -1131,12 +1137,18 @@ static int gelic_net_open(struct net_dev
+ 
+ 	card->tx_dma_progress = 0;
+ 	card->ghiintmask = GELIC_NET_RXINT | GELIC_NET_TXINT;
++#ifdef CONFIG_GELIC_WIRELESS
++	card->ghiintmask |= GELICW_DEVICE_CMD_COMP | GELICW_DEVICE_EVENT_RECV;
++#endif
+ 
+ 	gelic_net_set_irq_mask(card, card->ghiintmask);
+ 	gelic_net_enable_rxdmac(card);
+ 
+ 	netif_start_queue(netdev);
+ 	netif_carrier_on(netdev);
++#ifdef CONFIG_GELIC_WIRELESS
++ 	gelicw_up(netdev);
++#endif
+ 
+ 	return 0;
+ 
+@@ -1214,7 +1226,12 @@ static u32 gelic_net_get_link(struct net
+ 		link = 1;
+ 	else
+ 		link = 0;
+-
++#ifdef CONFIG_GELIC_WIRELESS
++	/* (v1 & GELIC_NET_LINK_UP) is always 0 in wireless mode */
++	if (gelicw_is_associated(netdev)) {
++		link = 1;
++	}
++#endif
+ 	return link;
+ }
+ 
+@@ -1404,6 +1421,12 @@ static int gelic_net_setup_netdev(struct
+ 		netdev->hard_header_len += VLAN_HLEN;
+ 	}
+ 
++#ifdef CONFIG_GELIC_WIRELESS
++	card->w.card = card;
++	/* init wireless extension */
++	/* No wireless vlan_index:-1 */
++	gelicw_setup_netdev(netdev, card->vlan_index);
++#endif
+ 	status = register_netdev(netdev);
+ 	if (status) {
+ 		dev_err(ctodev(card), "%s:Couldn't register net_device: %d\n",
+@@ -1533,6 +1556,9 @@ static int ps3_gelic_driver_remove (stru
+ {
+ 	struct gelic_net_card *card = ps3_system_bus_get_driver_data(dev);
+ 
++#ifdef CONFIG_GELIC_WIRELESS
++	gelicw_remove(card->netdev);
++#endif
+ 	wait_event(card->waitq,
+ 		   atomic_read(&card->tx_timeout_task_counter) == 0);
+ 
+--- a/drivers/net/ps3_gelic_net.h
++++ b/drivers/net/ps3_gelic_net.h
+@@ -28,6 +28,8 @@
+ #ifndef _GELIC_NET_H
+ #define _GELIC_NET_H
+ 
++#include "ps3_gelic_wireless.h"
++
+ /* descriptors */
+ #define GELIC_NET_RX_DESCRIPTORS        128 /* num of descriptors */
+ #define GELIC_NET_TX_DESCRIPTORS        128 /* num of descriptors */
+@@ -223,6 +225,9 @@ struct gelic_net_card {
+ 	wait_queue_head_t waitq;
+ 
+ 	struct gelic_net_descr *tx_top, *rx_top;
++#ifdef CONFIG_GELIC_WIRELESS
++	struct gelic_wireless w;
++#endif
+ 	struct gelic_net_descr descr[0];
+ };
+ 
+--- /dev/null
++++ b/drivers/net/ps3_gelic_wireless.c
+@@ -0,0 +1,2126 @@
++/*
++ * gelic_wireless.c: wireless extension for gelic_net
++ *
++ * Copyright (C) 2007 Sony Computer Entertainment Inc.
++ * Copyright 2006, 2007 Sony Corporation
++ *
++ * This program is free software; you can redistribute it and/or modify it
++ * under the terms of the GNU General Public License as published
++ * by the Free Software Foundation; version 2 of the License.
++ *
++ * This program is distributed in the hope that it will be useful, but
++ * WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License along
++ * with this program; if not, write to the Free Software Foundation, Inc.,
++ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
++ */
++
++#undef DEBUG
++
++#include <linux/kernel.h>
++#include <linux/netdevice.h>
++#include <linux/if_vlan.h>
++#include <linux/completion.h>
++#include <linux/ctype.h>
++#include <asm/ps3.h>
++#include <asm/lv1call.h>
++
++#include "ps3_gelic_net.h"
++
++static struct iw_handler_def gelicw_handler_def;
++
++/*
++ * Data tables
++ */
++static const u32 freq_list[] = {
++	2412, 2417, 2422, 2427, 2432,
++	2437, 2442, 2447, 2452, 2457,
++	2462, 2467, 2472, 2484 };
++
++static const u32 bitrate_list[] = {
++	 1000000,  2000000,  5500000, 11000000, /* 11b */
++	 6000000,  9000000, 12000000, 18000000,
++	24000000, 36000000, 48000000, 54000000
++};
++#define GELICW_NUM_11B_BITRATES 4 /* 802.11b: 1 ~ 11 Mb/s */
++
++static inline struct device * ntodev(struct net_device *netdev)
++{
++	return &((struct gelic_net_card *)netdev_priv(netdev))->dev->core;
++}
++
++static inline struct device * wtodev(struct gelic_wireless *w)
++{
++	return &w->card->dev->core;
++}
++static inline struct gelic_wireless *gelicw_priv(struct net_device *netdev)
++{
++	struct gelic_net_card *card = netdev_priv(netdev);
++	return &card->w;
++}
++static inline unsigned int bus_id(struct gelic_wireless *w)
++{
++	return w->card->dev->bus_id;
++}
++static inline unsigned int dev_id(struct gelic_wireless *w)
++{
++	return w->card->dev->dev_id;
++}
++
++/* control wired or wireless */
++static void gelicw_vlan_mode(struct net_device *netdev, int mode)
++{
++	struct gelic_net_card *card = netdev_priv(netdev);
++
++	if ((mode < GELIC_NET_VLAN_WIRED) ||
++	    (mode > GELIC_NET_VLAN_WIRELESS))
++		return;
++
++	card->vlan_index = mode - 1;
++}
++
++/* wireless_send_event */
++static void notify_assoc_event(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	union iwreq_data wrqu;
++
++	cancel_delayed_work(&w->work_scan_all);
++	cancel_delayed_work(&w->work_scan_essid);
++
++	wrqu.ap_addr.sa_family = ARPHRD_ETHER;
++	if (w->state < GELICW_STATE_ASSOCIATED) {
++		dev_dbg(ntodev(netdev), "notify disassociated\n");
++		w->is_assoc = 0;
++		memset(w->bssid, 0, ETH_ALEN);
++	} else {
++		dev_dbg(ntodev(netdev), "notify associated\n");
++		w->channel = w->current_bss.channel;
++		w->is_assoc = 1;
++		memcpy(w->bssid, w->current_bss.bssid, ETH_ALEN);
++	}
++	memcpy(wrqu.ap_addr.sa_data, w->bssid, ETH_ALEN);
++	wireless_send_event(netdev, SIOCGIWAP, &wrqu, NULL);
++}
++
++/* association/disassociation */
++static void gelicw_assoc(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (w->state == GELICW_STATE_ASSOCIATED)
++		return;
++	schedule_delayed_work(&w->work_start, 0);
++}
++
++static int gelicw_disassoc(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	memset(w->bssid, 0, ETH_ALEN); /* clear current bssid */
++	if (w->state < GELICW_STATE_ASSOCIATED)
++		return 0;
++
++	schedule_delayed_work(&w->work_stop, 0);
++	w->state = GELICW_STATE_SCAN_DONE;
++	/* notify disassociation */
++	notify_assoc_event(netdev);
++
++	return 0;
++}
++
++static void gelicw_reassoc(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (w->cmd_send_flg != GELICW_CMD_SEND_ALL)
++		return;
++
++	if (!gelicw_disassoc(netdev))
++		gelicw_assoc(netdev);
++}
++
++
++/*
++ * lv1_net_control command
++ */
++/* control Ether port */
++static int gelicw_cmd_set_port(struct net_device *netdev, int mode)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 tag, val;
++	int status;
++
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_SET_PORT, GELICW_ETHER_PORT, mode, 0,
++			&tag, &val);
++	if (status)
++		dev_dbg(ntodev(netdev), "GELICW_SET_PORT failed:%d\n", status);
++	return status;
++}
++
++/* check support channels */
++static int gelicw_cmd_get_ch_info(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 ch_info, val;
++	int status;
++
++	if (w->state < GELICW_STATE_UP)
++		return -EIO;
++
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_GET_INFO, 0, 0 , 0,
++			&ch_info, &val);
++	if (status) {
++		if (status == LV1_NO_ENTRY)
++			w->ch_info = 0x07ff; /* 11ch */
++		else {
++			dev_dbg(ntodev(netdev), "GELICW_GET_INFO failed:%d\n", status);
++			w->ch_info = CH_INFO_FAIL;
++		}
++	} else {
++		dev_dbg(ntodev(netdev), "ch_info:%lx val:%lx\n", ch_info, val);
++		w->ch_info = ch_info >> 48; /* MSB 16bit shows supported channnels */
++	}
++	return status;
++}
++
++
++/*
++ * lv1_net_control GELICW_SET_CMD command
++ * queued using schedule_work()
++ */
++/* association */
++static void gelicw_cmd_start(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 val;
++	int status;
++
++	if (w->state < GELICW_STATE_SCAN_DONE)
++		return;
++
++	dev_dbg(ntodev(netdev), "GELICW_CMD_START\n");
++	w->cmd_id = GELICW_CMD_START;
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_SET_CMD, w->cmd_id, 0, 0,
++			&w->cmd_tag, &val);
++	if (status) {
++		w->cmd_tag = 0;
++		dev_dbg(ntodev(netdev), "GELICW_CMD_START failed:%d\n", status);
++	}
++}
++
++/* association done */
++static void gelicw_cmd_start_done(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 res, val;
++	int status;
++
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_GET_RES, w->cmd_tag, 0, 0,
++			&res, &val);
++	w->cmd_tag = 0;
++	wake_up_interruptible(&w->waitq_cmd);
++
++	if (status || res)
++		dev_dbg(ntodev(netdev), "GELICW_CMD_START res:%d,%ld\n", status, res);
++}
++
++/* disassociation */
++static void gelicw_cmd_stop(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 res, val;
++	int status;
++
++	if (w->state < GELICW_STATE_SCAN_DONE)
++		return;
++
++	dev_dbg(ntodev(netdev), "GELICW_CMD_STOP\n");
++	init_completion(&w->cmd_done);
++	w->cmd_id = GELICW_CMD_STOP;
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_SET_CMD, w->cmd_id, 0, 0,
++			&w->cmd_tag, &val);
++	if (status) {
++		w->cmd_tag = 0;
++		dev_dbg(ntodev(netdev), "GELICW_CMD_STOP failed:%d\n", status);
++		return;
++	}
++	wait_for_completion_interruptible(&w->cmd_done);
++
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_GET_RES, w->cmd_tag, 0, 0,
++			&res, &val);
++	w->cmd_tag = 0;
++	if (status || res) {
++		dev_dbg(ntodev(netdev), "GELICW_CMD_STOP res:%d,%ld\n", status, res);
++		return;
++	}
++}
++
++/* get rssi */
++static void gelicw_cmd_rssi(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 lpar, res, val;
++	int status;
++	struct rssi_desc *rssi;
++
++	if (w->state < GELICW_STATE_ASSOCIATED) {
++		w->rssi = 0;
++		complete(&w->rssi_done);
++		return;
++	}
++
++	lpar = ps3_mm_phys_to_lpar(__pa(w->data_buf));
++	init_completion(&w->cmd_done);
++	w->cmd_id = GELICW_CMD_GET_RSSI;
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_SET_CMD, w->cmd_id, 0, 0,
++			&w->cmd_tag, &val);
++	if (status) {
++		w->cmd_tag = 0;
++		w->rssi = 0;
++		dev_dbg(ntodev(netdev), "GELICW_CMD_GET_RSSI failed:%d\n", status);
++	} else {
++		wait_for_completion_interruptible(&w->cmd_done);
++		status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_GET_RES, w->cmd_tag, lpar, sizeof(*rssi),
++				&res, &val);
++		w->cmd_tag = 0;
++		if (status || res) {
++			w->rssi = 0;
++			dev_dbg(ntodev(netdev), "GELICW_CMD_GET_RSSI res:%d,%ld\n", status, res);
++		}
++		rssi = w->data_buf;
++		w->rssi = rssi->rssi;
++		dev_dbg(ntodev(netdev), "GELICW_CMD_GET_RSSI:%d\n", rssi->rssi);
++	}
++
++	complete(&w->rssi_done);
++}
++
++/* set common configuration */
++static int gelicw_cmd_common(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 lpar, res, val;
++	int status;
++	struct common_config *config;
++
++	if (w->state < GELICW_STATE_SCAN_DONE)
++		return -EIO;
++
++	lpar = ps3_mm_phys_to_lpar(__pa(w->data_buf));
++	config = w->data_buf;
++	config->scan_index = w->bss_index;
++	config->bss_type = (w->iw_mode == IW_MODE_ADHOC) ?
++				GELICW_BSS_ADHOC : GELICW_BSS_INFRA;
++	config->auth_method = (w->auth_mode == IW_AUTH_ALG_SHARED_KEY) ?
++				GELICW_AUTH_SHARED : GELICW_AUTH_OPEN;
++	switch (w->wireless_mode) {
++	case IEEE_B:
++		config->op_mode = GELICW_OP_MODE_11B;
++		break;
++	case IEEE_G:
++		config->op_mode = GELICW_OP_MODE_11G;
++		break;
++	case IEEE_B | IEEE_G:
++	default:
++		/* default 11bg mode */
++		config->op_mode = GELICW_OP_MODE_11BG;
++		break;
++	}
++
++	dev_dbg(ntodev(netdev), "GELICW_CMD_SET_CONFIG: index:%d type:%d auth:%d mode:%d\n",\
++		config->scan_index, config->bss_type,\
++		config->auth_method,config->op_mode);
++	init_completion(&w->cmd_done);
++	w->cmd_id = GELICW_CMD_SET_CONFIG;
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_SET_CMD, w->cmd_id,
++			lpar, sizeof(struct common_config),
++			&w->cmd_tag, &val);
++	if (status) {
++		w->cmd_tag = 0;
++		dev_dbg(ntodev(netdev), "GELICW_CMD_SET_CONFIG failed:%d\n", status);
++		return status;
++	}
++	wait_for_completion_interruptible(&w->cmd_done);
++
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_GET_RES, w->cmd_tag, 0, 0,
++			&res, &val);
++	w->cmd_tag = 0;
++	if (status || res) {
++		dev_dbg(ntodev(netdev), "GELICW_CMD_SET_CONFIG res:%d,%ld\n", status, res);
++		return -EFAULT;
++	}
++
++	w->cmd_send_flg |= GELICW_CMD_SEND_COMMON;
++
++	return 0;
++}
++
++#define h2i(c)    (isdigit(c) ? c - '0' : toupper(c) - 'A' + 10)
++/* send WEP/WPA configuration */
++static int gelicw_cmd_encode(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 res, val, lpar;
++	int status;
++	struct wep_config *config;
++	struct wpa_config *wpa_config;
++
++	u8 *key, key_len;
++
++	if (w->state < GELICW_STATE_SCAN_DONE)
++		return -EIO;
++
++	lpar = ps3_mm_phys_to_lpar(__pa(w->data_buf));
++
++	if (w->key_alg == IW_ENCODE_ALG_WEP ||
++	    w->key_alg == IW_ENCODE_ALG_NONE) {
++		/* WEP */
++		config = w->data_buf;
++		memset(config, 0, sizeof(struct wep_config));
++
++		/* check key len */
++		key_len = w->key_len[w->key_index];
++		key = w->key[w->key_index];
++		if (w->key_alg == IW_ENCODE_ALG_NONE)
++			config->sec = GELICW_WEP_SEC_NONE;
++		else
++			config->sec = (key_len == 5) ? GELICW_WEP_SEC_40BIT :
++							GELICW_WEP_SEC_104BIT;
++		/* copy key */
++		memcpy(config->key[w->key_index], key, key_len);
++
++		/* send wep config */
++		dev_dbg(ntodev(netdev), "GELICW_CMD_SET_WEP\n");
++		init_completion(&w->cmd_done);
++		w->cmd_id = GELICW_CMD_SET_WEP;
++		status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_SET_CMD, w->cmd_id,
++			lpar, sizeof(struct wep_config),
++			&w->cmd_tag, &val);
++		if (status) {
++			w->cmd_tag = 0;
++			dev_dbg(ntodev(netdev), "GELICW_CMD_SET_WEP failed:%d\n", status);
++			goto err;
++		}
++		wait_for_completion_interruptible(&w->cmd_done);
++
++		status = lv1_net_control(bus_id(w), dev_id(w),
++				GELICW_GET_RES, w->cmd_tag, 0, 0,
++				&res, &val);
++		w->cmd_tag = 0;
++		if (status || res) {
++			dev_dbg(ntodev(netdev), "GELICW_CMD_SET_WEP res:%d,%ld\n", status, res);
++			status = -EFAULT;
++			goto err;
++		}
++	} else {
++		/* WPA */
++		wpa_config = w->data_buf;
++		memset(wpa_config, 0, sizeof(struct wpa_config));
++
++		switch (w->key_alg) {
++		case IW_ENCODE_ALG_TKIP:
++			wpa_config->sec = GELICW_WPA_SEC_TKIP;
++			break;
++		default:
++		case IW_ENCODE_ALG_CCMP:
++			wpa_config->sec = GELICW_WPA_SEC_AES;
++			break;
++		}
++		/* check key len */
++		key = w->key[w->key_index];
++		key_len = w->key_len[w->key_index];
++
++		if (key_len > 64) key_len = 64;
++		if (key_len != 64) {
++			/* if key_len isn't 64byte ,it should be passphrase */
++			/* pass phrase */
++			memcpy(wpa_config->psk_material, key, key_len);
++			wpa_config->psk_type = GELICW_PSK_PASSPHRASE;
++		} else {
++			int i;
++			/* 64 hex */
++			for (i = 0; i < 32; i++)
++				wpa_config->psk_material[i] =
++						h2i(key[2 * i]) * 16
++						+ h2i(key[2 * i + 1]);
++			wpa_config->psk_type = GELICW_PSK_64HEX;
++		}
++
++		/* send wpa config */
++		dev_dbg(ntodev(netdev), "GELICW_CMD_SET_WPA:type:%d\n", wpa_config->psk_type);
++		init_completion(&w->cmd_done);
++		w->cmd_id = GELICW_CMD_SET_WPA;
++		status = lv1_net_control(bus_id(w), dev_id(w),
++				GELICW_SET_CMD, w->cmd_id,
++				lpar, sizeof(struct wpa_config),
++				&w->cmd_tag, &val);
++		if (status) {
++			w->cmd_tag = 0;
++			dev_dbg(ntodev(netdev), "GELICW_CMD_SET_WPA failed:%d\n", status);
++			goto err;
++		}
++		wait_for_completion_interruptible(&w->cmd_done);
++
++		status = lv1_net_control(bus_id(w), dev_id(w),
++				GELICW_GET_RES, w->cmd_tag, 0, 0, &res, &val);
++		w->cmd_tag = 0;
++		if (status || res) {
++			dev_dbg(ntodev(netdev), "GELICW_CMD_SET_WPA res:%d,%ld\n", status, res);
++			status = -EFAULT;
++			goto err;
++		}
++	}
++
++	w->cmd_send_flg |= GELICW_CMD_SEND_ENCODE;
++	/* (re)associate */
++	gelicw_reassoc(netdev);
++
++	return 0;
++err:
++	gelicw_disassoc(netdev);
++	return status;
++}
++
++static int gelicw_is_ap_11b(struct gelicw_bss *list)
++{
++	if (list->rates_len + list->rates_ex_len == GELICW_NUM_11B_BITRATES)
++		return 1;
++	else
++		return 0;
++}
++
++/* get scan results */
++static int gelicw_cmd_get_scan(struct gelic_wireless *w)
++{
++	u64 lpar, res, val;
++	int status;
++	struct scan_desc *desc;
++	int i, j;
++	u8 *p;
++
++
++	/* get scan */
++	dev_dbg(wtodev(w), "GELICW_CMD_GET_SCAN\n");
++	init_completion(&w->cmd_done);
++	w->cmd_id = GELICW_CMD_GET_SCAN;
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_SET_CMD, w->cmd_id, 0, 0,
++			&w->cmd_tag, &val);
++	if (status) {
++		w->cmd_tag = 0;
++		dev_dbg(wtodev(w), "GELICW_CMD_GET_SCAN failed:%d\n", status);
++		return status;
++	}
++	wait_for_completion_interruptible(&w->cmd_done);
++
++	lpar = ps3_mm_phys_to_lpar(__pa(w->data_buf));
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_GET_RES, w->cmd_tag, lpar, PAGE_SIZE,
++			&res, &val);
++	w->cmd_tag = 0;
++	if (status || res) {
++		dev_dbg(wtodev(w), "GELICW_CMD_GET_SCAN res:%d,%ld\n",
++			status, res);
++		return -EFAULT;
++	}
++
++	for (i = 0, desc = w->data_buf;
++	     ((void *)desc < (w->data_buf + val)) && i < MAX_SCAN_BSS;
++	     i++, desc = ((void *)desc + desc->size)) {
++		struct gelicw_bss *bss = &w->bss_list[i];
++
++		bss->rates_len = 0;
++		for (j = 0; j < MAX_RATES_LENGTH; j++)
++			if (desc->rate[j])
++				bss->rates[bss->rates_len++] = desc->rate[j];
++		bss->rates_ex_len = 0;
++		for (j = 0; j < MAX_RATES_EX_LENGTH; j++)
++			if (desc->ext_rate[j])
++				bss->rates_ex[bss->rates_ex_len++]
++						= desc->ext_rate[j];
++
++		if (desc->capability & 0x3) {
++			if (desc->capability & 0x1)
++				bss->mode = IW_MODE_INFRA;
++			else
++				bss->mode = IW_MODE_ADHOC;
++		}
++		bss->channel = desc->channel;
++		bss->essid_len = strnlen(desc->essid, IW_ESSID_MAX_SIZE);
++		bss->rssi = (u8)desc->rssi;
++		bss->capability = desc->capability;
++		bss->beacon_interval = desc->beacon_period;
++		memset(bss->essid, 0, sizeof(bss->essid));
++		memcpy(bss->essid, desc->essid, bss->essid_len);
++		p = (u8 *)&desc->bssid;
++		memcpy(bss->bssid, &p[2], ETH_ALEN);/* bssid:64bit in desc */
++		bss->sec_info = desc->security;
++	}
++	w->num_bss_list = i;
++
++	if (w->num_bss_list)
++		return 0; /* ap found */
++	else
++		return -1; /* no ap found */
++}
++
++/* search bssid in bss list */
++static int gelicw_search_bss_list(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	static const u8 off[] = { 0x00, 0x00, 0x00, 0x00, 0x00, 0x00 };
++	int i;
++	int check_bss = 0, check_11g = 0, found_bss, found_11g;
++
++	if (!w->num_bss_list)
++		return -1;	/* no bss list */
++
++	if (memcmp(off, w->wap_bssid, ETH_ALEN))
++		check_bss = 1;
++
++	/* wireless op_mode seems not working with CMD_SET_CONFIG */
++	if (w->wireless_mode == IEEE_G)
++		check_11g = 1;
++
++	if (!check_bss && !check_11g)
++		return 0;	/* no check bssid, wmode */
++
++	for (i = 0; i < w->num_bss_list; i++) {
++		found_bss = found_11g = 1;
++		if (check_bss &&
++		    memcmp(w->bss_list[i].bssid, w->wap_bssid, ETH_ALEN))
++			found_bss = 0; /* not found */
++
++		if (check_11g &&
++		    gelicw_is_ap_11b(&w->bss_list[i]))
++			found_11g = 0; /* not found */
++
++		if (found_bss && found_11g)
++			break;
++	}
++
++	if (i == w->num_bss_list)
++		return -1; /* not found */
++	else
++		return i;
++}
++
++/* scan done */
++static void gelicw_scan_complete(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	int res;
++	int bss_index;
++
++	/* get scan results */
++	res = gelicw_cmd_get_scan(w);
++	if (w->is_assoc)
++		w->state = GELICW_STATE_ASSOCIATED;
++	else
++		w->state = GELICW_STATE_SCAN_DONE;
++
++	if (res) {
++		/* No AP found */
++		if (!w->scan_all) {
++			/* no specified AP */
++			gelicw_disassoc(netdev);
++			/* rescan */
++			if (w->essid_search && w->essid_len)
++				schedule_delayed_work(&w->work_scan_essid,
++							GELICW_SCAN_INTERVAL);
++			return;
++		}
++	}
++
++	if (w->scan_all) {
++		/* all params should be set again after scan */
++		w->cmd_send_flg = 0;
++		return;
++	}
++
++	bss_index = gelicw_search_bss_list(netdev);
++	if (bss_index < 0) {
++		/* no wap_bssid in bss_list */
++		if (w->essid_search && w->essid_len)
++			schedule_delayed_work(&w->work_scan_essid,
++						GELICW_SCAN_INTERVAL);
++		return;
++	}
++	w->bss_index = (u8)bss_index;
++	w->current_bss = w->bss_list[w->bss_index];
++
++	/* essid search complete */
++	w->essid_search = 0;
++	w->cmd_send_flg |= GELICW_CMD_SEND_SCAN;
++
++	/* (re)connect to AP */
++	if (w->is_assoc) {
++		/* notify disassociation */
++		w->state = GELICW_STATE_SCAN_DONE;
++		notify_assoc_event(netdev);
++	}
++	schedule_delayed_work(&w->work_common, 0);
++	schedule_delayed_work(&w->work_encode, 0);
++}
++
++/* start scan */
++static int gelicw_cmd_set_scan(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 res, val, lpar;
++	int status;
++	u8 *p;
++
++	if (w->state < GELICW_STATE_UP) {
++		w->scan_all = 0;
++		return -EIO;
++	}
++	if (!w->scan_all && !w->essid_len)
++		return -EINVAL; /* unsupported essid ANY */
++
++	/* set device wired to wireless when essid is set */
++	if (!w->scan_all && w->wireless < GELICW_WIRELESS_ON) {
++		gelicw_vlan_mode(netdev, GELIC_NET_VLAN_WIRELESS);
++		gelicw_cmd_set_port(netdev, GELICW_PORT_DOWN);
++		w->wireless = GELICW_WIRELESS_ON;
++	}
++
++	p = w->data_buf;
++	lpar = ps3_mm_phys_to_lpar(__pa(w->data_buf));
++
++	/* avoid frequent scanning */
++	if (!w->essid_search && /* background scan off */
++	    w->scan_all &&
++	    (time_before64(get_jiffies_64(), w->last_scan + 5 * HZ)))
++		return 0;
++
++	w->bss_key_alg = IW_ENCODE_ALG_NONE;
++
++	init_completion(&w->cmd_done);
++	w->state = GELICW_STATE_SCANNING;
++	w->cmd_id = GELICW_CMD_SCAN;
++
++	if (w->scan_all) {
++		/* scan all ch */
++		dev_dbg(ntodev(netdev), "GELICW_CMD_SCAN all\n");
++		w->last_scan = get_jiffies_64(); /* last scan time */
++		status = lv1_net_control(bus_id(w), dev_id(w),
++				GELICW_SET_CMD, w->cmd_id, 0, 0,
++				&w->cmd_tag, &val);
++	} else {
++		/* scan essid */
++		memset(p, 0, 32);
++		memcpy(p, w->essid, w->essid_len);
++		dev_dbg(ntodev(netdev), "GELICW_CMD_SCAN essid\n");
++		status = lv1_net_control(bus_id(w), dev_id(w),
++				GELICW_SET_CMD, w->cmd_id, lpar, 32,
++				&w->cmd_tag, &val);
++	}
++
++	if (status) {
++		w->cmd_tag = 0;
++		dev_dbg(ntodev(netdev), "GELICW_CMD_SCAN failed:%d\n", status);
++		return status;
++	}
++	wait_for_completion_interruptible(&w->cmd_done);
++
++	status = lv1_net_control(bus_id(w), dev_id(w),
++			GELICW_GET_RES, w->cmd_tag, 0, 0,
++			&res, &val);
++	w->cmd_tag = 0;
++	if (status || res) {
++		dev_dbg(ntodev(netdev), "GELICW_CMD_SCAN res:%d,%ld\n", status, res);
++		return -EFAULT;
++	}
++
++	return 0;
++}
++
++static void gelicw_send_common_config(struct net_device *netdev,
++					u8 *cur, u8 mode)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (*cur != mode) {
++		*cur = mode;
++		if (w->state < GELICW_STATE_SCAN_DONE)
++			return;
++
++		if (!(w->cmd_send_flg & GELICW_CMD_SEND_SCAN) &&
++		    w->essid_len)
++			/* scan essid and set other params */
++			schedule_delayed_work(&w->work_scan_essid, 0);
++		else {
++			schedule_delayed_work(&w->work_common, 0);
++			if (w->cmd_send_flg
++			    & GELICW_CMD_SEND_ENCODE)
++				/* (re)send encode key */
++				schedule_delayed_work(&w->work_encode, 0);
++		}
++	}
++}
++
++
++/*
++ * work queue
++ */
++static void gelicw_work_rssi(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_rssi.work);
++	struct net_device *netdev = w->card->netdev;
++
++	if (w->cmd_tag) {
++		schedule_delayed_work(&w->work_rssi, HZ / 5);
++		return;
++	}
++
++	gelicw_cmd_rssi(netdev);
++}
++
++static void gelicw_work_scan_all(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_scan_all.work);
++	struct net_device *netdev = w->card->netdev;
++
++	if (w->cmd_tag || w->state == GELICW_STATE_SCANNING) {
++		schedule_delayed_work(&w->work_scan_all, HZ / 5);
++		return;
++	}
++
++	w->scan_all = 1;
++	gelicw_cmd_set_scan(netdev);
++}
++
++static void gelicw_work_scan_essid(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_scan_essid.work);
++	struct net_device *netdev = w->card->netdev;
++
++	if (w->cmd_tag || w->scan_all || w->state == GELICW_STATE_SCANNING) {
++		schedule_delayed_work(&w->work_scan_essid, HZ / 5);
++		return;
++	}
++	w->bss_index = 0;
++	w->scan_all = 0;
++	gelicw_cmd_set_scan(netdev);
++}
++
++static void gelicw_work_common(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_common.work);
++	struct net_device *netdev = w->card->netdev;
++
++	if (w->cmd_tag) {
++		schedule_delayed_work(&w->work_common, HZ / 5);
++		return;
++	}
++	gelicw_cmd_common(netdev);
++}
++
++static void gelicw_work_encode(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_encode.work);
++	struct net_device *netdev = w->card->netdev;
++
++	if (w->cmd_tag) {
++		schedule_delayed_work(&w->work_encode, HZ / 5);
++		return;
++	}
++	gelicw_cmd_encode(netdev);
++}
++
++static void gelicw_work_start(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_start.work);
++	struct net_device *netdev = w->card->netdev;
++
++	if (w->cmd_tag) {
++		schedule_delayed_work(&w->work_start, HZ / 5);
++		return;
++	}
++	gelicw_cmd_start(netdev);
++}
++
++static void gelicw_work_start_done(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_start_done);
++	struct net_device *netdev = w->card->netdev;
++
++	gelicw_cmd_start_done(netdev);
++}
++
++static void gelicw_work_stop(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_stop.work);
++	struct net_device *netdev = w->card->netdev;
++
++	if (w->cmd_tag) {
++		schedule_delayed_work(&w->work_stop, HZ / 5);
++		return;
++	}
++	gelicw_cmd_stop(netdev);
++}
++
++static void gelicw_work_roam(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_roam.work);
++	struct net_device *netdev = w->card->netdev;
++
++	if (w->cmd_tag || w->scan_all || w->state == GELICW_STATE_SCANNING) {
++		schedule_delayed_work(&w->work_roam, HZ / 5);
++		return;
++	}
++	gelicw_cmd_stop(netdev);
++	w->bss_index = 0;
++	w->scan_all = 0;
++	gelicw_cmd_set_scan(netdev);
++}
++
++/*
++ * Event handler
++ */
++#define GELICW_EVENT_LOOP_MAX 16
++static void gelicw_event(struct work_struct *work)
++{
++	struct gelic_wireless *w =
++		container_of(work, struct gelic_wireless, work_event);
++	struct net_device *netdev = w->card->netdev;
++	u64 event_type, val;
++	int i, status;
++
++	for (i = 0; i < GELICW_EVENT_LOOP_MAX; i++) {
++		status = lv1_net_control(bus_id(w), dev_id(w),
++				GELICW_GET_EVENT, 0, 0 , 0,
++				&event_type, &val);
++		if (status == GELICW_EVENT_NO_ENTRY)
++			/* got all events */
++			break;
++		else if (status){
++			dev_dbg(ntodev(netdev), "GELICW_GET_EVENT failed:%d\n", status);
++			return;
++		}
++		switch(event_type) {
++		case GELICW_EVENT_DEVICE_READY:
++			dev_dbg(ntodev(netdev), "  GELICW_EVENT_DEVICE_READY\n");
++			break;
++		case GELICW_EVENT_SCAN_COMPLETED:
++			dev_dbg(ntodev(netdev), "  GELICW_EVENT_SCAN_COMPLETED\n");
++			gelicw_scan_complete(netdev);
++			break;
++		case GELICW_EVENT_BEACON_LOST:
++			dev_dbg(ntodev(netdev), "  GELICW_EVENT_BEACON_LOST\n");
++			w->state = GELICW_STATE_SCAN_DONE;
++			notify_assoc_event(netdev);
++			/* roaming */
++			w->essid_search = 1;
++			schedule_delayed_work(&w->work_roam, 0);
++			break;
++		case GELICW_EVENT_CONNECTED:
++		{
++			u16 ap_sec;
++			dev_dbg(ntodev(netdev), "  GELICW_EVENT_CONNECTED\n");
++			/* this event ocuured with any key_alg */
++			ap_sec = w->current_bss.sec_info;
++			if (w->key_alg == IW_ENCODE_ALG_NONE) {
++				/* no encryption */
++				if (ap_sec == 0) {
++					w->state = GELICW_STATE_ASSOCIATED;
++					notify_assoc_event(netdev);
++				}
++			} else if (w->key_alg == IW_ENCODE_ALG_WEP){
++				if ((ap_sec & GELICW_SEC_TYPE_WEP_MASK)
++				    == GELICW_SEC_TYPE_WEP) {
++					/* wep */
++					w->state = GELICW_STATE_ASSOCIATED;
++					notify_assoc_event(netdev);
++				}
++			}
++			break;
++		}
++		case GELICW_EVENT_WPA_CONNECTED:
++			dev_dbg(ntodev(netdev), "  GELICW_EVENT_WPA_CONNECTED\n");
++			w->state = GELICW_STATE_ASSOCIATED;
++			notify_assoc_event(netdev);
++			break;
++		case GELICW_EVENT_WPA_ERROR:
++			dev_dbg(ntodev(netdev), "  GELICW_EVENT_WPA_ERROR\n");
++			break;
++		default:
++			dev_dbg(ntodev(netdev), "  GELICW_EVENT_UNKNOWN\n");
++			break;
++		}
++	}
++}
++
++static void gelicw_clear_event(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u64 event_type, val;
++	int i, status;
++
++	for (i = 0; i < GELICW_EVENT_LOOP_MAX; i++) {
++		status = lv1_net_control(bus_id(w), dev_id(w),
++				GELICW_GET_EVENT, 0, 0 , 0,
++				&event_type, &val);
++		if (status)
++			return;/* got all events */
++
++		switch(event_type) {
++		case GELICW_EVENT_SCAN_COMPLETED:
++			w->state = GELICW_STATE_SCAN_DONE;
++			wake_up_interruptible(&w->waitq_scan);
++			break;
++		default:
++			break;
++		}
++	}
++}
++
++/*
++ * gelic_net support function
++ */
++static void gelicw_clear_params(struct gelic_wireless *w)
++{
++	int i;
++
++	/* clear status */
++	w->state = GELICW_STATE_DOWN;
++	w->cmd_send_flg = 0;
++	w->scan_all = 0;
++	w->is_assoc = 0;
++	w->essid_search = 0;
++	w->cmd_tag = 0;
++	w->cmd_id = 0;
++	w->last_scan = 0;
++
++	/* default mode and settings */
++	w->essid_len = 0;
++	w->essid[0] = '\0';
++	w->nick[0] = '\0';
++	w->iw_mode = IW_MODE_INFRA;
++	w->auth_mode = IW_AUTH_ALG_OPEN_SYSTEM;
++	w->wireless_mode = IEEE_B | IEEE_G;
++	w->bss_index = 0;
++	memset(w->bssid, 0, ETH_ALEN);
++	memset(w->wap_bssid, 0, ETH_ALEN);
++
++	/* init key */
++	w->key_index = 0;
++	for (i = 0; i < WEP_KEYS; i++) {
++		w->key[i][0] = '\0';
++		w->key_len[i] = 0;
++	}
++	w->key_alg = IW_ENCODE_ALG_NONE;
++	w->bss_key_alg = IW_ENCODE_ALG_NONE;
++}
++
++int gelicw_setup_netdev(struct net_device *netdev, int wi)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (wi < 0) {
++		/* PS3 low model has no wireless */
++		dev_info(ntodev(netdev), "No wireless dvice in this system\n");
++		w->wireless = 0;
++		return 0;
++	}
++	/* version check */
++	if (ps3_compare_firmware_version(1, 6, 0) < 0) {
++		dev_info(ntodev(netdev),
++			 "firmware is too old for wireless.\n");
++		w->wireless = 0;
++		return 0;
++	}
++	/* we need 4K aligned, 16 units of scan_desc sized */
++	BUILD_BUG_ON(PAGE_SIZE < sizeof(struct scan_desc) * MAX_SCAN_BSS);
++	w->data_buf = (void *)get_zeroed_page(GFP_KERNEL);
++	if (!w->data_buf) {
++		w->wireless = 0;
++		dev_info(ntodev(netdev), "%s:get_page failed\n", __func__);
++		return -ENOMEM;
++	}
++
++	w->wireless = GELICW_WIRELESS_SUPPORTED;
++
++	w->ch_info = 0;
++	w->channel = 0;
++	netdev->wireless_data = &w->wireless_data;
++	netdev->wireless_handlers = &gelicw_handler_def;
++	INIT_WORK(&w->work_event, gelicw_event);
++	INIT_WORK(&w->work_start_done, gelicw_work_start_done);
++	INIT_DELAYED_WORK(&w->work_rssi, gelicw_work_rssi);
++	INIT_DELAYED_WORK(&w->work_scan_all, gelicw_work_scan_all);
++	INIT_DELAYED_WORK(&w->work_scan_essid, gelicw_work_scan_essid);
++	INIT_DELAYED_WORK(&w->work_common, gelicw_work_common);
++	INIT_DELAYED_WORK(&w->work_encode, gelicw_work_encode);
++	INIT_DELAYED_WORK(&w->work_start, gelicw_work_start);
++	INIT_DELAYED_WORK(&w->work_stop, gelicw_work_stop);
++	INIT_DELAYED_WORK(&w->work_roam, gelicw_work_roam);
++	init_waitqueue_head(&w->waitq_cmd);
++	init_waitqueue_head(&w->waitq_scan);
++
++	gelicw_clear_params(w);
++
++	return 0;
++}
++
++void gelicw_up(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (!w->wireless)
++		return;
++
++	dev_dbg(ntodev(netdev), "gelicw_up\n");
++	if (w->state < GELICW_STATE_UP)
++		w->state = GELICW_STATE_UP;
++
++	/* start essid scanning */
++	if (w->essid_len)
++		schedule_delayed_work(&w->work_scan_essid, 0);
++}
++
++int gelicw_down(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (!w->wireless || w->state == GELICW_STATE_DOWN)
++		return 0;
++
++	dev_dbg(ntodev(netdev), "gelicw_down\n");
++	w->wireless = GELICW_WIRELESS_SHUTDOWN;
++	flush_scheduled_work();
++
++	/* check cmd_tag of CMD_START */
++	if (w->cmd_id == GELICW_CMD_START)
++		wait_event_interruptible(w->waitq_cmd, !w->cmd_tag);
++	/* wait scan done */
++	if (w->state == GELICW_STATE_SCANNING) {
++		wait_event_interruptible(w->waitq_scan,
++					w->state != GELICW_STATE_SCANNING);
++		gelicw_cmd_get_scan(w);
++	}
++
++	gelicw_cmd_stop(netdev);
++	if (w->is_assoc) {
++		w->state = GELICW_STATE_DOWN;
++		notify_assoc_event(netdev);
++	}
++	gelicw_clear_params(w);
++
++	/* set device wireless to wired */
++	gelicw_vlan_mode(netdev, GELIC_NET_VLAN_WIRED);
++	gelicw_cmd_set_port(netdev, GELICW_PORT_UP);
++	w->wireless = GELICW_WIRELESS_SUPPORTED;
++
++	return 0;
++}
++
++void gelicw_remove(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (!w->wireless)
++		return;
++
++	dev_dbg(ntodev(netdev), "gelicw_remove\n");
++	gelicw_down(netdev);
++	w->wireless = 0;
++	netdev->wireless_handlers = NULL;
++	free_page((unsigned long)w->data_buf);
++}
++
++void gelicw_interrupt(struct net_device *netdev, u64 status)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (!w->wireless)
++		return;
++
++	if (status & GELICW_DEVICE_CMD_COMP) {
++		dev_dbg(ntodev(netdev), "GELICW_DEVICE_CMD_COMP\n");
++		if (w->cmd_id == GELICW_CMD_START)
++			schedule_work(&w->work_start_done);
++		else
++			complete(&w->cmd_done);
++	}
++	if (status & GELICW_DEVICE_EVENT_RECV) {
++		dev_dbg(ntodev(netdev), "GELICW_DEVICE_EVENT_RECV\n");
++		if (w->wireless == GELICW_WIRELESS_SHUTDOWN)
++			gelicw_clear_event(netdev);
++		else
++			schedule_work(&w->work_event);
++	}
++}
++
++int gelicw_is_associated(struct net_device *netdev)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	if (!w->wireless)
++		return 0;
++
++	return w->is_assoc;
++}
++
++
++/*
++ * Wireless externsions
++ */
++static int gelicw_get_name(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx: get_name\n");
++	if (w->state < GELICW_STATE_UP) {
++		strcpy(wrqu->name, "radio off");
++		return 0;
++	}
++
++	if (w->wireless_mode == IEEE_B ||
++	    (w->is_assoc && gelicw_is_ap_11b(&w->current_bss)))
++		strcpy(wrqu->name, "IEEE 802.11b");
++	else {
++		switch (w->wireless_mode) {
++		case IEEE_G:
++			strcpy(wrqu->name, "IEEE 802.11g");
++			break;
++		case IEEE_B | IEEE_G:
++		default:
++			strcpy(wrqu->name, "IEEE 802.11bg");
++			break;
++		}
++	}
++
++	return 0;
++}
++
++static int gelicw_set_freq(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	struct iw_freq *fwrq = &wrqu->freq;
++	int ch;
++
++	dev_dbg(ntodev(netdev), "wx: set_freq e:%d m:%d\n", fwrq->e, fwrq->m);
++	if (w->is_assoc || w->state < GELICW_STATE_UP)
++		return 0;
++
++	/* this setting has no effect for INFRA mode */
++	if (fwrq->e == 1) {
++		u32 f = fwrq->m / 100000;
++		int i;
++		for (i = 0; i < ARRAY_SIZE(freq_list); i++)
++			if (freq_list[i] == f)
++				break;
++		if (i == ARRAY_SIZE(freq_list))
++			return -EINVAL;
++		fwrq->m = i + 1; /* ch number */
++		fwrq->e = 0;
++	}
++	if (fwrq->e > 0)
++		return -EINVAL;
++
++	ch = fwrq->m;
++	if (ch < 1)
++		w->channel = 0; /* auto */
++	else if (ch > ARRAY_SIZE(freq_list))
++		return -EINVAL;
++	else {
++		/* check supported channnel */
++		if (!w->ch_info)
++			gelicw_cmd_get_ch_info(netdev);
++		if (w->ch_info & (1 << (ch - 1)))
++			w->channel = ch;
++		else
++			return -EINVAL;
++	}
++	dev_dbg(ntodev(netdev), " set cnannel: %d\n", w->channel);
++
++	return 0;
++}
++
++static int gelicw_get_freq(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx: get_freq:%d\n", w->channel);
++	if (w->channel == 0)
++		wrqu->freq.m = 0;
++	else
++		wrqu->freq.m = freq_list[w->channel - 1] * 100000;
++	wrqu->freq.e = 1;
++
++	return 0;
++}
++
++static int gelicw_set_mode(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	int mode = wrqu->mode;
++	u8 iw_mode = IW_MODE_INFRA;
++
++	dev_dbg(ntodev(netdev), "wx: set_mode:%x\n",mode);
++	switch (mode) {
++	case IW_MODE_ADHOC:
++		dev_dbg(ntodev(netdev), "IW_MODE_ADHOC\n");
++		iw_mode = mode;
++		return -EOPNOTSUPP; /* adhoc not supported */
++	case IW_MODE_INFRA:
++	default:
++		dev_dbg(ntodev(netdev), "IW_MODE_INFRA\n");
++		iw_mode = IW_MODE_INFRA;
++		break;
++	}
++
++	/* send common config */
++	gelicw_send_common_config(netdev, &w->iw_mode, iw_mode);
++
++	return 0;
++}
++
++static int gelicw_get_mode(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx: get_mode\n");
++	wrqu->mode = w->iw_mode;
++
++	return 0;
++}
++
++static int gelicw_get_range(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	struct iw_range *range = (struct iw_range *)extra;
++	int num_ch, i;
++
++	dev_dbg(ntodev(netdev), "wx: get_range\n");
++	wrqu->data.length = sizeof(*range);
++	memset(range, 0, sizeof(*range));
++
++	/* wireless extension */
++	range->we_version_compiled = WIRELESS_EXT;
++	range->we_version_source = 19;
++
++	/* supported bitrates */
++	if (w->wireless_mode == IEEE_B)
++		range->num_bitrates = GELICW_NUM_11B_BITRATES;
++	else
++		range->num_bitrates = ARRAY_SIZE(bitrate_list);
++	range->throughput = bitrate_list[range->num_bitrates -1] / 2; /* half */
++	for (i = 0; i < range->num_bitrates; i++)
++		range->bitrate[i] = bitrate_list[i];
++
++	range->max_qual.qual = 100; /* relative value */
++	range->max_qual.level = 100;
++	range->avg_qual.qual = 0;
++	range->avg_qual.level = 0;
++	range->sensitivity = 0;
++
++	/* encryption capabilities */
++	range->encoding_size[0] = 5;	/* 40bit WEP */
++	range->encoding_size[1] = 13;	/* 104bit WEP */
++	range->encoding_size[2] = 64;	/* WPA-PSK */
++	range->num_encoding_sizes = 3;
++	range->max_encoding_tokens = WEP_KEYS;
++	range->enc_capa = IW_ENC_CAPA_WPA | IW_ENC_CAPA_WPA2 |
++			  IW_ENC_CAPA_CIPHER_TKIP | IW_ENC_CAPA_CIPHER_CCMP;
++
++	/* freq */
++	if (!w->ch_info)
++		gelicw_cmd_get_ch_info(netdev); /* get supported freq */
++
++	num_ch = 0;
++	for (i = 0; i < ARRAY_SIZE(freq_list); i++)
++		if (w->ch_info & (1 << i)) {
++			range->freq[num_ch].i = i + 1;
++			range->freq[num_ch].m = freq_list[i];
++			range->freq[num_ch].e = 6;
++			if (++num_ch == IW_MAX_FREQUENCIES)
++				break;
++		}
++
++	range->num_channels = num_ch;
++	range->num_frequency = num_ch;
++
++	/* event capabilities */
++	range->event_capa[0] = (IW_EVENT_CAPA_K_0 |
++				IW_EVENT_CAPA_MASK(SIOCGIWAP));
++	range->event_capa[1] = IW_EVENT_CAPA_K_1;
++
++	return 0;
++}
++
++static int gelicw_set_wap(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	static const u8 any[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
++	static const u8 off[] = { 0x00, 0x00, 0x00, 0x00, 0x00, 0x00 };
++
++	dev_dbg(ntodev(netdev), "wx: set_wap\n");
++	if (wrqu->ap_addr.sa_family != ARPHRD_ETHER)
++		return -EINVAL;
++
++	if (!memcmp(any, wrqu->ap_addr.sa_data, ETH_ALEN) ||
++	    !memcmp(off, wrqu->ap_addr.sa_data, ETH_ALEN)) {
++		if (!memcmp(off, w->wap_bssid, ETH_ALEN))
++			return 0; /* ap off, no change */
++		else {
++			memset(w->wap_bssid, 0, ETH_ALEN);
++			/* start scan */
++		}
++	} else if (!memcmp(w->wap_bssid, wrqu->ap_addr.sa_data, ETH_ALEN))
++		/* no change */
++		return 0;
++	else if (!memcmp(w->bssid, wrqu->ap_addr.sa_data, ETH_ALEN)) {
++		/* current bss */
++		memcpy(w->wap_bssid, wrqu->ap_addr.sa_data, ETH_ALEN);
++		return 0;
++	} else
++		memcpy(w->wap_bssid, wrqu->ap_addr.sa_data, ETH_ALEN);
++
++	/* start scan */
++	if (w->essid_len && w->state >= GELICW_STATE_SCAN_DONE) {
++		gelicw_disassoc(netdev);
++		/* scan essid */
++		cancel_delayed_work(&w->work_scan_all);
++		cancel_delayed_work(&w->work_scan_essid);
++		w->essid_search = 1;
++		schedule_delayed_work(&w->work_scan_essid, 0);
++	}
++
++	return 0;
++}
++
++static int gelicw_get_wap(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx: get_wap\n");
++	wrqu->ap_addr.sa_family = ARPHRD_ETHER;
++	memcpy(wrqu->ap_addr.sa_data, w->bssid, ETH_ALEN);
++
++	return 0;
++}
++
++static int gelicw_set_scan(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx: set_scan\n");
++	if (w->state < GELICW_STATE_UP)
++		return -EIO;
++
++	/* cancel scan */
++	cancel_delayed_work(&w->work_scan_all);
++	cancel_delayed_work(&w->work_scan_essid);
++
++	schedule_delayed_work(&w->work_scan_all, 0);
++
++	return 0;
++}
++
++#define MAX_CUSTOM_LEN 64
++static char *gelicw_translate_scan(struct net_device *netdev,
++				char *start, char *stop,
++				struct gelicw_bss *list)
++{
++	char custom[MAX_CUSTOM_LEN];
++	struct iw_event iwe;
++	int i;
++	char *p, *current_val;
++
++	/* BSSID */
++	iwe.cmd = SIOCGIWAP;
++	iwe.u.ap_addr.sa_family = ARPHRD_ETHER;
++	memcpy(iwe.u.ap_addr.sa_data, list->bssid, ETH_ALEN);
++	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_ADDR_LEN);
++
++	/* ESSID */
++	iwe.cmd = SIOCGIWESSID;
++	iwe.u.data.flags = 1;
++	iwe.u.data.length = list->essid_len;
++	start = iwe_stream_add_point(start, stop, &iwe, list->essid);
++
++	/* protocol name */
++	iwe.cmd = SIOCGIWNAME;
++	if (gelicw_is_ap_11b(list))
++		snprintf(iwe.u.name, IFNAMSIZ, "IEEE 802.11b");
++	else
++		snprintf(iwe.u.name, IFNAMSIZ, "IEEE 802.11bg");
++	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_CHAR_LEN);
++
++	/* MODE */
++	iwe.cmd = SIOCGIWMODE;
++	iwe.u.mode = list->mode;
++	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_UINT_LEN);
++
++	/* FREQ */
++	iwe.cmd = SIOCGIWFREQ;
++	iwe.u.freq.m = freq_list[list->channel - 1];
++	iwe.u.freq.e = 6;
++	iwe.u.freq.i = 0;
++	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_FREQ_LEN);
++
++	/* ENCODE */
++	iwe.cmd = SIOCGIWENCODE;
++	if (list->capability & WLAN_CAPABILITY_PRIVACY)
++		iwe.u.data.flags = IW_ENCODE_ENABLED | IW_ENCODE_NOKEY;
++	else
++		iwe.u.data.flags = IW_ENCODE_DISABLED;
++	iwe.u.data.length = 0;
++	start = iwe_stream_add_point(start, stop, &iwe, list->essid);
++
++	/* QUAL */
++	iwe.cmd = IWEVQUAL;
++	iwe.u.qual.updated  = IW_QUAL_ALL_UPDATED |
++			IW_QUAL_QUAL_INVALID | IW_QUAL_NOISE_INVALID;
++	iwe.u.qual.level = list->rssi;
++	iwe.u.qual.qual = list->rssi;
++	iwe.u.qual.noise = 0;
++	start = iwe_stream_add_event(start, stop, &iwe, IW_EV_QUAL_LEN);
++
++	/* RATE */
++	current_val = start + IW_EV_LCP_LEN;
++	iwe.cmd = SIOCGIWRATE;
++	iwe.u.bitrate.fixed = iwe.u.bitrate.disabled = 0;
++	for (i = 0; i < list->rates_len; i++) {
++		iwe.u.bitrate.value = ((list->rates[i] & 0x7f) * 500000);
++		current_val = iwe_stream_add_value(start, current_val, stop,
++					&iwe, IW_EV_PARAM_LEN);
++	}
++	for (i = 0; i < list->rates_ex_len; i++) {
++		iwe.u.bitrate.value = ((list->rates_ex[i] & 0x7f) * 500000);
++		current_val = iwe_stream_add_value(start, current_val, stop,
++					&iwe, IW_EV_PARAM_LEN);
++	}
++	if ((current_val - start) > IW_EV_LCP_LEN)
++		start = current_val;
++
++	/* Extra */
++	/* BEACON */
++	memset(&iwe, 0, sizeof(iwe));
++	iwe.cmd = IWEVCUSTOM;
++	p = custom;
++	p += snprintf(p, MAX_CUSTOM_LEN, "bcn_int=%d", list->beacon_interval);
++	iwe.u.data.length = p - custom;
++	start = iwe_stream_add_point(start, stop, &iwe, custom);
++
++	/* AP security */
++	memset(&iwe, 0, sizeof(iwe));
++	iwe.cmd = IWEVCUSTOM;
++	p = custom;
++	p += snprintf(p, MAX_CUSTOM_LEN, "ap_sec=%04X", list->sec_info);
++	iwe.u.data.length = p - custom;
++	start = iwe_stream_add_point(start, stop, &iwe, custom);
++
++	return start;
++}
++
++static int gelicw_get_scan(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	int i;
++	char *ev = extra;
++	char *stop = ev + wrqu->data.length;
++
++	dev_dbg(ntodev(netdev), "wx: get_scan \n");
++	switch (w->state) {
++	case GELICW_STATE_DOWN:
++	case GELICW_STATE_UP:
++		return 0; /* no scan results */
++	case GELICW_STATE_SCANNING:
++		return -EAGAIN; /* now scanning */
++	case GELICW_STATE_SCAN_DONE:
++		if (!w->scan_all) /* essid scan */
++			return -EAGAIN;
++		break;
++	default:
++		break;
++	}
++
++	w->scan_all = 0;
++	for (i = 0; i < w->num_bss_list; i++)
++		ev = gelicw_translate_scan(netdev, ev, stop, &w->bss_list[i]);
++	wrqu->data.length = ev - extra;
++	wrqu->data.flags = 0;
++
++	/* start background scan */
++	if (w->essid_search)
++		schedule_delayed_work(&w->work_scan_essid,
++					GELICW_SCAN_INTERVAL);
++
++	return 0;
++}
++
++static int gelicw_set_essid(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u16 length = 0;
++
++	dev_dbg(ntodev(netdev), "wx:set_essid\n");
++	/* cancel scan */
++	w->essid_search = 0;
++	cancel_delayed_work(&w->work_scan_all);
++	cancel_delayed_work(&w->work_scan_essid);
++
++	if (wrqu->essid.flags && wrqu->essid.length)
++		length = wrqu->essid.length;
++
++	if (length == 0) {
++		/* essid ANY scan not supported */
++		dev_dbg(ntodev(netdev), "ESSID ANY\n");
++		w->essid_len = 0; /* clear essid */
++		w->essid[0] = '\0';
++		return 0;
++	} else {
++		/* check essid */
++		if (length > IW_ESSID_MAX_SIZE)
++			return -EINVAL;
++		if (w->essid_len == length &&
++		    !strncmp(w->essid, extra, length)) {
++			/* same essid */
++			if (w->is_assoc)
++				return 0;
++		} else {
++			/* set new essid */
++			w->essid_len = length;
++			memcpy(w->essid, extra, length);
++		}
++	}
++	/* start essid scan */
++	w->essid_search = 1;
++	schedule_delayed_work(&w->work_scan_essid, 0);
++
++	return 0;
++}
++
++static int gelicw_get_essid(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx:get_essid\n");
++	if (w->essid_len) {
++		memcpy(extra, w->essid, w->essid_len);
++		wrqu->essid.length = w->essid_len;
++		wrqu->essid.flags = 1;
++	} else {
++		wrqu->essid.length = 0;
++		wrqu->essid.flags = 0;
++	}
++
++	return 0;
++}
++
++static int gelicw_set_nick(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	u32 len = wrqu->data.length;
++
++	dev_dbg(ntodev(netdev), "wx:set_nick\n");
++	if (len > IW_ESSID_MAX_SIZE)
++		return -EINVAL;
++
++	memset(w->nick, 0, sizeof(w->nick));
++	memcpy(w->nick, extra, len);
++
++	return 0;
++}
++
++static int gelicw_get_nick(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx:get_nick\n");
++	wrqu->data.length = strlen(w->nick);
++	memcpy(extra, w->nick, wrqu->data.length);
++	wrqu->data.flags = 1;
++
++	return 0;
++}
++
++static int gelicw_set_rate(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	dev_dbg(ntodev(netdev), "wx:set_rate:%d\n", wrqu->bitrate.value);
++	if (wrqu->bitrate.value == -1)
++		return 0;	/* auto rate only */
++
++	return -EOPNOTSUPP;
++}
++
++static int gelicw_get_rate(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx:get_rate\n");
++
++	if (w->wireless_mode == IEEE_B ||
++	    (w->is_assoc && gelicw_is_ap_11b(&w->current_bss)))
++		wrqu->bitrate.value = bitrate_list[GELICW_NUM_11B_BITRATES -1];
++	else
++		wrqu->bitrate.value = bitrate_list[ARRAY_SIZE(bitrate_list) -1];
++
++	wrqu->bitrate.fixed = 0;
++
++	return 0;
++}
++
++static int gelicw_set_encode(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	struct iw_point *enc = &wrqu->encoding;
++	int i, index, key_index;
++
++	dev_dbg(ntodev(netdev), "wx:set_encode: flags:%x\n", enc->flags );
++	index = enc->flags & IW_ENCODE_INDEX;
++	if (index < 0 || index > WEP_KEYS)
++		return -EINVAL;
++	index--;
++
++	if (enc->length > IW_ENCODING_TOKEN_MAX)
++		return -EINVAL;
++
++	if (index != -1)
++		w->key_index = index;
++	key_index = w->key_index;
++
++	if (enc->flags & IW_ENCODE_DISABLED) {
++		/* disable encryption */
++		if (index == -1) {
++			/* disable all */
++			w->key_alg = IW_ENCODE_ALG_NONE;
++			for (i = 0; i < WEP_KEYS; i++)
++				w->key_len[i] = 0;
++		} else
++			w->key_len[key_index] = 0;
++	} else if (enc->flags & IW_ENCODE_NOKEY) {
++		/* key not changed */
++		if (w->key_alg == IW_ENCODE_ALG_NONE)
++			w->key_alg = IW_ENCODE_ALG_WEP; /* default wep */
++	} else {
++		/* enable encryption */
++		w->key_len[key_index] = enc->length;
++		if (w->key_alg == IW_ENCODE_ALG_NONE)
++			w->key_alg = IW_ENCODE_ALG_WEP; /* default wep */
++		memcpy(w->key[key_index], extra, w->key_len[key_index]);
++	}
++	dev_dbg(ntodev(netdev), "key %d len:%d alg:%x\n",\
++		key_index, w->key_len[key_index], w->key_alg);
++
++	if (w->state >= GELICW_STATE_SCAN_DONE &&
++	    w->cmd_send_flg == 0 && w->essid_len)
++		/* scan essid and set other params */
++		schedule_delayed_work(&w->work_scan_essid, 0);
++	else
++		schedule_delayed_work(&w->work_encode, 0);
++
++	return 0;
++}
++
++static int gelicw_get_encode(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	struct iw_point *enc = &wrqu->encoding;
++	int index, key_index;
++
++	dev_dbg(ntodev(netdev), "wx:get_encode\n");
++	index = enc->flags & IW_ENCODE_INDEX;
++	if (index < 0 || index > WEP_KEYS)
++		return -EINVAL;
++
++	index--;
++	key_index = (index == -1 ? w->key_index : index);
++	enc->flags = key_index + 1;
++
++	if (w->key_alg == IW_ENCODE_ALG_NONE || !w->key_len[key_index]) {
++		/* no encryption */
++		enc->flags |= IW_ENCODE_DISABLED;
++		enc->length = 0;
++	} else {
++		enc->flags |= IW_ENCODE_NOKEY;
++		enc->length = w->key_len[key_index];
++		memset(extra, 0, w->key_len[key_index]);
++	}
++
++	return 0;
++}
++
++static int gelicw_set_auth(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	struct iw_param *param = &wrqu->param;
++	int value = param->value;
++	int ret = 0;
++
++	dev_dbg(ntodev(netdev), "wx:set_auth:%x\n", param->flags & IW_AUTH_INDEX);
++	switch(param->flags & IW_AUTH_INDEX) {
++	case IW_AUTH_WPA_VERSION:
++	case IW_AUTH_CIPHER_PAIRWISE:
++	case IW_AUTH_CIPHER_GROUP:
++	case IW_AUTH_KEY_MGMT:
++	case IW_AUTH_TKIP_COUNTERMEASURES:
++	case IW_AUTH_DROP_UNENCRYPTED:
++	case IW_AUTH_WPA_ENABLED:
++	case IW_AUTH_RX_UNENCRYPTED_EAPOL:
++	case IW_AUTH_ROAMING_CONTROL:
++	case IW_AUTH_PRIVACY_INVOKED:
++		/* ignore */
++		dev_dbg(ntodev(netdev), "IW_AUTH(%x)\n", param->flags & IW_AUTH_INDEX);
++		break;
++	case IW_AUTH_80211_AUTH_ALG:
++		dev_dbg(ntodev(netdev), "IW_AUTH_80211_AUTH_ALG:\n");
++		if (value & IW_AUTH_ALG_SHARED_KEY)
++			w->auth_mode = IW_AUTH_ALG_SHARED_KEY;
++		else if (value & IW_AUTH_ALG_OPEN_SYSTEM)
++			w->auth_mode = IW_AUTH_ALG_OPEN_SYSTEM;
++		else
++			ret = -EINVAL;
++		break;
++	default:
++		dev_dbg(ntodev(netdev), "IW_AUTH_UNKNOWN flags:%x\n", param->flags);
++		ret = -EOPNOTSUPP;
++	}
++
++	return ret;
++}
++
++static int gelicw_get_auth(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	struct iw_param *param = &wrqu->param;
++
++	dev_dbg(ntodev(netdev), "wx:get_auth\n");
++	switch(param->flags & IW_AUTH_INDEX) {
++	case IW_AUTH_80211_AUTH_ALG:
++		param->value = w->auth_mode;
++		break;
++	case IW_AUTH_WPA_ENABLED:
++		if ((w->key_alg & IW_ENCODE_ALG_TKIP) ||
++		    (w->key_alg & IW_ENCODE_ALG_CCMP))
++			param->value = 1;
++		else
++			param->value = 0;
++		break;
++	default:
++		return -EOPNOTSUPP;
++	}
++
++	return 0;
++}
++
++static int gelicw_set_encodeext(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	struct iw_point *enc = &wrqu->encoding;
++	struct iw_encode_ext *ext = (struct iw_encode_ext *)extra;
++	int i, index, key_index;
++
++	dev_dbg(ntodev(netdev), "wx:set_encodeext\n");
++	index = enc->flags & IW_ENCODE_INDEX;
++	if (index < 0 || index > WEP_KEYS)
++		return -EINVAL;
++
++	index--;
++	if (ext->key_len > IW_ENCODING_TOKEN_MAX)
++		return -EINVAL;
++
++	if (index != -1)
++		w->key_index = index;
++	key_index = w->key_index;
++
++	if (enc->flags & IW_ENCODE_DISABLED) {
++		/* disable encryption */
++		if (index == -1) {
++			/* disable all */
++			w->key_alg = IW_ENCODE_ALG_NONE;
++			for (i = 0; i < WEP_KEYS; i++)
++				w->key_len[i] = 0;
++		} else
++			w->key_len[key_index] = 0;
++	} else if (enc->flags & IW_ENCODE_NOKEY)
++		/* key not changed */
++		w->key_alg = ext->alg;
++	else {
++		w->key_len[key_index] = ext->key_len;
++		w->key_alg = ext->alg;
++		if (w->key_alg != IW_ENCODE_ALG_NONE && w->key_len[key_index])
++			memcpy(w->key[key_index], ext->key, w->key_len[key_index]);
++	}
++	dev_dbg(ntodev(netdev), "key %d len:%d alg:%x\n",\
++		key_index, w->key_len[key_index], w->key_alg);
++
++	if (w->state >= GELICW_STATE_SCAN_DONE &&
++	    w->cmd_send_flg == 0 && w->essid_len)
++		/* scan essid and set other params */
++		schedule_delayed_work(&w->work_scan_essid, 0);
++	else
++		schedule_delayed_work(&w->work_encode, 0);
++
++	return 0;
++}
++
++static int gelicw_get_encodeext(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	struct iw_point *enc = &wrqu->encoding;
++	struct iw_encode_ext *ext = (struct iw_encode_ext *)extra;
++	int index, key_index, key_len;
++
++	dev_dbg(ntodev(netdev), "wx:get_encodeext\n");
++	key_len = enc->length - sizeof(*ext);
++	if (key_len < 0)
++		return -EINVAL;
++
++	index = enc->flags & IW_ENCODE_INDEX;
++	if (index < 0 || index > WEP_KEYS)
++		return -EINVAL;
++
++	index--;
++	key_index = (index == -1 ? w->key_index : index);
++
++	memset(ext, 0, sizeof(*ext));
++	enc->flags = key_index + 1;
++
++	if (w->key_alg == IW_ENCODE_ALG_NONE || !w->key_len[key_index]) {
++		/* no encryption */
++		enc->flags |= IW_ENCODE_DISABLED;
++		ext->alg = IW_ENCODE_ALG_NONE;
++		ext->key_len = 0;
++	} else {
++		enc->flags |= IW_ENCODE_NOKEY;
++		ext->alg = w->key_alg;
++		ext->key_len = w->key_len[key_index];
++	}
++
++	return 0;
++}
++
++/*
++ * wireless stats
++ */
++static struct iw_statistics *gelicw_get_wireless_stats(struct net_device *netdev)
++{
++	static struct iw_statistics wstats;
++	struct gelic_wireless *w = gelicw_priv(netdev);
++
++	dev_dbg(ntodev(netdev), "wx:wireless_stats\n");
++	if (w->state < GELICW_STATE_ASSOCIATED) {
++		wstats.qual.updated  = IW_QUAL_ALL_UPDATED |
++				IW_QUAL_QUAL_INVALID | IW_QUAL_NOISE_INVALID;
++		wstats.qual.qual = 0;
++		wstats.qual.level = 0;
++		wstats.qual.noise = 0;
++		return &wstats;
++	}
++	init_completion(&w->rssi_done);
++	schedule_delayed_work(&w->work_rssi, 0);
++
++	wait_for_completion_interruptible(&w->rssi_done);
++	wstats.qual.updated  = IW_QUAL_ALL_UPDATED |
++			IW_QUAL_QUAL_INVALID | IW_QUAL_NOISE_INVALID;
++	wstats.qual.level = w->rssi;
++	wstats.qual.qual = 0;
++	wstats.qual.noise = 0;
++
++	return &wstats;
++}
++
++/*
++ * private handler
++ */
++static int gelicw_priv_set_alg_mode(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	int mode = *(int *)extra;
++
++	dev_dbg(ntodev(netdev), "wx:priv_set_alg\n");
++	switch (mode) {
++	case IW_ENCODE_ALG_NONE:
++	case IW_ENCODE_ALG_WEP:
++	case IW_ENCODE_ALG_TKIP:
++	case IW_ENCODE_ALG_CCMP:
++		break;
++	default:
++		return -EINVAL;
++	}
++	/* send common config */
++	gelicw_send_common_config(netdev, &w->key_alg, (u8)mode);
++
++	return 0;
++}
++
++static int gelicw_priv_get_alg_mode(struct net_device *netdev,
++			   struct iw_request_info *info,
++			   union iwreq_data *wrqu, char *extra)
++{
++	struct gelic_wireless *w = gelicw_priv(netdev);
++	char *p;
++
++	dev_dbg(ntodev(netdev), "wx:priv_get_alg\n");
++	switch (w->key_alg) {
++	case IW_ENCODE_ALG_NONE:
++		strncpy(extra, "OFF", MAX_IW_PRIV_SIZE);
++		break;
++	case IW_ENCODE_ALG_WEP:
++		strncpy(extra, "WEP", MAX_IW_PRIV_SIZE);
++		break;
++	case IW_ENCODE_ALG_TKIP:
++		strncpy(extra, "TKIP", MAX_IW_PRIV_SIZE);
++		break;
++	case IW_ENCODE_ALG_CCMP:
++		strncpy(extra, "AES-CCMP", MAX_IW_PRIV_SIZE);
++		break;
++	default:
++		break;
++	}
++	p = extra + strlen(extra);
++
++	if (w->key_alg == IW_ENCODE_ALG_TKIP ||
++	    w->key_alg == IW_ENCODE_ALG_CCMP) {
++		if (w->key_len[w->key_index] == 64) /* current key index */
++			strncpy(p, " hex", MAX_IW_PRIV_SIZE);
++		else
++			strncpy(p, " passphrase", MAX_IW_PRIV_SIZE);
++	}
++	wrqu->data.length = strlen(extra);
++
++	return 0;
++}
++
++
++/*
++ * Wireless handlers
++ */
++static const iw_handler gelicw_handler[] =
++{
++	[IW_IOCTL_IDX(SIOCGIWNAME)]      = gelicw_get_name,
++	[IW_IOCTL_IDX(SIOCSIWFREQ)]      = gelicw_set_freq,
++	[IW_IOCTL_IDX(SIOCGIWFREQ)]      = gelicw_get_freq,
++	[IW_IOCTL_IDX(SIOCSIWMODE)]      = gelicw_set_mode,
++	[IW_IOCTL_IDX(SIOCGIWMODE)]      = gelicw_get_mode,
++	[IW_IOCTL_IDX(SIOCGIWRANGE)]     = gelicw_get_range,
++	[IW_IOCTL_IDX(SIOCSIWAP)]        = gelicw_set_wap,
++	[IW_IOCTL_IDX(SIOCGIWAP)]        = gelicw_get_wap,
++	[IW_IOCTL_IDX(SIOCSIWSCAN)]      = gelicw_set_scan,
++	[IW_IOCTL_IDX(SIOCGIWSCAN)]      = gelicw_get_scan,
++	[IW_IOCTL_IDX(SIOCSIWESSID)]     = gelicw_set_essid,
++	[IW_IOCTL_IDX(SIOCGIWESSID)]     = gelicw_get_essid,
++	[IW_IOCTL_IDX(SIOCSIWNICKN)]     = gelicw_set_nick,
++	[IW_IOCTL_IDX(SIOCGIWNICKN)]     = gelicw_get_nick,
++	[IW_IOCTL_IDX(SIOCSIWRATE)]      = gelicw_set_rate,
++	[IW_IOCTL_IDX(SIOCGIWRATE)]      = gelicw_get_rate,
++	[IW_IOCTL_IDX(SIOCSIWENCODE)]    = gelicw_set_encode,
++	[IW_IOCTL_IDX(SIOCGIWENCODE)]    = gelicw_get_encode,
++	[IW_IOCTL_IDX(SIOCSIWAUTH)]      = gelicw_set_auth,
++	[IW_IOCTL_IDX(SIOCGIWAUTH)]      = gelicw_get_auth,
++	[IW_IOCTL_IDX(SIOCSIWENCODEEXT)] = gelicw_set_encodeext,
++	[IW_IOCTL_IDX(SIOCGIWENCODEEXT)] = gelicw_get_encodeext,
++};
++
++/*
++ * Private wireless handlers
++ */
++enum {
++	GELICW_PRIV_SET_AUTH  = SIOCIWFIRSTPRIV,
++	GELICW_PRIV_GET_AUTH
++};
++
++static struct iw_priv_args gelicw_private_args[] = {
++	{
++	 .cmd = GELICW_PRIV_SET_AUTH,
++	 .set_args = IW_PRIV_TYPE_INT | IW_PRIV_SIZE_FIXED | 1,
++	 .name = "set_alg"
++	},
++	{
++	 .cmd = GELICW_PRIV_GET_AUTH,
++	 .get_args = IW_PRIV_TYPE_CHAR | IW_PRIV_SIZE_FIXED | MAX_IW_PRIV_SIZE,
++	 .name = "get_alg"
++	},
++};
++
++static const iw_handler gelicw_private_handler[] =
++{
++	gelicw_priv_set_alg_mode,
++	gelicw_priv_get_alg_mode,
++};
++
++static struct iw_handler_def gelicw_handler_def =
++{
++	.num_standard	= ARRAY_SIZE(gelicw_handler),
++	.num_private	= ARRAY_SIZE(gelicw_private_handler),
++	.num_private_args = ARRAY_SIZE(gelicw_private_args),
++	.standard	= (iw_handler *)gelicw_handler,
++	.private	= (iw_handler *)gelicw_private_handler,
++	.private_args	= (struct iw_priv_args *)gelicw_private_args,
++	.get_wireless_stats = gelicw_get_wireless_stats
++};
+--- /dev/null
++++ b/drivers/net/ps3_gelic_wireless.h
+@@ -0,0 +1,246 @@
++/*
++ *  PS3 Platfom gelic network driver.
++ *
++ * Copyright (C) 2007 Sony Computer Entertainment Inc.
++ * Copyright 2006, 2007 Sony Corporation.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License as published by
++ * the Free Software Foundation; either version 2, or (at your option)
++ * any later version.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
++ */
++#ifndef _GELIC_NET_WIRELESS_H
++#define _GELIC_NET_WIRELESS_H
++#include <linux/wireless.h>
++#include <net/ieee80211.h>
++
++/* wireless */
++#define GELICW_WIRELESS_NOT_EXIST	0
++#define GELICW_WIRELESS_SUPPORTED	1
++#define GELICW_WIRELESS_ON		2
++#define GELICW_WIRELESS_SHUTDOWN	3
++/* state */
++#define GELICW_STATE_DOWN		0
++#define GELICW_STATE_UP			1
++#define GELICW_STATE_SCANNING		2
++#define GELICW_STATE_SCAN_DONE		3
++#define GELICW_STATE_ASSOCIATED		4
++
++/* cmd_send_flg */
++#define GELICW_CMD_SEND_NONE		0x00
++#define GELICW_CMD_SEND_COMMON		0x01
++#define GELICW_CMD_SEND_ENCODE		0x02
++#define GELICW_CMD_SEND_SCAN		0x04
++#define GELICW_CMD_SEND_ALL		(GELICW_CMD_SEND_COMMON \
++					| GELICW_CMD_SEND_ENCODE \
++					| GELICW_CMD_SEND_SCAN)
++
++#define GELICW_SCAN_INTERVAL		(HZ)
++
++#ifdef DEBUG
++#define CH_INFO_FAIL 0x0600 /* debug */
++#else
++#define CH_INFO_FAIL 0
++#endif
++
++/* net_control command */
++#define GELICW_SET_PORT			3 /* control Ether port */
++#define GELICW_GET_INFO			6 /* get supported channels */
++#define GELICW_SET_CMD			9 /* set configuration */
++#define GELICW_GET_RES			10 /* get command response */
++#define GELICW_GET_EVENT		11 /* get event from device */
++/* net_control command data buffer */
++#define GELICW_DATA_BUF_SIZE		0x1000
++
++/* GELICW_SET_CMD params */
++#define GELICW_CMD_START		1
++#define GELICW_CMD_STOP			2
++#define GELICW_CMD_SCAN			3
++#define GELICW_CMD_GET_SCAN		4
++#define GELICW_CMD_SET_CONFIG		5
++#define GELICW_CMD_GET_CONFIG		6
++#define GELICW_CMD_SET_WEP		7
++#define GELICW_CMD_GET_WEP		8
++#define GELICW_CMD_SET_WPA		9
++#define GELICW_CMD_GET_WPA		10
++#define GELICW_CMD_GET_RSSI		11
++
++/* GELICW_SET_PORT params */
++#define GELICW_ETHER_PORT		2
++#define GELICW_PORT_DOWN		0 /* Ether port off */
++#define GELICW_PORT_UP			4 /* Ether port on (auto neg) */
++
++/* interrupt status bit */
++#define GELICW_DEVICE_CMD_COMP		(1UL << 31)
++#define GELICW_DEVICE_EVENT_RECV	(1UL << 30)
++
++/* GELICW_GET_EVENT ID */
++#define GELICW_EVENT_UNKNOWN		0x00
++#define GELICW_EVENT_DEVICE_READY	0x01
++#define GELICW_EVENT_SCAN_COMPLETED	0x02
++#define GELICW_EVENT_DEAUTH		0x04
++#define GELICW_EVENT_BEACON_LOST	0x08
++#define GELICW_EVENT_CONNECTED		0x10
++#define GELICW_EVENT_WPA_CONNECTED	0x20
++#define GELICW_EVENT_WPA_ERROR		0x40
++#define GELICW_EVENT_NO_ENTRY		(-6)
++
++#define MAX_IW_PRIV_SIZE		32
++
++/* structure of data buffer for lv1_net_contol */
++/* wep_config: sec */
++#define GELICW_WEP_SEC_NONE		0
++#define GELICW_WEP_SEC_40BIT		1
++#define GELICW_WEP_SEC_104BIT		2
++struct wep_config {
++	u16 sec;
++	u8  key[4][16];
++} __attribute__ ((packed));
++
++/* wpa_config: sec */
++#define GELICW_WPA_SEC_NONE		0
++#define GELICW_WPA_SEC_TKIP		1
++#define GELICW_WPA_SEC_AES		2
++/* wpa_config: psk_type */
++#define GELICW_PSK_PASSPHRASE		0
++#define GELICW_PSK_64HEX		1
++struct wpa_config {
++	u16 sec;
++	u16 psk_type;
++	u8  psk_material[64]; /* key */
++} __attribute__ ((packed));
++
++/* common_config: bss_type */
++#define GELICW_BSS_INFRA		0
++#define GELICW_BSS_ADHOC		1
++/* common_config: auth_method */
++#define GELICW_AUTH_OPEN		0
++#define GELICW_AUTH_SHARED		1
++/* common_config: op_mode */
++#define GELICW_OP_MODE_11BG		0
++#define GELICW_OP_MODE_11B		1
++#define GELICW_OP_MODE_11G		2
++struct common_config {
++	u16 scan_index; /* index of scan_desc list */
++	u16 bss_type;
++	u16 auth_method;
++	u16 op_mode;
++} __attribute__ ((packed));
++
++/* scan_descriptor: security */
++#define GELICW_SEC_TYPE_NONE		0x0000
++#define GELICW_SEC_TYPE_WEP		0x0100
++#define GELICW_SEC_TYPE_WEP40		0x0101
++#define GELICW_SEC_TYPE_WEP104		0x0102
++#define GELICW_SEC_TYPE_TKIP		0x0201
++#define GELICW_SEC_TYPE_AES		0x0202
++#define GELICW_SEC_TYPE_WEP_MASK	0xFF00
++struct scan_desc {
++	u16 size;
++	u16 rssi;
++	u16 channel;
++	u16 beacon_period;
++	u16 capability;
++	u16 security;
++	u64 bssid;
++	u8  essid[32];
++	u8  rate[16];
++	u8  ext_rate[16];
++	u32 reserved1;
++	u32 reserved2;
++	u32 reserved3;
++	u32 reserved4;
++} __attribute__ ((packed));
++
++/* rssi_descriptor */
++struct rssi_desc {
++	u16 rssi; /* max rssi = 100 */
++} __attribute__ ((packed));
++
++
++struct gelicw_bss {
++	u8 bssid[ETH_ALEN];
++	u8 channel;
++	u8 mode;
++	u8 essid_len;
++	u8 essid[IW_ESSID_MAX_SIZE + 1]; /* null terminated for debug msg */
++
++	u16 capability;
++	u16 beacon_interval;
++
++	u8 rates_len;
++	u8 rates[MAX_RATES_LENGTH];
++	u8 rates_ex_len;
++	u8 rates_ex[MAX_RATES_EX_LENGTH];
++	u8 rssi;
++
++	/* scan results have sec_info instead of rsn_ie or wpa_ie */
++	u16 sec_info;
++};
++
++/* max station count of station list which hvc returns */
++#define MAX_SCAN_BSS	(16)
++
++struct gelic_wireless {
++	struct gelic_net_card *card;
++	struct completion cmd_done, rssi_done;
++	struct work_struct work_event, work_start_done;
++	struct delayed_work work_rssi, work_scan_all, work_scan_essid;
++	struct delayed_work work_common, work_encode;
++	struct delayed_work work_start, work_stop, work_roam;
++	wait_queue_head_t waitq_cmd, waitq_scan;
++
++	u64 cmd_tag, cmd_id;
++	u8 cmd_send_flg;
++
++	struct iw_public_data wireless_data;
++	void *data_buf; /* data buffer for lv1_net_control */
++
++	u8 wireless; /* wireless support */
++	u8 state;
++	u8 scan_all; /* essid scan or all scan */
++	u8 essid_search; /* essid background scan */
++	u8 is_assoc;
++
++	u16 ch_info; /* supoprted channels */
++	u8 wireless_mode; /* 11b/g */
++	u8 channel; /* current ch */
++	u8 iw_mode; /* INFRA or Ad-hoc */
++	u8 rssi;
++	u8 essid_len;
++	u8 essid[IW_ESSID_MAX_SIZE + 1]; /* null terminated for debug msg */
++	u8 nick[IW_ESSID_MAX_SIZE + 1];
++	u8 bssid[ETH_ALEN];
++
++	u8 key_index;
++	u8 key[WEP_KEYS][IW_ENCODING_TOKEN_MAX]; /* 4 * 64byte */
++	u8 key_len[WEP_KEYS];
++	u8 key_alg; /* key algorithm  */
++	u8 auth_mode; /* authenticaton mode */
++
++	u8 bss_index; /* current bss in bss_list */
++	u8 num_bss_list;
++	u8 bss_key_alg; /* key alg of bss */
++	u8 wap_bssid[ETH_ALEN];
++	unsigned long last_scan; /* last scan time */
++	struct gelicw_bss current_bss;
++	struct gelicw_bss bss_list[MAX_SCAN_BSS];
++};
++
++extern int gelicw_setup_netdev(struct net_device *netdev, int wi);
++extern void gelicw_up(struct net_device *netdev);
++extern int gelicw_down(struct net_device *netdev);
++extern void gelicw_remove(struct net_device *netdev);
++extern void gelicw_interrupt(struct net_device *netdev, u64 status);
++extern int gelicw_is_associated(struct net_device *netdev);
++
++#endif /* _GELIC_NET_WIRELESS_H */
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-gelic-wpa2.diff linux-2.6.25-id/patches/ps3-wip/ps3-gelic-wpa2.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-gelic-wpa2.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-gelic-wpa2.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,92 @@
+---
+ drivers/net/ps3_gelic_wireless.c |   31 ++++++++++++++++++++++++++++---
+ drivers/net/ps3_gelic_wireless.h |    5 +++++
+ 2 files changed, 33 insertions(+), 3 deletions(-)
+
+--- a/drivers/net/ps3_gelic_wireless.c
++++ b/drivers/net/ps3_gelic_wireless.c
+@@ -432,10 +432,20 @@ static int gelicw_cmd_encode(struct net_
+ 		case IW_ENCODE_ALG_TKIP:
+ 			wpa_config->sec = GELICW_WPA_SEC_TKIP;
+ 			break;
+-		default:
+ 		case IW_ENCODE_ALG_CCMP:
+ 			wpa_config->sec = GELICW_WPA_SEC_AES;
+ 			break;
++		case GELICW_ENCODE_WPA2_TKIP:
++			wpa_config->sec = GELICW_WPA2_SEC_TKIP;
++			break;
++		case GELICW_ENCODE_WPA2_CCMP:
++			wpa_config->sec = GELICW_WPA2_SEC_AES;
++			break;
++		default:
++			if (ps3_compare_firmware_version(2, 0, 0) < 0)
++				wpa_config->sec = GELICW_WPA_SEC_AES;
++			else
++				wpa_config->sec = GELICW_WPA2_SEC_AES;
+ 		}
+ 		/* check key len */
+ 		key = w->key[w->key_index];
+@@ -2007,6 +2017,11 @@ static int gelicw_priv_set_alg_mode(stru
+ 	case IW_ENCODE_ALG_TKIP:
+ 	case IW_ENCODE_ALG_CCMP:
+ 		break;
++	case GELICW_ENCODE_WPA2_TKIP:
++	case GELICW_ENCODE_WPA2_CCMP:
++		if (ps3_compare_firmware_version(2, 0, 0) < 0)
++			return -EINVAL;
++		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+@@ -2022,6 +2037,7 @@ static int gelicw_priv_get_alg_mode(stru
+ {
+ 	struct gelic_wireless *w = gelicw_priv(netdev);
+ 	char *p;
++	int return_key = 0;
+ 
+ 	dev_dbg(ntodev(netdev), "wx:priv_get_alg\n");
+ 	switch (w->key_alg) {
+@@ -2033,17 +2049,26 @@ static int gelicw_priv_get_alg_mode(stru
+ 		break;
+ 	case IW_ENCODE_ALG_TKIP:
+ 		strncpy(extra, "TKIP", MAX_IW_PRIV_SIZE);
++		return_key = 1;
+ 		break;
+ 	case IW_ENCODE_ALG_CCMP:
+ 		strncpy(extra, "AES-CCMP", MAX_IW_PRIV_SIZE);
++		return_key = 1;
++		break;
++	case GELICW_ENCODE_WPA2_TKIP:
++		strncpy(extra, "TKIP WPA2", MAX_IW_PRIV_SIZE);
++		return_key = 1;
++		break;
++	case GELICW_ENCODE_WPA2_CCMP:
++		strncpy(extra, "AES-CCMP WPA2", MAX_IW_PRIV_SIZE);
++		return_key = 1;
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	p = extra + strlen(extra);
+ 
+-	if (w->key_alg == IW_ENCODE_ALG_TKIP ||
+-	    w->key_alg == IW_ENCODE_ALG_CCMP) {
++	if (return_key) {
+ 		if (w->key_len[w->key_index] == 64) /* current key index */
+ 			strncpy(p, " hex", MAX_IW_PRIV_SIZE);
+ 		else
+--- a/drivers/net/ps3_gelic_wireless.h
++++ b/drivers/net/ps3_gelic_wireless.h
+@@ -110,6 +110,11 @@ struct wep_config {
+ #define GELICW_WPA_SEC_NONE		0
+ #define GELICW_WPA_SEC_TKIP		1
+ #define GELICW_WPA_SEC_AES		2
++#define GELICW_WPA2_SEC_TKIP		4
++#define GELICW_WPA2_SEC_AES		8
++
++#define GELICW_ENCODE_WPA2_TKIP         (IW_ENCODE_ALG_CCMP + 1)
++#define GELICW_ENCODE_WPA2_CCMP         (IW_ENCODE_ALG_CCMP + 2)
+ /* wpa_config: psk_type */
+ #define GELICW_PSK_PASSPHRASE		0
+ #define GELICW_PSK_64HEX		1
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-htab-rework.diff linux-2.6.25-id/patches/ps3-wip/ps3-htab-rework.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-htab-rework.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-htab-rework.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,425 @@
+A work in progress.
+
+---
+ arch/powerpc/platforms/ps3/htab.c     |  279 +++++++++++++---------------------
+ arch/powerpc/platforms/ps3/platform.h |   13 +
+ arch/powerpc/platforms/ps3/setup.c    |    3 
+ 3 files changed, 127 insertions(+), 168 deletions(-)
+
+--- ps3-linux-mokuno.orig/arch/powerpc/platforms/ps3/htab.c
++++ ps3-linux-mokuno/arch/powerpc/platforms/ps3/htab.c
+@@ -24,8 +24,9 @@
+ #include <asm/lmb.h>
+ #include <asm/udbg.h>
+ #include <asm/lv1call.h>
++#ifdef CONFIG_FB_PS3
+ #include <asm/ps3fb.h>
+-
++#endif
+ #include "platform.h"
+ 
+ #if defined(DEBUG)
+@@ -34,191 +35,162 @@
+ #define DBG(fmt...) do{if(0)printk(fmt);}while(0)
+ #endif
+ 
+-static hpte_t *htab;
+-static unsigned long htab_addr;
+-static unsigned char *bolttab;
+-static unsigned char *inusetab;
+-
+-static spinlock_t ps3_bolttab_lock = SPIN_LOCK_UNLOCKED;
+-
+-#define debug_dump_hpte(_a, _b, _c, _d, _e, _f, _g) \
+-	_debug_dump_hpte(_a, _b, _c, _d, _e, _f, _g, __func__, __LINE__)
+-static void _debug_dump_hpte(unsigned long pa, unsigned long va,
+-	unsigned long group, unsigned long bitmap, hpte_t lhpte, int psize,
+-	unsigned long slot, const char* func, int line)
+-{
+-	DBG("%s:%d: pa     = %lxh\n", func, line, pa);
+-	DBG("%s:%d: lpar   = %lxh\n", func, line,
+-		ps3_mm_phys_to_lpar(pa));
+-	DBG("%s:%d: va     = %lxh\n", func, line, va);
+-	DBG("%s:%d: group  = %lxh\n", func, line, group);
+-	DBG("%s:%d: bitmap = %lxh\n", func, line, bitmap);
+-	DBG("%s:%d: hpte.v = %lxh\n", func, line, lhpte.v);
+-	DBG("%s:%d: hpte.r = %lxh\n", func, line, lhpte.r);
+-	DBG("%s:%d: psize  = %xh\n", func, line, psize);
+-	DBG("%s:%d: slot   = %lxh\n", func, line, slot);
+-}
++static spinlock_t ps3_htab_lock = SPIN_LOCK_UNLOCKED;
+ 
+ static long ps3_hpte_insert(unsigned long hpte_group, unsigned long va,
+ 	unsigned long pa, unsigned long rflags, unsigned long vflags, int psize)
+ {
+-	unsigned long slot;
+-	hpte_t lhpte;
+-	int secondary = 0;
+-	unsigned long result;
+-	unsigned long bitmap;
+-	unsigned long flags;
+-	unsigned long p_pteg, s_pteg, b_index, b_mask, cb, ci;
+-
+-	vflags &= ~HPTE_V_SECONDARY; /* this bit is ignored */
++	u64 result;
++	u64 hpte_v, hpte_r;
++	u64 inserted_index;
++	u64 evicted_v, evicted_r;
++	u64 hpte_v_array[4], hpte_rs;
++	long ret = -1;
+ 
+-	lhpte.v = hpte_encode_v(va, psize) | vflags | HPTE_V_VALID;
+-	lhpte.r = hpte_encode_r(ps3_mm_phys_to_lpar(pa), psize) | rflags;
++	/*
++	 * lv1_insert_htab_entry() will search for victim
++	 * entry in both primary and secondary pte group
++	 */
++	vflags &= ~HPTE_V_SECONDARY;
+ 
+-	p_pteg = hpte_group / HPTES_PER_GROUP;
+-	s_pteg = ~p_pteg & htab_hash_mask;
++	BUG_ON(hpte_group % HPTES_PER_GROUP);
+ 
+-	spin_lock_irqsave(&ps3_bolttab_lock, flags);
++	hpte_v = hpte_encode_v(va, psize) | vflags | HPTE_V_VALID;
++	hpte_r = hpte_encode_r(ps3_mm_phys_to_lpar(pa), psize) | rflags;
+ 
+-	BUG_ON(bolttab[p_pteg] == 0xff && bolttab[s_pteg] == 0xff);
++	spin_lock(&ps3_htab_lock);
+ 
+-	bitmap = (inusetab[p_pteg] << 8) | inusetab[s_pteg];
+-
+-	if (bitmap == 0xffff) {
+-		/*
+-		 * PTEG is full. Search for victim.
+-		 */
+-		bitmap &= ~((bolttab[p_pteg] << 8) | bolttab[s_pteg]);
+-		do {
+-			ci = mftb() & 15;
+-			cb = 0x8000UL >> ci;
+-		} while ((cb & bitmap) == 0);
+-	} else {
+-		/*
+-		 * search free slot in hardware order
+-		 *	[primary]	0, 2, 4, 6, 1, 3, 5, 7
+-		 *	[secondary]	0, 2, 4, 6, 1, 3, 5, 7
+-		 */
+-		for (ci = 0; ci < HPTES_PER_GROUP; ci += 2) {
+-			cb = 0x8000UL >> ci;
+-			if ((cb & bitmap) == 0)
+-				goto found;
+-		}
+-		for (ci = 1; ci < HPTES_PER_GROUP; ci += 2) {
+-			cb = 0x8000UL >> ci;
+-			if ((cb & bitmap) == 0)
+-				goto found;
+-		}
+-		for (ci = HPTES_PER_GROUP; ci < HPTES_PER_GROUP*2; ci += 2) {
+-			cb = 0x8000UL >> ci;
+-			if ((cb & bitmap) == 0)
+-				goto found;
+-		}
+-		for (ci = HPTES_PER_GROUP+1; ci < HPTES_PER_GROUP*2; ci += 2) {
+-			cb = 0x8000UL >> ci;
+-			if ((cb & bitmap) == 0)
+-				goto found;
+-		}
+-	}
+-
+-found:
+-	if (ci < HPTES_PER_GROUP) {
+-		slot = p_pteg * HPTES_PER_GROUP + ci;
+-	} else {
+-		slot = s_pteg * HPTES_PER_GROUP + (ci & 7);
+-		/* lhpte.dw0.dw0.h = 1; */
+-		vflags |= HPTE_V_SECONDARY;
+-		lhpte.v |= HPTE_V_SECONDARY;
+-	}
+-
+-	result = lv1_write_htab_entry(0, slot, lhpte.v, lhpte.r);
++	/* talk hvc to replace entries BOLTED == 0 */
++	result = lv1_insert_htab_entry(PS3_LPAR_VAS_ID_CURRENT, hpte_group,
++				       hpte_v, hpte_r,
++				       HPTE_V_BOLTED, 0,
++				       &inserted_index,
++				       &evicted_v, &evicted_r);
+ 
+ 	if (result) {
+-		debug_dump_hpte(pa, va, hpte_group, bitmap, lhpte, psize, slot);
++		/* all entries bolted !*/
++		DBG("va=%lx pa=%lx ix=%lx v=%lx r=%lx\n",
++		    va, pa, hpte_group, hpte_v, hpte_r);
+ 		BUG();
+ 	}
+ 
+ 	/*
+-	 * If used slot is not in primary HPTE group,
+-	 * the slot should be in secondary HPTE group.
++	 * see if the entry is inserted into secondary pteg
+ 	 */
++	result = lv1_read_htab_entries(PS3_LPAR_VAS_ID_CURRENT,
++				       inserted_index & ~0x3UL,
++				       &hpte_v_array[0], &hpte_v_array[1],
++				       &hpte_v_array[2], &hpte_v_array[3],
++				       &hpte_rs);
++	BUG_ON(result);
+ 
+-	if ((hpte_group ^ slot) & ~(HPTES_PER_GROUP - 1)) {
+-		secondary = 1;
+-		b_index = s_pteg;
++	if (hpte_v_array[inserted_index % 4] & HPTE_V_SECONDARY) {
++		ret = (inserted_index & 7) | (1 << 3);
+ 	} else {
+-		secondary = 0;
+-		b_index = p_pteg;
++		ret = inserted_index & 7;
+ 	}
+ 
+-	b_mask = (lhpte.v & HPTE_V_BOLTED) ? 1 << 7 : 0 << 7;
+-	bolttab[b_index] |= b_mask >> (slot & 7);
+-	b_mask = 1 << 7;
+-	inusetab[b_index] |= b_mask >> (slot & 7);
+-	spin_unlock_irqrestore(&ps3_bolttab_lock, flags);
++	spin_unlock(&ps3_htab_lock);
+ 
+-	return (slot & 7) | (secondary << 3);
++	return ret;
+ }
+ 
+ static long ps3_hpte_remove(unsigned long hpte_group)
+ {
+-	panic("ps3_hpte_remove() not implemented");
+-	return 0;
++	int i;
++	int slot_offset;
++	u64 hpte_v, hpte_rs;
++	u64 hpte_v_array[HPTES_PER_GROUP];
++	long ret = 0;
++
++	spin_lock(&ps3_htab_lock);
++	lv1_read_htab_entries(PS3_LPAR_VAS_ID_CURRENT, hpte_group,
++			      &hpte_v_array[0], &hpte_v_array[1],
++			      &hpte_v_array[2], &hpte_v_array[3],
++			      &hpte_rs);
++	lv1_read_htab_entries(PS3_LPAR_VAS_ID_CURRENT, hpte_group + 4,
++			      &hpte_v_array[4], &hpte_v_array[5],
++			      &hpte_v_array[6], &hpte_v_array[7],
++			      &hpte_rs);
++	/* pick a random entry to start at */
++	slot_offset = mftb();
++
++	for (i = 0; i < HPTES_PER_GROUP; i++, slot_offset++) {
++		slot_offset &= 0x7;
++		hpte_v = hpte_v_array[slot_offset];
++
++		if ((hpte_v & HPTE_V_VALID) && !(hpte_v & HPTE_V_BOLTED)) {
++			/* find the victim */
++			break;
++		}
++	}
++
++	if (i == HPTES_PER_GROUP) {
++		ret = -1;
++	} else {
++		lv1_write_htab_entry(PS3_LPAR_VAS_ID_CURRENT,
++				     hpte_group + slot_offset, 0, 0);
++	}
++
++	spin_unlock(&ps3_htab_lock);
++
++	return ret;
+ }
+ 
+ static long ps3_hpte_updatepp(unsigned long slot, unsigned long newpp,
+ 	unsigned long va, int psize, int local)
+ {
+-	unsigned long flags;
+-	unsigned long result;
+-	unsigned long pteg, bit;
+-	unsigned long hpte_v, want_v;
++	u64 result;
++	u64 hpte_v, want_v, hpte_rs;
++	u64 hpte_v_array[4];
++	long ret = -1;
+ 
+ 	want_v = hpte_encode_v(va, psize);
+ 
+-	spin_lock_irqsave(&ps3_bolttab_lock, flags);
++	spin_lock(&ps3_htab_lock);
+ 
+-	hpte_v = htab[slot].v;
+-	if (!HPTE_V_COMPARE(hpte_v, want_v) || !(hpte_v & HPTE_V_VALID)) {
+-		spin_unlock_irqrestore(&ps3_bolttab_lock, flags);
+-
+-		/* ps3_hpte_insert() will be used to update PTE */
+-		return -1;
+-	}
+-
+-	result = lv1_write_htab_entry(0, slot, 0, 0);
++	result = lv1_read_htab_entries(PS3_LPAR_VAS_ID_CURRENT, slot & ~0x3UL,
++				       &hpte_v_array[0], &hpte_v_array[1],
++				       &hpte_v_array[2], &hpte_v_array[3],
++				       &hpte_rs);
+ 
+ 	if (result) {
+-		DBG("%s: va=%lx slot=%lx psize=%d result = %ld (0x%lx)\n",
+-		       __func__, va, slot, psize, result, result);
++		DBG("%s: read va=%lx slot=%lx psize=%d result = %ld (0x%lx)\n",
++		    __FUNCTION__, va, slot, psize, result, result);
+ 		BUG();
+ 	}
+ 
+-	pteg = slot / HPTES_PER_GROUP;
+-	bit = slot % HPTES_PER_GROUP;
+-	inusetab[pteg] &= ~(0x80 >> bit);
++	hpte_v = hpte_v_array[slot % 4];
+ 
+-	spin_unlock_irqrestore(&ps3_bolttab_lock, flags);
++	if (!HPTE_V_COMPARE(hpte_v, want_v) || !(hpte_v & HPTE_V_VALID)) {
++		/* ps3_hpte_insert() will be used to update PTE */
++		ret = -1;
++	} else {
++		result = lv1_write_htab_entry(PS3_LPAR_VAS_ID_CURRENT,
++					      slot, 0, 0);
++		ret = -1;
++	}
+ 
+-	/* ps3_hpte_insert() will be used to update PTE */
+-	return -1;
++	spin_unlock(&ps3_htab_lock);
++	return ret;
+ }
+ 
+ static void ps3_hpte_updateboltedpp(unsigned long newpp, unsigned long ea,
+ 	int psize)
+ {
+-	panic("ps3_hpte_updateboltedpp() not implemented");
++	DBG("%s:newpp=%ld ea=%ld", __FUNCTION__, newpp, ea);
+ }
+ 
+ static void ps3_hpte_invalidate(unsigned long slot, unsigned long va,
+ 	int psize, int local)
+ {
+ 	unsigned long flags;
+-	unsigned long result;
+-	unsigned long pteg, bit;
++	u64 result;
++
++	local_irq_save(flags);
++	spin_lock(&ps3_htab_lock);
+ 
+-	spin_lock_irqsave(&ps3_bolttab_lock, flags);
+-	result = lv1_write_htab_entry(0, slot, 0, 0);
++	result = lv1_write_htab_entry(PS3_LPAR_VAS_ID_CURRENT, slot, 0, 0);
+ 
+ 	if (result) {
+ 		DBG("%s: va=%lx slot=%lx psize=%d result = %ld (0x%lx)\n",
+@@ -226,25 +198,25 @@ static void ps3_hpte_invalidate(unsigned
+ 		BUG();
+ 	}
+ 
+-	pteg = slot / HPTES_PER_GROUP;
+-	bit = slot % HPTES_PER_GROUP;
+-	inusetab[pteg] &= ~(0x80 >> bit);
+-	spin_unlock_irqrestore(&ps3_bolttab_lock, flags);
++	spin_unlock(&ps3_htab_lock);
++	local_irq_restore(flags);
+ }
+ 
+ static void ps3_hpte_clear(void)
+ {
++	unsigned long hpte_count = (1UL << ppc64_pft_size) >> 4;
++	u64 i;
++
+ 	/* Make sure to clean up the frame buffer device first */
++#ifdef CONFIG_FB_PS3
+ 	ps3fb_cleanup();
+-
+-	lv1_unmap_htab(htab_addr);
++#endif
++	for (i = 0; i < hpte_count; i++)
++		lv1_write_htab_entry(PS3_LPAR_VAS_ID_CURRENT, i, 0, 0);
+ }
+ 
+ void __init ps3_hpte_init(unsigned long htab_size)
+ {
+-	long bitmap_size;
+-
+-	DBG(" -> %s:%d\n", __func__, __LINE__);
+ 
+ 	ppc_md.hpte_invalidate = ps3_hpte_invalidate;
+ 	ppc_md.hpte_updatepp = ps3_hpte_updatepp;
+@@ -254,27 +226,4 @@ void __init ps3_hpte_init(unsigned long 
+ 	ppc_md.hpte_clear_all = ps3_hpte_clear;
+ 
+ 	ppc64_pft_size = __ilog2(htab_size);
+-
+-	bitmap_size = htab_size / sizeof(hpte_t) / 8;
+-
+-	bolttab = __va(lmb_alloc(bitmap_size, 1));
+-	inusetab = __va(lmb_alloc(bitmap_size, 1));
+-
+-	memset(bolttab, 0, bitmap_size);
+-	memset(inusetab, 0, bitmap_size);
+-
+-	DBG(" <- %s:%d\n", __func__, __LINE__);
+-}
+-
+-void __init ps3_map_htab(void)
+-{
+-	long result;
+-	unsigned long htab_size = (1UL << ppc64_pft_size);
+-
+-	result = lv1_map_htab(0, &htab_addr);
+-
+-	htab = (hpte_t *)__ioremap(htab_addr, htab_size, PAGE_READONLY_X);
+-
+-	DBG("%s:%d: lpar %016lxh, virt %016lxh\n", __func__, __LINE__,
+-		htab_addr, (unsigned long)htab);
+ }
+--- ps3-linux-mokuno.orig/arch/powerpc/platforms/ps3/platform.h
++++ ps3-linux-mokuno/arch/powerpc/platforms/ps3/platform.h
+@@ -2,7 +2,7 @@
+  *  PS3 platform declarations.
+  *
+  *  Copyright (C) 2006 Sony Computer Entertainment Inc.
+- *  Copyright 2006 Sony Corp.
++ *  Copyright 2006, 2007 Sony Corp.
+  *
+  *  This program is free software; you can redistribute it and/or modify
+  *  it under the terms of the GNU General Public License as published by
+@@ -226,4 +226,15 @@ int ps3_repository_read_num_spu_resource
+ int ps3_repository_read_spu_resource_id(unsigned int res_index,
+ 	enum ps3_spu_resource_type* resource_type, unsigned int *resource_id);
+ 
++/**
++ * enum lpar_vas_id - id of LPAR virtual address space.
++ * @lpar_vas_id_current: Current selected virtual address space
++ *
++ * Identify the target LPAR address space.
++ */
++
++enum ps3_lpar_vas_id {
++	PS3_LPAR_VAS_ID_CURRENT = 0,
++};
++
+ #endif
+--- ps3-linux-mokuno.orig/arch/powerpc/platforms/ps3/setup.c
++++ ps3-linux-mokuno/arch/powerpc/platforms/ps3/setup.c
+@@ -2,7 +2,7 @@
+  *  PS3 platform setup routines.
+  *
+  *  Copyright (C) 2006 Sony Computer Entertainment Inc.
+- *  Copyright 2006 Sony Corp.
++ *  Copyright 2006, 2007 Sony Corp.
+  *
+  *  This program is free software; you can redistribute it and/or modify
+  *  it under the terms of the GNU General Public License as published by
+@@ -161,7 +161,6 @@ static void __init ps3_setup_arch(void)
+ 		v.rev);
+ 
+ 	ps3_spu_set_platform();
+-	ps3_map_htab();
+ 
+ #ifdef CONFIG_SMP
+ 	smp_init_ps3();
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-oprofile.patch linux-2.6.25-id/patches/ps3-wip/ps3-oprofile.patch
--- linux-2.6.25-org/patches/ps3-wip/ps3-oprofile.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-oprofile.patch	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,858 @@
+Subject: PS3: Add oprofile support
+
+This is a WIP.
+
+Add PS3 oprofile support.
+
+wip-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/oprofile/Makefile       |    1 
+ arch/powerpc/oprofile/common.c       |   19 
+ arch/powerpc/oprofile/op_model_ps3.c |  767 +++++++++++++++++++++++++++++++++++
+ arch/powerpc/platforms/ps3/Kconfig   |    5 
+ include/asm-powerpc/oprofile_impl.h  |    1 
+ 5 files changed, 791 insertions(+), 2 deletions(-)
+
+--- a/arch/powerpc/oprofile/Makefile
++++ b/arch/powerpc/oprofile/Makefile
+@@ -14,6 +14,7 @@ oprofile-y := $(DRIVER_OBJS) common.o ba
+ oprofile-$(CONFIG_OPROFILE_CELL) += op_model_cell.o \
+ 		cell/spu_profiler.o cell/vma_map.o \
+ 		cell/spu_task_sync.o
++oprofile-$(CONFIG_OPROFILE_PS3) += op_model_ps3.o
+ oprofile-$(CONFIG_PPC64) += op_model_rs64.o op_model_power4.o op_model_pa6t.o
+ oprofile-$(CONFIG_FSL_BOOKE) += op_model_fsl_booke.o
+ oprofile-$(CONFIG_6xx) += op_model_7450.o
+--- a/arch/powerpc/oprofile/common.c
++++ b/arch/powerpc/oprofile/common.c
+@@ -178,10 +178,23 @@ int __init oprofile_arch_init(struct opr
+ 
+ 	switch (cur_cpu_spec->oprofile_type) {
+ #ifdef CONFIG_PPC64
+-#ifdef CONFIG_OPROFILE_CELL
++#if defined(CONFIG_OPROFILE_CELL) || defined(CONFIG_OPROFILE_PS3)
+ 		case PPC_OPROFILE_CELL:
+-			if (firmware_has_feature(FW_FEATURE_LPAR))
++			printk("%s:%d: \n", __func__, __LINE__);
++			if (firmware_has_feature(FW_FEATURE_PS3_LV1)) {
++				printk("%s:%d: \n", __func__, __LINE__);
++				model = &op_model_ps3;
++				ops->sync_start = model->sync_start;
++				ops->sync_stop = model->sync_stop;
++				break;
++			}
++
++			if (firmware_has_feature(FW_FEATURE_LPAR)) {
++				printk("%s:%d: \n", __func__, __LINE__);
+ 				return -ENODEV;
++			}
++
++			printk("%s:%d: \n", __func__, __LINE__);
+ 			model = &op_model_cell;
+ 			ops->sync_start = model->sync_start;
+ 			ops->sync_stop = model->sync_stop;
+@@ -208,8 +221,10 @@ int __init oprofile_arch_init(struct opr
+ 			break;
+ #endif
+ 		default:
++			printk("%s:%d: \n", __func__, __LINE__);
+ 			return -ENODEV;
+ 	}
++	printk("%s:%d: \n", __func__, __LINE__);
+ 
+ 	model->num_counters = cur_cpu_spec->num_pmcs;
+ 
+--- /dev/null
++++ b/arch/powerpc/oprofile/op_model_ps3.c
+@@ -0,0 +1,767 @@
++/*
++ * PS3 OProfile support
++ *
++ * This file based on op_model_cell.c, but the spu profiling has not
++ * been implemented yet.
++ *
++ * Copyright (C) 2007 Sony Computer Entertainment Inc.
++ * Copyright 2007 Sony Corporation.
++ *
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of the GNU General Public License as
++ * published by the Free Software Foundation; version 2 of the License.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
++ * 02111-1307 USA
++ */
++
++#include <linux/cpufreq.h>
++#include <linux/delay.h>
++#include <linux/init.h>
++#include <linux/jiffies.h>
++#include <linux/kthread.h>
++#include <linux/oprofile.h>
++#include <linux/percpu.h>
++#include <linux/smp.h>
++#include <linux/spinlock.h>
++#include <linux/timer.h>
++#include <linux/firmware.h>
++#include <linux/io.h>
++#include <linux/ptrace.h>
++#include <asm/cell-pmu.h>
++#include <asm/cputable.h>
++#include <asm/oprofile_impl.h>
++#include <asm/processor.h>
++#include <asm/prom.h>
++#include <asm/reg.h>
++#include <asm/system.h>
++#include <asm/cell-regs.h>
++#include <asm/pmc.h>
++#include <asm/irq_regs.h>
++#include <asm/ps3.h>
++
++#include "../platforms/cell/interrupt.h"
++#include "cell/pr_util.h"
++
++#define OP_ERR(f, x...)  pr_info("pmu: " f "\n", ## x)
++#define OP_DBG(f, x...)  pr_debug("pmu: " f "\n", ## x)
++
++/*
++ * spu_cycle_reset is the number of cycles between samples.
++ * This variable is used for SPU profiling and should ONLY be set
++ * at the beginning of cell_reg_setup; otherwise, it's read-only.
++ */
++static unsigned int spu_cycle_reset;
++
++#define NUM_SPUS_PER_NODE    8
++#define SPU_CYCLES_EVENT_NUM 2	/*  event number for SPU_CYCLES */
++
++#define PPU_CYCLES_EVENT_NUM 1	/*  event number for CYCLES */
++#define PPU_CYCLES_GRP_NUM   1	/* special group number for identifying
++				 * PPU_CYCLES event
++				 */
++#define CBE_COUNT_ALL_CYCLES 0x42800000 /* PPU cycle event specifier */
++
++#define NUM_THREADS 2         /* number of physical threads in
++			       * physical processor
++			       */
++#define NUM_DEBUG_BUS_WORDS 4
++#define NUM_INPUT_BUS_WORDS 2
++
++#define MAX_SPU_COUNT 0xFFFFFF	/* maximum 24 bit LFSR value */
++
++struct pmc_cntrl_data {
++	unsigned long vcntr;
++	unsigned long evnts;
++	unsigned long masks;
++	unsigned long enabled;
++};
++
++struct pm_signal {
++	u16 cpu;		/* Processor to modify */
++	u16 sub_unit;		/* hw subunit this applies to (if applicable)*/
++	short int signal_group; /* Signal Group to Enable/Disable */
++	u8 bus_word;		/* Enable/Disable on this Trace/Trigger/Event
++				 * Bus Word(s) (bitmask)
++				 */
++	u8 bit;			/* Trigger/Event bit (if applicable) */
++};
++
++struct pm_cntrl {
++	u16 enable;
++	u16 stop_at_max;
++	u16 trace_mode;
++	u16 freeze;
++	u16 count_mode;
++};
++
++static struct {
++	u32 group_control;
++	u32 debug_bus_control;
++	struct pm_cntrl pm_cntrl;
++	u32 pm07_cntrl[NR_PHYS_CTRS];
++} pm_regs;
++
++#define GET_SUB_UNIT(x) ((x & 0x0000f000) >> 12)
++#define GET_BUS_WORD(x) ((x & 0x000000f0) >> 4)
++#define GET_BUS_TYPE(x) ((x & 0x00000300) >> 8)
++#define GET_POLARITY(x) ((x & 0x00000002) >> 1)
++#define GET_COUNT_CYCLES(x) (x & 0x00000001)
++#define GET_INPUT_CONTROL(x) ((x & 0x00000004) >> 2)
++
++static DEFINE_PER_CPU(unsigned long[NR_PHYS_CTRS], pmc_values);
++
++static struct pmc_cntrl_data pmc_cntrl[NUM_THREADS][NR_PHYS_CTRS];
++
++/*
++ * Interpetation of hdw_thread:
++ * 0 - even virtual cpus 0, 2, 4,...
++ * 1 - odd virtual cpus 1, 3, 5, ...
++ *
++ * FIXME: this is strictly wrong, we need to clean this up in a number
++ * of places. It works for now. -arnd
++ */
++static u32 hdw_thread;
++
++static u32 virt_cntr_inter_mask;
++static struct timer_list timer_virt_cntr;
++
++/*
++ * pm_signal needs to be global since it is initialized in
++ * cell_reg_setup at the time when the necessary information
++ * is available.
++ */
++static struct pm_signal pm_signal[NR_PHYS_CTRS];
++
++static u32 reset_value[NR_PHYS_CTRS];
++static u32 count_value[NR_PHYS_CTRS];
++static int num_counters;
++static int oprofile_running;
++static DEFINE_SPINLOCK(virt_cntr_lock);
++
++static u32 ctr_enabled;
++
++static unsigned char input_bus[NUM_INPUT_BUS_WORDS];
++
++static u32 ps3_cpu_to_node(int cpu)
++{
++	return 0;
++}
++
++static int pm_activate_signals(u32 node, u32 count)
++{
++	int i, j;
++	struct pm_signal pm_signal_local[NR_PHYS_CTRS];
++
++	/*
++	 * There is no debug setup required for the cycles event.
++	 * Note that only events in the same group can be used.
++	 * Otherwise, there will be conflicts in correctly routing
++	 * the signals on the debug bus.  It is the responsiblity
++	 * of the OProfile user tool to check the events are in
++	 * the same group.
++	 */
++	i = 0;
++	for (j = 0; j < count; j++) {
++		if (pm_signal[j].signal_group != PPU_CYCLES_GRP_NUM) {
++
++			/* fw expects physical cpu # */
++			pm_signal_local[i].cpu = node;
++			pm_signal_local[i].signal_group
++			    = pm_signal[j].signal_group;
++			pm_signal_local[i].bus_word =
++			    pm_signal[j].bus_word;
++			pm_signal_local[i].sub_unit =
++			    pm_signal[j].sub_unit;
++			pm_signal_local[i].bit = pm_signal[j].bit;
++			ps3_set_signal(pm_signal[j].signal_group,
++				       pm_signal[j].bit,
++				       pm_signal[j].sub_unit,
++				       pm_signal[j].bus_word);
++			i++;
++		}
++	}
++	return 0;
++}
++
++/*
++ * PM Signal functions
++ */
++static void set_pm_event(u32 ctr, int event, u32 unit_mask)
++{
++	struct pm_signal *p;
++	u32 signal_bit;
++	u32 bus_word, bus_type, count_cycles, polarity, input_control;
++	int j, i;
++
++	if (event == PPU_CYCLES_EVENT_NUM) {
++		/* Special Event: Count all cpu cycles */
++		pm_regs.pm07_cntrl[ctr] = CBE_COUNT_ALL_CYCLES;
++		p = &(pm_signal[ctr]);
++		p->signal_group = PPU_CYCLES_GRP_NUM;
++		p->bus_word = 1;
++		p->sub_unit = 0;
++		p->bit = 0;
++		goto out;
++	} else {
++		pm_regs.pm07_cntrl[ctr] = 0;
++	}
++
++	bus_word = GET_BUS_WORD(unit_mask);
++	bus_type = GET_BUS_TYPE(unit_mask);
++	count_cycles = GET_COUNT_CYCLES(unit_mask);
++	polarity = GET_POLARITY(unit_mask);
++	input_control = GET_INPUT_CONTROL(unit_mask);
++	signal_bit = (event % 100);
++
++	p = &(pm_signal[ctr]);
++
++	p->signal_group = event / 100;
++	p->bus_word = bus_word;
++	p->sub_unit = GET_SUB_UNIT(unit_mask);
++
++	/*
++	 * This parameter is used to specify the target physical/logical
++	 * PPE/SPE object.
++	 */
++	if (p->signal_group < 42 || 56 < p->signal_group)
++		p->sub_unit = 1;
++
++	pm_regs.pm07_cntrl[ctr] = 0;
++	pm_regs.pm07_cntrl[ctr] |= PM07_CTR_COUNT_CYCLES(count_cycles);
++	pm_regs.pm07_cntrl[ctr] |= PM07_CTR_POLARITY(polarity);
++	pm_regs.pm07_cntrl[ctr] |= PM07_CTR_INPUT_CONTROL(input_control);
++
++	/*
++	 * Some of the islands signal selection is based on 64 bit words.
++	 * The debug bus words are 32 bits, the input words to the performance
++	 * counters are defined as 32 bits.  Need to convert the 64 bit island
++	 * specification to the appropriate 32 input bit and bus word for the
++	 * performance counter event selection.  See the CELL Performance
++	 * monitoring signals manual and the Perf cntr hardware descriptions
++	 * for the details.
++	 */
++	if (input_control == 0) {
++		if (signal_bit > 31) {
++			signal_bit -= 32;
++			if (bus_word == 0x3)
++				bus_word = 0x2;
++			else if (bus_word == 0xc)
++				bus_word = 0x8;
++		}
++
++		if ((bus_type == 0) && p->signal_group >= 60)
++			bus_type = 2;
++		if ((bus_type == 0) &&
++		    (30 <= p->signal_group && p->signal_group <= 40))
++			bus_type = 2;
++		if ((bus_type == 1) && p->signal_group >= 50)
++			bus_type = 0;
++
++		pm_regs.pm07_cntrl[ctr] |= PM07_CTR_INPUT_MUX(signal_bit);
++	} else {
++		pm_regs.pm07_cntrl[ctr] = 0;
++		p->bit = signal_bit;
++	}
++
++	for (i = 0; i < NUM_DEBUG_BUS_WORDS; i++) {
++		if (bus_word & (1 << i)) {
++			pm_regs.debug_bus_control |=
++				(bus_type << (30 - (2 * i)));
++			for (j = 0; j < NUM_INPUT_BUS_WORDS; j++) {
++				if (input_bus[j] == 0xff) {
++					input_bus[j] = i;
++					pm_regs.group_control |=
++						(i << (30 - (2 * j)));
++
++					break;
++				}
++			}
++		}
++	}
++	OP_DBG("pm07_ctrl[%d] : 0x%x", ctr, pm_regs.pm07_cntrl[ctr]);
++	OP_DBG("group_control : 0x%x", pm_regs.group_control);
++	OP_DBG("debug_bus_control : 0x%x", pm_regs.debug_bus_control);
++out:
++	;
++}
++
++static void write_pm_cntrl(int cpu)
++{
++	/*
++	 * Oprofile will use 32 bit counters, set bits 7:10 to 0
++	 * pmregs.pm_cntrl is a global
++	 */
++
++	u32 val = 0;
++	if (pm_regs.pm_cntrl.enable == 1)
++		val |= CBE_PM_ENABLE_PERF_MON;
++
++	if (pm_regs.pm_cntrl.stop_at_max == 1)
++		val |= CBE_PM_STOP_AT_MAX;
++
++	if (pm_regs.pm_cntrl.trace_mode == 1)
++		val |= CBE_PM_TRACE_MODE_SET(pm_regs.pm_cntrl.trace_mode);
++
++	if (pm_regs.pm_cntrl.freeze == 1)
++		val |= CBE_PM_FREEZE_ALL_CTRS;
++
++	/*
++	 * Routine set_count_mode must be called previously to set
++	 * the count mode based on the user selection of user and kernel.
++	 */
++	val |= CBE_PM_COUNT_MODE_SET(pm_regs.pm_cntrl.count_mode);
++	ps3_write_pm(cpu, pm_control, val);
++}
++
++static inline void
++set_count_mode(u32 kernel, u32 user)
++{
++	OP_DBG("set_count_mode k:%d u:%d", kernel, user);
++	/*
++	 * The user must specify user and kernel if they want them. If
++	 *  neither is specified, OProfile will count in hypervisor mode.
++	 *  pm_regs.pm_cntrl is a global
++	 *
++	 *  NOTE : PS3 hypervisor rejects ALL_MODES and HYPERVISOR_MODE.
++	 *         So, ALL_MODES and HYPERVISOR_MODE are changed to
++	 *         PROBLEM_MODE.
++	 */
++	if (kernel) {
++		if (user)
++			pm_regs.pm_cntrl.count_mode =
++				CBE_COUNT_PROBLEM_MODE;
++		else
++			pm_regs.pm_cntrl.count_mode =
++				CBE_COUNT_SUPERVISOR_MODE;
++	} else {
++		if (user)
++			pm_regs.pm_cntrl.count_mode =
++				CBE_COUNT_PROBLEM_MODE;
++		else
++			pm_regs.pm_cntrl.count_mode =
++				CBE_COUNT_PROBLEM_MODE;
++	}
++}
++
++static inline void enable_ctr(u32 cpu, u32 ctr, u32 *pm07_cntrl)
++{
++
++	pm07_cntrl[ctr] |= CBE_PM_CTR_ENABLE;
++	ps3_write_pm07_control(cpu, ctr, pm07_cntrl[ctr]);
++}
++
++static void add_sample(u32 cpu)
++{
++	struct pt_regs *regs;
++	u64 pc;
++	int is_kernel;
++	int i;
++	u32 value;
++
++	regs = get_irq_regs();
++	if (oprofile_running == 1) {
++		pc = regs->nip;
++		is_kernel = is_kernel_addr(pc);
++
++		for (i = 0; i < num_counters; ++i) {
++			value = ps3_read_ctr(cpu, i);
++			if (value >= count_value[i] && count_value[i] != 0) {
++				OP_DBG("pmu:add_sample ctr:%d"
++				       " value:0x%x reset:0x%x count:0x%x",
++				       i, value, reset_value[i],
++				       count_value[i]);
++				oprofile_add_pc(pc, is_kernel, i);
++				ps3_write_ctr(cpu, i, reset_value[i]);
++			}
++		}
++	}
++}
++
++/*
++ * Oprofile is expected to collect data on all CPUs simultaneously.
++ * However, there is one set of performance counters per node.	There are
++ * two hardware threads or virtual CPUs on each node.  Hence, OProfile must
++ * multiplex in time the performance counter collection on the two virtual
++ * CPUs.  The multiplexing of the performance counters is done by this
++ * virtual counter routine.
++ *
++ * The pmc_values used below is defined as 'per-cpu' but its use is
++ * more akin to 'per-node'.  We need to store two sets of counter
++ * values per node -- one for the previous run and one for the next.
++ * The per-cpu[NR_PHYS_CTRS] gives us the storage we need.  Each odd/even
++ * pair of per-cpu arrays is used for storing the previous and next
++ * pmc values for a given node.
++ * NOTE: We use the per-cpu variable to improve cache performance.
++ *
++ * This routine will alternate loading the virtual counters for
++ * virtual CPUs
++ */
++static void cell_virtual_cntr(unsigned long data)
++{
++	int i, prev_hdw_thread, next_hdw_thread;
++	u32 cpu;
++	unsigned long flags;
++
++	/*
++	 * Make sure that the interrupt_hander and the virt counter are
++	 * not both playing with the counters on the same node.
++	 */
++
++	spin_lock_irqsave(&virt_cntr_lock, flags);
++
++	prev_hdw_thread = hdw_thread;
++
++	/* switch the cpu handling the interrupts */
++	hdw_thread = 1 ^ hdw_thread;
++	next_hdw_thread = hdw_thread;
++
++	pm_regs.group_control = 0;
++	pm_regs.debug_bus_control = 0;
++
++	for (i = 0; i < NUM_INPUT_BUS_WORDS; i++)
++		input_bus[i] = 0xff;
++
++	/*
++	 * There are some per thread events.  Must do the
++	 * set event, for the thread that is being started
++	 */
++	for (i = 0; i < num_counters; i++)
++		set_pm_event(i,
++			pmc_cntrl[next_hdw_thread][i].evnts,
++			pmc_cntrl[next_hdw_thread][i].masks);
++
++	/*
++	 * The following is done only once per each node, but
++	 * we need cpu #, not node #, to pass to the cbe_xxx functions.
++	 */
++	for_each_online_cpu(cpu) {
++		if (ps3_get_hw_thread_id(cpu))
++			continue;
++
++		/*
++		 * stop counters, save counter values, restore counts
++		 * for previous thread
++		 */
++		ps3_disable_pm(cpu);
++		ps3_disable_pm_interrupts(cpu);
++
++		/*
++		 * Add sample data at here.
++		 * Because PS3 hypervisor does not have
++		 * the performance monitor interrupt feature.
++		 */
++		add_sample(cpu);
++
++		/*
++		 * Switch to the other thread. Change the interrupt
++		 * and control regs to be scheduled on the CPU
++		 * corresponding to the thread to execute.
++		 */
++		for (i = 0; i < num_counters; i++) {
++			if (pmc_cntrl[next_hdw_thread][i].enabled) {
++				/*
++				 * There are some per thread events.
++				 * Must do the set event, enable_cntr
++				 * for each cpu.
++				 */
++				enable_ctr(cpu, i,
++					   pm_regs.pm07_cntrl);
++			} else {
++				ps3_write_pm07_control(cpu, i, 0);
++			}
++		}
++
++		/* Enable interrupts on the CPU thread that is starting */
++		ps3_enable_pm_interrupts(cpu, next_hdw_thread,
++					 virt_cntr_inter_mask);
++		ps3_enable_pm(cpu);
++	}
++
++	spin_unlock_irqrestore(&virt_cntr_lock, flags);
++
++	mod_timer(&timer_virt_cntr, jiffies + HZ / 10);
++}
++
++static void start_virt_cntrs(void)
++{
++	init_timer(&timer_virt_cntr);
++	timer_virt_cntr.function = cell_virtual_cntr;
++	timer_virt_cntr.data = 0UL;
++	timer_virt_cntr.expires = jiffies + HZ / 10;
++	add_timer(&timer_virt_cntr);
++}
++
++/* This function is called once for all cpus combined */
++static int cell_reg_setup(struct op_counter_config *ctr,
++			struct op_system_config *sys, int num_ctrs)
++{
++	int i, j, cpu;
++	int ret;
++
++	spu_cycle_reset = 0;
++
++	ret = ps3_lpm_open(PS3_LPM_TB_TYPE_NONE, NULL, 0);
++	if (ret) {
++		OP_ERR("lpm_open error. %d", ret);
++		return -EFAULT;
++	}
++
++	if (ctr[0].event == SPU_CYCLES_EVENT_NUM)
++		spu_cycle_reset = ctr[0].count;
++
++	num_counters = num_ctrs;
++
++	pm_regs.group_control = 0;
++	pm_regs.debug_bus_control = 0;
++
++	/* setup the pm_control register */
++	memset(&pm_regs.pm_cntrl, 0, sizeof(struct pm_cntrl));
++	pm_regs.pm_cntrl.stop_at_max = 1;
++	pm_regs.pm_cntrl.trace_mode = 0;
++	pm_regs.pm_cntrl.freeze = 1;
++
++	set_count_mode(sys->enable_kernel, sys->enable_user);
++
++	/* Setup the thread 0 events */
++	for (i = 0; i < num_ctrs; ++i) {
++
++		pmc_cntrl[0][i].evnts = ctr[i].event;
++		pmc_cntrl[0][i].masks = ctr[i].unit_mask;
++		pmc_cntrl[0][i].enabled = ctr[i].enabled;
++		pmc_cntrl[0][i].vcntr = i;
++
++		for_each_possible_cpu(j)
++			per_cpu(pmc_values, j)[i] = 0;
++	}
++
++	/*
++	 * Setup the thread 1 events, map the thread 0 event to the
++	 * equivalent thread 1 event.
++	 */
++	for (i = 0; i < num_ctrs; ++i) {
++		if ((ctr[i].event >= 2100) && (ctr[i].event <= 2111))
++			pmc_cntrl[1][i].evnts = ctr[i].event + 19;
++		else if (ctr[i].event == 2203)
++			pmc_cntrl[1][i].evnts = ctr[i].event;
++		else if ((ctr[i].event >= 2200) && (ctr[i].event <= 2215))
++			pmc_cntrl[1][i].evnts = ctr[i].event + 16;
++		else
++			pmc_cntrl[1][i].evnts = ctr[i].event;
++
++		pmc_cntrl[1][i].masks = ctr[i].unit_mask;
++		pmc_cntrl[1][i].enabled = ctr[i].enabled;
++		pmc_cntrl[1][i].vcntr = i;
++	}
++
++	for (i = 0; i < NUM_INPUT_BUS_WORDS; i++)
++		input_bus[i] = 0xff;
++
++	/*
++	 * Our counters count up, and "count" refers to
++	 * how much before the next interrupt, and we interrupt
++	 * on overflow.	 So we calculate the starting value
++	 * which will give us "count" until overflow.
++	 * Then we set the events on the enabled counters.
++	 */
++	for (i = 0; i < num_counters; ++i) {
++		/* start with virtual counter set 0 */
++		if (pmc_cntrl[0][i].enabled) {
++			reset_value[i] = 0;
++			count_value[i] = ctr[i].count;
++			set_pm_event(i,
++				     pmc_cntrl[0][i].evnts,
++				     pmc_cntrl[0][i].masks);
++
++			/* global, used by cell_cpu_setup */
++			ctr_enabled |= (1 << i);
++		}
++	}
++
++	/* initialize the previous counts for the virtual cntrs */
++	for_each_online_cpu(cpu)
++		for (i = 0; i < num_counters; ++i)
++			per_cpu(pmc_values, cpu)[i] = reset_value[i];
++
++	return 0;
++}
++
++
++
++/* This function is called once for each cpu */
++static int cell_cpu_setup(struct op_counter_config *cntr)
++{
++	u32 cpu = smp_processor_id();
++	u32 num_enabled = 0;
++	int i;
++
++	if (spu_cycle_reset)
++		return 0;
++
++	/* There is one performance monitor per processor chip (i.e. node),
++	 * so we only need to perform this function once per node.
++	 */
++	if (ps3_get_hw_thread_id(cpu))
++		return 0;
++
++	/* Stop all counters */
++	ps3_disable_pm(cpu);
++	ps3_disable_pm_interrupts(cpu);
++
++	ps3_write_pm(cpu, pm_interval, 0);
++	ps3_write_pm(cpu, pm_start_stop, 0);
++	ps3_write_pm(cpu, group_control, pm_regs.group_control);
++	ps3_write_pm(cpu, debug_bus_control, pm_regs.debug_bus_control);
++	write_pm_cntrl(cpu);
++
++	for (i = 0; i < num_counters; ++i) {
++		if (ctr_enabled & (1 << i)) {
++			pm_signal[num_enabled].cpu = ps3_cpu_to_node(cpu);
++			num_enabled++;
++		}
++	}
++
++	return pm_activate_signals(ps3_cpu_to_node(cpu), num_enabled);
++}
++
++
++static int cell_global_start_spu(struct op_counter_config *ctr)
++{
++	return -ENOSYS;
++}
++
++static int cell_global_start_ppu(struct op_counter_config *ctr)
++{
++	u32 cpu, i;
++	u32 interrupt_mask = 0;
++
++	/* This routine gets called once for the system.
++	 * There is one performance monitor per node, so we
++	 * only need to perform this function once per node.
++	 */
++	for_each_online_cpu(cpu) {
++		if (ps3_get_hw_thread_id(cpu))
++			continue;
++
++		interrupt_mask = 0;
++
++		for (i = 0; i < num_counters; ++i) {
++			if (ctr_enabled & (1 << i)) {
++				ps3_write_ctr(cpu, i, reset_value[i]);
++				enable_ctr(cpu, i, pm_regs.pm07_cntrl);
++				interrupt_mask |=
++				    CBE_PM_CTR_OVERFLOW_INTR(i);
++			} else {
++				/* Disable counter */
++				ps3_write_pm07_control(cpu, i, 0);
++			}
++		}
++
++		ps3_get_and_clear_pm_interrupts(cpu);
++		ps3_enable_pm_interrupts(cpu, hdw_thread, interrupt_mask);
++		ps3_enable_pm(cpu);
++	}
++
++	virt_cntr_inter_mask = interrupt_mask;
++	oprofile_running = 1;
++	/* complete the previous store */
++	smp_wmb();
++
++	/*
++	 * NOTE: start_virt_cntrs will result in cell_virtual_cntr() being
++	 * executed which manipulates the PMU.	We start the "virtual counter"
++	 * here so that we do not need to synchronize access to the PMU in
++	 * the above for-loop.
++	 */
++	start_virt_cntrs();
++
++	return 0;
++}
++
++static int cell_global_start(struct op_counter_config *ctr)
++{
++	if (spu_cycle_reset)
++		return cell_global_start_spu(ctr);
++	else
++		return cell_global_start_ppu(ctr);
++}
++
++static void cell_global_stop_spu(void)
++{
++}
++
++static void cell_global_stop_ppu(void)
++{
++	int cpu;
++
++	/*
++	 * This routine will be called once for the system.
++	 * There is one performance monitor per node, so we
++	 * only need to perform this function once per node.
++	 */
++	del_timer_sync(&timer_virt_cntr);
++	oprofile_running = 0;
++	/* complete the previous store */
++	smp_wmb();
++
++	for_each_online_cpu(cpu) {
++		if (ps3_get_hw_thread_id(cpu))
++			continue;
++
++		/* Stop the counters */
++		ps3_disable_pm(cpu);
++
++		/* Deactivate the signals */
++		ps3_set_signal(0, 0, 0, 0);	/*clear all */
++
++		/* Deactivate interrupts */
++		ps3_disable_pm_interrupts(cpu);
++	}
++}
++
++static void cell_global_stop(void)
++{
++	if (spu_cycle_reset)
++		cell_global_stop_spu();
++	else
++		cell_global_stop_ppu();
++}
++
++
++/*
++ * This function is called from the generic OProfile
++ * driver.  When profiling PPUs, we need to do the
++ * generic sync start; otherwise, do spu_sync_start.
++ */
++static int cell_sync_start(void)
++{
++	OP_ERR("PS3 oprofile support");
++	return DO_GENERIC_SYNC;
++}
++
++static int cell_sync_stop(void)
++{
++	int ret;
++
++	ret = ps3_lpm_close();
++	if (ret)
++		OP_ERR("lpm_close error. %d", ret);
++
++	return 1;
++}
++
++struct op_powerpc_model op_model_ps3 = {
++	.reg_setup = cell_reg_setup,
++	.cpu_setup = cell_cpu_setup,
++	.global_start = cell_global_start,
++	.global_stop = cell_global_stop,
++	.sync_start = cell_sync_start,
++	.sync_stop = cell_sync_stop,
++};
+--- a/arch/powerpc/platforms/ps3/Kconfig
++++ b/arch/powerpc/platforms/ps3/Kconfig
+@@ -127,6 +127,11 @@ config PS3_FLASH
+ 	  be disabled on the kernel command line using "ps3flash=off", to
+ 	  not allocate this fixed buffer.
+ 
++config OPROFILE_PS3
++	def_bool y
++	depends on PPC_PS3 && (OPROFILE = m || OPROFILE = y)
++	select PS3_LPM
++
+ config PS3_LPM
+ 	tristate "PS3 Logical Performance Monitor support"
+ 	depends on PPC_PS3
+--- a/include/asm-powerpc/oprofile_impl.h
++++ b/include/asm-powerpc/oprofile_impl.h
+@@ -59,6 +59,7 @@ extern struct op_powerpc_model op_model_
+ extern struct op_powerpc_model op_model_power4;
+ extern struct op_powerpc_model op_model_7450;
+ extern struct op_powerpc_model op_model_cell;
++extern struct op_powerpc_model op_model_ps3;
+ extern struct op_powerpc_model op_model_pa6t;
+ 
+ 
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-os-area-debug-only.diff linux-2.6.25-id/patches/ps3-wip/ps3-os-area-debug-only.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-os-area-debug-only.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-os-area-debug-only.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,17 @@
+for debug only
+
+
+---
+ arch/powerpc/platforms/ps3/os-area.c |    1 +
+ 1 file changed, 1 insertion(+)
+
+--- a/arch/powerpc/platforms/ps3/os-area.c
++++ b/arch/powerpc/platforms/ps3/os-area.c
+@@ -17,6 +17,7 @@
+  *  along with this program; if not, write to the Free Software
+  *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+  */
++#define DEBUG
+ 
+ #include <linux/kernel.h>
+ #include <linux/io.h>
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-quick-fix-boot-game-os.diff linux-2.6.25-id/patches/ps3-wip/ps3-quick-fix-boot-game-os.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-quick-fix-boot-game-os.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-quick-fix-boot-game-os.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,17 @@
+Quick fix to make boot-game-os work.
+
+---
+ drivers/ps3/ps3-sys-manager.c |    2 +-
+ 1 files changed, 1 insertion(+), 1 deletion(-)
+
+--- a/drivers/ps3/ps3-sys-manager.c
++++ b/drivers/ps3/ps3-sys-manager.c
+@@ -622,7 +622,7 @@ static void ps3_sys_manager_final_restar
+ 	ps3_vuart_cancel_async(dev);
+ 
+ 	ps3_sys_manager_send_attr(dev, 0);
+-	ps3_sys_manager_send_next_op(dev, PS3_SM_NEXT_OP_LPAR_REBOOT,
++	ps3_sys_manager_send_next_op(dev, PS3_SM_NEXT_OP_SYS_REBOOT,
+ 		PS3_SM_WAKE_DEFAULT);
+ 	ps3_sys_manager_send_request_shutdown(dev);
+ 
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-save-power-in-busy-loops-on-halt.diff linux-2.6.25-id/patches/ps3-wip/ps3-save-power-in-busy-loops-on-halt.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-save-power-in-busy-loops-on-halt.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-save-power-in-busy-loops-on-halt.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,168 @@
+Subject: ps3: Save power in busy loops on halt
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3: Save power on halt:
+  - Replace infinite busy loops by smarter loops calling lv1_pause() to save
+    power
+  - Add ps3_halt() and ps3_sys_manager_halt()
+  - Add __noreturn annotations
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ arch/powerpc/platforms/ps3/setup.c |   12 +++++++++++-
+ drivers/ps3/ps3-sys-manager.c      |   30 ++++++++++++++++++++----------
+ drivers/ps3/sys-manager-core.c     |   16 ++++++++++------
+ include/asm-powerpc/ps3.h          |    5 +++--
+ 4 files changed, 44 insertions(+), 19 deletions(-)
+
+--- a/arch/powerpc/platforms/ps3/setup.c
++++ b/arch/powerpc/platforms/ps3/setup.c
+@@ -95,6 +95,14 @@ static void ps3_power_off(void)
+ 	ps3_sys_manager_power_off(); /* never returns */
+ }
+ 
++static void ps3_halt(void)
++{
++	DBG("%s:%d\n", __func__, __LINE__);
++
++	smp_send_stop();
++	ps3_sys_manager_halt(); /* never returns */
++}
++
+ static void ps3_panic(char *str)
+ {
+ 	DBG("%s:%d %s\n", __func__, __LINE__, str);
+@@ -105,7 +113,8 @@ static void ps3_panic(char *str)
+ 	printk("   Please press POWER button.\n");
+ 	printk("\n");
+ 
+-	while(1);
++	while(1)
++		lv1_pause(1);
+ }
+ 
+ #if defined(CONFIG_FB_PS3) || defined(CONFIG_FB_PS3_MODULE) || \
+@@ -268,6 +277,7 @@ define_machine(ps3) {
+ 	.progress			= ps3_progress,
+ 	.restart			= ps3_restart,
+ 	.power_off			= ps3_power_off,
++	.halt				= ps3_halt,
+ #if defined(CONFIG_KEXEC)
+ 	.kexec_cpu_down			= ps3_kexec_cpu_down,
+ 	.machine_kexec			= default_machine_kexec,
+--- a/drivers/ps3/ps3-sys-manager.c
++++ b/drivers/ps3/ps3-sys-manager.c
+@@ -24,6 +24,7 @@
+ #include <linux/reboot.h>
+ 
+ #include <asm/firmware.h>
++#include <asm/lv1call.h>
+ #include <asm/ps3.h>
+ 
+ #include "vuart.h"
+@@ -566,6 +567,23 @@ fail_id:
+ 	return -EIO;
+ }
+ 
++static void ps3_sys_manager_fin(struct ps3_system_bus_device *dev)
++{
++	ps3_sys_manager_send_request_shutdown(dev);
++
++	pr_emerg("System Halted, OK to turn off power\n");
++
++	while (ps3_sys_manager_handle_msg(dev)) {
++		/* pause until next DEC interrupt */
++		lv1_pause(0);
++	}
++
++	while (1) {
++		/* pause, ignoring DEC interrupt */
++		lv1_pause(1);
++	}
++}
++
+ /**
+  * ps3_sys_manager_final_power_off - The final platform machine_power_off routine.
+  *
+@@ -587,12 +605,8 @@ static void ps3_sys_manager_final_power_
+ 
+ 	ps3_sys_manager_send_next_op(dev, PS3_SM_NEXT_OP_SYS_SHUTDOWN,
+ 		PS3_SM_WAKE_DEFAULT);
+-	ps3_sys_manager_send_request_shutdown(dev);
+-
+-	pr_emerg("System Halted, OK to turn off power\n");
+ 
+-	while (1)
+-		ps3_sys_manager_handle_msg(dev);
++	ps3_sys_manager_fin(dev);
+ }
+ 
+ /**
+@@ -624,12 +638,8 @@ static void ps3_sys_manager_final_restar
+ 	ps3_sys_manager_send_attr(dev, 0);
+ 	ps3_sys_manager_send_next_op(dev, PS3_SM_NEXT_OP_SYS_REBOOT,
+ 		PS3_SM_WAKE_DEFAULT);
+-	ps3_sys_manager_send_request_shutdown(dev);
+-
+-	pr_emerg("System Halted, OK to turn off power\n");
+ 
+-	while (1)
+-		ps3_sys_manager_handle_msg(dev);
++	ps3_sys_manager_fin(dev);
+ }
+ 
+ /**
+--- a/drivers/ps3/sys-manager-core.c
++++ b/drivers/ps3/sys-manager-core.c
+@@ -19,6 +19,7 @@
+  */
+ 
+ #include <linux/kernel.h>
++#include <asm/lv1call.h>
+ #include <asm/ps3.h>
+ 
+ /**
+@@ -50,10 +51,7 @@ void ps3_sys_manager_power_off(void)
+ 	if (ps3_sys_manager_ops.power_off)
+ 		ps3_sys_manager_ops.power_off(ps3_sys_manager_ops.dev);
+ 
+-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
+-	local_irq_disable();
+-	while (1)
+-		(void)0;
++	ps3_sys_manager_halt();
+ }
+ 
+ void ps3_sys_manager_restart(void)
+@@ -61,8 +59,14 @@ void ps3_sys_manager_restart(void)
+ 	if (ps3_sys_manager_ops.restart)
+ 		ps3_sys_manager_ops.restart(ps3_sys_manager_ops.dev);
+ 
+-	printk(KERN_EMERG "System Halted, OK to turn off power\n");
++	ps3_sys_manager_halt();
++}
++
++void ps3_sys_manager_halt(void)
++{
++	pr_emerg("System Halted, OK to turn off power\n");
+ 	local_irq_disable();
+ 	while (1)
+-		(void)0;
++		lv1_pause(1);
+ }
++
+--- a/include/asm-powerpc/ps3.h
++++ b/include/asm-powerpc/ps3.h
+@@ -425,8 +425,9 @@ struct ps3_sys_manager_ops {
+ };
+ 
+ void ps3_sys_manager_register_ops(const struct ps3_sys_manager_ops *ops);
+-void ps3_sys_manager_power_off(void);
+-void ps3_sys_manager_restart(void);
++void __noreturn ps3_sys_manager_power_off(void);
++void __noreturn ps3_sys_manager_restart(void);
++void __noreturn ps3_sys_manager_halt(void);
+ 
+ struct ps3_prealloc {
+     const char *name;
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-sys-man-cleanup.diff linux-2.6.25-id/patches/ps3-wip/ps3-sys-man-cleanup.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-sys-man-cleanup.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-sys-man-cleanup.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,53 @@
+Subject: PS3: Sys-manager code cleanup
+
+General code cleanups for PS3 system-manager:
+ o Move all MODULE_ macros to bottom.
+ o Correct PS3_SM_WAKE_P_O_R value.
+ o Enhance comment on wakeup source values.
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ drivers/ps3/ps3-sys-manager.c |   13 +++++++------
+ 1 file changed, 7 insertions(+), 6 deletions(-)
+
+--- a/drivers/ps3/ps3-sys-manager.c
++++ b/drivers/ps3/ps3-sys-manager.c
+@@ -29,10 +29,6 @@
+ 
+ #include "vuart.h"
+ 
+-MODULE_AUTHOR("Sony Corporation");
+-MODULE_LICENSE("GPL v2");
+-MODULE_DESCRIPTION("PS3 System Manager");
+-
+ /**
+  * ps3_sys_manager - PS3 system manager driver.
+  *
+@@ -182,7 +178,9 @@ enum ps3_sys_manager_next_op {
+  * @PS3_SM_WAKE_P_O_R: Power on reset.
+  *
+  * Additional wakeup sources when specifying PS3_SM_NEXT_OP_SYS_SHUTDOWN.
+- * System will always wake from the PS3_SM_WAKE_DEFAULT sources.
++ * The system will always wake from the PS3_SM_WAKE_DEFAULT sources.
++ * Sources listed here are the only ones available to guests in the
++ * other-os lpar.
+  */
+ 
+ enum ps3_sys_manager_wake_source {
+@@ -190,7 +188,7 @@ enum ps3_sys_manager_wake_source {
+ 	PS3_SM_WAKE_DEFAULT   = 0,
+ 	PS3_SM_WAKE_RTC       = 0x00000040,
+ 	PS3_SM_WAKE_RTC_ERROR = 0x00000080,
+-	PS3_SM_WAKE_P_O_R     = 0x10000000,
++	PS3_SM_WAKE_P_O_R     = 0x80000000,
+ };
+ 
+ /**
+@@ -709,4 +707,7 @@ static int __init ps3_sys_manager_init(v
+ module_init(ps3_sys_manager_init);
+ /* Module remove not supported. */
+ 
++MODULE_AUTHOR("Sony Corporation");
++MODULE_LICENSE("GPL v2");
++MODULE_DESCRIPTION("PS3 System Manager");
+ MODULE_ALIAS(PS3_MODULE_ALIAS_SYSTEM_MANAGER);
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-system-bus-status.diff linux-2.6.25-id/patches/ps3-wip/ps3-system-bus-status.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3-system-bus-status.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-system-bus-status.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,87 @@
+---
+ arch/powerpc/platforms/ps3/system-bus-rework.txt |   80 +++++++++++++++++++++++
+ 1 files changed, 80 insertions(+)
+
+--- /dev/null
++++ b/arch/powerpc/platforms/ps3/system-bus-rework.txt
+@@ -0,0 +1,80 @@
++* Status of the system-bus re-work
++
++o=working
++x=not working
++NA=no support planned
++				1st	shut	2nd	insmod	rmmod
++				boot	down	boot
++
++CONFIG_USB_EHCI_HCD		o	o	o	o	o
++CONFIG_USB_OHCI_HCD		o	o	o	o	o
++CONFIG_GELIC_NET		o	o	o	o	o
++CONFIG_FB_PS3			o	o	o	o	x
++CONFIG_SND_PS3			o	o	o	o	o
++CONFIG_PS3_PS3AV		o	o	o	o	o
++CONFIG_PS3_SYS_MANAGER		o	o	o	o	NA(1)
++CONFIG_PS3_VUART		o	o	o	o	o
++CONFIG_PS3_FLASH		o	o	o	o	o
++CONFIG_PS3_DISK			o	o	o	o	o
++CONFIG_PS3_ROM			o	o	o	o	o
++CONFIG_PS3_STORAGE		o	o	o	o	o
++CONFIG_SPU_FS			o	o	o	o	o
++
++-- commands --
++
++64 bit kexec	o
++32 bit kexec	x(2)
++reboot:  	o
++halt		o
++shutdown	o
++poweroff	o
++power button	o
++
++-- notes --
++
++(1) loaded as 'permanent'.
++(2) not working, WIP
++
++--------------------------------------------------------------------------------
++* Bus layout
++
++system-bus --+-- sb -------+-- usb0 --+-- ehci0
++             |             |          +-- ohci0
++             |             |
++             |             +-- usb1 --+-- ehci1
++             |             |          +-- ohci1
++             |             |
++             |             +-- gelic
++             |
++             +-- storage --+-- disk
++             |             +-- flash
++             |             +-- rom
++             |
++             +-- ioc0 -----+-- gfx
++             |             +-- sound
++             |
++             +-- vuart ----+-- av_settings
++                           +-- system_manager
++
++
++bus       | bus  | in   |
++name      | num  | repo |
++----------+------+------+---------------------------------------
++sb        | 04h  | yes  |
++storage   | 05h  | yes  |
++ioc0      | 81h  | no   |
++vuart     | 82h  | no   |
++----------+------+------+---------------------------------------
++
++device        | type   | irq  | dma | mmio |
++--------------+--------+------+-----+------+--------------------
++usb0            sb_04
++usb1            sb_04
++gelic           sb_03
++disk            st_00
++flash           st_0e
++rom             st_05
++gfx
++sound
++av_settings     vu_00
++system_manager  vu_02
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3-wip-defconfig-updates.patch linux-2.6.25-id/patches/ps3-wip/ps3-wip-defconfig-updates.patch
--- linux-2.6.25-org/patches/ps3-wip/ps3-wip-defconfig-updates.patch	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3-wip-defconfig-updates.patch	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,52 @@
+Adds defconfig values for the PS3 drivers.
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+---
+ arch/powerpc/configs/ps3_defconfig |   16 ++++++++++++++--
+ 1 file changed, 14 insertions(+), 2 deletions(-)
+
+--- a/arch/powerpc/configs/ps3_defconfig
++++ b/arch/powerpc/configs/ps3_defconfig
+@@ -1,7 +1,7 @@
+ #
+ # Automatically generated make config: don't edit
+-# Linux kernel version: 2.6.24-rc8
+-# Wed Jan 16 14:31:21 2008
++# Linux kernel version: 2.6.24-rc6
++# Fri Jan  4 15:30:55 2008
+ #
+ CONFIG_PPC64=y
+ 
+@@ -162,6 +162,7 @@ CONFIG_PS3_STORAGE=y
+ CONFIG_PS3_DISK=y
+ CONFIG_PS3_ROM=y
+ CONFIG_PS3_FLASH=y
++CONFIG_OPROFILE_PS3=y
+ CONFIG_PS3_LPM=m
+ CONFIG_PPC_CELL=y
+ # CONFIG_PPC_CELL_NATIVE is not set
+@@ -188,6 +189,16 @@ CONFIG_SPU_BASE=y
+ # CONFIG_FSL_ULI1575 is not set
+ 
+ #
++# Hardware Performance Monitoring support
++#
++CONFIG_PERFMON=y
++# CONFIG_PERFMON_DEBUG is not set
++# CONFIG_PERFMON_POWER4 is not set
++# CONFIG_PERFMON_POWER5 is not set
++# CONFIG_PERFMON_POWER6 is not set
++CONFIG_PERFMON_CELL=m
++
++#
+ # Kernel options
+ #
+ # CONFIG_TICK_ONESHOT is not set
+@@ -473,6 +484,7 @@ CONFIG_MII=m
+ # CONFIG_B44 is not set
+ CONFIG_NETDEV_1000=y
+ CONFIG_GELIC_NET=y
++CONFIG_GELIC_WIRELESS=y
+ # CONFIG_NETDEV_10000 is not set
+ 
+ #
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3av-mode-defines.diff linux-2.6.25-id/patches/ps3-wip/ps3av-mode-defines.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3av-mode-defines.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3av-mode-defines.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,218 @@
+Subject: ps3: Use symbolic names for video modes
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3: Use symbolic names for video modes
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/ps3/ps3av.c         |   39 ++++++++++++++++++++-------------------
+ drivers/video/ps3fb.c       |   17 ++++++++++-------
+ include/asm-powerpc/ps3av.h |   41 ++++++++++++++++++++++++++++++-----------
+ 3 files changed, 60 insertions(+), 37 deletions(-)
+
+--- a/drivers/ps3/ps3av.c
++++ b/drivers/ps3/ps3av.c
+@@ -542,7 +542,7 @@ static void ps3av_set_videomode_packet(u
+ 
+ static void ps3av_set_videomode_cont(u32 id, u32 old_id)
+ {
+-	static int vesa = 0;
++	static int vesa;
+ 	int res;
+ 
+ 	/* video signal off */
+@@ -552,9 +552,9 @@ static void ps3av_set_videomode_cont(u32
+ 	 * AV backend needs non-VESA mode setting at least one time
+ 	 * when VESA mode is used.
+ 	 */
+-	if (vesa == 0 && (id & PS3AV_MODE_MASK) >= 11) {
++	if (vesa == 0 && (id & PS3AV_MODE_MASK) >= PS3AV_MODE_WXGA) {
+ 		/* vesa mode */
+-		ps3av_set_videomode_packet(2);	/* 480P */
++		ps3av_set_videomode_packet(PS3AV_MODE_480P);
+ 	}
+ 	vesa = 1;
+ 
+@@ -594,20 +594,21 @@ static const struct {
+ 	unsigned mask : 19;
+ 	unsigned id :  4;
+ } ps3av_preferred_modes[] = {
+-	{ .mask = PS3AV_RESBIT_WUXGA		<< SHIFT_VESA,	.id = 13 },
+-	{ .mask = PS3AV_RESBIT_1920x1080P	<< SHIFT_60,	.id = 5 },
+-	{ .mask = PS3AV_RESBIT_1920x1080P	<< SHIFT_50,	.id = 10 },
+-	{ .mask = PS3AV_RESBIT_1920x1080I	<< SHIFT_60,	.id = 4 },
+-	{ .mask = PS3AV_RESBIT_1920x1080I	<< SHIFT_50,	.id = 9 },
+-	{ .mask = PS3AV_RESBIT_SXGA		<< SHIFT_VESA,	.id = 12 },
+-	{ .mask = PS3AV_RESBIT_WXGA		<< SHIFT_VESA,	.id = 11 },
+-	{ .mask = PS3AV_RESBIT_1280x720P	<< SHIFT_60,	.id = 3 },
+-	{ .mask = PS3AV_RESBIT_1280x720P	<< SHIFT_50,	.id = 8 },
+-	{ .mask = PS3AV_RESBIT_720x480P		<< SHIFT_60,	.id = 2 },
+-	{ .mask = PS3AV_RESBIT_720x576P		<< SHIFT_50,	.id = 7 },
++	{ PS3AV_RESBIT_WUXGA      << SHIFT_VESA, PS3AV_MODE_WUXGA   },
++	{ PS3AV_RESBIT_1920x1080P << SHIFT_60,   PS3AV_MODE_1080P60 },
++	{ PS3AV_RESBIT_1920x1080P << SHIFT_50,   PS3AV_MODE_1080P50 },
++	{ PS3AV_RESBIT_1920x1080I << SHIFT_60,   PS3AV_MODE_1080I60 },
++	{ PS3AV_RESBIT_1920x1080I << SHIFT_50,   PS3AV_MODE_1080I50 },
++	{ PS3AV_RESBIT_SXGA       << SHIFT_VESA, PS3AV_MODE_SXGA    },
++	{ PS3AV_RESBIT_WXGA       << SHIFT_VESA, PS3AV_MODE_WXGA    },
++	{ PS3AV_RESBIT_1280x720P  << SHIFT_60,   PS3AV_MODE_720P60  },
++	{ PS3AV_RESBIT_1280x720P  << SHIFT_50,   PS3AV_MODE_720P50  },
++	{ PS3AV_RESBIT_720x480P   << SHIFT_60,   PS3AV_MODE_480P    },
++	{ PS3AV_RESBIT_720x576P   << SHIFT_50,   PS3AV_MODE_576P    },
+ };
+ 
+-static int ps3av_resbit2id(u32 res_50, u32 res_60, u32 res_vesa)
++static enum ps3av_mode_num ps3av_resbit2id(u32 res_50, u32 res_60,
++					   u32 res_vesa)
+ {
+ 	unsigned int i;
+ 	u32 res_all;
+@@ -636,9 +637,9 @@ static int ps3av_resbit2id(u32 res_50, u
+ 	return 0;
+ }
+ 
+-static int ps3av_hdmi_get_id(struct ps3av_info_monitor *info)
++static enum ps3av_mode_num ps3av_hdmi_get_id(struct ps3av_info_monitor *info)
+ {
+-	int id;
++	enum ps3av_mode_num id;
+ 
+ 	if (safe_mode)
+ 		return PS3AV_DEFAULT_HDMI_MODE_ID_REG_60;
+@@ -852,7 +853,7 @@ int ps3av_set_video_mode(u32 id)
+ 
+ 	/* auto mode */
+ 	option = id & ~PS3AV_MODE_MASK;
+-	if ((id & PS3AV_MODE_MASK) == 0) {
++	if ((id & PS3AV_MODE_MASK) == PS3AV_MODE_AUTO) {
+ 		id = ps3av_auto_videomode(&ps3av->av_hw_conf);
+ 		if (id < 1) {
+ 			printk(KERN_ERR "%s: invalid id :%d\n", __func__, id);
+@@ -958,7 +959,7 @@ static int ps3av_probe(struct ps3_system
+ 		return -ENOMEM;
+ 
+ 	mutex_init(&ps3av->mutex);
+-	ps3av->ps3av_mode = 0;
++	ps3av->ps3av_mode = PS3AV_MODE_AUTO;
+ 	ps3av->dev = dev;
+ 
+ 	INIT_WORK(&ps3av->work, ps3avd);
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -338,7 +338,7 @@ static int ps3fb_get_res_table(u32 xres,
+ static unsigned int ps3fb_find_mode(const struct fb_var_screeninfo *var,
+ 				    u32 *ddr_line_length, u32 *xdr_line_length)
+ {
+-	unsigned int i, mode;
++	unsigned int i, fi, mode;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(ps3fb_modedb); i++)
+ 		if (var->xres == ps3fb_modedb[i].xres &&
+@@ -359,7 +359,8 @@ static unsigned int ps3fb_find_mode(cons
+ 
+ found:
+ 	/* Cropped broadcast modes use the full line length */
+-	*ddr_line_length = ps3fb_modedb[i < 10 ? i + 13 : i].xres * BPP;
++	fi = i < PS3AV_MODE_1080P50 ? i + PS3AV_MODE_WUXGA : i;
++	*ddr_line_length = ps3fb_modedb[fi].xres * BPP;
+ 
+ 	if (ps3_compare_firmware_version(1, 9, 0) >= 0) {
+ 		*xdr_line_length = GPU_ALIGN_UP(max(var->xres,
+@@ -370,7 +371,9 @@ found:
+ 		*xdr_line_length = *ddr_line_length;
+ 
+ 	/* Full broadcast modes have the full mode bit set */
+-	mode = i > 12 ? (i - 12) | PS3FB_FULL_MODE_BIT : i + 1;
++	mode = i+1;
++	if (mode > PS3AV_MODE_WUXGA)
++		mode = (mode - PS3AV_MODE_WUXGA) | PS3FB_FULL_MODE_BIT;
+ 
+ 	pr_debug("ps3fb_find_mode: mode %u\n", mode);
+ 
+@@ -382,14 +385,14 @@ static const struct fb_videomode *ps3fb_
+ 	u32 mode = id & PS3AV_MODE_MASK;
+ 	u32 flags;
+ 
+-	if (mode < 1 || mode > 13)
++	if (mode < PS3AV_MODE_480I || mode > PS3AV_MODE_WUXGA)
+ 		return NULL;
+ 
+ 	flags = id & ~PS3AV_MODE_MASK;
+ 
+-	if (mode <= 10 && flags & PS3FB_FULL_MODE_BIT) {
++	if (mode <= PS3AV_MODE_1080P50 && flags & PS3FB_FULL_MODE_BIT) {
+ 		/* Full broadcast mode */
+-		return &ps3fb_modedb[mode + 12];
++		return &ps3fb_modedb[mode + PS3AV_MODE_WUXGA - 1];
+ 	}
+ 
+ 	return &ps3fb_modedb[mode - 1];
+@@ -1076,7 +1079,7 @@ static int __devinit ps3fb_probe(struct 
+ 
+ 	if (!ps3fb_mode)
+ 		ps3fb_mode = ps3av_get_mode();
+-	dev_dbg(&dev->core, "ps3av_mode:%d\n", ps3fb_mode);
++	dev_dbg(&dev->core, "ps3fb_mode: %d\n", ps3fb_mode);
+ 
+ 	if (ps3fb_mode > 0 &&
+ 	    !ps3av_video_mode2res(ps3fb_mode, &xres, &yres)) {
+--- a/include/asm-powerpc/ps3av.h
++++ b/include/asm-powerpc/ps3av.h
+@@ -310,19 +310,25 @@
+ #define PS3AV_MONITOR_TYPE_HDMI			1	/* HDMI */
+ #define PS3AV_MONITOR_TYPE_DVI			2	/* DVI */
+ 
+-#define PS3AV_DEFAULT_HDMI_MODE_ID_REG_60	2	/* 480p */
+-#define PS3AV_DEFAULT_AVMULTI_MODE_ID_REG_60	1	/* 480i */
+-#define PS3AV_DEFAULT_HDMI_MODE_ID_REG_50	7	/* 576p */
+-#define PS3AV_DEFAULT_AVMULTI_MODE_ID_REG_50	6	/* 576i */
+-
+-#define PS3AV_REGION_60				0x01
+-#define PS3AV_REGION_50				0x02
+-#define PS3AV_REGION_RGB			0x10
+-
+-#define get_status(buf)				(((__u32 *)buf)[2])
+-#define PS3AV_HDR_SIZE				4	/* version + size */
+ 
+ /* for video mode */
++enum ps3av_mode_num {
++	PS3AV_MODE_AUTO				= 0,
++	PS3AV_MODE_480I				= 1,
++	PS3AV_MODE_480P				= 2,
++	PS3AV_MODE_720P60			= 3,
++	PS3AV_MODE_1080I60			= 4,
++	PS3AV_MODE_1080P60			= 5,
++	PS3AV_MODE_576I				= 6,
++	PS3AV_MODE_576P				= 7,
++	PS3AV_MODE_720P50			= 8,
++	PS3AV_MODE_1080I50			= 9,
++	PS3AV_MODE_1080P50			= 10,
++	PS3AV_MODE_WXGA				= 11,
++	PS3AV_MODE_SXGA				= 12,
++	PS3AV_MODE_WUXGA			= 13,
++};
++
+ #define PS3AV_MODE_MASK				0x000F
+ #define PS3AV_MODE_HDCP_OFF			0x1000	/* Retail PS3 product doesn't support this */
+ #define PS3AV_MODE_DITHER			0x0800
+@@ -333,6 +339,19 @@
+ #define PS3AV_MODE_RGB				0x0020
+ 
+ 
++#define PS3AV_DEFAULT_HDMI_MODE_ID_REG_60	PS3AV_MODE_480P
++#define PS3AV_DEFAULT_AVMULTI_MODE_ID_REG_60	PS3AV_MODE_480I
++#define PS3AV_DEFAULT_HDMI_MODE_ID_REG_50	PS3AV_MODE_576P
++#define PS3AV_DEFAULT_AVMULTI_MODE_ID_REG_50	PS3AV_MODE_576I
++
++#define PS3AV_REGION_60				0x01
++#define PS3AV_REGION_50				0x02
++#define PS3AV_REGION_RGB			0x10
++
++#define get_status(buf)				(((__u32 *)buf)[2])
++#define PS3AV_HDR_SIZE				4	/* version + size */
++
++
+ /** command packet structure **/
+ struct ps3av_send_hdr {
+ 	u16 version;
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3av-remove-unused.diff linux-2.6.25-id/patches/ps3-wip/ps3av-remove-unused.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3av-remove-unused.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3av-remove-unused.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,100 @@
+Subject: ps3av: ps3av_get_scanmode() and ps3av_get_refresh_rate() are unused
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3av: ps3av_get_scanmode() and ps3av_get_refresh_rate() are unused, so remove
+them
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/ps3/ps3av.c         |   58 +++++++++-----------------------------------
+ include/asm-powerpc/ps3av.h |    2 -
+ 2 files changed, 13 insertions(+), 47 deletions(-)
+
+--- a/drivers/ps3/ps3av.c
++++ b/drivers/ps3/ps3av.c
+@@ -78,23 +78,21 @@ static const struct avset_video_mode {
+ 	u32 aspect;
+ 	u32 x;
+ 	u32 y;
+-	u32 interlace;
+-	u32 freq;
+ } video_mode_table[] = {
+ 	{     0, }, /* auto */
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_480I,       A_N,  720,  480, 1, 60},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_480P,       A_N,  720,  480, 0, 60},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_720P_60HZ,  A_N, 1280,  720, 0, 60},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_1080I_60HZ, A_W, 1920, 1080, 1, 60},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_1080P_60HZ, A_W, 1920, 1080, 0, 60},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_576I,       A_N,  720,  576, 1, 50},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_576P,       A_N,  720,  576, 0, 50},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_720P_50HZ,  A_N, 1280,  720, 0, 50},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_1080I_50HZ, A_W, 1920, 1080, 1, 50},
+-	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_1080P_50HZ, A_W, 1920, 1080, 0, 50},
+-	{  RGB8, XRGB, PS3AV_CMD_VIDEO_VID_WXGA,       A_W, 1280,  768, 0, 60},
+-	{  RGB8, XRGB, PS3AV_CMD_VIDEO_VID_SXGA,       A_N, 1280, 1024, 0, 60},
+-	{  RGB8, XRGB, PS3AV_CMD_VIDEO_VID_WUXGA,      A_W, 1920, 1200, 0, 60},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_480I,       A_N,  720,  480},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_480P,       A_N,  720,  480},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_720P_60HZ,  A_N, 1280,  720},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_1080I_60HZ, A_W, 1920, 1080},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_1080P_60HZ, A_W, 1920, 1080},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_576I,       A_N,  720,  576},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_576P,       A_N,  720,  576},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_720P_50HZ,  A_N, 1280,  720},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_1080I_50HZ, A_W, 1920, 1080},
++	{YUV444, XRGB, PS3AV_CMD_VIDEO_VID_1080P_50HZ, A_W, 1920, 1080},
++	{  RGB8, XRGB, PS3AV_CMD_VIDEO_VID_WXGA,       A_W, 1280,  768},
++	{  RGB8, XRGB, PS3AV_CMD_VIDEO_VID_SXGA,       A_N, 1280, 1024},
++	{  RGB8, XRGB, PS3AV_CMD_VIDEO_VID_WUXGA,      A_W, 1920, 1200},
+ };
+ 
+ /* supported CIDs */
+@@ -889,36 +887,6 @@ int ps3av_get_mode(void)
+ 
+ EXPORT_SYMBOL_GPL(ps3av_get_mode);
+ 
+-int ps3av_get_scanmode(int id)
+-{
+-	int size;
+-
+-	id = id & PS3AV_MODE_MASK;
+-	size = ARRAY_SIZE(video_mode_table);
+-	if (id > size - 1 || id < 0) {
+-		printk(KERN_ERR "%s: invalid mode %d\n", __func__, id);
+-		return -EINVAL;
+-	}
+-	return video_mode_table[id].interlace;
+-}
+-
+-EXPORT_SYMBOL_GPL(ps3av_get_scanmode);
+-
+-int ps3av_get_refresh_rate(int id)
+-{
+-	int size;
+-
+-	id = id & PS3AV_MODE_MASK;
+-	size = ARRAY_SIZE(video_mode_table);
+-	if (id > size - 1 || id < 0) {
+-		printk(KERN_ERR "%s: invalid mode %d\n", __func__, id);
+-		return -EINVAL;
+-	}
+-	return video_mode_table[id].freq;
+-}
+-
+-EXPORT_SYMBOL_GPL(ps3av_get_refresh_rate);
+-
+ /* get resolution by video_mode */
+ int ps3av_video_mode2res(u32 id, u32 *xres, u32 *yres)
+ {
+--- a/include/asm-powerpc/ps3av.h
++++ b/include/asm-powerpc/ps3av.h
+@@ -713,8 +713,6 @@ extern int ps3av_set_video_mode(u32);
+ extern int ps3av_set_audio_mode(u32, u32, u32, u32, u32);
+ extern int ps3av_get_auto_mode(void);
+ extern int ps3av_get_mode(void);
+-extern int ps3av_get_scanmode(int);
+-extern int ps3av_get_refresh_rate(int);
+ extern int ps3av_video_mode2res(u32, u32 *, u32 *);
+ extern int ps3av_video_mute(int);
+ extern int ps3av_audio_mute(int);
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-cleanup.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-cleanup.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-cleanup.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-cleanup.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,184 @@
+Subject: ps3fb: cleanup sweep
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3fb: cleanup sweep:
+  - Kill xdr_ea and xdr_size, use info->screen_base and info->fix.smem_len
+    instead.
+  - Kill superfluous assignments to info->fix.smem_start, info->fix.smem_len,
+    and info->screen_base in ps3fb_set_par(). Their values never change.
+  - Add sparse annotations to casts to kill address space warnings
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/video/ps3fb.c |   64 +++++++++++++++++++++-----------------------------
+ 1 files changed, 27 insertions(+), 37 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -116,8 +116,6 @@ struct ps3fb_priv {
+ 	unsigned int irq_no;
+ 
+ 	u64 context_handle, memory_handle;
+-	void *xdr_ea;
+-	size_t xdr_size;
+ 	struct gpu_driver_info *dinfo;
+ 
+ 	u64 vblank_count;	/* frame count */
+@@ -598,7 +596,7 @@ static int ps3fb_check_var(struct fb_var
+ 	}
+ 
+ 	/* Memory limit */
+-	if (var->yres_virtual * xdr_line_length > ps3fb.xdr_size) {
++	if (var->yres_virtual * xdr_line_length > info->fix.smem_len) {
+ 		dev_dbg(info->device, "Not enough memory\n");
+ 		return -ENOMEM;
+ 	}
+@@ -627,19 +625,15 @@ static int ps3fb_set_par(struct fb_info 
+ 
+ 	vmode = ps3fb_native_vmode(mode & PS3AV_MODE_MASK);
+ 
+-	info->fix.smem_start = virt_to_abs(ps3fb.xdr_ea);
+-	info->fix.smem_len = ps3fb.xdr_size;
+ 	info->fix.xpanstep = info->var.xres_virtual > info->var.xres ? 1 : 0;
+ 	info->fix.ypanstep = info->var.yres_virtual > info->var.yres ? 1 : 0;
+ 	info->fix.line_length = xdr_line_length;
+ 
+-	info->screen_base = (char __iomem *)ps3fb.xdr_ea;
+-
+ 	par->ddr_line_length = ddr_line_length;
+ 	par->ddr_frame_size = vmode->yres * ddr_line_length;
+ 	par->xdr_frame_size = info->var.yres_virtual * xdr_line_length;
+ 
+-	par->num_frames = ps3fb.xdr_size /
++	par->num_frames = info->fix.smem_len /
+ 			  max(par->ddr_frame_size, par->xdr_frame_size);
+ 
+ 	/* Keep the special bits we cannot set using fb_var_screeninfo */
+@@ -667,13 +661,13 @@ static int ps3fb_set_par(struct fb_info 
+ 	}
+ 
+ 	/* Clear XDR frame buffer memory */
+-	memset(ps3fb.xdr_ea, 0, ps3fb.xdr_size);
++	memset((void __force *)info->screen_base, 0, info->fix.smem_len);
+ 
+ 	/* Clear DDR frame buffer memory */
+ 	lines = vmode->yres * par->num_frames;
+ 	if (par->full_offset)
+ 		lines++;
+-	maxlines = ps3fb.xdr_size / ddr_line_length;
++	maxlines = info->fix.smem_len / ddr_line_length;
+ 	for (dst = 0; lines; dst += maxlines * ddr_line_length) {
+ 		unsigned int l = min(lines, maxlines);
+ 		ps3fb_sync_image(info->device, 0, dst, 0, vmode->xres, l,
+@@ -1013,10 +1007,9 @@ static int ps3fb_xdr_settings(u64 xdr_lp
+ 			__func__, status);
+ 		return -ENXIO;
+ 	}
+-	dev_dbg(dev,
+-		"video:%p xdr_ea:%p ioif:%lx lpar:%lx phys:%lx size:%lx\n",
+-		ps3fb_videomemory.address, ps3fb.xdr_ea, GPU_IOIF, xdr_lpar,
+-		virt_to_abs(ps3fb.xdr_ea), ps3fb_videomemory.size);
++	dev_dbg(dev, "video:%p ioif:%lx lpar:%lx size:%lx\n",
++		ps3fb_videomemory.address, GPU_IOIF, xdr_lpar,
++		ps3fb_videomemory.size);
+ 
+ 	status = lv1_gpu_context_attribute(ps3fb.context_handle,
+ 					   L1GPU_CONTEXT_ATTRIBUTE_FB_SETUP,
+@@ -1099,6 +1092,7 @@ static int __devinit ps3fb_probe(struct 
+ 	u64 lpar_reports = 0;
+ 	u64 lpar_reports_size = 0;
+ 	u64 xdr_lpar;
++	void *fb_start;
+ 	int status;
+ 	struct task_struct *task;
+ 	unsigned long max_ps3fb_size;
+@@ -1154,7 +1148,7 @@ static int __devinit ps3fb_probe(struct 
+ 	}
+ 
+ 	/* vsync interrupt */
+-	ps3fb.dinfo = ioremap(lpar_driver_info, 128 * 1024);
++	ps3fb.dinfo = (void __force *)ioremap(lpar_driver_info, 128 * 1024);
+ 	if (!ps3fb.dinfo) {
+ 		dev_err(&dev->core, "%s: ioremap failed\n", __func__);
+ 		goto err_gpu_context_free;
+@@ -1164,22 +1158,10 @@ static int __devinit ps3fb_probe(struct 
+ 	if (retval)
+ 		goto err_iounmap_dinfo;
+ 
+-	/* XDR frame buffer */
+-	ps3fb.xdr_ea = ps3fb_videomemory.address;
+-	xdr_lpar = ps3_mm_phys_to_lpar(__pa(ps3fb.xdr_ea));
+-
+ 	/* Clear memory to prevent kernel info leakage into userspace */
+-	memset(ps3fb.xdr_ea, 0, ps3fb_videomemory.size);
+-
+-	/*
+-	 * The GPU command buffer is at the start of video memory
+-	 * As we don't use the full command buffer, we can put the actual
+-	 * frame buffer at offset GPU_FB_START and save some precious XDR
+-	 * memory
+-	 */
+-	ps3fb.xdr_ea += GPU_FB_START;
+-	ps3fb.xdr_size = ps3fb_videomemory.size - GPU_FB_START;
++	memset(ps3fb_videomemory.address, 0, ps3fb_videomemory.size);
+ 
++	xdr_lpar = ps3_mm_phys_to_lpar(__pa(ps3fb_videomemory.address));
+ 	retval = ps3fb_xdr_settings(xdr_lpar, &dev->core);
+ 	if (retval)
+ 		goto err_free_irq;
+@@ -1193,12 +1175,20 @@ static int __devinit ps3fb_probe(struct 
+ 	par->new_mode_id = ps3fb_mode;
+ 	par->num_frames = 1;
+ 
+-	info->screen_base = (char __iomem *)ps3fb.xdr_ea;
+ 	info->fbops = &ps3fb_ops;
+-
+ 	info->fix = ps3fb_fix;
+-	info->fix.smem_start = virt_to_abs(ps3fb.xdr_ea);
+-	info->fix.smem_len = ps3fb.xdr_size;
++
++	/*
++	 * The GPU command buffer is at the start of video memory
++	 * As we don't use the full command buffer, we can put the actual
++	 * frame buffer at offset GPU_FB_START and save some precious XDR
++	 * memory
++	 */
++	fb_start = ps3fb_videomemory.address + GPU_FB_START;
++	info->screen_base = (char __force __iomem *)fb_start;
++	info->fix.smem_start = virt_to_abs(fb_start);
++	info->fix.smem_len = ps3fb_videomemory.size - GPU_FB_START;
++
+ 	info->pseudo_palette = par->pseudo_palette;
+ 	info->flags = FBINFO_DEFAULT | FBINFO_READS_FAST |
+ 		      FBINFO_HWACCEL_XPAN | FBINFO_HWACCEL_YPAN;
+@@ -1223,9 +1213,9 @@ static int __devinit ps3fb_probe(struct 
+ 
+ 	dev->core.driver_data = info;
+ 
+-	dev_info(info->device, "%s %s, using %lu KiB of video memory\n",
++	dev_info(info->device, "%s %s, using %u KiB of video memory\n",
+ 		 dev_driver_string(info->dev), info->dev->bus_id,
+-		 ps3fb.xdr_size >> 10);
++		 info->fix.smem_len >> 10);
+ 
+ 	task = kthread_run(ps3fbd, info, DEVICE_NAME);
+ 	if (IS_ERR(task)) {
+@@ -1248,7 +1238,7 @@ err_free_irq:
+ 	free_irq(ps3fb.irq_no, &dev->core);
+ 	ps3_irq_plug_destroy(ps3fb.irq_no);
+ err_iounmap_dinfo:
+-	iounmap((u8 __iomem *)ps3fb.dinfo);
++	iounmap((u8 __force __iomem *)ps3fb.dinfo);
+ err_gpu_context_free:
+ 	lv1_gpu_context_free(ps3fb.context_handle);
+ err_gpu_memory_free:
+@@ -1283,7 +1273,7 @@ static int ps3fb_shutdown(struct ps3_sys
+ 		free_irq(ps3fb.irq_no, &dev->core);
+ 		ps3_irq_plug_destroy(ps3fb.irq_no);
+ 	}
+-	iounmap((u8 __iomem *)ps3fb.dinfo);
++	iounmap((u8 __force __iomem *)ps3fb.dinfo);
+ 
+ 	status = lv1_gpu_context_free(ps3fb.context_handle);
+ 	if (status)
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-configurable-black-borders.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-configurable-black-borders.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-configurable-black-borders.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-configurable-black-borders.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,107 @@
+Subject: ps3fb: Configurable black borders
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3fb: Allow all video modes where the visible resolution plus the black
+borders matches a native resolution
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+--
+ drivers/video/ps3fb.c |   69 +++++++++++++++++++++++++++++++++++---------------
+ 1 files changed, 49 insertions(+), 20 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -270,32 +270,57 @@ module_param(ps3fb_mode, int, 0);
+ 
+ static char *mode_option __devinitdata;
+ 
+-static unsigned int ps3fb_find_mode(const struct fb_var_screeninfo *var,
++static int ps3fb_cmp_mode(const struct fb_videomode *vmode,
++			  const struct fb_var_screeninfo *var)
++{
++	/* resolution + black border must match a native resolution */
++	if (vmode->left_margin + vmode->xres + vmode->right_margin !=
++	    var->left_margin + var->xres + var->right_margin ||
++	    vmode->upper_margin + vmode->yres + vmode->lower_margin !=
++	    var->upper_margin + var->yres + var->lower_margin)
++		return -1;
++
++	/* minimum limits for margins */
++	if (vmode->left_margin > var->left_margin ||
++	    vmode->right_margin > var->right_margin ||
++	    vmode->upper_margin > var->upper_margin ||
++	    vmode->lower_margin > var->lower_margin)
++		return -1;
++
++	/* these fields must match exactly */
++	if (vmode->pixclock != var->pixclock ||
++	    vmode->hsync_len != var->hsync_len ||
++	    vmode->vsync_len != var->vsync_len ||
++	    vmode->sync != var->sync ||
++	    vmode->vmode != (var->vmode & FB_VMODE_MASK))
++		return -1;
++
++	return 0;
++}
++
++static unsigned int ps3fb_find_mode(struct fb_var_screeninfo *var,
+ 				    u32 *ddr_line_length, u32 *xdr_line_length)
+ {
+-	unsigned int i, fi, mode;
++	unsigned int i, mode;
+ 
+-	for (i = 0; i < ARRAY_SIZE(ps3fb_modedb); i++)
+-		if (var->xres == ps3fb_modedb[i].xres &&
+-		    var->yres == ps3fb_modedb[i].yres &&
+-		    var->pixclock == ps3fb_modedb[i].pixclock &&
+-		    var->hsync_len == ps3fb_modedb[i].hsync_len &&
+-		    var->vsync_len == ps3fb_modedb[i].vsync_len &&
+-		    var->left_margin == ps3fb_modedb[i].left_margin &&
+-		    var->right_margin == ps3fb_modedb[i].right_margin &&
+-		    var->upper_margin == ps3fb_modedb[i].upper_margin &&
+-		    var->lower_margin == ps3fb_modedb[i].lower_margin &&
+-		    var->sync == ps3fb_modedb[i].sync &&
+-		    (var->vmode & FB_VMODE_MASK) == ps3fb_modedb[i].vmode)
++	for (i = PS3AV_MODE_1080P50; i < ARRAY_SIZE(ps3fb_modedb); i++)
++		if (!ps3fb_cmp_mode(&ps3fb_modedb[i], var))
+ 			goto found;
+ 
+ 	pr_debug("ps3fb_find_mode: mode not found\n");
+ 	return 0;
+ 
+ found:
+-	/* Cropped broadcast modes use the full line length */
+-	fi = i < PS3AV_MODE_1080P50 ? i + PS3AV_MODE_WUXGA : i;
+-	*ddr_line_length = ps3fb_modedb[fi].xres * BPP;
++	*ddr_line_length = ps3fb_modedb[i].xres * BPP;
++
++	if (!var->xres) {
++		var->xres = 1;
++		var->right_margin--;
++	}
++	if (!var->yres) {
++		var->yres = 1;
++		var->lower_margin--;
++	}
+ 
+ 	if (ps3_compare_firmware_version(1, 9, 0) >= 0) {
+ 		*xdr_line_length = GPU_ALIGN_UP(max(var->xres,
+@@ -305,10 +330,14 @@ found:
+ 	} else
+ 		*xdr_line_length = *ddr_line_length;
+ 
+-	/* Full broadcast modes have the full mode bit set */
+ 	mode = i+1;
+-	if (mode > PS3AV_MODE_WUXGA)
+-		mode = (mode - PS3AV_MODE_WUXGA) | PS3AV_MODE_FULL;
++	if (mode > PS3AV_MODE_WUXGA) {
++		mode -= PS3AV_MODE_WUXGA;
++		/* Full broadcast modes have the full mode bit set */
++		if (ps3fb_modedb[i].xres == var->xres &&
++		    ps3fb_modedb[i].yres == var->yres)
++			mode |= PS3AV_MODE_FULL;
++	}
+ 
+ 	pr_debug("ps3fb_find_mode: mode %u\n", mode);
+ 
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-inline-macros.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-inline-macros.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-inline-macros.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-inline-macros.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,42 @@
+Subject: ps3fb: Inline macros that are used only once
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3fb: inline the X_OFF(), Y_OFF(), WIDTH(), HEIGHT(), and VP_OFF() macros,
+as they're used in one place only
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/video/ps3fb.c |   12 ++++--------
+ 1 files changed, 4 insertions(+), 8 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -287,15 +287,8 @@ static const struct fb_videomode ps3fb_m
+ #define HEAD_A
+ #define HEAD_B
+ 
+-#define X_OFF(i)	(ps3fb_res[i].xoff)	/* left/right margin (pixel) */
+-#define Y_OFF(i)	(ps3fb_res[i].yoff)	/* top/bottom margin (pixel) */
+-#define WIDTH(i)	(ps3fb_res[i].xres)	/* width of FB */
+-#define HEIGHT(i)	(ps3fb_res[i].yres)	/* height of FB */
+ #define BPP		4			/* number of bytes per pixel */
+ 
+-/* Start of the virtual frame buffer (relative to fullscreen ) */
+-#define VP_OFF(i)	((WIDTH(i) * Y_OFF(i) + X_OFF(i)) * BPP)
+-
+ 
+ static int ps3fb_mode;
+ module_param(ps3fb_mode, int, 0);
+@@ -611,7 +604,10 @@ static int ps3fb_set_par(struct fb_info 
+ 
+ 	par->width = info->var.xres;
+ 	par->height = info->var.yres;
+-	offset = VP_OFF(i);
++
++	/* Start of the virtual frame buffer (relative to fullscreen) */
++	offset = ps3fb_res[i].yoff * ddr_line_length + ps3fb_res[i].xoff * BPP;
++
+ 	par->fb_offset = GPU_ALIGN_UP(offset);
+ 	par->full_offset = par->fb_offset - offset;
+ 	par->pan_offset = info->var.yoffset * xdr_line_length +
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-kill-PS3FB_FULL_MODE_BIT.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-kill-PS3FB_FULL_MODE_BIT.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-kill-PS3FB_FULL_MODE_BIT.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-kill-PS3FB_FULL_MODE_BIT.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,49 @@
+Subject: ps3fb: Kill PS3FB_FULL_MODE_BIT
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3fb: Kill PS3FB_FULL_MODE_BIT, use PS3AV_MODE_FULL instead
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/video/ps3fb.c |    8 +++-----
+ 1 files changed, 3 insertions(+), 5 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -57,8 +57,6 @@
+ #define GPU_ALIGN_UP(x)				_ALIGN_UP((x), 64)
+ #define GPU_MAX_LINE_LENGTH			(65536 - 64)
+ 
+-#define PS3FB_FULL_MODE_BIT			0x80
+-
+ #define GPU_INTR_STATUS_VSYNC_0			0	/* vsync on head A */
+ #define GPU_INTR_STATUS_VSYNC_1			1	/* vsync on head B */
+ #define GPU_INTR_STATUS_FLIP_0			3	/* flip head A */
+@@ -310,7 +308,7 @@ static int ps3fb_get_res_table(u32 xres,
+ 	unsigned int i;
+ 	u32 x, y, f;
+ 
+-	full_mode = (mode & PS3FB_FULL_MODE_BIT) ? PS3FB_RES_FULL : 0;
++	full_mode = (mode & PS3AV_MODE_FULL) ? PS3FB_RES_FULL : 0;
+ 	for (i = 0;; i++) {
+ 		x = ps3fb_res[i].xres;
+ 		y = ps3fb_res[i].yres;
+@@ -373,7 +371,7 @@ found:
+ 	/* Full broadcast modes have the full mode bit set */
+ 	mode = i+1;
+ 	if (mode > PS3AV_MODE_WUXGA)
+-		mode = (mode - PS3AV_MODE_WUXGA) | PS3FB_FULL_MODE_BIT;
++		mode = (mode - PS3AV_MODE_WUXGA) | PS3AV_MODE_FULL;
+ 
+ 	pr_debug("ps3fb_find_mode: mode %u\n", mode);
+ 
+@@ -390,7 +388,7 @@ static const struct fb_videomode *ps3fb_
+ 
+ 	flags = id & ~PS3AV_MODE_MASK;
+ 
+-	if (mode <= PS3AV_MODE_1080P50 && flags & PS3FB_FULL_MODE_BIT) {
++	if (mode <= PS3AV_MODE_1080P50 && flags & PS3AV_MODE_FULL) {
+ 		/* Full broadcast mode */
+ 		return &ps3fb_modedb[mode + PS3AV_MODE_WUXGA - 1];
+ 	}
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-kill-ps3fb_res.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-kill-ps3fb_res.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-kill-ps3fb_res.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-kill-ps3fb_res.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,230 @@
+Subject: ps3fb: Kill ps3fb_res[]
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3fb: kill ps3fb_res[], as all information it contains can be obtained in
+some other way.
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/video/ps3fb.c |  108 ++++++++++----------------------------------------
+ 1 files changed, 22 insertions(+), 86 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -134,42 +134,17 @@ static struct ps3fb_priv ps3fb;
+ struct ps3fb_par {
+ 	u32 pseudo_palette[16];
+ 	int mode_id, new_mode_id;
+-	int res_index;
+ 	unsigned int num_frames;	/* num of frame buffers */
+ 	unsigned int width;
+ 	unsigned int height;
++	unsigned int ddr_line_length;
++	unsigned int ddr_frame_size;
++	unsigned int xdr_frame_size;
+ 	unsigned long full_offset;	/* start of fullscreen DDR fb */
+ 	unsigned long fb_offset;	/* start of actual DDR fb */
+ 	unsigned long pan_offset;
+ };
+ 
+-struct ps3fb_res_table {
+-	u32 xres;
+-	u32 yres;
+-	u32 xoff;
+-	u32 yoff;
+-	u32 type;
+-};
+-#define PS3FB_RES_FULL 1
+-static const struct ps3fb_res_table ps3fb_res[] = {
+-	/* res_x,y   margin_x,y  full */
+-	{  720,  480,  72,  48 , 0},
+-	{  720,  576,  72,  58 , 0},
+-	{ 1280,  720,  78,  38 , 0},
+-	{ 1920, 1080, 116,  58 , 0},
+-	/* full mode */
+-	{  720,  480,   0,   0 , PS3FB_RES_FULL},
+-	{  720,  576,   0,   0 , PS3FB_RES_FULL},
+-	{ 1280,  720,   0,   0 , PS3FB_RES_FULL},
+-	{ 1920, 1080,   0,   0 , PS3FB_RES_FULL},
+-	/* vesa: normally full mode */
+-	{ 1280,  768,   0,   0 , 0},
+-	{ 1280, 1024,   0,   0 , 0},
+-	{ 1920, 1200,   0,   0 , 0},
+-	{    0,    0,   0,   0 , 0} };
+-
+-/* default resolution */
+-#define GPU_RES_INDEX	0		/* 720 x 480 */
+ 
+ static const struct fb_videomode ps3fb_modedb[] = {
+     /* 60 Hz broadcast modes (modes "1" to "5") */
+@@ -295,37 +270,6 @@ module_param(ps3fb_mode, int, 0);
+ 
+ static char *mode_option __devinitdata;
+ 
+-static int ps3fb_get_res_table(u32 xres, u32 yres, int mode)
+-{
+-	int full_mode;
+-	unsigned int i;
+-	u32 x, y, f;
+-
+-	full_mode = (mode & PS3AV_MODE_FULL) ? PS3FB_RES_FULL : 0;
+-	for (i = 0;; i++) {
+-		x = ps3fb_res[i].xres;
+-		y = ps3fb_res[i].yres;
+-		f = ps3fb_res[i].type;
+-
+-		if (!x) {
+-			pr_debug("ERROR: ps3fb_get_res_table()\n");
+-			return -1;
+-		}
+-
+-		if (full_mode == PS3FB_RES_FULL && f != PS3FB_RES_FULL)
+-			continue;
+-
+-		if (x == xres && (yres == 0 || y == yres))
+-			break;
+-
+-		x = x - 2 * ps3fb_res[i].xoff;
+-		y = y - 2 * ps3fb_res[i].yoff;
+-		if (x == xres && (yres == 0 || y == yres))
+-			break;
+-	}
+-	return i;
+-}
+-
+ static unsigned int ps3fb_find_mode(const struct fb_var_screeninfo *var,
+ 				    u32 *ddr_line_length, u32 *xdr_line_length)
+ {
+@@ -433,8 +377,7 @@ static void ps3fb_sync_image(struct devi
+ static int ps3fb_sync(struct fb_info *info, u32 frame)
+ {
+ 	struct ps3fb_par *par = info->par;
+-	int i, error = 0;
+-	u32 ddr_line_length, xdr_line_length;
++	int error = 0;
+ 	u64 ddr_base, xdr_base;
+ 
+ 	acquire_console_sem();
+@@ -446,16 +389,13 @@ static int ps3fb_sync(struct fb_info *in
+ 		goto out;
+ 	}
+ 
+-	i = par->res_index;
+-	xdr_line_length = info->fix.line_length;
+-	ddr_line_length = ps3fb_res[i].xres * BPP;
+-	xdr_base = frame * info->var.yres_virtual * xdr_line_length;
+-	ddr_base = frame * ps3fb_res[i].yres * ddr_line_length;
++	xdr_base = frame * par->xdr_frame_size;
++	ddr_base = frame * par->ddr_frame_size;
+ 
+ 	ps3fb_sync_image(info->device, ddr_base + par->full_offset,
+ 			 ddr_base + par->fb_offset, xdr_base + par->pan_offset,
+-			 par->width, par->height, ddr_line_length,
+-			 xdr_line_length);
++			 par->width, par->height, par->ddr_line_length,
++			 info->fix.line_length);
+ 
+ out:
+ 	release_console_sem();
+@@ -572,8 +512,9 @@ static int ps3fb_set_par(struct fb_info 
+ {
+ 	struct ps3fb_par *par = info->par;
+ 	unsigned int mode, ddr_line_length, xdr_line_length, lines, maxlines;
+-	int i;
++	unsigned int ddr_xoff, ddr_yoff;
+ 	unsigned long offset;
++	const struct fb_videomode *vmode;
+ 	u64 dst;
+ 
+ 	dev_dbg(info->device, "xres:%d xv:%d yres:%d yv:%d clock:%d\n",
+@@ -584,8 +525,7 @@ static int ps3fb_set_par(struct fb_info 
+ 	if (!mode)
+ 		return -EINVAL;
+ 
+-	i = ps3fb_get_res_table(info->var.xres, info->var.yres, mode);
+-	par->res_index = i;
++	vmode = ps3fb_default_mode(mode | PS3AV_MODE_FULL);
+ 
+ 	info->fix.smem_start = virt_to_abs(ps3fb.xdr_ea);
+ 	info->fix.smem_len = ps3fb.xdr_size;
+@@ -595,9 +535,12 @@ static int ps3fb_set_par(struct fb_info 
+ 
+ 	info->screen_base = (char __iomem *)ps3fb.xdr_ea;
+ 
++	par->ddr_line_length = ddr_line_length;
++	par->ddr_frame_size = vmode->yres * ddr_line_length;
++	par->xdr_frame_size = info->var.yres_virtual * xdr_line_length;
++
+ 	par->num_frames = ps3fb.xdr_size /
+-			  max(ps3fb_res[i].yres * ddr_line_length,
+-			      info->var.yres_virtual * xdr_line_length);
++			  max(par->ddr_frame_size, par->xdr_frame_size);
+ 
+ 	/* Keep the special bits we cannot set using fb_var_screeninfo */
+ 	par->new_mode_id = (par->new_mode_id & ~PS3AV_MODE_MASK) | mode;
+@@ -606,7 +549,9 @@ static int ps3fb_set_par(struct fb_info 
+ 	par->height = info->var.yres;
+ 
+ 	/* Start of the virtual frame buffer (relative to fullscreen) */
+-	offset = ps3fb_res[i].yoff * ddr_line_length + ps3fb_res[i].xoff * BPP;
++	ddr_xoff = info->var.left_margin - vmode->left_margin;
++	ddr_yoff = info->var.upper_margin - vmode->upper_margin;
++	offset = ddr_yoff * ddr_line_length + ddr_xoff * BPP;
+ 
+ 	par->fb_offset = GPU_ALIGN_UP(offset);
+ 	par->full_offset = par->fb_offset - offset;
+@@ -625,13 +570,13 @@ static int ps3fb_set_par(struct fb_info 
+ 	memset(ps3fb.xdr_ea, 0, ps3fb.xdr_size);
+ 
+ 	/* Clear DDR frame buffer memory */
+-	lines = ps3fb_res[i].yres * par->num_frames;
++	lines = vmode->yres * par->num_frames;
+ 	if (par->full_offset)
+ 		lines++;
+ 	maxlines = ps3fb.xdr_size / ddr_line_length;
+ 	for (dst = 0; lines; dst += maxlines * ddr_line_length) {
+ 		unsigned int l = min(lines, maxlines);
+-		ps3fb_sync_image(info->device, 0, dst, 0, ps3fb_res[i].xres, l,
++		ps3fb_sync_image(info->device, 0, dst, 0, vmode->xres, l,
+ 				 ddr_line_length, ddr_line_length);
+ 		lines -= l;
+ 	}
+@@ -1048,14 +993,13 @@ static int __devinit ps3fb_probe(struct 
+ 	struct fb_info *info;
+ 	struct ps3fb_par *par;
+ 	int retval = -ENOMEM;
+-	u32 xres, yres;
+ 	u64 ddr_lpar = 0;
+ 	u64 lpar_dma_control = 0;
+ 	u64 lpar_driver_info = 0;
+ 	u64 lpar_reports = 0;
+ 	u64 lpar_reports_size = 0;
+ 	u64 xdr_lpar;
+-	int status, res_index;
++	int status;
+ 	struct task_struct *task;
+ 	unsigned long max_ps3fb_size;
+ 
+@@ -1075,13 +1019,6 @@ static int __devinit ps3fb_probe(struct 
+ 		ps3fb_mode = ps3av_get_mode();
+ 	dev_dbg(&dev->core, "ps3fb_mode: %d\n", ps3fb_mode);
+ 
+-	if (ps3fb_mode > 0 &&
+-	    !ps3av_video_mode2res(ps3fb_mode, &xres, &yres)) {
+-		res_index = ps3fb_get_res_table(xres, yres, ps3fb_mode);
+-		dev_dbg(&dev->core, "res_index:%d\n", res_index);
+-	} else
+-		res_index = GPU_RES_INDEX;
+-
+ 	atomic_set(&ps3fb.f_count, -1);	/* fbcon opens ps3fb */
+ 	atomic_set(&ps3fb.ext_flip, 0);	/* for flip with vsync */
+ 	init_waitqueue_head(&ps3fb.wait_vsync);
+@@ -1154,7 +1091,6 @@ static int __devinit ps3fb_probe(struct 
+ 	par = info->par;
+ 	par->mode_id = ~ps3fb_mode;	/* != ps3fb_mode, to trigger change */
+ 	par->new_mode_id = ps3fb_mode;
+-	par->res_index = res_index;
+ 	par->num_frames = 1;
+ 
+ 	info->screen_base = (char __iomem *)ps3fb.xdr_ea;
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-modedb-typos.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-modedb-typos.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-modedb-typos.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-modedb-typos.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,32 @@
+Subject: ps3fb: Fix modedb typos
+
+From: Geoff Levand <geoffrey.levand@am.sony.com>
+
+ps3fb: Fix modedb typos
+
+Signed-off-by: Geoff Levand <geoffrey.levand@am.sony.com>
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/video/ps3fb.c |    4 ++--
+ 1 file changed, 2 insertions(+), 2 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -184,7 +184,7 @@ static const struct fb_videomode ps3fb_m
+         "720p", 50, 1124, 644, 13468, 298, 478, 57, 44, 80, 5,
+         FB_SYNC_BROADCAST, FB_VMODE_NONINTERLACED
+     },    {
+-        /* 1080 */
++        /* 1080i */
+         "1080i", 50, 1688, 964, 13468, 264, 600, 94, 62, 88, 5,
+         FB_SYNC_BROADCAST, FB_VMODE_INTERLACED
+     },    {
+@@ -232,7 +232,7 @@ static const struct fb_videomode ps3fb_m
+ 	FB_SYNC_BROADCAST, FB_VMODE_NONINTERLACED
+     }, {
+ 	/* 1080if */
+-	"1080f", 50, 1920, 1080, 13468, 148, 484, 36, 4, 88, 5,
++	"1080if", 50, 1920, 1080, 13468, 148, 484, 36, 4, 88, 5,
+ 	FB_SYNC_BROADCAST, FB_VMODE_INTERLACED
+     }, {
+ 	/* 1080pf */
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-reorganize-modedb-handling.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-reorganize-modedb-handling.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-reorganize-modedb-handling.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-reorganize-modedb-handling.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,210 @@
+Subject: ps3fb: Reorganize modedb handling
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3fb: Reorganize modedb handling:
+  - Reorder the video modes in ps3fb_modedb, for easier indexing using
+    PS3AV_MODE_* numbers,
+  - Introduce ps3fb_native_vmode(), to convert from native (PS3AV_MODE_*) mode
+    numbers to struct fb_videomode *,
+  - Rename and move ps3fb_default_mode() to ps3fb_vmode().
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/video/ps3fb.c |  116 +++++++++++++++++++++++++-------------------------
+ 1 files changed, 60 insertions(+), 56 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -146,6 +146,8 @@ struct ps3fb_par {
+ };
+ 
+ 
++#define FIRST_NATIVE_MODE_INDEX	10
++
+ static const struct fb_videomode ps3fb_modedb[] = {
+     /* 60 Hz broadcast modes (modes "1" to "5") */
+     {
+@@ -193,24 +195,7 @@ static const struct fb_videomode ps3fb_m
+         FB_SYNC_BROADCAST, FB_VMODE_NONINTERLACED
+     },
+ 
+-    /* VESA modes (modes "11" to "13") */
+-    {
+-	/* WXGA */
+-	"wxga", 60, 1280, 768, 12924, 160, 24, 29, 3, 136, 6,
+-	0, FB_VMODE_NONINTERLACED,
+-	FB_MODE_IS_VESA
+-    }, {
+-	/* SXGA */
+-	"sxga", 60, 1280, 1024, 9259, 248, 48, 38, 1, 112, 3,
+-	FB_SYNC_HOR_HIGH_ACT | FB_SYNC_VERT_HIGH_ACT, FB_VMODE_NONINTERLACED,
+-	FB_MODE_IS_VESA
+-    }, {
+-	/* WUXGA */
+-	"wuxga", 60, 1920, 1200, 6494, 80, 48, 26, 3, 32, 6,
+-	FB_SYNC_HOR_HIGH_ACT, FB_VMODE_NONINTERLACED,
+-	FB_MODE_IS_VESA
+-    },
+-
++    [FIRST_NATIVE_MODE_INDEX] =
+     /* 60 Hz broadcast modes (full resolution versions of modes "1" to "5") */
+     {
+ 	/* 480if */
+@@ -255,6 +240,24 @@ static const struct fb_videomode ps3fb_m
+ 	/* 1080pf */
+ 	"1080pf", 50, 1920, 1080, 6734, 148, 484, 36, 4, 88, 5,
+ 	FB_SYNC_BROADCAST, FB_VMODE_NONINTERLACED
++    },
++
++    /* VESA modes (modes "11" to "13") */
++    {
++	/* WXGA */
++	"wxga", 60, 1280, 768, 12924, 160, 24, 29, 3, 136, 6,
++	0, FB_VMODE_NONINTERLACED,
++	FB_MODE_IS_VESA
++    }, {
++	/* SXGA */
++	"sxga", 60, 1280, 1024, 9259, 248, 48, 38, 1, 112, 3,
++	FB_SYNC_HOR_HIGH_ACT | FB_SYNC_VERT_HIGH_ACT, FB_VMODE_NONINTERLACED,
++	FB_MODE_IS_VESA
++    }, {
++	/* WUXGA */
++	"wuxga", 60, 1920, 1200, 6494, 80, 48, 26, 3, 32, 6,
++	FB_SYNC_HOR_HIGH_ACT, FB_VMODE_NONINTERLACED,
++	FB_MODE_IS_VESA
+     }
+ };
+ 
+@@ -298,20 +301,43 @@ static int ps3fb_cmp_mode(const struct f
+ 	return 0;
+ }
+ 
++static const struct fb_videomode *ps3fb_native_vmode(enum ps3av_mode_num id)
++{
++	return &ps3fb_modedb[FIRST_NATIVE_MODE_INDEX + id - 1];
++}
++
++static const struct fb_videomode *ps3fb_vmode(int id)
++{
++	u32 mode = id & PS3AV_MODE_MASK;
++
++	if (mode < PS3AV_MODE_480I || mode > PS3AV_MODE_WUXGA)
++		return NULL;
++
++	if (mode <= PS3AV_MODE_1080P50 && !(id & PS3AV_MODE_FULL)) {
++		/* Non-fullscreen broadcast mode */
++		return &ps3fb_modedb[mode - 1];
++	}
++
++	return ps3fb_native_vmode(mode);
++}
++
+ static unsigned int ps3fb_find_mode(struct fb_var_screeninfo *var,
+ 				    u32 *ddr_line_length, u32 *xdr_line_length)
+ {
+-	unsigned int i, mode;
++	unsigned int id;
++	const struct fb_videomode *vmode;
+ 
+-	for (i = PS3AV_MODE_1080P50; i < ARRAY_SIZE(ps3fb_modedb); i++)
+-		if (!ps3fb_cmp_mode(&ps3fb_modedb[i], var))
++	for (id = PS3AV_MODE_480I; id <= PS3AV_MODE_WUXGA; id++) {
++		vmode = ps3fb_native_vmode(id);
++		if (!ps3fb_cmp_mode(vmode, var))
+ 			goto found;
++	}
+ 
+-	pr_debug("ps3fb_find_mode: mode not found\n");
++	pr_debug("%s: mode not found\n", __func__);
+ 	return 0;
+ 
+ found:
+-	*ddr_line_length = ps3fb_modedb[i].xres * BPP;
++	*ddr_line_length = vmode->xres * BPP;
+ 
+ 	if (!var->xres) {
+ 		var->xres = 1;
+@@ -330,36 +356,14 @@ found:
+ 	} else
+ 		*xdr_line_length = *ddr_line_length;
+ 
+-	mode = i+1;
+-	if (mode > PS3AV_MODE_WUXGA) {
+-		mode -= PS3AV_MODE_WUXGA;
++	if (vmode->sync & FB_SYNC_BROADCAST) {
+ 		/* Full broadcast modes have the full mode bit set */
+-		if (ps3fb_modedb[i].xres == var->xres &&
+-		    ps3fb_modedb[i].yres == var->yres)
+-			mode |= PS3AV_MODE_FULL;
+-	}
+-
+-	pr_debug("ps3fb_find_mode: mode %u\n", mode);
+-
+-	return mode;
+-}
+-
+-static const struct fb_videomode *ps3fb_default_mode(int id)
+-{
+-	u32 mode = id & PS3AV_MODE_MASK;
+-	u32 flags;
+-
+-	if (mode < PS3AV_MODE_480I || mode > PS3AV_MODE_WUXGA)
+-		return NULL;
+-
+-	flags = id & ~PS3AV_MODE_MASK;
+-
+-	if (mode <= PS3AV_MODE_1080P50 && flags & PS3AV_MODE_FULL) {
+-		/* Full broadcast mode */
+-		return &ps3fb_modedb[mode + PS3AV_MODE_WUXGA - 1];
++		if (vmode->xres == var->xres && vmode->yres == var->yres)
++			id |= PS3AV_MODE_FULL;
+ 	}
+ 
+-	return &ps3fb_modedb[mode - 1];
++	pr_debug("%s: mode %u\n", __func__, id);
++	return id;
+ }
+ 
+ static void ps3fb_sync_image(struct device *dev, u64 frame_offset,
+@@ -553,7 +557,7 @@ static int ps3fb_set_par(struct fb_info 
+ 	if (!mode)
+ 		return -EINVAL;
+ 
+-	vmode = ps3fb_default_mode(mode | PS3AV_MODE_FULL);
++	vmode = ps3fb_native_vmode(mode & PS3AV_MODE_MASK);
+ 
+ 	info->fix.smem_start = virt_to_abs(ps3fb.xdr_ea);
+ 	info->fix.smem_len = ps3fb.xdr_size;
+@@ -767,7 +771,7 @@ static int ps3fb_ioctl(struct fb_info *i
+ 	case PS3FB_IOCTL_SETMODE:
+ 		{
+ 			struct ps3fb_par *par = info->par;
+-			const struct fb_videomode *mode;
++			const struct fb_videomode *vmode;
+ 			struct fb_var_screeninfo var;
+ 
+ 			if (copy_from_user(&val, argp, sizeof(val)))
+@@ -780,10 +784,10 @@ static int ps3fb_ioctl(struct fb_info *i
+ 			}
+ 			dev_dbg(info->device, "PS3FB_IOCTL_SETMODE:%x\n", val);
+ 			retval = -EINVAL;
+-			mode = ps3fb_default_mode(val);
+-			if (mode) {
++			vmode = ps3fb_vmode(val);
++			if (vmode) {
+ 				var = info->var;
+-				fb_videomode_to_var(&var, mode);
++				fb_videomode_to_var(&var, vmode);
+ 				acquire_console_sem();
+ 				info->flags |= FBINFO_MISC_USEREVENT;
+ 				/* Force, in case only special bits changed */
+@@ -1137,7 +1141,7 @@ static int __devinit ps3fb_probe(struct 
+ 
+ 	if (!fb_find_mode(&info->var, info, mode_option, ps3fb_modedb,
+ 			  ARRAY_SIZE(ps3fb_modedb),
+-			  ps3fb_default_mode(par->new_mode_id), 32)) {
++			  ps3fb_vmode(par->new_mode_id), 32)) {
+ 		retval = -EINVAL;
+ 		goto err_fb_dealloc;
+ 	}
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-round-video-modes.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-round-video-modes.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-round-video-modes.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-round-video-modes.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,225 @@
+Subject: ps3fb: Round up video modes
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3fb: Round up arbitrary video modes until they fit (if possible)
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/video/ps3fb.c |  160 +++++++++++++++++++++++++++++++++++---------------
+ 1 files changed, 114 insertions(+), 46 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -276,29 +276,49 @@ static char *mode_option __devinitdata;
+ static int ps3fb_cmp_mode(const struct fb_videomode *vmode,
+ 			  const struct fb_var_screeninfo *var)
+ {
+-	/* resolution + black border must match a native resolution */
+-	if (vmode->left_margin + vmode->xres + vmode->right_margin !=
+-	    var->left_margin + var->xres + var->right_margin ||
+-	    vmode->upper_margin + vmode->yres + vmode->lower_margin !=
+-	    var->upper_margin + var->yres + var->lower_margin)
++	long xres, yres, left_margin, right_margin, upper_margin, lower_margin;
++	long dx, dy;
++
++	/* maximum values */
++	if (var->xres > vmode->xres || var->yres > vmode->yres ||
++	    var->pixclock > vmode->pixclock ||
++	    var->hsync_len > vmode->hsync_len ||
++	    var->vsync_len > vmode->vsync_len)
+ 		return -1;
+ 
+-	/* minimum limits for margins */
+-	if (vmode->left_margin > var->left_margin ||
+-	    vmode->right_margin > var->right_margin ||
+-	    vmode->upper_margin > var->upper_margin ||
+-	    vmode->lower_margin > var->lower_margin)
++	/* progressive/interlaced must match */
++	if ((var->vmode & FB_VMODE_MASK) != vmode->vmode)
+ 		return -1;
+ 
+-	/* these fields must match exactly */
+-	if (vmode->pixclock != var->pixclock ||
+-	    vmode->hsync_len != var->hsync_len ||
+-	    vmode->vsync_len != var->vsync_len ||
+-	    vmode->sync != var->sync ||
+-	    vmode->vmode != (var->vmode & FB_VMODE_MASK))
++	/* minimum resolution */
++	xres = max(var->xres, 1U);
++	yres = max(var->yres, 1U);
++
++	/* minimum margins */
++	left_margin = max(var->left_margin, vmode->left_margin);
++	right_margin = max(var->right_margin, vmode->right_margin);
++	upper_margin = max(var->upper_margin, vmode->upper_margin);
++	lower_margin = max(var->lower_margin, vmode->lower_margin);
++
++	/* resolution + margins may not exceed native parameters */
++	dx = ((long)vmode->left_margin + (long)vmode->xres +
++	      (long)vmode->right_margin) -
++	     (left_margin + xres + right_margin);
++	if (dx < 0)
+ 		return -1;
+ 
+-	return 0;
++	dy = ((long)vmode->upper_margin + (long)vmode->yres +
++	      (long)vmode->lower_margin) -
++	     (upper_margin + yres + lower_margin);
++	if (dy < 0)
++		return -1;
++
++	/* exact match */
++	if (!dx && !dy)
++		return 0;
++
++	/* resolution difference */
++	return (vmode->xres - xres) * (vmode->yres - yres);
+ }
+ 
+ static const struct fb_videomode *ps3fb_native_vmode(enum ps3av_mode_num id)
+@@ -324,33 +344,96 @@ static const struct fb_videomode *ps3fb_
+ static unsigned int ps3fb_find_mode(struct fb_var_screeninfo *var,
+ 				    u32 *ddr_line_length, u32 *xdr_line_length)
+ {
+-	unsigned int id;
++	unsigned int id, best_id;
++	int diff, best_diff;
+ 	const struct fb_videomode *vmode;
++	long gap;
+ 
++	best_id = 0;
++	best_diff = INT_MAX;
++	pr_debug("%s: wanted %u [%u] %u x %u [%u] %u\n", __func__,
++		 var->left_margin, var->xres, var->right_margin,
++		 var->upper_margin, var->yres, var->lower_margin);
+ 	for (id = PS3AV_MODE_480I; id <= PS3AV_MODE_WUXGA; id++) {
+ 		vmode = ps3fb_native_vmode(id);
+-		if (!ps3fb_cmp_mode(vmode, var))
+-			goto found;
++		diff = ps3fb_cmp_mode(vmode, var);
++		pr_debug("%s: mode %u: %u [%u] %u x %u [%u] %u: diff = %d\n",
++			 __func__, id, vmode->left_margin, vmode->xres,
++			 vmode->right_margin, vmode->upper_margin,
++			 vmode->yres, vmode->lower_margin, diff);
++		if (diff < 0)
++			continue;
++		if (diff < best_diff) {
++			best_id = id;
++			if (!diff)
++				break;
++			best_diff = diff;
++		}
+ 	}
+ 
+-	pr_debug("%s: mode not found\n", __func__);
+-	return 0;
++	if (!best_id) {
++		pr_debug("%s: no suitable mode found\n", __func__);
++		return 0;
++	}
++
++	id = best_id;
++	vmode = ps3fb_native_vmode(id);
+ 
+-found:
+ 	*ddr_line_length = vmode->xres * BPP;
+ 
+-	if (!var->xres) {
++	/* minimum resolution */
++	if (!var->xres)
+ 		var->xres = 1;
+-		var->right_margin--;
+-	}
+-	if (!var->yres) {
++	if (!var->yres)
+ 		var->yres = 1;
+-		var->lower_margin--;
+-	}
++
++	/* minimum virtual resolution */
++	if (var->xres_virtual < var->xres)
++		var->xres_virtual = var->xres;
++	if (var->yres_virtual < var->yres)
++		var->yres_virtual = var->yres;
++
++	/* minimum margins */
++	if (var->left_margin < vmode->left_margin)
++		var->left_margin = vmode->left_margin;
++	if (var->right_margin < vmode->right_margin)
++		var->right_margin = vmode->right_margin;
++	if (var->upper_margin < vmode->upper_margin)
++		var->upper_margin = vmode->upper_margin;
++	if (var->lower_margin < vmode->lower_margin)
++		var->lower_margin = vmode->lower_margin;
++
++	/* extra margins */
++	gap = ((long)vmode->left_margin + (long)vmode->xres +
++	       (long)vmode->right_margin) -
++	      ((long)var->left_margin + (long)var->xres +
++	       (long)var->right_margin);
++	if (gap > 0) {
++		var->left_margin += gap/2;
++		var->right_margin += (gap+1)/2;
++		pr_debug("%s: rounded up H to %u [%u] %u\n", __func__,
++			 var->left_margin, var->xres, var->right_margin);
++	}
++
++	gap = ((long)vmode->upper_margin + (long)vmode->yres +
++	       (long)vmode->lower_margin) -
++	      ((long)var->upper_margin + (long)var->yres +
++	       (long)var->lower_margin);
++	if (gap > 0) {
++		var->upper_margin += gap/2;
++		var->lower_margin += (gap+1)/2;
++		pr_debug("%s: rounded up V to %u [%u] %u\n", __func__,
++			 var->upper_margin, var->yres, var->lower_margin);
++	}
++
++	/* fixed fields */
++	var->pixclock = vmode->pixclock;
++	var->hsync_len = vmode->hsync_len;
++	var->vsync_len = vmode->vsync_len;
++	var->sync = vmode->sync;
+ 
+ 	if (ps3_compare_firmware_version(1, 9, 0) >= 0) {
+-		*xdr_line_length = GPU_ALIGN_UP(max(var->xres,
+-						    var->xres_virtual) * BPP);
++		*xdr_line_length = GPU_ALIGN_UP(var->xres_virtual * BPP);
+ 		if (*xdr_line_length > GPU_MAX_LINE_LENGTH)
+ 			*xdr_line_length = GPU_MAX_LINE_LENGTH;
+ 	} else
+@@ -465,22 +548,11 @@ static int ps3fb_check_var(struct fb_var
+ 	u32 xdr_line_length, ddr_line_length;
+ 	int mode;
+ 
+-	dev_dbg(info->device, "var->xres:%u info->var.xres:%u\n", var->xres,
+-		info->var.xres);
+-	dev_dbg(info->device, "var->yres:%u info->var.yres:%u\n", var->yres,
+-		info->var.yres);
+-
+-	/* FIXME For now we do exact matches only */
+ 	mode = ps3fb_find_mode(var, &ddr_line_length, &xdr_line_length);
+ 	if (!mode)
+ 		return -EINVAL;
+ 
+ 	/* Virtual screen */
+-	if (var->xres_virtual < var->xres)
+-		var->xres_virtual = var->xres;
+-	if (var->yres_virtual < var->yres)
+-		var->yres_virtual = var->yres;
+-
+ 	if (var->xres_virtual > xdr_line_length / BPP) {
+ 		dev_dbg(info->device,
+ 			"Horizontal virtual screen size too large\n");
+@@ -549,10 +621,6 @@ static int ps3fb_set_par(struct fb_info 
+ 	const struct fb_videomode *vmode;
+ 	u64 dst;
+ 
+-	dev_dbg(info->device, "xres:%d xv:%d yres:%d yv:%d clock:%d\n",
+-		info->var.xres, info->var.xres_virtual,
+-		info->var.yres, info->var.yres_virtual, info->var.pixclock);
+-
+ 	mode = ps3fb_find_mode(&info->var, &ddr_line_length, &xdr_line_length);
+ 	if (!mode)
+ 		return -EINVAL;
diff -Naur linux-2.6.25-org/patches/ps3-wip/ps3fb-unsigned-int-offsets.diff linux-2.6.25-id/patches/ps3-wip/ps3fb-unsigned-int-offsets.diff
--- linux-2.6.25-org/patches/ps3-wip/ps3fb-unsigned-int-offsets.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/ps3-wip/ps3fb-unsigned-int-offsets.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,37 @@
+Subject: ps3fb: Make frame buffer offset unsigned int
+
+From: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+
+ps3fb: Frame buffer offsets don't have to be `unsigned long', `unsigned int' is
+sufficient
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+---
+ drivers/video/ps3fb.c |    9 ++++-----
+ 1 files changed, 4 insertions(+), 5 deletions(-)
+
+--- a/drivers/video/ps3fb.c
++++ b/drivers/video/ps3fb.c
+@@ -140,9 +140,9 @@ struct ps3fb_par {
+ 	unsigned int ddr_line_length;
+ 	unsigned int ddr_frame_size;
+ 	unsigned int xdr_frame_size;
+-	unsigned long full_offset;	/* start of fullscreen DDR fb */
+-	unsigned long fb_offset;	/* start of actual DDR fb */
+-	unsigned long pan_offset;
++	unsigned int full_offset;	/* start of fullscreen DDR fb */
++	unsigned int fb_offset;		/* start of actual DDR fb */
++	unsigned int pan_offset;
+ };
+ 
+ 
+@@ -512,8 +512,7 @@ static int ps3fb_set_par(struct fb_info 
+ {
+ 	struct ps3fb_par *par = info->par;
+ 	unsigned int mode, ddr_line_length, xdr_line_length, lines, maxlines;
+-	unsigned int ddr_xoff, ddr_yoff;
+-	unsigned long offset;
++	unsigned int ddr_xoff, ddr_yoff, offset;
+ 	const struct fb_videomode *vmode;
+ 	u64 dst;
+ 
diff -Naur linux-2.6.25-org/patches/series linux-2.6.25-id/patches/series
--- linux-2.6.25-org/patches/series	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/series	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,92 @@
+# Source: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6.git
+#
+
+other/powerpc-add-cell-srpn-bkmk.patch
+other/powerpc-asm-mmu-hash64-sparse.diff
+ps3-stable/ps3-make-dev_id-and-bus_id-u64.diff
+ps3-stable/ps3_repository_find_device_by_id.diff
+ps3-stable/ps3-storage-correct-notification-mechanism.diff
+ps3-stable/ps3-storage-fw190-workaround.diff
+ps3-stable/kill-unused-ps3_repository_bump_device.diff
+ps3-stable/ps3-refactor-ps3_repository_find_device.diff
+ps3-stable/ps3-checkpatch-sys-manager.diff
+ps3-stable/ps3-checkpatch-vuart.diff
+ps3-stable/ps3-checkpatch-repository.diff
+ps3-stable/ps3-lpm-repository-support.patch
+ps3-stable/ps3-lpm-device-support.patch
+ps3-stable/ps3-lpm-driver-support.patch
+ps3-stable/ps3-vuart-change-sem-to-mutex.patch
+ps3-stable/ps3-remove-use-lpar-addr.patch
+ps3-stable/ps3-defconfig-updates.patch
+
+other/boot-create-otheros-bld-in-output-dir.patch
+ps3-stable/spufs-wrap-master-run-bit.diff
+
+other/kbuild-includecheck-versioncheck-src-prefix.diff
+other/usb-fix-ehci-iso-transfer.patch
+ps3-stable/ps3disk-superfluous-cast.diff
+
+ps3-wip/ps3av-remove-unused.diff
+ps3-wip/ps3av-mode-defines.diff
+ps3-wip/ps3fb-kill-PS3FB_FULL_MODE_BIT.diff
+ps3-wip/ps3fb-inline-macros.diff
+ps3-wip/ps3fb-kill-ps3fb_res.diff
+ps3-wip/ps3fb-unsigned-int-offsets.diff
+ps3-wip/ps3fb-configurable-black-borders.diff
+ps3-wip/ps3fb-reorganize-modedb-handling.diff
+ps3-wip/ps3fb-round-video-modes.diff
+ps3-wip/ps3fb-cleanup.diff
+ps3-wip/ps3fb-modedb-typos.diff
+
+ps3-stable/ps3-lpm-fix-set-bookmark.patch
+
+# -- cut for 2.6.25 --
+
+ps3-wip/ps3-save-power-in-busy-loops-on-halt.diff
+ps3-wip/ps3-quick-fix-boot-game-os.diff
+ps3-wip/ps3-oprofile.patch
+other/powerpc-fix-slb-debug-warnings.diff
+
+# -- cut for 2.6.26 --
+
+ps3-wip/ps3-gelic-fix-fallback.diff
+ps3-wip/ps3-gelic-endianness.patch
+ps3-wip/ps3-gelic-cleanup.patch
+ps3-wip/ps3-gelic-remove-duplicate-ethtool-handlers.patch
+ps3-wip/ps3-gelic-ethernet-linkstatus.patch
+ps3-wip/ps3-gelic-multiple-interface.patch
+ps3-wip/ps3-gelic-wireless-v2.patch
+#ps3-wip/ps3-gelic-wireless.diff
+#ps3-wip/ps3-gelic-wpa2.diff
+
+ps3-hacks/ps3-oprofile-test-script.patch
+perfmon/perfmon-core.diff
+perfmon/perfmon-cell-Initial-PS3-perfmon2-support.patch
+ps3-wip/powerpc-verbose-bootwrapper.diff
+#ps3-wip/ps3-htab-rework.diff
+
+#spe-logo/logo-extern-in-header.diff
+
+# debugging junk
+#ps3-hacks/ps3-debug-zimage-initrd-in-userspace.diff
+#ps3-hacks/sync-ps3fb-on-panic.diff
+ps3-wip/ps3-system-bus-status.diff
+ps3-hacks/ps3-free-irq-check.patch
+ps3-wip/ps3-os-area-debug-only.diff
+
+other/powerpc-warn-on-emulation.diff
+
+# This patch was applied to Eranian's perfmon2 git tree. 12/20
+ps3-wip/perfmon/cell-add-hw-thid-check-for-ppu-event.patch
+ps3-wip/perfmon/ps3-rename-lpm-open-close.patch
+
+ps3-wip/ps3-wip-defconfig-updates.patch
+
+# Source: git://git.kernel.org/pub/scm/linux/kernel/git/geoff/ps3-linux.git
+#
+
+ps3-private/perfmon/cell-spe-add-get-speid-func.patch
+ps3-private/perfmon/ps3-print-spe-id.patch
+ps3-private/perfmon/cell-spe-other.patch
+ps3-private/perfmon/cell-spe-add-hook-functions.patch
+ps3-private/perfmon/cell-spe-call-hook-functions.patch
diff -Naur linux-2.6.25-org/patches/spe-logo/logo-extern-in-header.diff linux-2.6.25-id/patches/spe-logo/logo-extern-in-header.diff
--- linux-2.6.25-org/patches/spe-logo/logo-extern-in-header.diff	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/patches/spe-logo/logo-extern-in-header.diff	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,90 @@
+Subject: fbdev: move logo externs to header file
+
+Move the external declarations for the various linux logo structures to
+<linux/linux_logo.h>. As a consequence, I had to change scripts/pnmtologo.c to
+add the appropriate `const' to the generated logo source code.
+
+FIXME: This one is really tricky and cannot be applied!
+  - gcc 3.4.6 20060404 (Red Hat 3.4.6-3) needs xxx_data[] and
+    xxx_clut[] to be const too, else it complains about a section type conflict
+  - ppu-gcc 4.1.0 20060304 (Red Hat 4.1.0-3) needs xxx_data[] and xxx_clut[] to
+    be non-const, else it complains about a section type conflict
+Anyone with a suggestion how to fix this?
+
+Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
+Acked-By: James Simmons <jsimmons@infradead.org>
+---
+ arch/powerpc/kernel/prom_init.c |    3 ---
+ drivers/video/logo/logo.c       |   13 -------------
+ include/linux/linux_logo.h      |   13 +++++++++++++
+ scripts/pnmtologo.c             |    2 +-
+ 4 files changed, 14 insertions(+), 17 deletions(-)
+
+--- a/arch/powerpc/kernel/prom_init.c
++++ b/arch/powerpc/kernel/prom_init.c
+@@ -44,10 +44,7 @@
+ #include <asm/sections.h>
+ #include <asm/machdep.h>
+ 
+-#ifdef CONFIG_LOGO_LINUX_CLUT224
+ #include <linux/linux_logo.h>
+-extern const struct linux_logo logo_linux_clut224;
+-#endif
+ 
+ /*
+  * Properties whose value is longer than this get excluded from our
+--- a/drivers/video/logo/logo.c
++++ b/drivers/video/logo/logo.c
+@@ -21,19 +21,6 @@
+ #include <asm/bootinfo.h>
+ #endif
+ 
+-extern const struct linux_logo logo_linux_mono;
+-extern const struct linux_logo logo_linux_vga16;
+-extern const struct linux_logo logo_linux_clut224;
+-extern const struct linux_logo logo_dec_clut224;
+-extern const struct linux_logo logo_mac_clut224;
+-extern const struct linux_logo logo_parisc_clut224;
+-extern const struct linux_logo logo_sgi_clut224;
+-extern const struct linux_logo logo_sun_clut224;
+-extern const struct linux_logo logo_superh_mono;
+-extern const struct linux_logo logo_superh_vga16;
+-extern const struct linux_logo logo_superh_clut224;
+-extern const struct linux_logo logo_m32r_clut224;
+-
+ 
+ const struct linux_logo *fb_find_logo(int depth)
+ {
+--- a/include/linux/linux_logo.h
++++ b/include/linux/linux_logo.h
+@@ -32,6 +32,19 @@ struct linux_logo {
+ 	const unsigned char *data;
+ };
+ 
++extern const struct linux_logo logo_linux_mono;
++extern const struct linux_logo logo_linux_vga16;
++extern const struct linux_logo logo_linux_clut224;
++extern const struct linux_logo logo_dec_clut224;
++extern const struct linux_logo logo_mac_clut224;
++extern const struct linux_logo logo_parisc_clut224;
++extern const struct linux_logo logo_sgi_clut224;
++extern const struct linux_logo logo_sun_clut224;
++extern const struct linux_logo logo_superh_mono;
++extern const struct linux_logo logo_superh_vga16;
++extern const struct linux_logo logo_superh_clut224;
++extern const struct linux_logo logo_m32r_clut224;
++
+ extern const struct linux_logo *fb_find_logo(int depth);
+ 
+ #endif /* _LINUX_LINUX_LOGO_H */
+--- a/scripts/pnmtologo.c
++++ b/scripts/pnmtologo.c
+@@ -244,7 +244,7 @@ static void write_header(void)
+ static void write_footer(void)
+ {
+     fputs("\n};\n\n", out);
+-    fprintf(out, "struct linux_logo %s __initdata = {\n", logoname);
++    fprintf(out, "const struct linux_logo %s __initdata = {\n", logoname);
+     fprintf(out, "    .type\t= %s,\n", logo_types[logo_type]);
+     fprintf(out, "    .width\t= %d,\n", logo_width);
+     fprintf(out, "    .height\t= %d,\n", logo_height);
diff -Naur linux-2.6.25-org/perfmon/Makefile linux-2.6.25-id/perfmon/Makefile
--- linux-2.6.25-org/perfmon/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/Makefile	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,8 @@
+#
+# Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+# Contributed by Stephane Eranian <eranian@hpl.hp.com>
+#
+obj-$(CONFIG_PERFMON) = perfmon.o perfmon_rw.o perfmon_res.o perfmon_fmt.o \
+			perfmon_pmu.o perfmon_sysfs.o perfmon_syscalls.o   \
+			perfmon_file.o perfmon_ctxsw.o perfmon_intr.o	   \
+			perfmon_dfl_smpl.o perfmon_sets.o perfmon_debugfs.o
diff -Naur linux-2.6.25-org/perfmon/perfmon.c linux-2.6.25-id/perfmon/perfmon.c
--- linux-2.6.25-org/perfmon/perfmon.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,1821 @@
+/*
+ * perfmon.c: perfmon2 core functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://www.hpl.hp.com/research/linux/perfmon
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/vmalloc.h>
+#include <linux/poll.h>
+#include <linux/ptrace.h>
+#include <linux/perfmon.h>
+#include <linux/cpu.h>
+#include <linux/random.h>
+
+/*
+ * internal variables
+ */
+static struct kmem_cache *pfm_ctx_cachep;
+
+/*
+ * external variables
+ */
+DEFINE_PER_CPU(struct task_struct *, pmu_owner);
+DEFINE_PER_CPU(struct pfm_context  *, pmu_ctx);
+DEFINE_PER_CPU(u64, pmu_activation_number);
+DEFINE_PER_CPU(struct pfm_stats, pfm_stats);
+DEFINE_PER_CPU(struct hrtimer, pfm_hrtimer);
+
+#define PFM_INVALID_ACTIVATION	((u64)~0)
+
+int perfmon_disabled;	/* >0 if perfmon is disabled */
+
+/*
+ * Reset PMD register flags
+ */
+#define PFM_PMD_RESET_NONE	0	/* do not reset (pfm_switch_set) */
+#define PFM_PMD_RESET_SHORT	1	/* use short reset value */
+#define PFM_PMD_RESET_LONG	2	/* use long reset value  */
+
+/*
+ * get a new message slot from the queue. If the queue is full NULL
+ * is returned and monitoring stops.
+ */
+static union pfarg_msg *pfm_get_new_msg(struct pfm_context *ctx)
+{
+	int next;
+
+	next = ctx->msgq_head & PFM_MSGQ_MASK;
+
+	if ((ctx->msgq_head - ctx->msgq_tail) == PFM_MSGS_COUNT)
+		return NULL;
+
+	/*
+	 * move to next possible slot
+	 */
+	ctx->msgq_head++;
+
+	PFM_DBG_ovfl("head=%d tail=%d msg=%d",
+		ctx->msgq_head & PFM_MSGQ_MASK,
+		ctx->msgq_tail & PFM_MSGQ_MASK,
+		next);
+
+	return ctx->msgq+next;
+}
+
+void pfm_context_free(struct pfm_context *ctx)
+{
+	struct pfm_smpl_fmt *fmt;
+
+	pfm_arch_context_free(ctx);
+
+#if CONFIG_PPC
+	pfm_arch_remove_ctxsw_hook(ctx);
+	if (ctx)
+		ctx->task = NULL;
+	PFM_DBG("unhook SPE ctxsw. context_free");
+#endif
+
+	fmt = ctx->smpl_fmt;
+
+	pfm_free_sets(ctx);
+
+	if (ctx->smpl_addr) {
+		PFM_DBG("freeing sampling buffer @%p size=%zu",
+			ctx->real_smpl_addr,
+			ctx->real_smpl_size);
+
+		pfm_release_buf_space(ctx, ctx->real_smpl_size);
+
+		if (fmt->fmt_exit)
+			(*fmt->fmt_exit)(ctx->smpl_addr);
+
+		vfree(ctx->real_smpl_addr);
+	}
+
+	PFM_DBG("free ctx @%p", ctx);
+	kmem_cache_free(pfm_ctx_cachep, ctx);
+	/*
+	 * decrease refcount on:
+	 * 	- PMU description table
+	 * 	- sampling format
+	 */
+	pfm_pmu_conf_put();
+	pfm_smpl_fmt_put(fmt);
+	pfm_pmu_release();
+}
+
+/*
+ * only called in for the current task
+ */
+static int pfm_setup_smpl_fmt(struct pfm_smpl_fmt *fmt, void *fmt_arg,
+				struct pfm_context *ctx, u32 ctx_flags,
+				int mode, struct file *filp)
+{
+	size_t size = 0;
+	int ret = 0;
+
+	/*
+	 * validate parameters
+	 */
+	if (fmt->fmt_validate) {
+		ret = (*fmt->fmt_validate)(ctx_flags, pfm_pmu_conf->regs.num_pmds,
+					   fmt_arg);
+		PFM_DBG("validate(0x%x,%p)=%d", ctx_flags, fmt_arg, ret);
+		if (ret)
+			goto error;
+	}
+
+	/*
+	 * check if buffer format wants to use perfmon
+	 * buffer allocation/mapping service
+	 */
+	size = 0;
+	if (fmt->fmt_getsize) {
+		ret = (*fmt->fmt_getsize)(ctx_flags, fmt_arg, &size);
+		if (ret) {
+			PFM_DBG("cannot get size ret=%d", ret);
+			goto error;
+		}
+	}
+
+	if (size) {
+		if (mode == PFM_COMPAT)
+			ret = pfm_smpl_buffer_alloc_compat(ctx, size, filp);
+		else
+			ret = pfm_smpl_buffer_alloc(ctx, size);
+		if (ret)
+			goto error;
+
+	}
+
+	if (fmt->fmt_init) {
+		ret = (*fmt->fmt_init)(ctx, ctx->smpl_addr, ctx_flags,
+				       pfm_pmu_conf->regs.num_pmds,
+				       fmt_arg);
+		if (ret)
+			goto error_buffer;
+	}
+	return 0;
+
+error_buffer:
+	pfm_release_buf_space(ctx, ctx->real_smpl_size);
+	/*
+	 * we do not call fmt_exit, if init has failed
+	 */
+	vfree(ctx->real_smpl_addr);
+error:
+	return ret;
+}
+
+/*
+ * interrupts are masked when entering this function.
+ * context must be in MASKED state when calling.
+ */
+static void pfm_unmask_monitoring(struct pfm_context *ctx,
+				  struct pfm_event_set *set)
+{
+	if (ctx->state != PFM_CTX_MASKED)
+		return;
+
+	PFM_DBG_ovfl("unmasking monitoring");
+
+	/*
+	 * must be done before calling
+	 * pfm_arch_unmask_monitoring()
+	 */
+	ctx->state = PFM_CTX_LOADED;
+
+	/*
+	 * we need to restore the PMDs because they
+	 * may have been modified by user while MASKED in which
+	 * case the actual registers were not updated
+	 *
+	 * XXX: could be avoided in system-wide mode
+	 */
+	pfm_arch_restore_pmds(ctx, set);
+
+	pfm_arch_unmask_monitoring(ctx, set);
+
+	set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
+
+	/*
+	 * reset set duration timer
+	 */
+	set->duration_start = sched_clock();
+}
+
+/*
+ * called from pfm_smpl_buffer_alloc_old() (IA64-COMPAT)
+ * and pfm_setup_smpl_fmt()
+ *
+ * interrupts are enabled, context is not locked.
+ */
+int pfm_smpl_buffer_alloc(struct pfm_context *ctx, size_t rsize)
+{
+#if PFM_ARCH_SMPL_ALIGN_SIZE > 0
+#define PFM_ALIGN_SMPL(a, f) (void *)((((unsigned long)(a))+(f-1)) & ~(f-1))
+#else
+#define PFM_ALIGN_SMPL(a, f) (a)
+#endif
+	void *addr, *real_addr;
+	size_t size, real_size;
+	int ret;
+
+	might_sleep();
+
+	/*
+	 * align page boundary
+	 */
+	size = PAGE_ALIGN(rsize);
+
+	/*
+	 * On some arch, it may be necessary to get an alignment greater
+	 * than page size to avoid certain cache effects (e.g., MIPS).
+	 * This is the reason for PFM_ARCH_SMPL_ALIGN_SIZE.
+	 */
+	real_size = size + PFM_ARCH_SMPL_ALIGN_SIZE;
+
+	PFM_DBG("buffer req_size=%zu actual_size=%zu before", rsize, size);
+
+	ret = pfm_reserve_buf_space(real_size);
+	if (ret)
+		return ret;
+
+	PFM_DBG("buffer req_size=%zu size=%zu real_size=%zu",
+		rsize,
+		size,
+		real_size);
+
+	/*
+	 * vmalloc can sleep. we do not hold
+	 * any spinlock and interrupts are enabled
+	 */
+	real_addr = addr = vmalloc(real_size);
+	if (!real_addr) {
+		PFM_DBG("cannot allocate sampling buffer");
+		goto unres;
+	}
+
+	/*
+	 * align the useable sampling buffer address to the arch requirement
+	 * This is a nop on most architectures
+	 */
+	addr = PFM_ALIGN_SMPL(real_addr, PFM_ARCH_SMPL_ALIGN_SIZE);
+
+	memset(addr, 0, real_size);
+
+	/*
+	 * due to cache aliasing, it may be necessary to flush the pages
+	 * on certain architectures (e.g., MIPS)
+	 */
+	pfm_cacheflush(addr, real_size);
+
+	/*
+	 * what needs to be freed
+	 */
+	ctx->real_smpl_addr = real_addr;
+	ctx->real_smpl_size = real_size;
+
+	/*
+	 * what is actually available to user
+	 */
+	ctx->smpl_addr = addr;
+	ctx->smpl_size = size;
+
+	PFM_DBG("kernel smpl buffer @ used=%p real=%p", addr, real_addr);
+
+	return 0;
+unres:
+	PFM_DBG("buffer req_size=%zu actual_size=%zu error", rsize, size);
+	pfm_release_buf_space(ctx, real_size);
+	return -ENOMEM;
+}
+
+void pfm_reset_pmds(struct pfm_context *ctx,
+		    struct pfm_event_set *set,
+		    int num_pmds,
+		    int reset_mode)
+{
+	u64 val, mask, new_seed;
+	struct pfm_pmd *reg;
+	unsigned int i, not_masked;
+
+	not_masked = ctx->state != PFM_CTX_MASKED;
+
+	PFM_DBG_ovfl("%s r_pmds=0x%llx not_masked=%d",
+		reset_mode == PFM_PMD_RESET_LONG ? "long" : "short",
+		(unsigned long long)set->reset_pmds[0],
+		not_masked);
+
+	pfm_stats_inc(reset_pmds_count);
+
+	for (i = 0; num_pmds; i++) {
+		if (test_bit(i, cast_ulp(set->reset_pmds))) {
+			num_pmds--;
+
+			reg = set->pmds + i;
+
+			val = reset_mode == PFM_PMD_RESET_LONG ? reg->long_reset : reg->short_reset;
+
+			if (reg->flags & PFM_REGFL_RANDOM) {
+				mask = reg->mask;
+				new_seed = random32();
+
+				/* construct a full 64-bit random value: */
+				if ((unlikely(mask >> 32) != 0))
+					new_seed |= (u64)random32() << 32;
+
+				/* counter values are negative numbers! */
+				val -= (new_seed & mask);
+			}
+
+			set->pmds[i].value = val;
+			reg->lval = val;
+
+			/*
+			 * not all PMD to reset are necessarily
+			 * counters
+			 */
+			if (not_masked)
+				pfm_write_pmd(ctx, i, val);
+
+			PFM_DBG_ovfl("set%u pmd%u sval=0x%llx",
+					set->id,
+					i,
+					(unsigned long long)val);
+		}
+	}
+
+	/*
+	 * done with reset
+	 */
+	bitmap_zero(cast_ulp(set->reset_pmds), i);
+
+	/*
+	 * make changes visible
+	 */
+	if (not_masked)
+		pfm_arch_serialize();
+}
+
+/*
+ * called from pfm_handle_work() and __pfm_restart()
+ * for system-wide and per-thread context to resume
+ * monitoring after a user level notification.
+ *
+ * In both cases, the context is locked and interrupts
+ * are disabled.
+ */
+static void pfm_resume_after_ovfl(struct pfm_context *ctx)
+{
+	struct pfm_smpl_fmt *fmt;
+	u32 rst_ctrl;
+	struct pfm_event_set *set;
+	u64 *reset_pmds;
+	void *hdr;
+	int state, ret;
+
+	hdr = ctx->smpl_addr;
+	fmt = ctx->smpl_fmt;
+	state = ctx->state;
+	set = ctx->active_set;
+	ret = 0;
+
+	if (hdr) {
+		rst_ctrl = 0;
+		prefetch(hdr);
+	} else
+		rst_ctrl= PFM_OVFL_CTRL_RESET;
+
+	/*
+	 * if using a sampling buffer format and it has a restart callback,
+	 * then invoke it. hdr may be NULL, if the format does not use a
+	 * perfmon buffer
+	 */
+	if (fmt && fmt->fmt_restart)
+		ret = (*fmt->fmt_restart)(state == PFM_CTX_LOADED, &rst_ctrl, hdr);
+
+	reset_pmds = set->reset_pmds;
+
+	PFM_DBG("fmt_restart=%d reset_count=%d set=%u r_pmds=0x%llx switch=%d ctx_state=%d",
+		ret,
+		ctx->flags.reset_count,
+		set->id,
+		(unsigned long long)reset_pmds[0],
+		(set->priv_flags & PFM_SETFL_PRIV_SWITCH),
+		state);
+
+	if (!ret) {
+		/*
+		 * switch set if needed
+		 */
+		if (set->priv_flags & PFM_SETFL_PRIV_SWITCH) {
+			set->priv_flags &= ~PFM_SETFL_PRIV_SWITCH;
+			pfm_switch_sets(ctx, NULL, PFM_PMD_RESET_LONG, 0);
+			set = ctx->active_set;
+		} else if (rst_ctrl & PFM_OVFL_CTRL_RESET) {
+			int nn;
+			nn = bitmap_weight(cast_ulp(set->reset_pmds),
+					   pfm_pmu_conf->regs.max_pmd);
+			if (nn)
+				pfm_reset_pmds(ctx, set, nn, PFM_PMD_RESET_LONG);
+		}
+
+		if (!(rst_ctrl & PFM_OVFL_CTRL_MASK))
+			pfm_unmask_monitoring(ctx, set);
+		else
+			PFM_DBG("stopping monitoring?");
+		ctx->state = PFM_CTX_LOADED;
+	}
+}
+
+/*
+ * This function is always called after pfm_stop has been issued
+ */
+static void pfm_flush_pmds(struct task_struct *task, struct pfm_context *ctx)
+{
+	struct pfm_event_set *set;
+	u64 *cnt_pmds;
+	u64 ovfl_mask;
+	u16 num_ovfls, i, first;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	first = pfm_pmu_conf->regs.first_intr_pmd;
+	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
+
+	/*
+	 * save active set
+	 * UP:
+	 * 	if not current task and due to lazy, state may
+	 * 	still be live
+	 * for system-wide, guaranteed to run on correct CPU
+	 */
+	if (__get_cpu_var(pmu_ctx) == ctx) {
+		/*
+		 * pending overflows have been saved by pfm_stop()
+		 */
+		pfm_save_pmds(ctx, ctx->active_set);
+		pfm_set_pmu_owner(NULL, NULL);
+		PFM_DBG("released ownership");
+	}
+
+	/*
+	 * look for pending interrupts
+	 */
+	list_for_each_entry(set, &ctx->set_list, list) {
+
+		if (!set->npend_ovfls)
+			continue;
+
+		num_ovfls = set->npend_ovfls;
+		PFM_DBG("set%u nintrs=%u", set->id, num_ovfls);
+
+		for (i = first; num_ovfls; i++) {
+			if (test_bit(i, cast_ulp(set->povfl_pmds))) {
+				/* only correct value for counters */
+				if(test_bit(i, cast_ulp(cnt_pmds))) {
+					set->pmds[i].value += 1 + ovfl_mask;
+				}
+				num_ovfls--;
+			}
+			PFM_DBG("pmd%u set=%u val=0x%llx",
+				i,
+				set->id,
+				(unsigned long long)set->pmds[i].value);
+		}
+		/*
+		 * we need to clear to prevent a pfm_getinfo_evtsets() from
+		 * returning stale data even after the context is unloaded
+		 */
+		set->npend_ovfls = 0;
+		bitmap_zero(cast_ulp(set->povfl_pmds),
+			    pfm_pmu_conf->regs.max_intr_pmd);
+	}
+}
+
+/*
+ * This function is called when we need to perform asynchronous
+ * work on a context. This function is called ONLY when about to
+ * return to user mode (very much like with signals handling).
+ *
+ * There are several reasons why we come here:
+ *
+ *  - per-thread mode, not self-monitoring, to reset the counters
+ *    after a pfm_restart() by the thread controlling the context
+ *
+ *  - because we are zombie and we need to cleanup our state
+ *
+ *  - because we need to block after an overflow notification
+ *    on a context with the PFM_OVFL_NOTIFY_BLOCK flag
+ *
+ * This function is never called for a system-wide context.
+ *
+ * pfm_handle_work() can be called with interrupts enabled
+ * (TIF_NEED_RESCHED) or disabled. The down_interruptible
+ * call may sleep, therefore we must re-enable interrupts
+ * to avoid deadlocks. It is safe to do so because this function
+ * is called ONLY when returning to user level, in which case
+ * there is no risk of kernel stack overflow due to deep
+ * interrupt nesting.
+ */
+void pfm_handle_work(struct pt_regs *regs)
+{
+	struct pfm_context *ctx;
+	unsigned long flags, dummy_flags;
+	int type, ret, release_info;
+
+#ifdef CONFIG_PPC
+	/*
+	 * This is just a temporary fix. Obviously we'd like to fix the powerpc
+	 * code to make that check before calling __pfm_handle_work() to
+	 * prevent the function call overhead, but the call is made from assembly
+	 * code, so it will take a little while to figure out how to perform the
+	 * check correctly.
+	 */
+	if (!test_thread_flag(TIF_PERFMON_WORK))
+		return;
+#endif
+
+	if (!user_mode(regs))
+		return;
+
+	clear_thread_flag(TIF_PERFMON_WORK);
+
+	pfm_stats_inc(handle_work_count);
+
+	ctx = current->pfm_context;
+	if (ctx == NULL) {
+		PFM_DBG("[%d] has no ctx", current->pid);
+		return;
+	}
+
+	BUG_ON(ctx->flags.system);
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	type = ctx->flags.work_type;
+	ctx->flags.work_type = PFM_WORK_NONE;
+
+	PFM_DBG("work_type=%d reset_count=%d",
+		type,
+		ctx->flags.reset_count);
+
+	switch(type) {
+		case PFM_WORK_ZOMBIE:
+			goto do_zombie;
+		case PFM_WORK_RESET:
+			/* simply reset, no blocking */
+			goto skip_blocking;
+		case PFM_WORK_NONE:
+			PFM_DBG("unexpected PFM_WORK_NONE");
+			goto nothing_todo;
+		case PFM_WORK_BLOCK:
+			break;
+		default:
+			PFM_DBG("unkown type=%d", type);
+			goto nothing_todo;
+	}
+
+	/*
+	 * restore interrupt mask to what it was on entry.
+	 * Could be enabled/disabled.
+	 */
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/*
+	 * force interrupt enable because of down_interruptible()
+	 */
+	local_irq_enable();
+
+	PFM_DBG("before block sleeping");
+
+	/*
+	 * may go through without blocking on SMP systems
+	 * if restart has been received already by the time we call down()
+	 */
+	ret = wait_for_completion_interruptible(&ctx->restart_complete);
+
+	PFM_DBG("after block sleeping ret=%d", ret);
+
+	/*
+	 * lock context and mask interrupts again
+	 * We save flags into a dummy because we may have
+	 * altered interrupts mask compared to entry in this
+	 * function.
+	 */
+	spin_lock_irqsave(&ctx->lock, dummy_flags);
+
+	if (ctx->state == PFM_CTX_ZOMBIE)
+		goto do_zombie;
+
+	/*
+	 * in case of interruption of down() we don't restart anything
+	 */
+	if (ret < 0)
+		goto nothing_todo;
+
+skip_blocking:
+	/*
+	 * iterate over the number of pending resets
+	 * There are certain situations where there may be
+	 * multiple notifications sent before a pfm_restart().
+	 * As such, it may be that multiple pfm_restart() are
+	 * issued before the monitored thread gets to
+	 * pfm_handle_work(). To avoid losing restarts, pfm_restart()
+	 * increments a counter (reset_counts). Here, we take this
+	 * into account by potentially calling pfm_resume_after_ovfl()
+	 * multiple times. It is up to the sampling format to take the
+	 * appropriate actions.
+	 */
+	while(ctx->flags.reset_count) {
+		pfm_resume_after_ovfl(ctx);
+		ctx->flags.reset_count--;
+	}
+
+nothing_todo:
+	/*
+	 * restore flags as they were upon entry
+	 */
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	return;
+
+do_zombie:
+	PFM_DBG("context is zombie, bailing out");
+
+	__pfm_unload_context(ctx, &release_info);
+
+	/*
+	 * keep the spinlock check happy
+	 */
+	spin_unlock(&ctx->lock);
+
+	/*
+	 * enable interrupt for vfree()
+	 */
+	local_irq_enable();
+
+	/*
+	 * cancel timer now that context is unlocked
+	 */
+	if (release_info & 0x2) {
+		ret = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG("timeout cancel=%d", ret);
+	}
+
+	/*
+	 * actual context free
+	 */
+	pfm_context_free(ctx);
+
+	/*
+	 * restore interrupts as they were upon entry
+	 */
+	local_irq_restore(flags);
+
+	/* always true */
+	if (release_info & 0x1)
+		pfm_release_session(0, 0);
+}
+
+static int pfm_notify_user(struct pfm_context *ctx)
+{
+	if (ctx->state == PFM_CTX_ZOMBIE) {
+		PFM_DBG("ignoring overflow notification, owner is zombie");
+		return 0;
+	}
+	PFM_DBG_ovfl("waking up somebody");
+
+	wake_up_interruptible(&ctx->msgq_wait);
+
+	/*
+	 * it is safe to call kill_fasync() from an interrupt
+	 * handler. kill_fasync()  grabs two RW locks (fasync_lock,
+	 * tasklist_lock) in read mode. There is conflict only in
+	 * case the PMU interrupt occurs during a write mode critical
+	 * section. This cannot happen because for both locks, the
+	 * write mode is always using interrupt masking (write_lock_irq).
+	 */
+	kill_fasync (&ctx->async_queue, SIGIO, POLL_IN);
+
+	return 0;
+}
+
+/*
+ * send a counter overflow notification message to
+ * user. First appends the message to the queue, then
+ * wake up ay waiter on the file descriptor
+ *
+ * context is locked and interrupts are disabled (no preemption).
+ */
+int pfm_ovfl_notify_user(struct pfm_context *ctx,
+			struct pfm_event_set *set,
+			unsigned long ip)
+{
+	union pfarg_msg *msg = NULL;
+	u64 *ovfl_pmds;
+
+	if (!ctx->flags.no_msg) {
+		msg = pfm_get_new_msg(ctx);
+		if (msg == NULL) {
+			/*
+			 * when message queue fills up it is because the user
+			 * did not extract the message, yet issued
+			 * pfm_restart(). At this point, we stop sending
+			 * notification, thus the user will not be able to get
+			 * new samples when using the default format.
+			 */
+			PFM_DBG_ovfl("no more notification msgs");
+			return -1;
+		}
+
+		msg->pfm_ovfl_msg.msg_type = PFM_MSG_OVFL;
+		msg->pfm_ovfl_msg.msg_ovfl_pid = current->pid;
+		msg->pfm_ovfl_msg.msg_active_set = set->id;
+
+		ovfl_pmds = msg->pfm_ovfl_msg.msg_ovfl_pmds;
+
+		/*
+		 * copy bitmask of all pmd that interrupted last
+		 */
+		bitmap_copy(cast_ulp(ovfl_pmds), cast_ulp(set->ovfl_pmds),
+			    pfm_pmu_conf->regs.max_intr_pmd);
+
+		msg->pfm_ovfl_msg.msg_ovfl_cpu = smp_processor_id();
+		msg->pfm_ovfl_msg.msg_ovfl_tid = current->tgid;
+		msg->pfm_ovfl_msg.msg_ovfl_ip = ip;
+
+		pfm_stats_inc(ovfl_notify_count);
+	}
+
+	PFM_DBG_ovfl("ip=0x%lx o_pmds=0x%llx",
+		     ip,
+		     (unsigned long long)set->ovfl_pmds[0]);
+
+	return pfm_notify_user(ctx);
+}
+
+/*
+ * In per-thread mode, when not self-monitoring, perfmon
+ * sends a 'end' notification message when the monitored
+ * thread where the context is attached is exiting.
+ *
+ * This helper message alleviate the need to track the activity
+ * of the thread/process when it is not directly related, i.e.,
+ * was attached vs was forked/execd. In other words, no need
+ * to keep the thread ptraced.
+ *
+ * the context must be locked and interrupts disabled.
+ */
+static int pfm_end_notify_user(struct pfm_context *ctx)
+{
+	union pfarg_msg *msg;
+
+	msg = pfm_get_new_msg(ctx);
+	if (msg == NULL) {
+		PFM_ERR("%s no more msgs", __FUNCTION__);
+		return -1;
+	}
+	/* no leak */
+	memset(msg, 0, sizeof(*msg));
+
+	msg->type = PFM_MSG_END;
+
+	PFM_DBG("end msg: msg=%p no_msg=%d",
+		msg,
+		ctx->flags.no_msg);
+
+	return pfm_notify_user(ctx);
+}
+
+/*
+ * called only from exit_thread(): task == current
+ * we come here only if current has a context
+ * attached (loaded or masked or zombie)
+ */
+void __pfm_exit_thread(struct task_struct *task)
+{
+	struct pfm_context *ctx;
+	unsigned long flags;
+	int free_ok = 0, release_info = 0;
+	int ret;
+
+	ctx  = task->pfm_context;
+
+	BUG_ON(ctx->flags.system);
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	PFM_DBG("state=%d", ctx->state);
+
+	/*
+	 * __pfm_unload_context() cannot fail
+	 * in the context states we are interested in
+	 */
+	switch (ctx->state) {
+	case PFM_CTX_LOADED:
+	case PFM_CTX_MASKED:
+		__pfm_unload_context(ctx, &release_info);
+		pfm_end_notify_user(ctx);
+		break;
+	case PFM_CTX_ZOMBIE:
+		__pfm_unload_context(ctx, &release_info);
+		free_ok = 1;
+		break;
+	default:
+		BUG_ON(ctx->state != PFM_CTX_LOADED);
+		break;
+	}
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+#ifdef CONFIG_PPC
+	pfm_arch_remove_ctxsw_hook(ctx);
+	PFM_DBG("unhook SPE ctxsw. exit_thread");
+#endif
+
+	/*
+	 * cancel timer now that context is unlocked
+	 */
+	if (release_info & 0x2) {
+		ret = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG("timeout cancel=%d", ret);
+	}
+
+	if (release_info & 0x1)
+		pfm_release_session(0, 0);
+
+	/*
+	 * All memory free operations (especially for vmalloc'ed memory)
+	 * MUST be done with interrupts ENABLED.
+	 */
+	if (free_ok)
+		pfm_context_free(ctx);
+}
+
+/*
+ * CPU hotplug event nofication callback
+ *
+ * We use the callback to do manage the sysfs interface.
+ * Note that the actual shutdown of monitoring on the CPU
+ * is done in pfm_cpu_disable(), see comments there for more
+ * information.
+ */
+static int pfm_cpu_notify(struct notifier_block *nfb,
+			  unsigned long action, void *hcpu)
+{
+	unsigned int cpu = (unsigned long)hcpu;
+	int ret = NOTIFY_OK;
+
+	pfm_pmu_conf_get(0);
+
+	switch (action) {
+	case CPU_ONLINE:
+		pfm_debugfs_add_cpu(cpu);
+		PFM_INFO("CPU%d is online", cpu);
+		break;
+	case CPU_UP_PREPARE:
+		PFM_INFO("CPU%d prepare online", cpu);
+		break;
+	case CPU_UP_CANCELED:
+		pfm_debugfs_del_cpu(cpu);
+		PFM_INFO("CPU%d is up canceled", cpu);
+		break;
+	case CPU_DOWN_PREPARE:
+		PFM_INFO("CPU%d prepare offline", cpu);
+		break;
+	case CPU_DOWN_FAILED:
+		PFM_INFO("CPU%d is down failed", cpu);
+		break;
+	case CPU_DEAD:
+		pfm_debugfs_del_cpu(cpu);
+		PFM_INFO("CPU%d is offline", cpu);
+		break;
+	}
+	pfm_pmu_conf_put();
+	return ret;
+}
+
+static struct notifier_block pfm_cpu_notifier ={
+	.notifier_call = pfm_cpu_notify
+};
+
+/*
+ * called from cpu_init() and pfm_pmu_register()
+ */
+void __pfm_init_percpu(void *dummy)
+{
+	struct hrtimer *h;
+
+	h = &__get_cpu_var(pfm_hrtimer);
+
+	pfm_arch_init_percpu();
+
+	/*
+	 * initialize per-cpu high res timer
+	 */
+	hrtimer_init(h, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	h->function = pfm_handle_switch_timeout;
+}
+
+/*
+ * global initialization routine, executed only once
+ */
+int __init pfm_init(void)
+{
+	int ret;
+
+	PFM_LOG("version %u.%u, compiled: " __DATE__ ", " __TIME__,
+		PFM_VERSION_MAJ, PFM_VERSION_MIN);
+
+	pfm_ctx_cachep = kmem_cache_create("pfm_context",
+				   sizeof(struct pfm_context)+PFM_ARCH_CTX_SIZE,
+				   SLAB_HWCACHE_ALIGN, 0, NULL);
+	if (pfm_ctx_cachep == NULL) {
+		PFM_ERR("cannot initialize context slab");
+		goto error_disable;
+	}
+
+	ret = pfm_sets_init();
+	if (ret)
+		goto error_disable;
+
+	if (pfm_init_fs())
+		goto error_disable;
+
+	if (pfm_init_sysfs())
+		goto error_disable;
+
+	/* not critical, so no error checking */
+	pfm_init_debugfs();
+
+	/*
+	 * one time, arch-specific global initialization
+	 */
+	if (pfm_arch_init())
+		goto error_disable;
+
+	/*
+	 * register CPU hotplug event notifier
+	 */
+	ret = register_cpu_notifier(&pfm_cpu_notifier);
+	if (!ret)
+		return 0;
+
+error_disable:
+	PFM_INFO("perfmon is disabled due to initialization error");
+	perfmon_disabled = 1;
+	return -1;
+}
+
+/*
+ * must use subsys_initcall() to ensure that the perfmon2 core
+ * is initialized before any PMU description module when they are
+ * compiled in.
+ */
+subsys_initcall(pfm_init);
+
+/*
+ * function used to start monitoring. When operating in per-thread
+ * mode and when not self-monitoring, the monitored thread must be
+ * stopped.
+ *
+ * The pfarg_start argument is optional and may be used to designate
+ * the initial event set to activate. Wehn missing, the last active
+ * set is used. For the first activation, set0 is used.
+ *
+ * On some architectures, e.g., IA-64, it may be possible to start monitoring
+ * without calling this function under certain conditions (per-thread and self
+ * monitoring).
+ *
+ * the context is locked and interrupts are disabled.
+ */
+int __pfm_start(struct pfm_context *ctx, struct pfarg_start *start)
+{
+	struct task_struct *task, *owner_task;
+	struct pfm_event_set *new_set, *old_set;
+	int is_self;
+
+	task = ctx->task;
+
+	/*
+	 * UNLOADED: error
+	 * LOADED  : normal start, nop if started unless set is different
+	 * MASKED  : nop or change set when unmasking
+	 * ZOMBIE  : cannot happen
+	 */
+	if (ctx->state == PFM_CTX_UNLOADED)
+		return -EINVAL;
+
+	old_set = new_set = ctx->active_set;
+
+	/*
+	 * always the case for system-wide
+	 */
+	if (task == NULL)
+		task = current;
+
+	is_self = task == current;
+
+	/*
+	 * argument is provided?
+	 */
+	if (start) {
+		/*
+		 * find the set to load first
+		 */
+		new_set = pfm_find_set(ctx, start->start_set, 0);
+		if (new_set == NULL) {
+			PFM_DBG("event set%u does not exist",
+				start->start_set);
+			return -EINVAL;
+		}
+	}
+
+	PFM_DBG("cur_set=%u req_set=%u",
+		old_set->id,
+		new_set->id);
+
+	/*
+	 * if we need to change the active set we need
+	 * to check if we can access the PMU
+	 */
+	if (new_set != old_set) {
+
+		owner_task = __get_cpu_var(pmu_owner);
+		/*
+		 * system-wide: must run on the right CPU
+		 * per-thread : must be the owner of the PMU context
+		 *
+		 * pfm_switch_sets() returns with monitoring stopped
+		 */
+		if (is_self) {
+			pfm_switch_sets(ctx, new_set, PFM_PMD_RESET_LONG, 1);
+		} else {
+			/*
+			 * In a UP kernel, the PMU may contain the state
+			 * of the task we want to operate on, yet the task
+			 * may be switched out (lazy save). We need to save
+			 * current state (old_set), switch active_set and
+			 * mark it for reload.
+			 */
+			if (owner_task == task)
+				pfm_save_pmds(ctx, old_set);
+			ctx->active_set = new_set;
+			new_set->priv_flags |= PFM_SETFL_PRIV_MOD_BOTH;
+		}
+	}
+	/*
+	 * mark as started
+	 * must be done before calling pfm_arch_start()
+	 */
+	ctx->flags.started = 1;
+
+
+	pfm_arch_start(task, ctx, new_set);
+
+	/*
+	 * we check whether we had a pending ovfl before restarting.
+	 * If so we need to regenerate the interrupt to make sure we
+	 * keep recorded samples. For non-self monitoring this check
+	 * is done in the pfm_ctxswin_thread() routine.
+	 *
+	 * we check new_set/old_set because pfm_switch_sets() already
+	 * takes care of replaying the pending interrupts
+	 */
+	if (is_self && new_set != old_set && new_set->npend_ovfls) {
+		pfm_arch_resend_irq();
+		pfm_stats_inc(ovfl_intr_replay_count);
+	}
+
+	/*
+	 * activate timeout for system-wide, self-montoring
+	 */
+	if (is_self && new_set->flags & PFM_SETFL_TIME_SWITCH) {
+		hrtimer_start(&__get_cpu_var(pfm_hrtimer), new_set->hrtimer_exp, HRTIMER_MODE_REL);
+		PFM_DBG("armed timeout for set%u", new_set->id);
+	}
+
+	/*
+	 * we restart total duration even if context was
+	 * already started. In that case, counts are simply
+	 * reset.
+	 *
+	 * For per-thread, if not self-monitoring, the statement
+	 * below will have no effect because thread is stopped.
+	 * The field is reset of ctxsw in.
+	 */
+	new_set->duration_start = sched_clock();
+
+	return 0;
+}
+
+/*
+ * function used to stop monitoring. When operating in per-thread
+ * mode and when not self-monitoring, the monitored thread must be
+ * stopped.
+ *
+ * the context is locked and interrupts are disabled.
+ */
+int __pfm_stop(struct pfm_context *ctx, int *release_info)
+{
+	struct pfm_event_set *set;
+	struct task_struct *task;
+	u64 now;
+	int state;
+
+	*release_info = 0;
+
+	now = sched_clock();
+	state = ctx->state;
+	set = ctx->active_set;
+
+	/*
+	 * context must be attached (zombie cannot happen)
+	 */
+	if (state == PFM_CTX_UNLOADED)
+		return -EINVAL;
+
+	task = ctx->task;
+
+	PFM_DBG("ctx_task=[%d] ctx_state=%d is_system=%d",
+		task ? task->pid : -1,
+		state,
+		ctx->flags.system);
+
+	/*
+	 * this happens for system-wide context
+	 */
+	if (task == NULL)
+		task = current;
+
+	/*
+	 * compute elapsed time
+	 *
+	 * unless masked, compute elapsed duration, stop timeout
+	 */
+	if (task == current && state == PFM_CTX_LOADED) {
+		/*
+		 * timeout cancel must be deferred until context is unlocked
+		 * to avoid race with pfm_handle_switch_timeout()
+		 */
+		if (set->flags & PFM_SETFL_TIME_SWITCH)
+			*release_info |= 0x2;
+
+		set->duration += now - set->duration_start;
+	}
+
+	pfm_arch_stop(task, ctx, set);
+
+	ctx->flags.started = 0;
+	/*
+	 * starting now, in-flight PMU interrupt for this context
+	 * are treated as spurious
+	 */
+	return 0;
+}
+
+/*
+ * function called from sys_pfm_restart(). It is used when overflow
+ * notification is requested. For each notification received, the user
+ * must call pfm_restart() to indicate to the kernel that it is done
+ * processing the notification.
+ *
+ * When the caller is doing user level sampling, this function resets
+ * the overflowed counters and resumes monitoring which is normally stopped
+ * during notification (always the consequence of a counter overflow).
+ *
+ * When using a sampling format, the format restart() callback is invoked,
+ * overflowed PMDS may be reset based upon decision from sampling format.
+ *
+ * When operating in per-thread mode, and when not self-monitoring, the
+ * monitored thread DOES NOT need to be stopped, unlike for many other calls.
+ *
+ * This means that the effect of the restart may not necessarily be observed
+ * right when returning from the call. For instance, counters may not already
+ * be reset in the other thread.
+ *
+ * When operating in system-wide, the caller must be running on the monitored
+ * CPU.
+ *
+ * The context is locked and interrupts are disabled.
+ *
+ */
+int __pfm_restart(struct pfm_context *ctx, int *unblock)
+{
+	int state;
+
+	state = ctx->state;
+
+	PFM_DBG("state=%d can_restart=%d reset_count=%d",
+		state,
+		ctx->flags.can_restart,
+		ctx->flags.reset_count);
+
+	*unblock = 0;
+
+	switch (state) {
+	case PFM_CTX_MASKED:
+		break;
+	case PFM_CTX_LOADED:
+		if (ctx->smpl_addr && ctx->smpl_fmt->fmt_restart)
+			break;
+	default:
+		PFM_DBG("invalid state=%d", state);
+		return -EBUSY;
+	}
+
+	/*
+	 * first check if allowed to restart, i.e., notifications received
+	 */
+	if (!ctx->flags.can_restart) {
+		PFM_DBG("no restart can_restart=0");
+		return -EBUSY;
+	}
+
+	pfm_stats_inc(pfm_restart_count);
+
+	/*
+	 * at this point, the context is either LOADED or MASKED
+	 */
+	ctx->flags.can_restart--;
+
+	/*
+	 * handle self-monitoring case and system-wide
+	 */
+	if (ctx->task == current || ctx->flags.system) {
+		pfm_resume_after_ovfl(ctx);
+		return 0;
+	}
+
+	/*
+	 * restart another task
+	 */
+
+	/*
+	 * if blocking, then post the semaphore if PFM_CTX_MASKED, i.e.
+	 * the task is blocked or on its way to block. That's the normal
+	 * restart path. If the monitoring is not masked, then the task
+	 * can be actively monitoring and we cannot directly intervene.
+	 * Therefore we use the trap mechanism to catch the task and
+	 * force it to reset the buffer/reset PMDs.
+	 *
+	 * if non-blocking, then we ensure that the task will go into
+	 * pfm_handle_work() before returning to user mode.
+	 *
+	 * We cannot explicitly reset another task, it MUST always
+	 * be done by the task itself. This works for system wide because
+	 * the tool that is controlling the session is logically doing
+	 * "self-monitoring".
+	 */
+	if (ctx->flags.block && state == PFM_CTX_MASKED) {
+		PFM_DBG("unblocking [%d]", ctx->task->pid);
+		/*
+		 * It is not possible to call complete() with the context locked
+		 * otherwise we have a potential deadlock with the PMU context
+		 * switch code due to a lock inversion between task_rq_lock()
+		 * and the context lock.
+		 * Instead we mark whether or not we need to issue the complete
+		 * and we invoke the function once the context lock is released
+		 * in sys_pfm_restart()
+		 */
+		*unblock = 1;
+	} else {
+		PFM_DBG("[%d] armed exit trap", ctx->task->pid);
+		ctx->flags.work_type = PFM_WORK_RESET;
+		set_tsk_thread_flag(ctx->task, TIF_PERFMON_WORK);
+	}
+	ctx->flags.reset_count++;
+	return 0;
+}
+
+/*
+ * function used to attach a context to either a CPU or a thread.
+ * In per-thread mode, and when not self-monitoring, the thread must be
+ * stopped. In system-wide mode, the cpu specified in the pfarg_load.load_tgt
+ * argument must be the current CPU.
+ *
+ * The function must be called with the context locked and interrupts disabled.
+ */
+int __pfm_load_context(struct pfm_context *ctx, struct pfarg_load *req,
+		       struct task_struct *task)
+{
+	struct pfm_event_set *set;
+	struct pfm_context *old;
+	int mycpu;
+	int ret;
+
+	mycpu = smp_processor_id();
+
+	/*
+	 * system-wide: check we are running on the desired CPU
+	 */
+	if (ctx->flags.system && req->load_pid != mycpu) {
+		PFM_DBG("running on wrong CPU: %u vs. %u",
+			mycpu, req->load_pid);
+		return -EINVAL;
+	}
+
+	/*
+	 * locate first set to activate
+	 */
+	set = pfm_find_set(ctx, req->load_set, 0);
+	if (set == NULL) {
+		PFM_DBG("event set%u does not exist", req->load_set);
+		return -EINVAL;
+	}
+
+	/*
+	 * assess sanity of event sets, initialize set state
+	 */
+	ret = pfm_prepare_sets(ctx, set);
+	if (ret) {
+		PFM_DBG("invalid next field pointers in the sets");
+		return -EINVAL;
+	}
+
+	PFM_DBG("load_pid=%d set=%u set_flags=0x%x",
+		req->load_pid,
+		set->id,
+		set->flags);
+
+	/*
+	 * per-thread:
+	 *   - task to attach to is checked in sys_pfm_load_context() to avoid
+	 *     locking issues. if found, and not self,  task refcount was incremented.
+	 */
+	if (ctx->flags.system) {
+		ctx->cpu = mycpu;
+		ctx->task = NULL;
+		task = current;
+	} else {
+		old = cmpxchg(&task->pfm_context, NULL, ctx);
+		if (old != NULL) {
+			PFM_DBG("load_pid=%d has a context "
+				"old=%p new=%p cur=%p",
+				req->load_pid,
+				old,
+				ctx,
+				task->pfm_context);
+			return -EEXIST;
+		}
+		ctx->task = task;
+		ctx->cpu = -1;
+	}
+
+	/*
+	 * perform any architecture specific actions
+	 */
+	ret = pfm_arch_load_context(ctx, set, ctx->task);
+	if (ret)
+		goto error_noload;
+
+	/*
+	 * now reserve the session, before we can proceed with
+	 * actually accessing the PMU hardware
+	 */
+	ret = pfm_reserve_session(ctx->flags.system, ctx->cpu);
+	if (ret)
+		goto error;
+
+	/*
+	 * commit active set
+	 */
+	ctx->active_set = set;
+
+	set->runs++;
+
+	/*
+	 * self-monitoring (incl. system-wide)
+	 */
+	if (task == current) {
+#ifndef CONFIG_SMP
+		/*
+		 * in UP mode, because of lazy save/restore
+		 * there may already be valid state on the PMU.
+		 * We need to push it out before we can load the
+		 * next state
+		 */
+		struct pfm_context *ctxp;
+		ctxp = __get_cpu_var(pmu_ctx);
+		if (ctxp)
+			pfm_save_prev_context(ctxp);
+#endif
+		pfm_set_last_cpu(ctx, mycpu);
+		pfm_inc_activation();
+		pfm_set_activation(ctx);
+
+		/*
+		 * load PMD from set
+		 * load PMC from set
+		 */
+		pfm_arch_restore_pmds(ctx, set);
+		pfm_arch_restore_pmcs(ctx, set);
+
+		/*
+		 * set new ownership
+		 */
+		pfm_set_pmu_owner(ctx->task, ctx);
+	} else {
+		/* force a full reload */
+		ctx->last_act = PFM_INVALID_ACTIVATION;
+		pfm_set_last_cpu(ctx, -1);
+		set->priv_flags |= PFM_SETFL_PRIV_MOD_BOTH;
+		PFM_DBG("context loaded next ctxswin for [%d]", task->pid);
+	}
+
+	if (!ctx->flags.system) {
+#ifdef CONFIG_PPC
+		/*pfm_arch_add_ctxsw_hook(ctx);*/
+#else
+		set_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
+		PFM_DBG("[%d] set TIF", task->pid);
+#endif
+	}
+
+	ctx->flags.work_type = PFM_WORK_NONE;
+	ctx->flags.reset_count = 0;
+
+	/*
+	 * reset message queue
+	 */
+	ctx->msgq_head = ctx->msgq_tail = 0;
+
+	ctx->state = PFM_CTX_LOADED;
+
+	return 0;
+
+error:
+	pfm_arch_unload_context(ctx, task);
+error_noload:
+	/*
+	 * detach context
+	 */
+	if (!ctx->flags.system)
+		task->pfm_context = NULL;
+
+	return ret;
+}
+
+/*
+ * Function used to detach a context from either a CPU or a thread.
+ * In the per-thread case and when not self-monitoring, the thread must be
+ * stopped. After the call, the context is detached and monitoring is stopped.
+ *
+ * The function must be called with the context locked and interrupts disabled.
+ *
+ * release_info value upon return:
+ * 	- 0x0 : cannot free context, no timer to cancel
+ * 	- 0x1 : must free context
+ * 	- 0x2 : cannot free context, must cancel timer
+ * 	- 0x3 : must free context, must cancel timer
+ */
+int __pfm_unload_context(struct pfm_context *ctx, int *release_info)
+{
+	struct task_struct *task;
+	struct pfm_event_set *set;
+	int ret, is_self;
+
+	PFM_DBG("ctx_state=%d task [%d]", ctx->state, ctx->task ? ctx->task->pid : -1);
+
+	*release_info = 0;
+
+	/*
+	 * unload only when necessary
+	 */
+	if (ctx->state == PFM_CTX_UNLOADED)
+		return 0;
+
+	task = ctx->task;
+	set = ctx->active_set;
+	is_self = ctx->flags.system || task == current;
+
+	/*
+	 * stop monitoring
+	 */
+	ret = __pfm_stop(ctx, release_info);
+	if (ret)
+		return ret;
+
+	ctx->state = PFM_CTX_UNLOADED;
+	ctx->flags.can_restart = 0;
+
+	/*
+	 * save PMD registers
+	 * release ownership
+	 */
+	pfm_flush_pmds(task, ctx);
+
+	/*
+	 * arch-specific unload operations
+	 */
+	pfm_arch_unload_context(ctx, task);
+
+	/*
+	 * per-thread: disconnect from monitored task
+	 */
+	if (task) {
+		task->pfm_context = NULL;
+		ctx->task = NULL;
+		clear_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
+		clear_tsk_thread_flag(task, TIF_PERFMON_WORK);
+	}
+
+	*release_info |= 0x1;
+
+	return 0;
+}
+
+static inline int pfm_ctx_flags_sane(u32 ctx_flags)
+{
+	if (ctx_flags & PFM_FL_SYSTEM_WIDE) {
+		if (ctx_flags & PFM_FL_NOTIFY_BLOCK) {
+			PFM_DBG("cannot use blocking mode in syswide mode");
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+/*
+ * check for permissions to create a context.
+ *
+ * A sysadmin may decide to restrict creation of per-thread
+ * and/or system-wide context to a group of users using the
+ * group id via /sys/kernel/perfmon/task_group  and
+ * /sys/kernel/perfmon/sys_group.
+ *
+ * Once we identify a user level package which can be used
+ * to grant/revoke Linux capabilites at login via PAM, we will
+ * be able to use capabilities. We would also need to increase
+ * the size of cap_t to support more than 32 capabilities (it
+ * is currently defined as u32 and 32 capabilities are alrady
+ * defined).
+ */
+static inline int pfm_ctx_permissions(u32 ctx_flags)
+{
+	if (  (ctx_flags & PFM_FL_SYSTEM_WIDE)
+	   && pfm_controls.sys_group != PFM_GROUP_PERM_ANY
+	   && !in_group_p(pfm_controls.sys_group)) {
+		PFM_DBG("user group not allowed to create a syswide ctx");
+		return -EPERM;
+	} else if (pfm_controls.task_group != PFM_GROUP_PERM_ANY
+		   && !in_group_p(pfm_controls.task_group)) {
+		PFM_DBG("user group not allowed to create a task context");
+		return -EPERM;
+	}
+	return 0;
+}
+
+/*
+ * function used to allocate a new context. A context is allocated along
+ * with the default event set. If a sampling format is used, the buffer
+ * may be allocated and initialized.
+ *
+ * The file descriptor identifying the context is allocated and returned
+ * to caller.
+ *
+ * This function operates with no locks and interrupts are enabled.
+ * return:
+ * 	>=0: the file descriptor to identify the context
+ * 	<0 : the error code
+ */
+int __pfm_create_context(struct pfarg_ctx *req,
+			 struct pfm_smpl_fmt *fmt,
+			 void *fmt_arg,
+			 int mode,
+			 struct pfm_context **new_ctx)
+{
+	struct pfm_context *ctx;
+	struct pfm_event_set *set;
+	struct file *filp = NULL;
+	u32 ctx_flags;
+	int fd = 0, ret;
+
+	ctx_flags = req->ctx_flags;
+
+	/* Increase refcount on PMU description */
+	ret = pfm_pmu_conf_get(1);
+	if (ret < 0)
+		goto error_conf;
+
+	ret = pfm_ctx_flags_sane(ctx_flags);
+	if (ret < 0)
+		goto error_alloc;
+
+	ret = pfm_ctx_permissions(ctx_flags);
+	if (ret < 0)
+		goto error_alloc;
+
+	/*
+	 * we can use GFP_KERNEL and potentially sleep because we do
+	 * not hold any lock at this point.
+	 */
+	might_sleep();
+	ret = -ENOMEM;
+	ctx = kmem_cache_zalloc(pfm_ctx_cachep, GFP_KERNEL);
+	if (!ctx)
+		goto error_alloc;
+
+	ret = pfm_pmu_acquire();
+	if (ret)
+		goto error_file;
+	/*
+	 * check if PMU is usable
+	 */
+	if (!(pfm_pmu_conf->regs.num_pmcs && pfm_pmu_conf->regs.num_pmcs)) {
+		PFM_DBG("no usable PMU registers");
+		ret = -EBUSY;
+		goto error_file;
+	}
+
+	/*
+	 * link to format, must be done first for correct
+	 * error handling in pfm_context_free()
+	 */
+	ctx->smpl_fmt = fmt;
+
+	ret = -ENFILE;
+	fd = pfm_alloc_fd(&filp);
+	if (fd < 0)
+		goto error_file;
+
+	/*
+	 * context is unloaded
+	 */
+	ctx->state = PFM_CTX_UNLOADED;
+
+	INIT_LIST_HEAD(&ctx->set_list);
+
+	/*
+	 * initialization of context's flags
+	 * must be done before pfm_find_set()
+	 */
+	ctx->flags.block = (ctx_flags & PFM_FL_NOTIFY_BLOCK) ? 1 : 0;
+	ctx->flags.system = (ctx_flags & PFM_FL_SYSTEM_WIDE) ? 1: 0;
+	ctx->flags.no_msg = (ctx_flags & PFM_FL_OVFL_NO_MSG) ? 1: 0;
+	ctx->flags.ia64_v20_compat = mode == PFM_COMPAT ? 1 : 0;
+
+	ctx->ctxsw_notifier.notifier_call = NULL;
+	ctx->ctxsw_notifier.next = NULL;
+	ctx->ctxsw_notifier.priority = 0;
+
+	/*
+	 * initialize arch-specific section
+	 * must be done before fmt_init()
+	 *
+	 * XXX: fix dependency with fmt_init()
+	 */
+	ret = pfm_arch_context_create(ctx, ctx_flags);
+	if (ret)
+		goto error_set;
+
+	ret = -ENOMEM;
+	/*
+	 * create initial set
+	 */
+	if (pfm_find_set(ctx, 0, 1) == NULL)
+		goto error_set;
+
+	set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+
+	pfm_init_evtset(set);
+
+	/*
+	 * does the user want to sample?
+	 */
+	if (fmt) {
+		ret = pfm_setup_smpl_fmt(fmt, fmt_arg, ctx, ctx_flags,
+					 mode, filp);
+		if (ret)
+			goto error_set;
+	}
+
+	filp->private_data = ctx;
+
+	spin_lock_init(&ctx->lock);
+	init_completion(&ctx->restart_complete);
+
+	ctx->last_act = PFM_INVALID_ACTIVATION;
+	pfm_set_last_cpu(ctx, -1);
+
+	/*
+	 * initialize notification message queue
+	 */
+	ctx->msgq_head = ctx->msgq_tail = 0;
+	init_waitqueue_head(&ctx->msgq_wait);
+
+	PFM_DBG("ctx=%p flags=0x%x system=%d notify_block=%d no_msg=%d"
+		" use_fmt=%d ctx_fd=%d mode=%d",
+		ctx,
+		ctx_flags,
+		ctx->flags.system,
+		ctx->flags.block,
+		ctx->flags.no_msg,
+		fmt != NULL,
+		fd, mode);
+
+	*new_ctx = ctx;
+
+	/*
+	 * we defer the fd_install until we are certain the call succeeded
+	 * to ensure we do not have to undo its effect. Neither put_filp()
+	 * nor put_unused_fd() undoes the effect of fd_install().
+	 */
+	fd_install(fd, filp);
+
+	return fd;
+
+error_set:
+	put_filp(filp);
+	put_unused_fd(fd);
+error_file:
+	/*
+	 * calls the right *_put() functions
+	 * calls pfm_release_pmu()
+	 */
+	pfm_context_free(ctx);
+	return ret;
+error_alloc:
+	pfm_pmu_conf_put();
+error_conf:
+	pfm_smpl_fmt_put(fmt);
+	return ret;
+}
+
+/*
+ * called from cpu_disable() to detach the perfmon context
+ * from the CPU going down.
+ *
+ * We cannot use the cpu hotplug notifier because we MUST run
+ * on the CPU that is going down to save the PMU state
+ */
+void pfm_cpu_disable(void)
+{
+	struct pfm_context *ctx;
+	unsigned long flags;
+	int is_system, release_info = 0;
+	u32 cpu;
+	int r;
+
+	ctx = __get_cpu_var(pmu_ctx);
+	if (ctx == NULL)
+		return;
+
+	is_system = ctx->flags.system;
+	cpu = ctx->cpu;
+
+	/*
+	 * context is LOADED or MASKED
+	 *
+	 * we unload from CPU. That stops monitoring and does
+	 * all the bookeeping of saving values and updating duration
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+	if (is_system)
+		__pfm_unload_context(ctx, &release_info);
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/*
+	 * cancel timer now that context is unlocked
+	 */
+	if (release_info & 0x2) {
+		r = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG("timeout cancel=%d", r);
+	}
+
+	if (release_info & 0x1)
+		pfm_release_session(is_system, cpu);
+}
diff -Naur linux-2.6.25-org/perfmon/perfmon_ctxsw.c linux-2.6.25-id/perfmon/perfmon_ctxsw.c
--- linux-2.6.25-org/perfmon/perfmon_ctxsw.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_ctxsw.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,378 @@
+/*
+ * perfmon_cxtsw.c: perfmon2 context switch code
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+/*
+ * used only in UP mode
+ */
+void pfm_save_prev_context(struct pfm_context *ctxp)
+{
+	struct pfm_event_set *set;
+
+	/*
+	 * in UP per-thread, due to lazy save
+	 * there could be a context from another
+	 * task. We need to push it first before
+	 * installing our new state
+	 */
+	set = ctxp->active_set;
+	pfm_save_pmds(ctxp, set);
+	/*
+	 * do not clear ownership because we rewrite
+	 * right away
+	 */
+}
+
+void pfm_save_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 val, ovfl_mask;
+	u64 *used_pmds, *cnt_pmds;
+	u16 i, num;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	num = set->nused_pmds;
+	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
+	used_pmds = set->used_pmds;
+
+	/*
+	 * save HW PMD, for counters, reconstruct 64-bit value
+	 */
+	for (i = 0; num; i++) {
+		if (test_bit(i, cast_ulp(used_pmds))) {
+			val = pfm_read_pmd(ctx, i);
+			if (likely(test_bit(i, cast_ulp(cnt_pmds))))
+				val = (set->pmds[i].value & ~ovfl_mask) |
+					(val & ovfl_mask);
+			set->pmds[i].value = val;
+			num--;
+		}
+	}
+}
+
+/*
+ * interrupts are  disabled (no preemption)
+ */
+static void __pfm_ctxswin_thread(struct task_struct *task,
+				 struct pfm_context *ctx, u64 now)
+{
+	u64 cur_act;
+	struct pfm_event_set *set;
+	int reload_pmcs, reload_pmds;
+	int mycpu, is_active;
+
+	mycpu = smp_processor_id();
+
+	cur_act = __get_cpu_var(pmu_activation_number);
+	/*
+	 * we need to lock context because it could be accessed
+	 * from another CPU
+	 */
+	spin_lock(&ctx->lock);
+
+	is_active = pfm_arch_is_active(ctx);
+
+	set = ctx->active_set;
+
+	/*
+	 * in case fo zombie, we do not complete ctswin of the
+	 * PMU, and we force a call to pfm_handle_work() to finish
+	 * cleanup, i.e., free context + smpl_buff. The reason for
+	 * deferring to pfm_handle_work() is that it is not possible
+	 * to vfree() with interrupts disabled.
+	 */
+	if (unlikely(ctx->state == PFM_CTX_ZOMBIE)) {
+		ctx->flags.work_type = PFM_WORK_ZOMBIE;
+		set_tsk_thread_flag(task, TIF_PERFMON_WORK);
+		spin_unlock(&ctx->lock);
+		return;
+	}
+
+	/*
+	 * if we were the last user of the PMU on that CPU,
+	 * then nothing to do except restore psr
+	 */
+	if (ctx->last_cpu == mycpu && ctx->last_act == cur_act) {
+		/*
+		 * check for forced reload conditions
+		 */
+		reload_pmcs = set->priv_flags & PFM_SETFL_PRIV_MOD_PMCS;
+		reload_pmds = set->priv_flags & PFM_SETFL_PRIV_MOD_PMDS;
+	} else {
+#ifndef CONFIG_SMP
+		struct pfm_context *ctxp;
+		ctxp = __get_cpu_var(pmu_ctx);
+		if (ctxp)
+			pfm_save_prev_context(ctxp);
+#endif
+		reload_pmcs = 1;
+		reload_pmds = 1;
+	}
+	/* consumed */
+	set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
+
+	if (reload_pmds)
+		pfm_arch_restore_pmds(ctx, set);
+
+	/*
+	 * need to check if had in-flight interrupt in
+	 * pfm_ctxswout_thread(). If at least one bit set, then we must replay
+	 * the interrupt to avoid losing some important performance data.
+	 *
+	 * npend_ovfls is cleared in interrupt handler
+	 */
+	if (set->npend_ovfls) {
+		pfm_arch_resend_irq();
+		pfm_stats_inc(ovfl_intr_replay_count);
+	}
+
+	if (reload_pmcs)
+		pfm_arch_restore_pmcs(ctx, set);
+
+	/*
+	 * record current activation for this context
+	 */
+	pfm_inc_activation();
+	pfm_set_last_cpu(ctx, mycpu);
+	pfm_set_activation(ctx);
+
+	/*
+	 * establish new ownership.
+	 */
+	pfm_set_pmu_owner(task, ctx);
+
+	pfm_arch_ctxswin_thread(task, ctx, set);
+	/*
+	 * set->duration does not count when context in MASKED state.
+	 * set->duration_start is reset in unmask_monitoring()
+	 */
+	set->duration_start = now;
+
+	/*
+	 * re-arm switch timeout, if necessary
+	 * Timeout is active only if monitoring is active, i.e., LOADED + started
+	 *
+	 * We reload the remainder timeout or the full timeout. Remainder is recorded
+	 * on context switch out.
+	 */
+	if (ctx->state == PFM_CTX_LOADED
+	    && (set->flags & PFM_SETFL_TIME_SWITCH) && is_active) {
+		struct hrtimer *h;
+		h = &__get_cpu_var(pfm_hrtimer);
+		if (set->hrtimer_rem.tv64) {
+			hrtimer_start(h, set->hrtimer_rem, HRTIMER_MODE_REL);
+			PFM_DBG_ovfl("armed rem %lld for [%d]",
+					(long long)set->hrtimer_rem.tv64,
+					task->pid);
+			set->hrtimer_rem.tv64 = 0;
+		} else {
+			hrtimer_start(h, set->hrtimer_exp, HRTIMER_MODE_REL);
+			PFM_DBG_ovfl("armed exp for [%d]", task->pid);
+		}
+	}
+	spin_unlock(&ctx->lock);
+}
+
+/*
+ * interrupts are masked, runqueue lock is held.
+ *
+ * In UP. we simply stop monitoring and leave the state
+ * in place, i.e., lazy save
+ */
+static void __pfm_ctxswout_thread(struct task_struct *task,
+				  struct pfm_context *ctx, u64 now)
+{
+	struct pfm_event_set *set;
+	struct hrtimer *h = NULL;
+	int need_save_pmds, is_active;
+
+	/*
+	 * we need to lock context because it could be accessed
+	 * from another CPU
+	 */
+	spin_lock(&ctx->lock);
+
+	is_active = pfm_arch_is_active(ctx);
+	set = ctx->active_set;
+
+	/*
+	 * stop monitoring and
+	 * collect pending overflow information
+	 * needed on ctxswin. We cannot afford to lose
+	 * a PMU interrupt.
+	 */
+	need_save_pmds = pfm_arch_ctxswout_thread(task, ctx, set);
+
+	/*
+	 * accumulate only when set is actively monitoring,
+	 */
+	if (ctx->state == PFM_CTX_LOADED) {
+		set->duration += now - set->duration_start;
+		/*
+		 * timeout only runs while LOADED + started
+		 * record remaining timeout. Used on context switch in
+		 */
+		if (is_active && (set->flags & PFM_SETFL_TIME_SWITCH)) {
+			h = &__get_cpu_var(pfm_hrtimer);
+			set->hrtimer_rem = hrtimer_get_remaining(h);
+		}
+	}
+
+#ifdef CONFIG_SMP
+	/*
+	 * in SMP, release ownership of this PMU.
+	 * PMU interrupts are masked, so nothing
+	 * can happen.
+	 */
+	pfm_set_pmu_owner(NULL, NULL);
+
+	/*
+	 * On some architectures, it is necessary to read the
+	 * PMD registers to check for pending overflow in
+	 * pfm_arch_ctxswout_thread(). In that case, saving of
+	 * the PMDs  may be  done there and not here.
+	 */
+	if (need_save_pmds)
+		pfm_save_pmds(ctx, set);
+#endif
+	spin_unlock(&ctx->lock);
+
+	/*
+	 * cancel timer if necessary
+	 * need to have context unlocked to avoid
+	 * a race condition with pfm_handle_switch_timeout()
+	 *
+	 * hrtimer_cancel() loops until timer is actually cancelled
+	 */
+	if (h)
+		hrtimer_cancel(h);
+}
+
+/*
+ * no need to lock the context. To operate on a system-wide
+ * context, the task has to run on the monitored CPU. In the
+ * case of close issued on another CPU, an IPI is sent but
+ * this routine runs with interrupts masked, so we are
+ * protected
+ *
+ * On some architectures, such as IA-64, it may be necessary
+ * to intervene during the context even in system-wide mode
+ * to modify some machine state.
+ */
+static void __pfm_ctxsw_sys(struct task_struct *prev,
+			    struct task_struct *next)
+{
+	struct pfm_context *ctx;
+	struct pfm_event_set *set;
+
+	ctx = __get_cpu_var(pmu_ctx);
+	if (!ctx) {
+		pr_info("prev=%d tif=%d ctx=%p next=%d tif=%d ctx=%p\n",
+			prev->pid,
+			test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW),
+			prev->pfm_context,
+			next->pid,
+			test_tsk_thread_flag(next, TIF_PERFMON_CTXSW),
+			next->pfm_context);
+		BUG_ON(!ctx);
+	}
+
+	set = ctx->active_set;
+
+	/*
+	 * propagate TIF_PERFMON_CTXSW to ensure that:
+	 * - previous task has TIF_PERFMON_CTXSW cleared, in case it is
+	 *   scheduled onto another CPU where there is syswide monitoring
+	 * - next task has TIF_PERFMON_CTXSW set to ensure it will come back
+	 *   here when context switched out
+	 */
+	clear_tsk_thread_flag(prev, TIF_PERFMON_CTXSW);
+	set_tsk_thread_flag(next, TIF_PERFMON_CTXSW);
+
+	/*
+	 * nothing to do until actually started
+	 * XXX: assumes no mean to start from user level
+	 */
+	if (!ctx->flags.started)
+		return;
+
+	pfm_arch_ctxswout_sys(prev, ctx, set);
+	pfm_arch_ctxswin_sys(next, ctx, set);
+}
+
+/*
+ * come here when either prev or next has TIF_PERFMON_CTXSW flag set
+ * Note that this is not because a task has TIF_PERFMON_CTXSW set that
+ * it has a context attached, e.g., in system-wide on certain arch.
+ */
+void pfm_ctxsw(struct task_struct *prev, struct task_struct *next)
+{
+	struct pfm_context *ctxp, *ctxn;
+	u64 now;
+
+	now = sched_clock();
+
+	ctxp = NULL;
+	ctxn = NULL;
+	if (prev)
+		ctxp = prev->pfm_context;
+
+	if (next)
+		ctxn = next->pfm_context;
+
+	if (ctxp)
+		__pfm_ctxswout_thread(prev, ctxp, now);
+
+	if (ctxn)
+		__pfm_ctxswin_thread(next, ctxn, now);
+
+	/*
+	 * given that prev and next can never be the same, this
+	 * test is checking that ctxp == ctxn == NULL which is
+	 * an indication we have an active system-wide session on
+	 * this CPU that needs ctxsw intervention. Not all processors
+	 * needs this, IA64 is one.
+	 */
+	if (ctxp == ctxn)
+		__pfm_ctxsw_sys(prev, next);
+
+	pfm_stats_inc(ctxsw_count);
+	pfm_stats_add(ctxsw_ns, sched_clock() - now);
+}
+EXPORT_SYMBOL_GPL(pfm_ctxsw);
diff -Naur linux-2.6.25-org/perfmon/perfmon_debugfs.c linux-2.6.25-id/perfmon/perfmon_debugfs.c
--- linux-2.6.25-org/perfmon/perfmon_debugfs.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_debugfs.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,186 @@
+/*
+ * perfmon_debugfs.c: perfmon2 statistics interface to debugfs
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon.h>
+#include <linux/debugfs.h>
+
+/*
+ * to make the statistics visible to user space:
+ * $ mount -t debugfs none /mnt
+ * $ cd /mnt/perfmon
+ * then choose a CPU subdir
+ */
+DECLARE_PER_CPU(struct pfm_stats, pfm_stats);
+
+static struct dentry *pfm_debugfs_dir;
+
+void pfm_reset_stats(int cpu)
+{
+	struct pfm_stats *st;
+	unsigned long flags;
+
+	st = &per_cpu(pfm_stats, cpu);
+
+	local_irq_save(flags);
+	memset(st->v, 0, sizeof(st->v));
+	local_irq_restore(flags);
+}
+
+static const char *pfm_stats_strs[] = {
+	"ovfl_intr_all_count",
+	"ovfl_intr_ns",
+	"ovfl_intr_p1_ns",
+	"ovfl_intr_p2_ns",
+	"ovfl_intr_p3_ns",
+	"ovfl_intr_spurious_count",
+	"ovfl_intr_replay_count",
+	"ovfl_intr_regular_count",
+	"handle_work_count",
+	"ovfl_notify_count",
+	"reset_pmds_count",
+	"pfm_restart_count",
+	"fmt_handler_calls",
+	"fmt_handler_ns",
+	"set_switch_count",
+	"set_switch_ns",
+	"ctxsw_count",
+	"ctxsw_ns",
+	"handle_timeout_count",
+	"ovfl_intr_nmi_count",
+};
+#define PFM_NUM_STRS ARRAY_SIZE(pfm_stats_strs)
+
+void pfm_debugfs_del_cpu(int cpu)
+{
+	struct pfm_stats *st;
+	int i;
+
+	st = &per_cpu(pfm_stats, cpu);
+
+	for(i=0; i < PFM_NUM_STATS; i++) {
+		if (st->dirs[i])
+			debugfs_remove(st->dirs[i]);
+		st->dirs[i] = NULL;
+	}
+	if (st->cpu_dir)
+		debugfs_remove(st->cpu_dir);
+	st->cpu_dir = NULL;
+}
+
+int pfm_debugfs_add_cpu(int cpu)
+{
+	struct pfm_stats *st;
+	int i;
+
+	/*
+	 * sanity check between stats names and the number
+	 * of entries in the pfm_stats value array.
+	 */
+	if (PFM_NUM_STRS != PFM_NUM_STATS) {
+		PFM_ERR("PFM_NUM_STRS != PFM_NUM_STATS error");
+		return -1;
+	}
+
+	st = &per_cpu(pfm_stats, cpu);
+	sprintf(st->cpu_name, "cpu%d", cpu);
+
+	st->cpu_dir = debugfs_create_dir(st->cpu_name, pfm_debugfs_dir);
+	if (!st->cpu_dir)
+		return -1;
+
+	for (i=0; i < PFM_NUM_STATS; i++) {
+		st->dirs[i] = debugfs_create_u64(pfm_stats_strs[i],
+						 S_IRUGO,
+						 st->cpu_dir,
+						 &st->v[i]);
+		if (!st->dirs[i])
+			goto error;
+	}
+	pfm_reset_stats(cpu);
+	return 0;
+error:
+	while(i >= 0) {
+		debugfs_remove(st->dirs[i]);
+		i--;
+	}
+	debugfs_remove(st->cpu_dir);
+	return -1;
+}
+
+/*
+ * called once from pfm_init()
+ */
+int __init pfm_init_debugfs(void)
+{
+	int cpu1, cpu2, ret;
+
+	pfm_debugfs_dir = debugfs_create_dir("perfmon", NULL);
+	if (!pfm_debugfs_dir)
+		return -1;
+
+	for_each_online_cpu(cpu1) {
+		ret = pfm_debugfs_add_cpu(cpu1);
+		if (ret)
+			goto error;
+	}
+	return 0;
+error:
+	for_each_online_cpu(cpu2) {
+		if (cpu2 == cpu1)
+			break;
+		pfm_debugfs_del_cpu(cpu2);
+	}
+	return -1;
+}
+
+#if 0
+static ssize_t ovfl_intr_spurious_count_show(void *info, char *buf)
+{
+	struct pfm_stats *st = info;
+	return snprintf(buf, PAGE_SIZE, "%llu\n",
+			(unsigned long long)(st->ovfl_intr_all_count
+					     - st->ovfl_intr_regular_count));
+}
+
+static ssize_t ovfl_intr_regular_count_show(void *info, char *buf)
+{
+	struct pfm_stats *st = info;
+	return snprintf(buf, PAGE_SIZE, "%llu\n",
+			(unsigned long long)(st->ovfl_intr_regular_count
+					     - st->ovfl_intr_replay_count));
+}
+#endif
diff -Naur linux-2.6.25-org/perfmon/perfmon_dfl_smpl.c linux-2.6.25-id/perfmon/perfmon_dfl_smpl.c
--- linux-2.6.25-org/perfmon/perfmon_dfl_smpl.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_dfl_smpl.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,294 @@
+/*
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file implements the new default sampling buffer format
+ * for the perfmon2 subsystem.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+
+#include <linux/perfmon.h>
+#include <linux/perfmon_dfl_smpl.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("new perfmon default sampling format");
+MODULE_LICENSE("GPL");
+
+static int pfm_dfl_fmt_validate(u32 ctx_flags, u16 npmds, void *data)
+{
+	struct pfm_dfl_smpl_arg *arg = data;
+	u64 min_buf_size;
+
+	if (data == NULL) {
+		PFM_DBG("no argument passed");
+		return -EINVAL;
+	}
+
+	/*
+	 * sanity check in case size_t is smaller then u64
+	 */
+#if BITS_PER_LONG == 4
+#define MAX_SIZE_T	(1ULL<<(sizeof(size_t)<<3))
+	if (sizeof(size_t) < sizeof(arg->buf_size)) {
+		if (arg->buf_size >= MAX_SIZE_T)
+			return -ETOOBIG;
+	}
+#endif
+
+	/*
+	 * compute min buf size. npmds is the maximum number
+	 * of implemented PMD registers.
+	 */
+	min_buf_size = sizeof(struct pfm_dfl_smpl_hdr)
+	             + (sizeof(struct pfm_dfl_smpl_entry) + (npmds*sizeof(u64)));
+
+	PFM_DBG("validate ctx_flags=0x%x flags=0x%x npmds=%u "
+		   "min_buf_size=%llu buf_size=%llu\n",
+		   ctx_flags,
+		   arg->buf_flags,
+		   npmds,
+		   (unsigned long long)min_buf_size,
+		   (unsigned long long)arg->buf_size);
+
+	/*
+	 * must hold at least the buffer header + one minimally sized entry
+	 */
+	if (arg->buf_size < min_buf_size)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int pfm_dfl_fmt_get_size(u32 flags, void *data, size_t *size)
+{
+	struct pfm_dfl_smpl_arg *arg = data;
+
+	/*
+	 * size has been validated in default_validate
+	 * we can never loose bits from buf_size.
+	 */
+	*size = (size_t)arg->buf_size;
+
+	return 0;
+}
+
+static int pfm_dfl_fmt_init(struct pfm_context *ctx, void *buf, u32 ctx_flags,
+			    u16 npmds, void *data)
+{
+	struct pfm_dfl_smpl_hdr *hdr;
+	struct pfm_dfl_smpl_arg *arg = data;
+
+	hdr = buf;
+
+	hdr->hdr_version = PFM_DFL_SMPL_VERSION;
+	hdr->hdr_buf_size = arg->buf_size;
+	hdr->hdr_buf_flags = arg->buf_flags;
+	hdr->hdr_cur_offs = sizeof(*hdr);
+	hdr->hdr_overflows = 0;
+	hdr->hdr_count = 0;
+	hdr->hdr_min_buf_space = sizeof(struct pfm_dfl_smpl_entry) + (npmds*sizeof(u64));
+	/*
+	 * due to cache aliasing, it may be necessary to flush the cache
+	 * on certain architectures (e.g., MIPS)
+	 */
+	pfm_cacheflush(hdr, sizeof(*hdr));
+
+	PFM_DBG("buffer=%p buf_size=%llu hdr_size=%zu hdr_version=%u.%u "
+		  "min_space=%llu npmds=%u",
+		  buf,
+		  (unsigned long long)hdr->hdr_buf_size,
+		  sizeof(*hdr),
+		  PFM_VERSION_MAJOR(hdr->hdr_version),
+		  PFM_VERSION_MINOR(hdr->hdr_version),
+		  (unsigned long long)hdr->hdr_min_buf_space,
+		  npmds);
+
+	return 0;
+}
+
+/*
+ * called from pfm_overflow_handler() to record a new sample
+ *
+ * context is locked, interrupts are disabled (no preemption)
+ */
+static int pfm_dfl_fmt_handler(void *buf, struct pfm_ovfl_arg *arg,
+			       unsigned long ip, u64 tstamp, void *data)
+{
+	struct pfm_dfl_smpl_hdr *hdr;
+	struct pfm_dfl_smpl_entry *ent;
+	void *cur, *last;
+	u64 *e;
+	size_t entry_size, min_size;
+	u16 npmds, i;
+	u16 ovfl_pmd;
+
+	hdr = buf;
+	cur = buf+hdr->hdr_cur_offs;
+	last = buf+hdr->hdr_buf_size;
+	ovfl_pmd = arg->ovfl_pmd;
+	min_size = hdr->hdr_min_buf_space;
+
+	/*
+	 * precheck for sanity
+	 */
+	if ((last - cur) < min_size)
+		goto full;
+
+	npmds = arg->num_smpl_pmds;
+
+	ent = (struct pfm_dfl_smpl_entry *)cur;
+
+	entry_size = sizeof(*ent) + (npmds << 3);
+
+	/* position for first pmd */
+	e = (u64 *)(ent+1);
+
+	hdr->hdr_count++;
+
+	PFM_DBG_ovfl("count=%llu cur=%p last=%p free_bytes=%zu ovfl_pmd=%d "
+		       "npmds=%u",
+		       (unsigned long long)hdr->hdr_count,
+		       cur, last,
+		       (last-cur),
+		       ovfl_pmd,
+		       npmds);
+
+	/*
+	 * current = task running at the time of the overflow.
+	 *
+	 * per-task mode:
+	 * 	- this is usually the task being monitored.
+	 * 	  Under certain conditions, it might be a different task
+	 *
+	 * system-wide:
+	 * 	- this is not necessarily the task controlling the session
+	 */
+	ent->pid = current->pid;
+	ent->ovfl_pmd = ovfl_pmd;
+	ent->last_reset_val = arg->pmd_last_reset;
+
+	/*
+	 * where did the fault happen (includes slot number)
+	 */
+	ent->ip = ip;
+
+	ent->tstamp = tstamp;
+	ent->cpu = smp_processor_id();
+	ent->set = arg->active_set;
+	ent->tgid = current->tgid;
+
+	/*
+	 * selectively store PMDs in increasing index number
+	 */
+	if (npmds) {
+		u64 *val = arg->smpl_pmds_values;
+		for(i=0; i < npmds; i++) {
+			*e++ = *val++;
+		}
+	}
+
+	/*
+	 * update position for next entry
+	 */
+	hdr->hdr_cur_offs += entry_size;
+	cur += entry_size;
+
+	pfm_cacheflush(hdr, sizeof(*hdr));
+	pfm_cacheflush(ent, entry_size);
+
+	/*
+	 * post check to avoid losing the last sample
+	 */
+	if ((last - cur) < min_size)
+		goto full;
+
+	/* reset before returning from interrupt handler */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+
+	return 0;
+full:
+	PFM_DBG_ovfl("sampling buffer full free=%zu, count=%llu",
+		     last-cur,
+		     (unsigned long long)hdr->hdr_count);
+
+	/*
+	 * increment number of buffer overflows.
+	 * important to detect duplicate set of samples.
+	 */
+	hdr->hdr_overflows++;
+
+	/*
+	 * request notification and masking of monitoring.
+	 * Notification is still subject to the overflowed
+	 * register having the FL_NOTIFY flag set.
+	 */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY| PFM_OVFL_CTRL_MASK;
+
+	return -ENOBUFS; /* we are full, sorry */
+}
+
+static int pfm_dfl_fmt_restart(int is_active, u32 *ovfl_ctrl, void *buf)
+{
+	struct pfm_dfl_smpl_hdr *hdr;
+
+	hdr = buf;
+
+	hdr->hdr_count = 0;
+	hdr->hdr_cur_offs = sizeof(*hdr);
+
+	pfm_cacheflush(hdr, sizeof(*hdr));
+
+	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+
+	return 0;
+}
+
+static int pfm_dfl_fmt_exit(void *buf)
+{
+	return 0;
+}
+
+static struct pfm_smpl_fmt dfl_fmt={
+	.fmt_name = "default",
+	.fmt_version = 0x10000,
+	.fmt_arg_size = sizeof(struct pfm_dfl_smpl_arg),
+	.fmt_validate = pfm_dfl_fmt_validate,
+	.fmt_getsize = pfm_dfl_fmt_get_size,
+	.fmt_init = pfm_dfl_fmt_init,
+	.fmt_handler = pfm_dfl_fmt_handler,
+	.fmt_restart = pfm_dfl_fmt_restart,
+	.fmt_exit = pfm_dfl_fmt_exit,
+	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
+	.owner = THIS_MODULE
+};
+
+static int pfm_dfl_fmt_init_module(void)
+{
+	return pfm_fmt_register(&dfl_fmt);
+}
+
+static void pfm_dfl_fmt_cleanup_module(void)
+{
+	pfm_fmt_unregister(&dfl_fmt);
+}
+
+module_init(pfm_dfl_fmt_init_module);
+module_exit(pfm_dfl_fmt_cleanup_module);
diff -Naur linux-2.6.25-org/perfmon/perfmon_file.c linux-2.6.25-id/perfmon/perfmon_file.c
--- linux-2.6.25-org/perfmon/perfmon_file.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_file.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,807 @@
+/*
+ * perfmon_file.c: perfmon2 file input/output functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/file.h>
+#include <linux/poll.h>
+#include <linux/vfs.h>
+#include <linux/pagemap.h>
+#include <linux/mount.h>
+#include <linux/perfmon.h>
+
+#define PFMFS_MAGIC 0xa0b4d889	/* perfmon filesystem magic number */
+
+static inline int pfm_msgq_is_empty(struct pfm_context *ctx)
+{
+	return ctx->msgq_head == ctx->msgq_tail;
+}
+
+static int pfmfs_delete_dentry(struct dentry *dentry)
+{
+	return 1;
+}
+
+static struct dentry_operations pfmfs_dentry_operations = {
+	.d_delete = pfmfs_delete_dentry,
+};
+
+static union pfarg_msg *pfm_get_next_msg(struct pfm_context *ctx)
+{
+	union pfarg_msg *msg;
+
+	PFM_DBG_ovfl("in head=%d tail=%d",
+		ctx->msgq_head & PFM_MSGQ_MASK,
+		ctx->msgq_tail & PFM_MSGQ_MASK);
+
+	if (pfm_msgq_is_empty(ctx))
+		return NULL;
+
+	/*
+	 * get oldest message
+	 */
+	msg = ctx->msgq + (ctx->msgq_tail & PFM_MSGQ_MASK);
+
+	/*
+	 * move tail forward
+	 */
+	ctx->msgq_tail++;
+
+	PFM_DBG_ovfl("out head=%d tail=%d type=%d",
+		ctx->msgq_head & PFM_MSGQ_MASK,
+		ctx->msgq_tail & PFM_MSGQ_MASK,
+		msg->type);
+
+	return msg;
+}
+
+static struct page *pfm_buf_map_pagefault(struct vm_area_struct *vma,
+					  unsigned long address, int *type)
+{
+	void *kaddr;
+	struct pfm_context *ctx;
+	struct page *page;
+	size_t size;
+
+	ctx = vma->vm_private_data;
+	if (ctx == NULL) {
+		PFM_DBG("no ctx");
+		return NOPAGE_SIGBUS;
+	}
+	/*
+	 * size available to user (maybe different from real_smpl_size
+	 */
+	size = ctx->smpl_size;
+
+	if ( (address < (unsigned long) vma->vm_start) ||
+	     (address >= (unsigned long) (vma->vm_start + size)) )
+		return NOPAGE_SIGBUS;
+
+	kaddr = ctx->smpl_addr + (address - vma->vm_start);
+
+	if (type)
+		*type = VM_FAULT_MINOR;
+
+	page = vmalloc_to_page(kaddr);
+	get_page(page);
+
+	PFM_DBG("[%d] start=%p ref_count=%d",
+		current->pid,
+		kaddr, page_count(page));
+
+	return page;
+}
+/*
+ * we need to determine whther or not we are closing the last reference
+ * to the file and thus are going to end up in pfm_close() which eventually
+ * calls pfm_release_buf_space(). In that function, we update the accouting
+ * for locked_vm given that we are actually freeing the sampling buffer. The
+ * issue is that there are multiple paths leading to pfm_release_buf_space(),
+ * from exit(), munmap(), close(). The path coming from munmap() is problematic
+ * becuse do_munmap() grabs mmap_sem in write-mode which is also what
+ * pfm_release_buf_space does. To avoid deadlock, we need to determine where
+ * we are calling from and skip the locking. The vm_ops->close() callback
+ * is invoked for each remove_vma() independently of the number of references
+ * left on the file descriptor, therefore simple reference counter does not
+ * work. We need to determine if this is the last call, and then set a flag
+ * to skip the locking.
+ */
+static void pfm_buf_map_close(struct vm_area_struct *vma)
+{
+	struct file *file;
+	struct pfm_context *ctx;
+
+	file = vma->vm_file;
+	ctx = vma->vm_private_data;
+
+	/*
+	 * if file is going to close, then pfm_close() will
+	 * be called, do not lock in pfm_release_buf
+	 */
+	if (atomic_read(&file->f_count) == 1)
+		ctx->flags.mmap_nlock = 1;
+}
+
+/*
+ * we do not have a close callback because, the locked
+ * memory accounting must be done when the actual buffer
+ * is freed. Munmap does not free the page backing the vma
+ * because they may still be in use by the PMU interrupt handler.
+ */
+struct vm_operations_struct pfm_buf_map_vm_ops = {
+	.nopage	= pfm_buf_map_pagefault,
+	.close = pfm_buf_map_close
+};
+
+static int pfm_mmap_buffer(struct pfm_context *ctx, struct vm_area_struct *vma,
+			   size_t size)
+{
+	if (ctx->smpl_addr == NULL) {
+		PFM_DBG("no sampling buffer to map");
+		return -EINVAL;
+	}
+
+	if (size > ctx->smpl_size) {
+		PFM_DBG("mmap size=%zu >= actual buf size=%zu",
+			size,
+			ctx->smpl_size);
+		return -EINVAL;
+	}
+
+	vma->vm_ops = &pfm_buf_map_vm_ops;
+	vma->vm_private_data = ctx;
+
+	return 0;
+}
+
+static int pfm_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	size_t size;
+	struct pfm_context *ctx;
+	unsigned long flags;
+	int ret;
+
+	PFM_DBG("pfm_file_ops");
+
+	ctx  = file->private_data;
+	size = (vma->vm_end - vma->vm_start);
+
+	if (ctx == NULL)
+		return -EINVAL;
+
+	ret = -EINVAL;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	if (vma->vm_flags & VM_WRITE) {
+		PFM_DBG("cannot map buffer for writing");
+		goto done;
+	}
+
+	PFM_DBG("vm_pgoff=%lu size=%zu vm_start=0x%lx",
+		vma->vm_pgoff,
+		size,
+		vma->vm_start);
+
+	ret = pfm_mmap_buffer(ctx, vma, size);
+	if (ret == 0)
+		vma->vm_flags |= VM_RESERVED;
+
+	PFM_DBG("ret=%d vma_flags=0x%lx vma_start=0x%lx vma_size=%lu",
+		ret,
+		vma->vm_flags,
+		vma->vm_start,
+		vma->vm_end-vma->vm_start);
+done:
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	return ret;
+}
+
+/*
+ * Extract one message from queue.
+ *
+ * return:
+ * 	-EAGAIN:  when non-blocking and nothing is* in the queue.
+ * 	-ERESTARTSYS: when blocking and signal is pending
+ * 	Otherwise returns size of message (sizeof(pfarg_msg))
+ */
+ssize_t __pfm_read(struct pfm_context *ctx, union pfarg_msg *msg_buf, int non_block)
+{
+	union pfarg_msg *msg;
+	ssize_t ret = 0;
+	unsigned long flags;
+	DECLARE_WAITQUEUE(wait, current);
+
+	/*
+	 * we must masks interrupts to avoid a race condition
+	 * with the PMU interrupt handler.
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	while (pfm_msgq_is_empty(ctx)) {
+
+		/*
+		 * handle non-blocking reads
+		 * return -EAGAIN
+		 */
+		ret = -EAGAIN;
+		if (non_block)
+			break;
+
+		add_wait_queue(&ctx->msgq_wait, &wait);
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		spin_unlock_irqrestore(&ctx->lock, flags);
+
+		schedule();
+
+		/*
+		 * during this window, another thread may call
+		 * pfm_read() and steal our message
+		 */
+
+		spin_lock_irqsave(&ctx->lock, flags);
+
+		remove_wait_queue(&ctx->msgq_wait, &wait);
+		set_current_state(TASK_RUNNING);
+
+		/*
+		 * check for pending signals
+		 * return -ERESTARTSYS
+		 */
+		ret = -ERESTARTSYS;
+		if(signal_pending(current))
+			break;
+
+		/*
+		 * we may have a message
+		 */
+		ret = 0;
+	}
+
+	/*
+	 * extract message
+	 */
+	if (ret == 0) {
+		msg = pfm_get_next_msg(ctx);
+		BUG_ON(msg == NULL);
+
+		/*
+		 * we must make a local copy before we unlock
+		 * to ensure that the message queue cannot fill
+		 * (overwriting our message) up before
+		 * we do copy_to_user() which cannot be done
+		 * with interrupts masked.
+		 */
+		*msg_buf = *msg;
+
+		ret = sizeof(*msg);
+
+		PFM_DBG("extracted type=%d", msg->type);
+	}
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	PFM_DBG("blocking=%d ret=%zd", non_block, ret);
+
+	return ret;
+}
+
+static ssize_t pfm_read(struct file *filp, char __user *buf, size_t size,
+			loff_t *ppos)
+{
+	struct pfm_context *ctx;
+	union pfarg_msg msg_buf;
+	int non_block, ret;
+
+	PFM_DBG_ovfl("buf=%p size=%zu", buf, size);
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("no ctx for pfm_read");
+		return -EINVAL;
+	}
+
+	non_block = filp->f_flags & O_NONBLOCK;
+
+	/*
+	 * detect IA-64 v2.0 context read (message size is different)
+	 * nops on all other architectures
+	 */
+	if (unlikely(ctx->flags.ia64_v20_compat))
+		return pfm_arch_compat_read(ctx,  buf, non_block, size);
+
+	/*
+	 * cannot extract partial messages.
+	 * check even when there is no message
+	 *
+	 * cannot extract more than one message per call. Bytes
+	 * above sizeof(msg) are ignored.
+	 */
+	if (size < sizeof(msg_buf)) {
+		PFM_DBG("message is too small size=%zu must be >=%zu)",
+			size,
+			sizeof(msg_buf));
+		return -EINVAL;
+	}
+
+	ret =  __pfm_read(ctx, &msg_buf, non_block);
+	if (ret > 0) {
+		if (copy_to_user(buf, &msg_buf, sizeof(msg_buf)))
+			ret = -EFAULT;
+	}
+	PFM_DBG_ovfl("ret=%d", ret);
+	return ret;
+}
+
+static ssize_t pfm_write(struct file *file, const char __user *ubuf,
+			  size_t size, loff_t *ppos)
+{
+	PFM_DBG("pfm_write called");
+	return -EINVAL;
+}
+
+static unsigned int pfm_poll(struct file *filp, poll_table *wait)
+{
+	struct pfm_context *ctx;
+	unsigned long flags;
+	unsigned int mask = 0;
+
+	PFM_DBG("pfm_file_ops");
+
+	if (filp->f_op != &pfm_file_ops) {
+		PFM_ERR("pfm_poll bad magic");
+		return 0;
+	}
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("pfm_poll no ctx");
+		return 0;
+	}
+
+	PFM_DBG("before poll_wait");
+
+	poll_wait(filp, &ctx->msgq_wait, wait);
+
+	/*
+	 * pfm_msgq_is_empty() is non-atomic
+	 *
+	 * filp is protected by fget() at upper level
+	 * context cannot be closed by another thread.
+	 *
+	 * There may be a race with a PMU interrupt adding
+	 * messages to the queue. But we are interested in
+	 * queue not empty, so adding more messages should
+	 * not really be a problem.
+	 *
+	 * There may be a race with another thread issuing
+	 * a read() and stealing messages from the queue thus
+	 * may return the wrong answer. This could potentially
+	 * lead to a blocking read, because nothing is
+	 * available in the queue
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	if (!pfm_msgq_is_empty(ctx))
+		mask =  POLLIN | POLLRDNORM;
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	PFM_DBG("after poll_wait mask=0x%x", mask);
+
+	return mask;
+}
+
+static int pfm_ioctl(struct inode *inode, struct file *file, unsigned int cmd,
+		     unsigned long arg)
+{
+	PFM_DBG("pfm_ioctl called");
+	return -EINVAL;
+}
+
+/*
+ * interrupt cannot be masked when entering this function
+ */
+static inline int __pfm_fasync(int fd, struct file *filp,
+			       struct pfm_context *ctx, int on)
+{
+	int ret;
+
+	ret = fasync_helper (fd, filp, on, &ctx->async_queue);
+
+	PFM_DBG("fd=%d on=%d async_q=%p ret=%d",
+		fd,
+		on,
+		ctx->async_queue, ret);
+
+	return ret;
+}
+
+static int pfm_fasync(int fd, struct file *filp, int on)
+{
+	struct pfm_context *ctx;
+	int ret;
+
+	PFM_DBG("pfm_file_ops");
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("pfm_fasync no ctx");
+		return -EBADF;
+	}
+
+	/*
+	 * we cannot mask interrupts during this call because this may
+	 * may go to sleep if memory is not readily avalaible.
+	 *
+	 * We are protected from the context disappearing by the
+	 * get_fd()/put_fd() done in caller. Serialization of this function
+	 * is ensured by caller.
+	 */
+	ret = __pfm_fasync(fd, filp, ctx, on);
+
+	PFM_DBG("pfm_fasync called on fd=%d on=%d async_queue=%p ret=%d",
+		fd,
+		on,
+		ctx->async_queue, ret);
+
+	return ret;
+}
+
+#ifdef CONFIG_SMP
+static void __pfm_close_remote_cpu(void *info)
+{
+	struct pfm_context *ctx = info;
+	int can_release;
+
+	BUG_ON(ctx != __get_cpu_var(pmu_ctx));
+
+	/*
+	 * we are in IPI interrupt handler which has always higher
+	 * priority than PMU interrupt, therefore we do not need to
+	 * mask interrupts. context locking is not needed because we
+	 * are in close(), no more user references.
+	 *
+	 * can_release is ignored, release done on calling CPU
+	 */
+	__pfm_unload_context(ctx, &can_release);
+
+	/*
+	 * we cannot free context here because we are in_interrupt().
+	 * we free on the calling CPU
+	 */
+}
+
+static int pfm_close_remote_cpu(u32 cpu, struct pfm_context *ctx)
+{
+	BUG_ON(irqs_disabled());
+	return smp_call_function_single(cpu, __pfm_close_remote_cpu, ctx, 0, 1);
+}
+#endif /* CONFIG_SMP */
+
+/*
+ * called either on explicit close() or from exit_files().
+ * Only the LAST user of the file gets to this point, i.e., it is
+ * called only ONCE.
+ *
+ * IMPORTANT: we get called ONLY when the refcnt on the file gets to zero
+ * (fput()),i.e, last task to access the file. Nobody else can access the
+ * file at this point.
+ *
+ * When called from exit_files(), the VMA has been freed because exit_mm()
+ * is executed before exit_files().
+ *
+ * When called from exit_files(), the current task is not yet ZOMBIE but we
+ * flush the PMU state to the context.
+ */
+int __pfm_close(struct pfm_context *ctx, struct file *filp)
+{
+	unsigned long flags;
+	int state;
+	int can_free = 1, can_unload = 1;
+	int is_system, can_release = 0;
+	u32 cpu;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	state = ctx->state;
+	is_system = ctx->flags.system;
+	cpu = ctx->cpu;
+
+	PFM_DBG("state=%d", state);
+
+	/*
+	 * check if unload is needed
+	 */
+	if (state == PFM_CTX_UNLOADED)
+		goto doit;
+
+#ifdef CONFIG_SMP
+	/*
+	 * we need to release the resource on the ORIGINAL cpu.
+	 * we need to release the context lock to avoid deadlocks
+	 * on the original CPU, especially in the context switch
+	 * routines. It is safe to unlock because we are in close(),
+	 * in other words, there is no more access from user level.
+	 * we can also unmask interrupts on this CPU because the
+	 * context is running on the original CPU. Context will be
+	 * unloaded and the session will be released on the original
+	 * CPU. Upon return, the caller is guaranteed that the context
+	 * is gone from original CPU.
+	 */
+	if (is_system && cpu != smp_processor_id()) {
+		spin_unlock_irqrestore(&ctx->lock, flags);
+		pfm_close_remote_cpu(cpu, ctx);
+		can_release = 1;
+		goto free_it;
+	}
+
+	if (!is_system && ctx->task != current) {
+		/*
+		 * switch context to zombie state
+		 */
+		ctx->state = PFM_CTX_ZOMBIE;
+
+		PFM_DBG("zombie ctx for [%d]", ctx->task->pid);
+		/*
+		 * must check if other thread is using block overflow
+		 * notification mode. If so make sure it will not block
+		 * because there will not be any pfm_restart() issued.
+		 * When the thread notices the ZOMBIE state, it will clean
+		 * up what is left of the context
+		 */
+		if (state == PFM_CTX_MASKED && ctx->flags.block) {
+			/*
+			 * force task to wake up from MASKED state
+			 */
+			PFM_DBG("waking up [%d]", ctx->task->pid);
+
+			complete(&ctx->restart_complete);
+		}
+		/*
+		 * PMU session will be release by monitored task when it notices
+		 * ZOMBIE state as part of pfm_unload_context()
+		 */
+		can_unload = can_free = 0;
+
+		if (!test_tsk_thread_flag(ctx->task, TIF_PERFMON_CTXSW)) {
+			/*
+			 * Cell specific change
+			 * The normal perfmon ctxsw mechanism is not used.
+			 * unload & free is executed in this function.
+			 *
+			 */
+			can_unload = 1;
+			can_free = 1;
+		}
+	}
+#endif
+	if (can_unload)
+		__pfm_unload_context(ctx, &can_release);
+doit:
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+#ifdef CONFIG_SMP
+free_it:
+#endif
+	if (can_release)
+		pfm_release_session(is_system, cpu);
+
+	if (can_free)
+		pfm_context_free(ctx);
+
+	return 0;
+}
+
+static int pfm_close(struct inode *inode, struct file *filp)
+{
+	struct pfm_context *ctx;
+
+	PFM_DBG("called filp=%p", filp);
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("no ctx");
+		return -EBADF;
+	}
+	return __pfm_close(ctx, filp);
+}
+
+static int pfm_no_open(struct inode *irrelevant, struct file *dontcare)
+{
+	PFM_DBG("pfm_file_ops");
+
+	return -ENXIO;
+}
+
+/*
+ * pfm_flush() is called from filp_close() on every call to
+ * close(). pfm_close() is only invoked when the last user
+ * calls close(). pfm_close() is never invoked without
+ * pfm_flush() being invoked first.
+ *
+ * Partially free resources:
+ * 	- remove from fasync queue
+ */
+static int pfm_flush(struct file *filp, fl_owner_t id)
+{
+	struct pfm_context *ctx;
+
+	PFM_DBG("called filp=%p", filp);
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("pfm_flush no ctx");
+		return -EBADF;
+	}
+
+	/*
+	 * remove our file from the async queue, if we use this mode.
+	 * This can be done without the context being protected. We come
+	 * here when the context has become unreacheable by other tasks.
+	 *
+	 * We may still have active monitoring at this point and we may
+	 * end up in pfm_overflow_handler(). However, fasync_helper()
+	 * operates with interrupts disabled and it cleans up the
+	 * queue. If the PMU handler is called prior to entering
+	 * fasync_helper() then it will send a signal. If it is
+	 * invoked after, it will find an empty queue and no
+	 * signal will be sent. In both case, we are safe
+	 */
+	if (filp->f_flags & FASYNC) {
+		PFM_DBG("cleaning up async_queue=%p", ctx->async_queue);
+		__pfm_fasync (-1, filp, ctx, 0);
+	}
+	return 0;
+}
+
+const struct file_operations pfm_file_ops = {
+	.llseek = no_llseek,
+	.read = pfm_read,
+	.write = pfm_write,
+	.poll = pfm_poll,
+	.ioctl = pfm_ioctl,
+	.open = pfm_no_open, /* special open to disallow open via /proc */
+	.fasync = pfm_fasync,
+	.release = pfm_close,
+	.flush= pfm_flush,
+	.mmap = pfm_mmap
+};
+
+static int pfmfs_get_sb(struct file_system_type *fs_type,
+			int flags, const char *dev_name,
+			void *data, struct vfsmount *mnt)
+{
+	return get_sb_pseudo(fs_type, "pfm:", NULL, PFMFS_MAGIC, mnt);
+}
+
+static struct file_system_type pfm_fs_type = {
+	.name     = "pfmfs",
+	.get_sb   = pfmfs_get_sb,
+	.kill_sb  = kill_anon_super,
+};
+
+/*
+ * pfmfs should _never_ be mounted by userland - too much of security hassle,
+ * no real gain from having the whole whorehouse mounted. So we don't need
+ * any operations on the root directory. However, we need a non-trivial
+ * d_name - pfm: will go nicely and kill the special-casing in procfs.
+ */
+static struct vfsmount *pfmfs_mnt;
+
+int __init pfm_init_fs(void)
+{
+	int err = register_filesystem(&pfm_fs_type);
+	if (!err) {
+		pfmfs_mnt = kern_mount(&pfm_fs_type);
+		err = PTR_ERR(pfmfs_mnt);
+		if (IS_ERR(pfmfs_mnt))
+			unregister_filesystem(&pfm_fs_type);
+		else
+			err = 0;
+	}
+	return err;
+}
+
+int pfm_alloc_fd(struct file **cfile)
+{
+	int fd, ret = 0;
+	struct file *file = NULL;
+	struct inode * inode;
+	char name[32];
+	struct qstr this;
+
+	fd = get_unused_fd();
+	if (fd < 0)
+		return -ENFILE;
+
+	ret = -ENFILE;
+
+	file = get_empty_filp();
+	if (!file)
+		goto out;
+
+	/*
+	 * allocate a new inode
+	 */
+	inode = new_inode(pfmfs_mnt->mnt_sb);
+	if (!inode)
+		goto out;
+
+	PFM_DBG("new inode ino=%ld @%p", inode->i_ino, inode);
+
+	inode->i_sb = pfmfs_mnt->mnt_sb;
+	inode->i_mode = S_IFCHR|S_IRUGO;
+	inode->i_uid = current->fsuid;
+	inode->i_gid = current->fsgid;
+
+	sprintf(name, "[%lu]", inode->i_ino);
+	this.name = name;
+	this.hash = inode->i_ino;
+	this.len = strlen(name);
+
+	ret = -ENOMEM;
+
+	/*
+	 * allocate a new dcache entry
+	 */
+	file->f_dentry = d_alloc(pfmfs_mnt->mnt_sb->s_root, &this);
+	if (!file->f_dentry)
+		goto out;
+
+	file->f_dentry->d_op = &pfmfs_dentry_operations;
+
+	d_add(file->f_dentry, inode);
+	file->f_vfsmnt = mntget(pfmfs_mnt);
+	file->f_mapping = inode->i_mapping;
+
+	file->f_op = &pfm_file_ops;
+	file->f_mode = FMODE_READ;
+	file->f_flags = O_RDONLY;
+	file->f_pos  = 0;
+
+	*cfile = file;
+
+	return fd;
+out:
+	if (file)
+		put_filp(file);
+	put_unused_fd(fd);
+	return ret;
+}
diff -Naur linux-2.6.25-org/perfmon/perfmon_fmt.c linux-2.6.25-id/perfmon/perfmon_fmt.c
--- linux-2.6.25-org/perfmon/perfmon_fmt.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_fmt.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,218 @@
+/*
+ * perfmon_fmt.c: perfmon2 sampling buffer format management
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_smpl_fmt_lock);
+static LIST_HEAD(pfm_smpl_fmt_list);
+
+static inline int fmt_is_mod(struct pfm_smpl_fmt *f)
+{
+	return !(f->fmt_flags & PFM_FMTFL_IS_BUILTIN);
+}
+
+static struct pfm_smpl_fmt *pfm_find_fmt(char *name)
+{
+	struct pfm_smpl_fmt * entry;
+
+	list_for_each_entry(entry, &pfm_smpl_fmt_list, fmt_list) {
+		if (!strcmp(entry->fmt_name, name))
+			return entry;
+	}
+	return NULL;
+}
+/*
+ * find a buffer format based on its name
+ */
+struct pfm_smpl_fmt *pfm_smpl_fmt_get(char *name)
+{
+	struct pfm_smpl_fmt * fmt;
+
+	spin_lock(&pfm_smpl_fmt_lock);
+
+	fmt = pfm_find_fmt(name);
+
+	/*
+	 * increase module refcount
+	 */
+	if (fmt && fmt_is_mod(fmt) && !try_module_get(fmt->owner))
+		fmt = NULL;
+
+	spin_unlock(&pfm_smpl_fmt_lock);
+
+	return fmt;
+}
+
+void pfm_smpl_fmt_put(struct pfm_smpl_fmt *fmt)
+{
+	if (fmt == NULL || !fmt_is_mod(fmt))
+		return;
+	BUG_ON(fmt->owner == NULL);
+
+	spin_lock(&pfm_smpl_fmt_lock);
+	module_put(fmt->owner);
+	spin_unlock(&pfm_smpl_fmt_lock);
+}
+
+int pfm_fmt_register(struct pfm_smpl_fmt *fmt)
+{
+	int ret = 0;
+
+	if (perfmon_disabled) {
+		PFM_INFO("perfmon disabled, cannot add sampling format");
+		return -ENOSYS;
+	}
+
+	/* some sanity checks */
+	if (fmt == NULL) {
+		PFM_INFO("perfmon: NULL format for register");
+		return -EINVAL;
+	}
+
+	if (fmt->fmt_name == NULL) {
+		PFM_INFO("perfmon: format has no name");
+		return -EINVAL;
+	}
+
+	if (fmt->fmt_qdepth > PFM_MSGS_COUNT) {
+		PFM_INFO("perfmon: format %s requires %u msg queue depth (max %d)",
+		       fmt->fmt_name,
+		       fmt->fmt_qdepth,
+		       PFM_MSGS_COUNT);
+		return -EINVAL;
+	}
+
+	/*
+	 * fmt is missing the initialization of .owner = THIS_MODULE
+	 * this is only valid when format is compiled as a module
+	 */
+	if (fmt->owner == NULL && fmt_is_mod(fmt)) {
+		PFM_INFO("format %s has no module owner", fmt->fmt_name);
+		return -EINVAL;
+	}
+	/*
+	 * we need at least a handler
+	 */
+	if (fmt->fmt_handler == NULL) {
+		PFM_INFO("format %s has no handler", fmt->fmt_name);
+		return -EINVAL;
+	}
+
+	/*
+	 * format argument size cannot be bigger than PAGE_SIZE
+	 */
+	if (fmt->fmt_arg_size > PAGE_SIZE) {
+		PFM_INFO("format %s arguments too big", fmt->fmt_name);
+		return -EINVAL;
+	}
+
+	spin_lock(&pfm_smpl_fmt_lock);
+
+	/*
+	 * because of sysfs, we cannot have two formats with the same name
+	 */
+	if (pfm_find_fmt(fmt->fmt_name)) {
+		PFM_INFO("format %s already registered", fmt->fmt_name);
+		ret = -EBUSY;
+		goto out;
+	}
+
+	ret = pfm_sysfs_add_fmt(fmt);
+	if (ret) {
+		PFM_INFO("sysfs cannot add format entry for %s", fmt->fmt_name);
+		goto out;
+	}
+
+	list_add(&fmt->fmt_list, &pfm_smpl_fmt_list);
+
+	PFM_INFO("added sampling format %s", fmt->fmt_name);
+out:
+	spin_unlock(&pfm_smpl_fmt_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(pfm_fmt_register);
+
+int pfm_fmt_unregister(struct pfm_smpl_fmt *fmt)
+{
+	struct pfm_smpl_fmt *fmt2;
+	int ret = 0;
+
+	if (!fmt || !fmt->fmt_name) {
+		PFM_DBG("invalid fmt");
+		return -EINVAL;
+	}
+
+	spin_lock(&pfm_smpl_fmt_lock);
+
+	fmt2 = pfm_find_fmt(fmt->fmt_name);
+	if (!fmt) {
+		PFM_INFO("unregister failed, format not registered");
+		ret = -EINVAL;
+		goto out;
+	}
+	list_del_init(&fmt->fmt_list);
+
+	pfm_sysfs_remove_fmt(fmt);
+
+	PFM_INFO("removed sampling format: %s", fmt->fmt_name);
+
+out:
+	spin_unlock(&pfm_smpl_fmt_lock);
+	return ret;
+
+}
+EXPORT_SYMBOL(pfm_fmt_unregister);
+
+/*
+ * we defer adding the builtin formats to /sys/kernel/perfmon/formats
+ * until after the pfm sysfs subsystem is initialized. This function
+ * is called from pfm_sysfs_init()
+ */
+void pfm_sysfs_builtin_fmt_add(void)
+{
+	struct pfm_smpl_fmt * entry;
+
+	/*
+	 * locking not needed, kernel not fully booted
+	 * when called
+	 */
+	list_for_each_entry(entry, &pfm_smpl_fmt_list, fmt_list) {
+		pfm_sysfs_add_fmt(entry);
+	}
+}
diff -Naur linux-2.6.25-org/perfmon/perfmon_intr.c linux-2.6.25-id/perfmon/perfmon_intr.c
--- linux-2.6.25-org/perfmon/perfmon_intr.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_intr.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,577 @@
+/*
+ * perfmon_intr.c: perfmon2 interrupt handling
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon.h>
+#include <linux/module.h>
+
+static inline void pfm_mask_monitoring(struct pfm_context *ctx,
+				       struct pfm_event_set *set)
+{
+	u64 now;
+
+
+	now = sched_clock();
+
+	/*
+	 * we save the PMD values such that we can read them while
+	 * MASKED without having the thread stopped
+	 * because monitoring is stopped
+	 *
+	 * XXX: could be avoided in system-wide
+	 */
+	pfm_save_pmds(ctx, set);
+	pfm_arch_mask_monitoring(ctx, set);
+	/*
+	 * accumulate the set duration up to this point
+	 */
+	set->duration += now - set->duration_start;
+
+	ctx->state = PFM_CTX_MASKED;
+
+	PFM_DBG_ovfl("can_restart=%u", ctx->flags.can_restart);
+}
+
+/*
+ * main overflow processing routine.
+ *
+ * set->num_ovfl_pmds is 0 when returning from this function even though
+ * set->ovfl_pmds[] may have bits set. When leaving set->num_ovfl_pmds
+ * must never be used to determine if there was a pending overflow.
+ */
+static void pfm_overflow_handler(struct pfm_context *ctx, struct pfm_event_set *set,
+				 unsigned long ip,
+				 struct pt_regs *regs)
+{
+	struct pfm_ovfl_arg *ovfl_arg;
+	struct pfm_event_set *set_orig;
+	void *hdr;
+	u64 old_val, ovfl_mask, new_val, ovfl_thres;
+	u64 *ovfl_notify, *ovfl_pmds, *pend_ovfls;
+	u64 *smpl_pmds, *reset_pmds, *cnt_pmds;
+	u64 now, t0, t1;
+	u32 ovfl_ctrl, num_ovfl, num_ovfl_orig;
+	u16 i, max_pmd, max_intr, first_intr;
+	u8 must_switch, has_64b_ovfl;
+	u8 ctx_block, has_notify, has_ovfl_sw;
+
+	now = t0 = sched_clock();
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+
+	max_pmd = pfm_pmu_conf->regs.max_pmd;
+	first_intr = pfm_pmu_conf->regs.first_intr_pmd;
+	max_intr = pfm_pmu_conf->regs.max_intr_pmd;
+	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
+
+	ovfl_pmds = set->ovfl_pmds;
+	num_ovfl = num_ovfl_orig = set->npend_ovfls;
+	pend_ovfls = set->povfl_pmds;
+	has_ovfl_sw = set->flags & PFM_SETFL_OVFL_SWITCH;
+	set_orig = set;
+
+	if (unlikely(ctx->state == PFM_CTX_ZOMBIE))
+		goto stop_monitoring;
+
+	must_switch = has_64b_ovfl = 0;
+
+	hdr = ctx->smpl_addr;
+
+	PFM_DBG_ovfl("intr_pmds=0x%llx npend=%u ip=%p, blocking=%d "
+		     "u_pmds=0x%llx use_fmt=%u",
+		     (unsigned long long)pend_ovfls[0],
+		     num_ovfl,
+		     (void *)ip,
+		     ctx->flags.block,
+		     (unsigned long long)set->used_pmds[0],
+		     ctx->smpl_fmt != NULL);
+
+	/*
+	 * initialize temporary bitvectors
+	 * we allocate bitvectors in the context
+	 * rather than on the stack to minimize stack
+	 * space consumption. PMU interrupt is very high
+	 * which implies possible deep nesting of interrupt
+	 * hence limited kernel stack space.
+	 *
+	 * This is safe because a context can only be in the
+	 * overflow handler once at a time
+	 */
+	reset_pmds = set->reset_pmds;
+	ovfl_notify = ctx->ovfl_ovfl_notify;
+
+	bitmap_zero(cast_ulp(reset_pmds), max_pmd);
+
+	/*
+	 * first we update the virtual counters
+	 *
+	 * we leverage num_ovfl to minimize number of
+	 * iterations of the loop.
+	 *
+	 * The i < max_intr is just a sanity check
+	 */
+	for (i = first_intr; num_ovfl && i < max_intr ; i++) {
+		/*
+		 * skip pmd which did not overflow
+		 */
+		if (!test_bit(i, cast_ulp(pend_ovfls)))
+			continue;
+
+		num_ovfl--;
+
+		/*
+		 * Update software value for counters ONLY
+		 *
+		 * Note that the pmd is not necessarily 0 at this point as
+		 * qualified events may have happened before the PMU was
+		 * frozen. The residual count is not taken into consideration
+		 * here but will be with any read of the pmd
+		 */
+		ovfl_thres = set->pmds[i].ovflsw_thres;
+
+		if (likely(test_bit(i, cast_ulp(cnt_pmds)))) {
+			old_val = new_val = set->pmds[i].value;
+			new_val += 1 + ovfl_mask;
+			set->pmds[i].value = new_val;
+		}  else {
+			/* for non counter which interrupt, we consider
+			 * this equivalent to a 64-bit counter overflow.
+			 */
+			old_val = 1; new_val = 0;
+		}
+
+		/*
+		 * check for overflow condition
+		 */
+		if (likely(old_val > new_val)) {
+			has_64b_ovfl = 1;
+			if (has_ovfl_sw && ovfl_thres > 0) {
+				if (ovfl_thres == 1)
+					must_switch = 1;
+				set->pmds[i].ovflsw_thres = ovfl_thres - 1;
+			}
+
+			/*
+			 * what to reset because of this overflow
+			 */
+			__set_bit(i, cast_ulp(reset_pmds));
+
+			bitmap_or(cast_ulp(reset_pmds),
+				  cast_ulp(reset_pmds),
+				  cast_ulp(set->pmds[i].reset_pmds),
+				  max_pmd);
+
+		} else {
+			/*
+			 * only keep track of 64-bit overflows or
+			 * assimilated
+			 */
+			__clear_bit(i, cast_ulp(pend_ovfls));
+
+			/*
+			 * on some PMU, it may be necessary to re-arm the PMD
+			 */
+			pfm_arch_ovfl_reset_pmd(ctx, i);
+		}
+
+		PFM_DBG_ovfl("ovfl=%s pmd%u new=0x%llx old=0x%llx "
+			     "hw_pmd=0x%llx o_pmds=0x%llx must_switch=%u "
+			     "o_thres=%llu o_thres_ref=%llu",
+			     old_val > new_val ? "64-bit" : "HW",
+			     i,
+			     (unsigned long long)new_val,
+			     (unsigned long long)old_val,
+			     (unsigned long long)pfm_read_pmd(ctx, i),
+			     (unsigned long long)ovfl_pmds[0],
+			     must_switch,
+			     (unsigned long long)set->pmds[i].ovflsw_thres,
+			     (unsigned long long)set->pmds[i].ovflsw_ref_thres);
+	}
+
+	/*
+	 * mark the overflows as consumed
+	 */
+	set->npend_ovfls = 0;
+
+	ctx_block = ctx->flags.block;
+
+	t1 = sched_clock();
+	pfm_stats_add(ovfl_intr_p1_ns, t1 - t0);
+	t0 = t1;
+
+	/*
+	 * there was no 64-bit overflow, nothing else to do
+	 */
+	if (!has_64b_ovfl)
+		return;
+
+	/*
+	 * copy pending_ovfls to ovfl_pmd. It is used in
+	 * the notification message or getinfo_evtsets().
+	 *
+	 * pend_ovfls modified to reflect only 64-bit overflows
+	 */
+	bitmap_copy(cast_ulp(ovfl_pmds),
+		    cast_ulp(pend_ovfls),
+		    max_intr);
+
+	/*
+	 * build ovfl_notify bitmask from ovfl_pmds
+	 */
+	bitmap_and(cast_ulp(ovfl_notify),
+		   cast_ulp(pend_ovfls),
+		   cast_ulp(set->ovfl_notify),
+		   max_intr);
+
+	has_notify = !bitmap_empty(cast_ulp(ovfl_notify), max_intr);
+	/*
+	 * must reset for next set of overflows
+	 */
+	bitmap_zero(cast_ulp(pend_ovfls), max_intr);
+
+	/*
+	 * check for format
+	 */
+	if (likely(ctx->smpl_fmt)) {
+		u64 start_cycles, end_cycles;
+		u64 *cnt_pmds;
+		int j, k, ret = 0;
+
+		ovfl_ctrl = 0;
+		num_ovfl = num_ovfl_orig;
+		ovfl_arg = &ctx->ovfl_arg;
+		cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
+
+		ovfl_arg->active_set = set->id;
+
+		for (i = first_intr; num_ovfl && !ret; i++) {
+
+			if (!test_bit(i, cast_ulp(ovfl_pmds)))
+				continue;
+
+			num_ovfl--;
+
+			ovfl_arg->ovfl_pmd = i;
+			ovfl_arg->ovfl_ctrl = 0;
+
+			ovfl_arg->pmd_last_reset = set->pmds[i].lval;
+			ovfl_arg->pmd_eventid = set->pmds[i].eventid;
+
+			/*
+			 * copy values of pmds of interest.
+			 * Sampling format may use them
+			 * We do not initialize the unused smpl_pmds_values
+			 */
+			k = 0;
+			smpl_pmds = set->pmds[i].smpl_pmds;
+			if (!bitmap_empty(cast_ulp(smpl_pmds), max_pmd)) {
+
+				for (j = 0; j < max_pmd; j++) {
+
+					if (!test_bit(j, cast_ulp(smpl_pmds)))
+						continue;
+
+					new_val = pfm_read_pmd(ctx, j);
+
+					/* for counters, build 64-bit value */
+					if (test_bit(j, cast_ulp(cnt_pmds))) {
+						new_val = (set->pmds[j].value & ~ovfl_mask)
+							| (new_val & ovfl_mask);
+					}
+					ovfl_arg->smpl_pmds_values[k++] = new_val;
+
+					PFM_DBG_ovfl("s_pmd_val[%u]="
+						     "pmd%u=0x%llx",
+						     k, j,
+						     (unsigned long long)new_val);
+				}
+			}
+			ovfl_arg->num_smpl_pmds = k;
+
+			pfm_stats_inc(fmt_handler_calls);
+
+			start_cycles = sched_clock();
+
+			/*
+			 * call custom buffer format record (handler) routine
+			 */
+			ret = (*ctx->smpl_fmt->fmt_handler)(hdr,
+							    ovfl_arg,
+							    ip,
+							    now,
+							    regs);
+
+			end_cycles = sched_clock();
+
+			/*
+			 * for PFM_OVFL_CTRL_MASK and PFM_OVFL_CTRL_NOTIFY
+			 * we take the union
+			 *
+			 * The reset_pmds mask is constructed automatically
+			 * on overflow. When the actual reset takes place
+			 * depends on the masking, switch and notification
+			 * status. It may be deferred until pfm_restart().
+			 */
+			ovfl_ctrl |= ovfl_arg->ovfl_ctrl;
+
+			pfm_stats_add(fmt_handler_ns, end_cycles - start_cycles);
+		}
+		/*
+		 * when the format cannot handle the rest of the overflow,
+		 * we abort right here
+		 */
+		if (ret) {
+			PFM_DBG_ovfl("handler aborted at PMD%u ret=%d",
+				     i, ret);
+		}
+	} else {
+		/*
+		 * When no sampling format is used, the default
+		 * is:
+		 * 	- mask monitoring
+		 * 	- notify user if requested
+		 *
+		 * If notification is not requested, monitoring is masked
+		 * and overflowed counters are not reset (saturation).
+		 * This mimics the behavior of the default sampling format.
+		 */
+		ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY;
+
+		if (!must_switch || has_notify)
+			ovfl_ctrl |= PFM_OVFL_CTRL_MASK;
+	}
+	t1 = sched_clock();
+	pfm_stats_add(ovfl_intr_p2_ns, t1 - t0);
+	t0 = t1;
+
+	PFM_DBG_ovfl("set%u o_notify=0x%llx o_pmds=0x%llx "
+		     "r_pmds=0x%llx ovfl_ctrl=0x%x",
+		     set->id,
+		     (unsigned long long)ovfl_notify[0],
+		     (unsigned long long)ovfl_pmds[0],
+		     (unsigned long long)reset_pmds[0],
+		     ovfl_ctrl);
+
+	/*
+	 * we only reset (short reset) when we are not masking. Otherwise
+	 * the reset is postponed until restart.
+	 */
+	if (likely(!(ovfl_ctrl & PFM_OVFL_CTRL_MASK))) {
+		if (must_switch) {
+			/*
+			 * pfm_switch_sets() takes care
+			 * of resetting new set if needed
+			 */
+			pfm_switch_sets_from_intr(ctx);
+
+			/*
+			 * update our view of the active set
+			 */
+			set = ctx->active_set;
+
+			must_switch = 0;
+		} else if (ovfl_ctrl & PFM_OVFL_CTRL_RESET) {
+			u16 nn;
+			t0 = sched_clock();
+			nn = bitmap_weight(cast_ulp(reset_pmds), max_pmd);
+			if (nn)
+				pfm_reset_pmds(ctx, set, nn, PFM_PMD_RESET_SHORT);
+		}
+		/*
+		 * do not block if not masked
+		 */
+		ctx_block = 0;
+	} else {
+		pfm_mask_monitoring(ctx, set);
+	}
+	/*
+	 * if we have not switched here, then remember for the
+	 * time monitoring is restarted
+	 */
+	if (must_switch)
+		set->priv_flags |= PFM_SETFL_PRIV_SWITCH;
+
+	/*
+	 * block only if CTRL_NOTIFY+CTRL_MASK and requested by user
+	 *
+	 * Defer notification until last operation in the handler
+	 * to avoid spinlock contention
+	 */
+	if (has_notify && (ovfl_ctrl & PFM_OVFL_CTRL_NOTIFY)) {
+		int ret;
+		if (ctx_block) {
+			ctx->flags.work_type = PFM_WORK_BLOCK;
+			set_thread_flag(TIF_PERFMON_WORK);
+		}
+		/*
+		 * Sanity check on the queue.
+		 * Should never happen because queue must be sized
+		 * appropriatly for format
+		 */
+		ret = pfm_ovfl_notify_user(ctx, set_orig, ip);
+		if (unlikely(ret)) {
+			if (ctx->state == PFM_CTX_LOADED)
+				pfm_mask_monitoring(ctx, set);
+		} else {
+			ctx->flags.can_restart++;
+			PFM_DBG_ovfl("can_restart=%u", ctx->flags.can_restart);
+		}
+	}
+
+	t1 = sched_clock();
+	pfm_stats_add(ovfl_intr_p3_ns, t1 - t0);
+
+	return;
+
+stop_monitoring:
+	/*
+	 * Does not happen for a system-wide context nor for a
+	 * self-monitored context. We cannot attach to kernel-only
+	 * thread, thus it is safe to set TIF bits, i.e., the thread
+	 * will eventually leave the kernel or die and either we will
+	 * catch the context and clean it up in pfm_handler_work() or
+	 * pfm_exit_thread().
+	 *
+	 * Mask until we get to pfm_handle_work()
+	 */
+	pfm_mask_monitoring(ctx, set);
+
+	PFM_DBG_ovfl("ctx is zombie, converted to spurious");
+	ctx->flags.work_type = PFM_WORK_ZOMBIE;
+	set_thread_flag(TIF_PERFMON_WORK);
+}
+
+/*
+ * interrupts are masked
+ *
+ * Context locking necessary to avoid concurrent accesses from other CPUs
+ * 	- For per-thread, we must prevent pfm_restart() which works when
+ * 	  context is LOADED or MASKED
+ */
+static void __pfm_interrupt_handler(unsigned long iip, struct pt_regs *regs)
+{
+	struct task_struct *task;
+	struct pfm_context *ctx;
+	struct pfm_event_set *set;
+
+	pfm_stats_inc(ovfl_intr_all_count);
+
+	task = __get_cpu_var(pmu_owner);
+	ctx = __get_cpu_var(pmu_ctx);
+
+	if (unlikely(ctx == NULL)) {
+		PFM_DBG_ovfl("no ctx");
+		goto spurious;
+	}
+
+	spin_lock(&ctx->lock);
+
+	set = ctx->active_set;
+
+	/*
+	 * For SMP per-thread, it is not possible to have
+	 * owner != NULL && task != current.
+	 *
+	 * For UP per-thread, because of lazy save, it
+	 * is possible to receive an interrupt in another task
+	 * which is not using the PMU. This means
+	 * that the interrupt was in-flight at the
+	 * time of pfm_ctxswout_thread(). In that
+	 * case it will be replayed when the task
+	 * is scheduled again. Hence we convert to spurious.
+	 *
+	 * The basic rule is that an overflow is always
+	 * processed in the context of the task that
+	 * generated it for all per-thread contexts.
+	 *
+	 * for system-wide, task is always NULL
+	 */
+#ifndef CONFIG_SMP
+	if (unlikely((task && current->pfm_context != ctx))) {
+		PFM_DBG_ovfl("spurious: not owned by current task");
+		goto spurious;
+	}
+#endif
+	if (unlikely(!pfm_arch_is_active(ctx))) {
+		PFM_DBG_ovfl("spurious: monitoring non active");
+		goto spurious;
+	}
+
+	/*
+	 * freeze PMU and collect overflowed PMD registers
+	 * into set->povfl_pmds. Number of overflowed PMDs reported
+	 * in set->npend_ovfls
+	 */
+	pfm_arch_intr_freeze_pmu(ctx, set);
+	if (unlikely(!set->npend_ovfls)) {
+		PFM_DBG_ovfl("no npend_ovfls");
+		goto spurious;
+	}
+
+	pfm_stats_inc(ovfl_intr_regular_count);
+
+	pfm_overflow_handler(ctx, set, iip, regs);
+
+	pfm_arch_intr_unfreeze_pmu(ctx);
+
+	spin_unlock(&ctx->lock);
+
+	return;
+
+spurious:
+	/* ctx may be NULL */
+	pfm_arch_intr_unfreeze_pmu(ctx);
+	if (ctx)
+		spin_unlock(&ctx->lock);
+
+	pfm_stats_inc(ovfl_intr_spurious_count);
+}
+
+void pfm_interrupt_handler(unsigned long iip, struct pt_regs *regs)
+{
+	u64 start;
+
+	BUG_ON(!irqs_disabled());
+
+	start = sched_clock();
+
+	__pfm_interrupt_handler(iip, regs);
+
+	pfm_stats_add(ovfl_intr_ns, sched_clock() - start);
+}
+EXPORT_SYMBOL(pfm_interrupt_handler);
+
diff -Naur linux-2.6.25-org/perfmon/perfmon_pmu.c linux-2.6.25-id/perfmon/perfmon_pmu.c
--- linux-2.6.25-org/perfmon/perfmon_pmu.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_pmu.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,599 @@
+/*
+ * perfmon_pmu.c: perfmon2 PMU configuration management
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon.h>
+
+#ifndef CONFIG_MODULE_UNLOAD
+#define module_refcount(n)	1
+#endif
+
+static __cacheline_aligned_in_smp int request_mod_in_progress;
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_pmu_conf_lock);
+
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_pmu_acq_lock);
+static u32 pfm_pmu_acquired;
+
+/*
+ * perfmon core must acces PMU information ONLY through pfm_pmu_conf
+ * if pfm_pmu_conf is NULL, then no description is registered
+ */
+struct pfm_pmu_config	*pfm_pmu_conf;
+EXPORT_SYMBOL(pfm_pmu_conf);
+
+static inline int pmu_is_module(struct pfm_pmu_config *c)
+{
+	return !(c->flags & PFM_PMUFL_IS_BUILTIN);
+}
+
+/*
+ * compute the following:
+ * 	- max_pmc, num_pmcs max_pmd, num_pmds
+ * 	- first_intr_pmd, max_rw_pmd
+ * based on existing regdesc->pmds and regdesc->pmcs
+ */
+static void pfm_pmu_regdesc_calc_limits(struct pfm_regdesc *d)
+{
+	u16 n, n2, n_counters, i;
+	int max1, max2, max3, first_intr, first_i;
+
+	n = 0;
+	max1 = max2 = -1;
+	for (i = 0; i < pfm_pmu_conf->num_pmc_entries;  i++) {
+		if (!test_bit(i, cast_ulp(d->pmcs)))
+			continue;
+		max1 = i;
+		n++;
+	}
+	d->max_pmc = max1 + 1;
+	d->num_pmcs = n;
+
+	n = n_counters = n2 = 0;
+	max1 = max2 = max3 = first_intr = first_i = -1;
+	for (i = 0; i < pfm_pmu_conf->num_pmd_entries;  i++) {
+		if (!test_bit(i, cast_ulp(d->pmds)))
+			continue;
+
+		if (first_i == -1)
+			first_i = i;
+
+		max1 = i;
+		n++;
+
+		/*
+		 * read-write registers
+		 */
+		if (!(pfm_pmu_conf->pmd_desc[i].type & PFM_REG_RO)) {
+			max3 = i;
+			n2++;
+		}
+
+		/*
+		 * counters registers
+		 */
+		if (pfm_pmu_conf->pmd_desc[i].type & PFM_REG_C64)
+			n_counters++;
+
+		if (pfm_pmu_conf->pmd_desc[i].type & PFM_REG_INTR) {
+			max2 = i;
+			if (first_intr == -1)
+				first_intr = i;
+		}
+	}
+	d->max_pmd = max1 + 1;
+	d->first_intr_pmd = first_intr == -1 ?  first_i : first_intr;
+
+	d->max_intr_pmd  = max2 + 1;
+
+	d->num_counters = n_counters;
+	d->num_pmds = n;
+	d->max_rw_pmd = max3 + 1;
+	d->num_rw_pmd = n2;
+}
+
+static int pfm_regdesc_init(struct pfm_regdesc *d, struct pfm_pmu_config *cfg)
+{
+	u16 n, n2, n_counters, i;
+	int max1, max2, max3, first_intr, first_i;
+
+	memset(d, 0 , sizeof(*d));
+	/*
+	 * compute the number of implemented PMC from the
+	 * description table
+	 */
+	n = 0;
+	max1 = max2 = -1;
+	for (i = 0; i < cfg->num_pmc_entries;  i++) {
+		if (!(cfg->pmc_desc[i].type & PFM_REG_I))
+			continue;
+
+		__set_bit(i, cast_ulp(d->pmcs));
+
+		max1 = i;
+		n++;
+	}
+
+	if (!n) {
+		PFM_INFO("%s PMU description has no PMC registers",
+			 cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	d->max_pmc = max1 + 1;
+	d->num_pmcs = n;
+
+	n = n_counters = n2 = 0;
+	max1 = max2 = max3 = first_intr = first_i = -1;
+	for (i = 0; i < cfg->num_pmd_entries;  i++) {
+		if (!(cfg->pmd_desc[i].type & PFM_REG_I))
+			continue;
+
+		if (first_i == -1)
+			first_i = i;
+
+		__set_bit(i, cast_ulp(d->pmds));
+		max1 = i;
+		n++;
+
+		/*
+		 * read-write registers
+		 */
+		if (!(cfg->pmd_desc[i].type & PFM_REG_RO)) {
+			__set_bit(i, cast_ulp(d->rw_pmds));
+			max3 = i;
+			n2++;
+		}
+
+		/*
+		 * counters registers
+		 */
+		if (cfg->pmd_desc[i].type & PFM_REG_C64) {
+			__set_bit(i, cast_ulp(d->cnt_pmds));
+			n_counters++;
+		}
+
+		/*
+		 * PMD with intr capabilities
+		 */
+		if (cfg->pmd_desc[i].type & PFM_REG_INTR) {
+			__set_bit(i, cast_ulp(d->intr_pmds));
+			max2 = i;
+			if (first_intr == -1)
+				first_intr = i;
+		}
+	}
+
+	if (!n) {
+		PFM_INFO("%s PMU description has no PMD registers",
+			 cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	d->max_pmd = max1 + 1;
+	d->first_intr_pmd = first_intr == -1 ?  first_i : first_intr;
+
+	d->max_intr_pmd  = max2 + 1;
+
+	d->num_counters = n_counters;
+	d->num_pmds = n;
+	d->max_rw_pmd = max3 + 1;
+	d->num_rw_pmd = n2;
+
+	return 0;
+}
+
+/*
+ * initialize PMU configuration from PMU config descriptor
+ */
+static int pfm_pmu_config_init(struct pfm_pmu_config *cfg)
+{
+	int ret;
+
+	/*
+	 * we build the register description using the full mapping
+	 * table as defined by the module. On first use, we update
+	 * the current description (regs) based on local constraints.
+	 */
+	ret = pfm_regdesc_init(&cfg->full_regs, cfg);
+	if (ret)
+		return ret;
+
+	if (!cfg->version)
+		cfg->version = "0.0";
+
+	pfm_pmu_conf = cfg;
+	pfm_pmu_conf->ovfl_mask = (1ULL << cfg->counter_width) -1;
+
+	PFM_INFO("%s PMU detected, %u PMCs, %u PMDs, %u counters (%u bits)",
+		 pfm_pmu_conf->pmu_name,
+		 pfm_pmu_conf->full_regs.num_pmcs,
+		 pfm_pmu_conf->full_regs.num_pmds,
+		 pfm_pmu_conf->full_regs.num_counters,
+		 pfm_pmu_conf->counter_width);
+
+	return 0;
+}
+
+int pfm_pmu_register(struct pfm_pmu_config *cfg)
+{
+	u16 i, nspec, nspec_ro, num_pmcs, num_pmds, num_wc = 0;
+	int type, ret = -EBUSY;
+
+	if (perfmon_disabled) {
+		PFM_INFO("perfmon disabled, cannot add PMU description");
+		return -ENOSYS;
+	}
+
+	nspec = nspec_ro = num_pmds = num_pmcs = 0;
+
+	/* some sanity checks */
+	if (cfg == NULL || cfg->pmu_name == NULL) {
+		PFM_INFO("PMU config descriptor is invalid");
+		return -EINVAL;
+	}
+
+	/* must have a probe */
+	if (cfg->probe_pmu == NULL) {
+		PFM_INFO("PMU config has no probe routine");
+		return -EINVAL;
+	}
+
+	/*
+	 * execute probe routine before anything else as it
+	 * may update configuration tables
+	 */
+	if ((*cfg->probe_pmu)() == -1) {
+		PFM_INFO("%s PMU detection failed", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	if (!(cfg->flags & PFM_PMUFL_IS_BUILTIN) && cfg->owner == NULL) {
+		PFM_INFO("PMU config %s is missing owner", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	if (!cfg->num_pmd_entries) {
+		PFM_INFO("%s needs to define num_pmd_entries", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	if (!cfg->num_pmc_entries) {
+		PFM_INFO("%s needs to define num_pmc_entries", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	if (!cfg->counter_width) {
+		PFM_INFO("PMU config %s, zero width counters", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	/*
+	 * REG_RO, REG_V not supported on PMC registers
+	 */
+	for (i = 0; i < cfg->num_pmc_entries;  i++) {
+
+		type = cfg->pmc_desc[i].type;
+
+		if (type & PFM_REG_I)
+			num_pmcs++;
+
+		if (type & PFM_REG_WC)
+			num_wc++;
+
+		if (type & PFM_REG_V) {
+			PFM_INFO("PFM_REG_V is not supported on "
+				 "PMCs (PMC%d)", i);
+			return -EINVAL;
+		}
+		if (type & PFM_REG_RO) {
+			PFM_INFO("PFM_REG_RO meaningless on "
+				 "PMCs (PMC%u)", i);
+			return -EINVAL;
+		}
+	}
+
+	if (num_wc && cfg->pmc_write_check == NULL) {
+		PFM_INFO("some PMCs have write-checker but no callback provided\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * check virtual PMD registers
+	 */
+	num_wc= 0;
+	for (i = 0; i < cfg->num_pmd_entries;  i++) {
+
+		type = cfg->pmd_desc[i].type;
+
+		if (type & PFM_REG_I)
+			num_pmds++;
+
+		if (type & PFM_REG_V) {
+			nspec++;
+			if (type & PFM_REG_RO)
+				nspec_ro++;
+		}
+
+		if (type & PFM_REG_WC)
+			num_wc++;
+	}
+
+	if (num_wc && cfg->pmd_write_check == NULL) {
+		PFM_INFO("PMD have write-checker but no callback provided\n");
+		return -EINVAL;
+	}
+
+	if (nspec && cfg->pmd_sread == NULL) {
+		PFM_INFO("PMU config is missing pmd_sread()");
+		return -EINVAL;
+	}
+
+	nspec = nspec - nspec_ro;
+	if (nspec && cfg->pmd_swrite == NULL) {
+		PFM_INFO("PMU config is missing pmd_swrite()");
+		return -EINVAL;
+	}
+
+	if (num_pmcs >= PFM_MAX_PMCS) {
+		PFM_INFO("%s PMCS registers exceed name space [0-%u]",
+			 cfg->pmu_name,
+			 PFM_MAX_PMCS);
+		return -EINVAL;
+	}
+	if (num_pmds >= PFM_MAX_PMDS) {
+		PFM_INFO("%s PMDS registers exceed name space [0-%u]",
+			 cfg->pmu_name,
+			 PFM_MAX_PMDS);
+		return -EINVAL;
+	}
+	spin_lock(&pfm_pmu_conf_lock);
+
+	if (pfm_pmu_conf)
+		goto unlock;
+
+	ret = pfm_pmu_config_init(cfg);
+	if (ret)
+		goto unlock;
+
+	ret = pfm_arch_pmu_config_init(pfm_pmu_conf);
+	if (ret)
+		goto unlock;
+
+	ret = pfm_sysfs_add_pmu(pfm_pmu_conf);
+	if (ret) {
+		pfm_arch_pmu_config_remove();
+		pfm_pmu_conf = NULL;
+	}
+
+unlock:
+	spin_unlock(&pfm_pmu_conf_lock);
+
+	if (ret) {
+		PFM_INFO("register %s PMU error %d", cfg->pmu_name, ret);
+	} else {
+		PFM_INFO("%s PMU installed", cfg->pmu_name);
+		/*
+		 * (re)initialize PMU on each PMU now that we have a description
+		 */
+		on_each_cpu(__pfm_init_percpu, cfg, 0, 0);
+	}
+	return ret;
+}
+EXPORT_SYMBOL(pfm_pmu_register);
+
+/*
+ * remove PMU description. Caller must pass address of current
+ * configuration. This is mostly for sanity checking as only
+ * one config can exist at any time.
+ *
+ * We are using the module refcount mechanism to protect against
+ * removal while the configuration is being used. As long as there is
+ * one context, a PMU configuration cannot be removed. The protection is
+ * managed in module logic.
+ */
+void pfm_pmu_unregister(struct pfm_pmu_config *cfg)
+{
+	if (!(cfg ||pfm_pmu_conf))
+		return;
+
+	spin_lock(&pfm_pmu_conf_lock);
+
+	BUG_ON(module_refcount(pfm_pmu_conf->owner));
+
+	if (cfg->owner == pfm_pmu_conf->owner) {
+		pfm_arch_pmu_config_remove();
+		pfm_sysfs_remove_pmu(pfm_pmu_conf);
+		pfm_pmu_conf = NULL;
+	}
+
+	spin_unlock(&pfm_pmu_conf_lock);
+}
+EXPORT_SYMBOL(pfm_pmu_unregister);
+
+static int pfm_pmu_request_module(void)
+{
+	char *mod_name;
+	int ret;
+
+	mod_name = pfm_arch_get_pmu_module_name();
+	if (mod_name == NULL)
+		return -ENOSYS;
+
+	ret = request_module(mod_name);
+
+	PFM_DBG("mod=%s ret=%d\n", mod_name, ret);
+	return ret;
+}
+
+/*
+ * autoload:
+ * 	0     : do not try to autoload the PMU description module
+ * 	not 0 : try to autoload the PMU description module
+ */
+int pfm_pmu_conf_get(int autoload)
+{
+	int ret;
+
+	spin_lock(&pfm_pmu_conf_lock);
+
+	if (request_mod_in_progress) {
+		ret = -ENOSYS;
+		goto skip;
+	}
+
+	if (autoload && pfm_pmu_conf == NULL) {
+
+		request_mod_in_progress = 1;
+
+		spin_unlock(&pfm_pmu_conf_lock);
+
+		pfm_pmu_request_module();
+
+		spin_lock(&pfm_pmu_conf_lock);
+
+		request_mod_in_progress = 0;
+
+		/*
+		 * request_module() may succeed but the module
+		 * may not have registered properly so we need
+		 * to check
+		 */
+	}
+
+	ret = pfm_pmu_conf == NULL ? -ENOSYS : 0;
+	if (!ret && pmu_is_module(pfm_pmu_conf)
+	    && !try_module_get(pfm_pmu_conf->owner))
+		ret = -ENOSYS;
+skip:
+	spin_unlock(&pfm_pmu_conf_lock);
+
+	return ret;
+}
+
+void pfm_pmu_conf_put(void)
+{
+	if (pfm_pmu_conf == NULL || !pmu_is_module(pfm_pmu_conf))
+		return;
+
+	spin_lock(&pfm_pmu_conf_lock);
+	module_put(pfm_pmu_conf->owner);
+	spin_unlock(&pfm_pmu_conf_lock);
+}
+
+
+/*
+ * acquire PMU resource from lower-level PMU register allocator
+ * (currently perfctr-watchdog.c)
+ *
+ * acquisition is done when the first context is created (and not
+ * when it is loaded). We grab all that is defined in the description
+ * module and then we make adjustments at the arch-specific level.
+ *
+ * The PMU resource is released when the last perfmon context is
+ * destroyed.
+ *
+ * interrupts are not masked
+ */
+int pfm_pmu_acquire(void)
+{
+	int ret = 0;
+
+	spin_lock(&pfm_pmu_acq_lock);
+
+	PFM_DBG("pmu_acquired=%d", pfm_pmu_acquired);
+
+	pfm_pmu_acquired++;
+
+	if (pfm_pmu_acquired == 1) {
+		/*
+		 * copy full description and then check if arch-specific
+		 * layer needs some adjustments
+		 */
+		pfm_pmu_conf->regs = pfm_pmu_conf->full_regs;
+
+		ret = pfm_arch_pmu_acquire();
+		if (ret) {
+			pfm_pmu_acquired--;
+		} else {
+			/*
+			 * calculate new limits (num, max)
+			 */
+			pfm_pmu_regdesc_calc_limits(&pfm_pmu_conf->regs);
+
+			/* available PMU ressources */
+			PFM_DBG("PMU acquired: %u PMCs, %u PMDs, %u counters",
+				pfm_pmu_conf->regs.num_pmcs,
+				pfm_pmu_conf->regs.num_pmds,
+				pfm_pmu_conf->regs.num_counters);
+		}
+	}
+	spin_unlock(&pfm_pmu_acq_lock);
+
+	return ret;
+}
+
+/*
+ * release the PMU resource
+ *
+ * actual release happens when last context is destroyed
+ *
+ * interrupts are not masked
+ */
+void pfm_pmu_release(void)
+{
+	BUG_ON(irqs_disabled());
+
+	/*
+	 * we need to use a spinlock because release takes some time
+	 * and we may have a race with pfm_pmu_acquire()
+	 */
+	spin_lock(&pfm_pmu_acq_lock);
+
+	PFM_DBG("pmu_acquired=%d", pfm_pmu_acquired);
+
+	/*
+	 * we decouple test and decrement because if we had errors
+	 * in pfm_pmu_acquire(), we still come here on pfm_context_free()
+	 * but with pfm_pmu_acquire=0
+	 */
+	if (pfm_pmu_acquired > 0 && --pfm_pmu_acquired == 0) {
+		pfm_arch_pmu_release();
+		memset(&pfm_pmu_conf->regs, 0, sizeof(pfm_pmu_conf->regs));
+		PFM_DBG("PMU released");
+	}
+	spin_unlock(&pfm_pmu_acq_lock);
+}
diff -Naur linux-2.6.25-org/perfmon/perfmon_res.c linux-2.6.25-id/perfmon/perfmon_res.c
--- linux-2.6.25-org/perfmon/perfmon_res.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_res.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,419 @@
+/*
+ * perfmon_res.c:  perfmon2 resource allocations
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/spinlock.h>
+#include <linux/perfmon.h>
+#include <linux/module.h>
+
+/*
+ * global information about all sessions
+ * mostly used to synchronize between system wide and per-process
+ */
+struct pfm_sessions {
+	u32		pfs_task_sessions;/* #num loaded per-thread sessions */
+	size_t		pfs_smpl_buffer_mem_cur; /* current smpl buf mem usage */
+	cpumask_t	pfs_sys_cpumask;  /* bitmask of used cpus (system-wide) */
+};
+
+static struct pfm_sessions pfm_sessions;
+
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_sessions_lock);
+
+/*
+ * sampling buffer allocated by perfmon must be
+ * checked against max usage thresholds for security
+ * reasons.
+ *
+ * The first level check is against the system wide limit
+ * as indicated by the system administrator in /proc/sys/kernel/perfmon
+ *
+ * The second level check is on a per-process basis using
+ * RLIMIT_MEMLOCK limit.
+ *
+ * Operating on the current task only.
+ */
+int pfm_reserve_buf_space(size_t size)
+{
+	struct mm_struct *mm;
+	unsigned long locked;
+	unsigned long buf_mem, buf_mem_max;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pfm_sessions_lock, flags);
+
+	/*
+	 * check against global buffer limit
+	 */
+	buf_mem_max = pfm_controls.smpl_buffer_mem_max;
+	buf_mem = pfm_sessions.pfs_smpl_buffer_mem_cur + size;
+
+	if (buf_mem <= buf_mem_max) {
+		pfm_sessions.pfs_smpl_buffer_mem_cur = buf_mem;
+
+		PFM_DBG("buf_mem_max=%lu current_buf_mem=%lu",
+			buf_mem_max,
+			buf_mem);
+	}
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+
+	if (buf_mem > buf_mem_max) {
+		PFM_DBG("smpl buffer memory threshold reached");
+		return -ENOMEM;
+	}
+
+	/*
+	 * check against RLIMIT_MEMLOCK
+	 */
+	mm = get_task_mm(current);
+
+	down_write(&mm->mmap_sem);
+
+	locked  = mm->locked_vm << PAGE_SHIFT;
+	locked += size;
+
+	if (locked > current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur) {
+
+		PFM_DBG("RLIMIT_MEMLOCK reached ask_locked=%lu rlim_cur=%lu",
+			locked,
+			current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
+
+		up_write(&mm->mmap_sem);
+		mmput(mm);
+		goto unres;
+	}
+
+	mm->locked_vm = locked >> PAGE_SHIFT;
+
+	up_write(&mm->mmap_sem);
+
+	mmput(mm);
+
+	return 0;
+
+unres:
+	/*
+	 * remove global buffer memory allocation
+	 */
+	spin_lock_irqsave(&pfm_sessions_lock, flags);
+
+	pfm_sessions.pfs_smpl_buffer_mem_cur -= size;
+
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+
+	return -ENOMEM;
+}
+/*
+ *There exist multiple paths leading to this function. We need to
+ * be very careful withlokcing on the mmap_sem as it may already be
+ * held by the time we come here.
+ * The following paths exist:
+ *
+ * exit path:
+ * sys_exit_group
+ *    do_group_exit
+ *     do_exit
+ *      exit_mm
+ *       mmput
+ *        exit_mmap
+ *         remove_vma
+ *          fput
+ *           __fput
+ *            pfm_close
+ *             __pfm_close
+ *              pfm_context_free
+ * 	         pfm_release_buf_space
+ * munmap path:
+ * sys_munmap
+ *  do_munmap
+ *   remove_vma
+ *    fput
+ *     __fput
+ *      pfm_close
+ *       __pfm_close
+ *        pfm_context_free
+ *         pfm_release_buf_space
+ *
+ * close path:
+ * sys_close
+ *  filp_close
+ *   fput
+ *    __fput
+ *     pfm_close
+ *      __pfm_close
+ *       pfm_context_free
+ *        pfm_release_buf_space
+ *
+ * The issue is that on the munmap() path, the mmap_sem is already held
+ * in write-mode by the time we come here. To avoid the deadlock, we need
+ * to know where we are coming from and skip down_write(). If is fairly
+ * difficult to know this because of the lack of good hooks and
+ * the fact that, there may not have been any mmap() of the sampling buffer
+ * (i.e. create_context() followed by close() or exit()).
+ *
+ * We use a set flag ctx->flags.mmap_nlock which is toggle in the vm_ops
+ * callback in remove_vma() which is called systematically for the call, so
+ * on all but the pure close() path. The exit path does not already hold
+ * the lock but this is exit so there is no task->mm by the time we come here.
+ *
+ * The mmap_nlock is set only when unmapping and this is the LAST reference
+ * to the file (i.e., close() followed by munmap()).
+ */
+void pfm_release_buf_space(struct pfm_context *ctx, size_t size)
+{
+	unsigned long flags;
+	struct mm_struct *mm;
+
+	mm = get_task_mm(current);
+	if (mm) {
+		if (ctx->flags.mmap_nlock == 0) {
+			PFM_DBG("doing down_write");
+			down_write(&mm->mmap_sem);
+		}
+
+		mm->locked_vm -= size >> PAGE_SHIFT;
+
+		PFM_DBG("locked_vm=%lu size=%zu", mm->locked_vm, size);
+
+		if (ctx->flags.mmap_nlock == 0)
+			up_write(&mm->mmap_sem);
+
+		mmput(mm);
+	}
+
+	spin_lock_irqsave(&pfm_sessions_lock, flags);
+
+	pfm_sessions.pfs_smpl_buffer_mem_cur -= size;
+
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+}
+
+int pfm_reserve_session(int is_system, u32 cpu)
+{
+	unsigned long flags;
+	u32 nsys_cpus;
+	int ret = 0;
+
+	/*
+	 * validy checks on cpu_mask have been done upstream
+	 */
+	spin_lock_irqsave(&pfm_sessions_lock, flags);
+
+	nsys_cpus = cpus_weight(pfm_sessions.pfs_sys_cpumask);
+
+	PFM_DBG("in  sys=%u task=%u is_sys=%d cpu=%u",
+		nsys_cpus,
+		pfm_sessions.pfs_task_sessions,
+		is_system,
+		cpu);
+
+	if (is_system) {
+		/*
+		 * cannot mix system wide and per-task sessions
+		 */
+		if (pfm_sessions.pfs_task_sessions > 0) {
+			PFM_DBG("%u conflicting task_sessions",
+				pfm_sessions.pfs_task_sessions);
+			ret = -EBUSY;
+			goto abort;
+		}
+
+		if (cpu_isset(cpu, pfm_sessions.pfs_sys_cpumask)) {
+			PFM_DBG("conflicting session on CPU%u", cpu);
+			ret = -EBUSY;
+			goto abort;
+		}
+
+		PFM_DBG("reserved session on CPU%u", cpu);
+
+		cpu_set(cpu, pfm_sessions.pfs_sys_cpumask);
+		nsys_cpus++;
+	} else {
+		if (nsys_cpus) {
+			ret = -EBUSY;
+			goto abort;
+		}
+		pfm_sessions.pfs_task_sessions++;
+	}
+
+	PFM_DBG("out sys=%u task=%u is_sys=%d cpu=%u",
+		nsys_cpus,
+		pfm_sessions.pfs_task_sessions,
+		is_system,
+		cpu);
+
+abort:
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+
+	return ret;
+}
+
+/*
+ * called from __pfm_unload_context()
+ */
+int pfm_release_session(int is_system, u32 cpu)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pfm_sessions_lock, flags);
+
+	PFM_DBG("in sys_sessions=%u task_sessions=%u syswide=%d cpu=%u",
+		cpus_weight(pfm_sessions.pfs_sys_cpumask),
+		pfm_sessions.pfs_task_sessions,
+		is_system, cpu);
+
+	if (is_system)
+		cpu_clear(cpu, pfm_sessions.pfs_sys_cpumask);
+	else
+		pfm_sessions.pfs_task_sessions--;
+
+	PFM_DBG("out sys_sessions=%u task_sessions=%u syswide=%d cpu=%u",
+		cpus_weight(pfm_sessions.pfs_sys_cpumask),
+		pfm_sessions.pfs_task_sessions,
+		is_system, cpu);
+
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+	return 0;
+}
+
+int pfm_reserve_allcpus(void)
+{
+	unsigned long flags;
+	u32 nsys_cpus, cpu;
+
+	spin_lock_irqsave(&pfm_sessions_lock, flags);
+
+	nsys_cpus = cpus_weight(pfm_sessions.pfs_sys_cpumask);
+
+	PFM_DBG("in  sys=%u task=%u",
+		nsys_cpus,
+		pfm_sessions.pfs_task_sessions);
+
+	if (nsys_cpus) {
+		PFM_DBG("already some system-wide sessions");
+		goto abort;
+	}
+
+	/*
+	 * cannot mix system wide and per-task sessions
+	 */
+	if (pfm_sessions.pfs_task_sessions) {
+		PFM_DBG("%u conflicting task_sessions",
+		  	pfm_sessions.pfs_task_sessions);
+		goto abort;
+	}
+
+	for_each_online_cpu(cpu) {
+		cpu_set(cpu, pfm_sessions.pfs_sys_cpumask);
+		nsys_cpus++;
+	}
+
+	PFM_DBG("out sys=%u task=%u",
+		nsys_cpus,
+		pfm_sessions.pfs_task_sessions);
+
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+
+	return 0;
+
+abort:
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+
+	return -EBUSY;
+}
+EXPORT_SYMBOL(pfm_reserve_allcpus);
+
+int pfm_release_allcpus(void)
+{
+	unsigned long flags;
+	u32 nsys_cpus, cpu;
+
+	spin_lock_irqsave(&pfm_sessions_lock, flags);
+
+	nsys_cpus = cpus_weight(pfm_sessions.pfs_sys_cpumask);
+
+	PFM_DBG("in  sys=%u task=%u",
+		nsys_cpus,
+		pfm_sessions.pfs_task_sessions);
+
+	/*
+	 * XXX: could use __cpus_clear() with nbits
+	 */
+	for_each_online_cpu(cpu) {
+		cpu_clear(cpu, pfm_sessions.pfs_sys_cpumask);
+		nsys_cpus--;
+	}
+
+	PFM_DBG("out sys=%u task=%u",
+		nsys_cpus,
+		pfm_sessions.pfs_task_sessions);
+
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(pfm_release_allcpus);
+
+/*
+ * called from perfmon_sysfs.c:
+ *  what=0 : pfs_task_sessions
+ *  what=1 : cpus_weight(pfs_sys_cpumask)
+ *  what=2 : smpl_buffer_mem_cur
+ *  what=3 : pmu model name
+ *
+ * return number of bytes written into buf (up to sz)
+ */
+ssize_t pfm_sysfs_session_show(char *buf, size_t sz, int what)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pfm_sessions_lock, flags);
+
+	switch (what) {
+	case 0: snprintf(buf, sz, "%u\n", pfm_sessions.pfs_task_sessions);
+		break;
+	case 1: snprintf(buf, sz, "%d\n", cpus_weight(pfm_sessions.pfs_sys_cpumask));
+		break;
+	case 2: snprintf(buf, sz, "%zu\n", pfm_sessions.pfs_smpl_buffer_mem_cur);
+		break;
+	case 3:
+		snprintf(buf, sz, "%s\n",
+			pfm_pmu_conf ?	pfm_pmu_conf->pmu_name
+				     :	"unknown\n");
+	}
+	spin_unlock_irqrestore(&pfm_sessions_lock, flags);
+	return strlen(buf);
+}
diff -Naur linux-2.6.25-org/perfmon/perfmon_rw.c linux-2.6.25-id/perfmon/perfmon_rw.c
--- linux-2.6.25-org/perfmon/perfmon_rw.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_rw.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,611 @@
+/*
+ * perfmon.c: perfmon2 PMC/PMD read/write system calls
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net/
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/perfmon.h>
+
+#define PFM_REGFL_PMC_ALL	(PFM_REGFL_NO_EMUL64)
+#define PFM_REGFL_PMD_ALL	(PFM_REGFL_RANDOM|PFM_REGFL_OVFL_NOTIFY)
+
+/*
+ * function called from sys_pfm_write_pmds() to write the
+ * requested PMD registers. The function succeeds whether the context is
+ * attached or not. When attached to another thread, that thread must be
+ * stopped.
+ *
+ * compat: is used only on IA-64 to maintain backward compatibility with v2.0
+ *
+ * The context is locked and interrupts are disabled.
+ */
+int __pfm_write_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count,
+		     int compat)
+{
+	struct pfm_event_set *set, *active_set;
+	u64 value, ovfl_mask;
+	u64 *smpl_pmds, *reset_pmds, *impl_pmds, *impl_rw_pmds;
+	u32 req_flags, flags;
+	u16 cnum, pmd_type, max_pmd, max_pmc;
+	u16 set_id, prev_set_id;
+	int i, can_access_pmu;
+	int is_counter;
+	int ret;
+	pfm_pmd_check_t	wr_func;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	active_set = ctx->active_set;
+	max_pmd	= pfm_pmu_conf->regs.max_pmd;
+	max_pmc	= pfm_pmu_conf->regs.max_pmc;
+	impl_pmds = pfm_pmu_conf->regs.pmds;
+	impl_rw_pmds = pfm_pmu_conf->regs.rw_pmds;
+	wr_func = pfm_pmu_conf->pmd_write_check;
+	set = NULL;
+
+	prev_set_id = 0;
+	can_access_pmu = 0;
+
+	/*
+	 * we cannot access the actual PMD registers when monitoring is masked
+	 */
+	if (unlikely(ctx->state == PFM_CTX_LOADED))
+		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
+			       || ctx->flags.system;
+
+	ret = -EINVAL;
+
+	for (i = 0; i < count; i++, req++) {
+
+		cnum = req->reg_num;
+		set_id = req->reg_set;
+		req_flags = req->reg_flags;
+		smpl_pmds = req->reg_smpl_pmds;
+		reset_pmds = req->reg_reset_pmds;
+		flags = 0;
+
+		/*
+		 * cannot write to unexisting
+		 * writes to read-only register are ignored
+		 */
+		if (unlikely(cnum >= max_pmd
+		    || !test_bit(cnum, cast_ulp(impl_pmds)))) {
+			PFM_DBG("pmd%u is not available", cnum);
+			goto error;
+		}
+
+		pmd_type = pfm_pmu_conf->pmd_desc[cnum].type;
+		is_counter = pmd_type & PFM_REG_C64;
+
+		if (likely(!compat)) {
+			/*
+			 * ensure only valid flags are set
+			 */
+			if (req_flags & ~(PFM_REGFL_PMD_ALL)) {
+				PFM_DBG("pmd%u: invalid flags=0x%x",
+					cnum, req_flags);
+				goto error;
+			}
+			/*
+			 * OVFL_NOTIFY is valid for all types of PMD.
+			 * non counting PMD may trigger PMU interrupt
+			 * and thus may trigger recording of a sample.
+			 * This is the case with AMD family 16 and the
+			 * IBS feature, for instance.
+			 */
+			if (req_flags & PFM_REGFL_OVFL_NOTIFY)
+				flags |= PFM_REGFL_OVFL_NOTIFY;
+
+			/*
+			 * We allow randomization to non counting
+			 * PMD as well.
+			 */
+			if (req_flags & PFM_REGFL_RANDOM)
+				flags |= PFM_REGFL_RANDOM;
+
+			/*
+			 * verify validity of smpl_pmds
+			 */
+			if (unlikely(!bitmap_subset(cast_ulp(smpl_pmds),
+						    cast_ulp(impl_pmds),
+						    max_pmd))) {
+				PFM_DBG("invalid smpl_pmds=0x%llx "
+					"for pmd%u",
+					(unsigned long long)smpl_pmds[0],
+					cnum);
+				goto error;
+			}
+
+			/*
+			 * verify validity of reset_pmds
+			 * check against impl_rw_pmds because it is not
+			 * possible to reset read-only PMDs
+			 */
+			if (unlikely(!bitmap_subset(cast_ulp(reset_pmds),
+						    cast_ulp(impl_rw_pmds),
+						    max_pmd))) {
+				PFM_DBG("invalid reset_pmds=0x%llx "
+					"for pmd%u",
+					(unsigned long long)reset_pmds[0],
+					cnum);
+				goto error;
+			}
+		}
+
+		/*
+		 * locate event set
+		 */
+		if (i == 0 || set_id != prev_set_id) {
+			set = pfm_find_set(ctx, set_id, 0);
+			if (set == NULL) {
+				PFM_DBG("event set%u does not exist",
+					set_id);
+				goto error;
+			}
+		}
+
+		/*
+		 * execute write checker, if any
+		 */
+		if (unlikely(wr_func && (pmd_type & PFM_REG_WC))) {
+			ret = (*wr_func)(ctx, set, req);
+			if (ret)
+				goto error;
+
+		}
+
+		value = req->reg_value;
+
+		/*
+		 * now commit changes to software state
+		 */
+
+		if (likely(!compat)) {
+			set->pmds[cnum].flags = flags;
+
+			/*
+			 * copy reset and sampling bitvectors
+			 */
+			bitmap_copy(cast_ulp(set->pmds[cnum].reset_pmds),
+					cast_ulp(reset_pmds),
+					max_pmd);
+
+			bitmap_copy(cast_ulp(set->pmds[cnum].smpl_pmds),
+					cast_ulp(smpl_pmds),
+					max_pmd);
+
+			set->pmds[cnum].eventid = req->reg_smpl_eventid;
+
+			/*
+			 * Mark reset/smpl PMDS as used.
+			 *
+			 * We do not keep track of PMC because we have to
+			 * systematically restore ALL of them.
+			 */
+			bitmap_or(cast_ulp(set->used_pmds),
+					cast_ulp(set->used_pmds),
+					cast_ulp(reset_pmds), max_pmd);
+
+			bitmap_or(cast_ulp(set->used_pmds),
+					cast_ulp(set->used_pmds),
+					cast_ulp(smpl_pmds), max_pmd);
+
+			/*
+			 * we reprogram the PMD hence, we clear any pending
+			 * ovfl. Does affect ovfl switch on restart but new
+			 * value has already been established here
+			 */
+			if (test_bit(cnum, cast_ulp(set->povfl_pmds))) {
+				set->npend_ovfls--;
+				__clear_bit(cnum, cast_ulp(set->povfl_pmds));
+			}
+			__clear_bit(cnum, cast_ulp(set->ovfl_pmds));
+
+			/*
+			 * update ovfl_notify
+			 */
+			if (flags & PFM_REGFL_OVFL_NOTIFY)
+				__set_bit(cnum, cast_ulp(set->ovfl_notify));
+			else
+				__clear_bit(cnum, cast_ulp(set->ovfl_notify));
+		}
+		/*
+		 * establish new switch count
+		 */
+		set->pmds[cnum].ovflsw_thres = req->reg_ovfl_switch_cnt;
+		set->pmds[cnum].ovflsw_ref_thres = req->reg_ovfl_switch_cnt;
+
+		/*
+		 * set last value to new value for all types of PMD
+		 */
+		set->pmds[cnum].lval = value;
+
+		/*
+		 * update reset values (not just for counters)
+		 */
+		set->pmds[cnum].long_reset = req->reg_long_reset;
+		set->pmds[cnum].short_reset = req->reg_short_reset;
+
+		/*
+		 * update randomization mask
+		 */
+		set->pmds[cnum].mask = req->reg_random_mask;
+
+		/*
+		 * update set values
+		 */
+		set->pmds[cnum].value = value;
+
+		__set_bit(cnum, cast_ulp(set->used_pmds));
+
+		if (set == active_set) {
+			set->priv_flags |= PFM_SETFL_PRIV_MOD_PMDS;
+			if (can_access_pmu)
+				pfm_write_pmd(ctx, cnum, value);
+		}
+
+		/*
+		 * update number of used PMD registers
+		 */
+		set->nused_pmds = bitmap_weight(cast_ulp(set->used_pmds), max_pmd);
+
+		prev_set_id = set_id;
+
+		PFM_DBG("set%u pmd%u=0x%llx flags=0x%x a_pmu=%d "
+			"ctx_pmd=0x%llx s_reset=0x%llx "
+			"l_reset=0x%llx u_pmds=0x%llx nu_pmds=%u "
+			"s_pmds=0x%llx r_pmds=0x%llx o_pmds=0x%llx "
+			"o_thres=%llu compat=%d eventid=%llx",
+			set->id,
+			cnum,
+			(unsigned long long)value,
+			set->pmds[cnum].flags,
+			can_access_pmu,
+			(unsigned long long)set->pmds[cnum].value,
+			(unsigned long long)set->pmds[cnum].short_reset,
+			(unsigned long long)set->pmds[cnum].long_reset,
+			(unsigned long long)set->used_pmds[0],
+			set->nused_pmds,
+			(unsigned long long)set->pmds[cnum].smpl_pmds[0],
+			(unsigned long long)set->pmds[cnum].reset_pmds[0],
+			(unsigned long long)set->ovfl_pmds[0],
+			(unsigned long long)set->pmds[cnum].ovflsw_thres,
+			compat,
+			(unsigned long long)set->pmds[cnum].eventid);
+	}
+	ret = 0;
+
+error:
+	/*
+	 * make changes visible
+	 */
+	if (can_access_pmu)
+		pfm_arch_serialize();
+
+	return ret;
+}
+
+/*
+ * function called from sys_pfm_write_pmcs() to write the
+ * requested PMC registers. The function succeeds whether the context is
+ * attached or not. When attached to another thread, that thread must be
+ * stopped.
+ *
+ * The context is locked and interrupts are disabled.
+ */
+int __pfm_write_pmcs(struct pfm_context *ctx, struct pfarg_pmc *req, int count)
+{
+	struct pfm_event_set *set, *active_set;
+	u64 value, dfl_val, rsvd_msk;
+	u64 *impl_pmcs;
+	int i, can_access_pmu;
+	int ret;
+	u16 set_id, prev_set_id;
+	u16 cnum, pmc_type, max_pmc;
+	u32 flags;
+	pfm_pmc_check_t	wr_func;
+
+	active_set = ctx->active_set;
+
+	wr_func = pfm_pmu_conf->pmc_write_check;
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+	impl_pmcs = pfm_pmu_conf->regs.pmcs;
+
+	set = NULL;
+	prev_set_id = 0;
+	can_access_pmu = 0;
+
+	/*
+	 * we cannot access the actual PMC registers when monitoring is masked
+	 */
+	if (unlikely(ctx->state == PFM_CTX_LOADED))
+		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
+			        || ctx->flags.system;
+
+	ret = -EINVAL;
+
+	for (i = 0; i < count; i++, req++) {
+
+		cnum = req->reg_num;
+		set_id = req->reg_set;
+		value = req->reg_value;
+		flags = req->reg_flags;
+
+		/*
+		 * no access to unavailable PMC register
+		 */
+		if (unlikely(cnum >= max_pmc
+			     || !test_bit(cnum, cast_ulp(impl_pmcs)))) {
+			PFM_DBG("pmc%u is not available", cnum);
+			goto error;
+		}
+
+		pmc_type = pfm_pmu_conf->pmc_desc[cnum].type;
+		dfl_val = pfm_pmu_conf->pmc_desc[cnum].dfl_val;
+		rsvd_msk = pfm_pmu_conf->pmc_desc[cnum].rsvd_msk;
+
+		/*
+		 * ensure only valid flags are set
+		 */
+		if (flags & ~PFM_REGFL_PMC_ALL) {
+			PFM_DBG("pmc%u: invalid flags=0x%x", cnum, flags);
+			goto error;
+		}
+
+		/*
+		 * locate event set
+		 */
+		if (i == 0 || set_id != prev_set_id) {
+			set = pfm_find_set(ctx, set_id, 0);
+			if (set == NULL) {
+				PFM_DBG("event set%u does not exist",
+					set_id);
+				goto error;
+			}
+		}
+
+		/*
+		 * set reserved bits to default values
+		 * (reserved bits must be 1 in rsvd_msk)
+		 */
+		value = (value & ~rsvd_msk) | (dfl_val & rsvd_msk);
+
+		if (flags & PFM_REGFL_NO_EMUL64) {
+			if (!(pmc_type & PFM_REG_NO64)) {
+				PFM_DBG("pmc%u no support for "
+					"PFM_REGFL_NO_EMUL64", cnum);
+				goto error;
+			}
+			value &= ~pfm_pmu_conf->pmc_desc[cnum].no_emul64_msk;
+		}
+
+		/*
+		 * execute write checker, if any
+		 */
+		if (likely(wr_func && (pmc_type & PFM_REG_WC))) {
+			req->reg_value = value;
+			ret = (*wr_func)(ctx, set, req);
+			if (ret)
+				goto error;
+			value = req->reg_value;
+		}
+
+		/*
+		 * Now we commit the changes
+		 */
+
+		/*
+		 * mark PMC register as used
+		 * We do not track associated PMC register based on
+		 * the fact that they will likely need to be written
+		 * in order to become useful at which point the statement
+		 * below will catch that.
+		 *
+		 * The used_pmcs bitmask is only useful on architectures where
+		 * the PMC needs to be modified for particular bits, especially
+		 * on overflow or to stop/start.
+		 */
+		if (!test_bit(cnum, cast_ulp(set->used_pmcs))) {
+			__set_bit(cnum, cast_ulp(set->used_pmcs));
+			set->nused_pmcs++;
+		}
+
+		set->pmcs[cnum] = value;
+
+		if (set == active_set) {
+			set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
+			if (can_access_pmu)
+				pfm_arch_write_pmc(ctx, cnum, value);
+		}
+
+		prev_set_id = set_id;
+
+		PFM_DBG("set%u pmc%u=0x%llx a_pmu=%d "
+			"u_pmcs=0x%llx nu_pmcs=%u",
+			set->id,
+			cnum,
+			(unsigned long long)value,
+			can_access_pmu,
+			(unsigned long long)set->used_pmcs[0],
+			set->nused_pmcs);
+	}
+	ret = 0;
+error:
+	/*
+	 * make sure the changes are visible
+	 */
+	if (can_access_pmu)
+		pfm_arch_serialize();
+
+	return ret;
+}
+
+/*
+ * function called from sys_pfm_read_pmds() to read the 64-bit value of
+ * requested PMD registers. The function succeeds whether the context is
+ * attached or not. When attached to another thread, that thread must be
+ * stopped.
+ *
+ * The context is locked and interrupts are disabled.
+ */
+int __pfm_read_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count)
+{
+	u64 val = 0, lval, ovfl_mask, hw_val;
+	u64 sw_cnt;
+	u64 *impl_pmds;
+	struct pfm_event_set *set, *active_set;
+	int i, ret, can_access_pmu = 0;
+	u16 cnum, pmd_type, set_id, prev_set_id, max_pmd;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	impl_pmds = pfm_pmu_conf->regs.pmds;
+	max_pmd   = pfm_pmu_conf->regs.max_pmd;
+	active_set = ctx->active_set;
+	set = NULL;
+	prev_set_id = 0;
+
+	if (likely(ctx->state == PFM_CTX_LOADED)) {
+		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
+			       || ctx->flags.system;
+
+		if (can_access_pmu)
+			pfm_arch_serialize();
+
+		/*
+		 * for CBE SPE events
+		 * The workaround for the read_pmds problem
+		 * which the result pmd value is 0.
+		 * Because can_access_pmu is 0.
+		 * This problem happnens under the following condition.
+		 *  --attach-task option
+		 *  Ctrl-C
+		 *  The target events are SPE event.
+		 *  no ctxsw happened while before Ctrl-C.
+		 *
+		 * The cause of this problem is that the pmd reader HW thread
+		 * is not same as the starter HW thread of the measurement.
+		 *
+		 * Save pmds.
+		 */
+		if (!can_access_pmu/* && pfm_is_ctxsw_type_spe(ctx->active_set)*/)
+			pfm_save_pmds(ctx, ctx->active_set);
+	}
+
+	/*
+	 * on both UP and SMP, we can only read the PMD from the hardware
+	 * register when the task is the owner of the local PMU.
+	 */
+	ret = -EINVAL;
+	for (i = 0; i < count; i++, req++) {
+
+		cnum = req->reg_num;
+		set_id = req->reg_set;
+
+		if (unlikely(cnum >= max_pmd
+			     || !test_bit(cnum, cast_ulp(impl_pmds)))) {
+			PFM_DBG("pmd%u is not implemented/unaccessible", cnum);
+			goto error;
+		}
+
+		pmd_type = pfm_pmu_conf->pmd_desc[cnum].type;
+
+		/*
+		 * locate event set
+		 */
+		if (i == 0 || set_id != prev_set_id) {
+			set = pfm_find_set(ctx, set_id, 0);
+			if (set == NULL) {
+				PFM_DBG("event set%u does not exist",
+					set_id);
+				goto error;
+			}
+		}
+		/*
+		 * it is not possible to read a PMD which was not requested:
+		 * 	- explicitly written via pfm_write_pmds()
+		 * 	- provided as a reg_smpl_pmds[] to another PMD during
+		 * 	  pfm_write_pmds()
+		 *
+		 * This is motivated by security and for optimization purposes:
+		 * 	- on context switch restore, we can restore only what we
+		 * 	  use (except when regs directly readable at user level,
+		 * 	  e.g., IA-64 self-monitoring, I386 RDPMC).
+		 * 	- do not need to maintain PMC -> PMD dependencies
+		 */
+		if (unlikely(!test_bit(cnum, cast_ulp(set->used_pmds)))) {
+			PFM_DBG("pmd%u cannot be read, because never "
+				"requested", cnum);
+			goto error;
+		}
+
+		val = set->pmds[cnum].value;
+		lval = set->pmds[cnum].lval;
+
+		/*
+		 * extract remaining ovfl to switch
+		 */
+		sw_cnt = set->pmds[cnum].ovflsw_thres;
+
+		/*
+		 * If the task is not the current one, then we check if the
+		 * PMU state is still in the local live register due to lazy
+		 * ctxsw. If true, then we read directly from the registers.
+		 */
+		if (set == active_set && can_access_pmu) {
+			hw_val = pfm_read_pmd(ctx, cnum);
+			if (pmd_type & PFM_REG_C64)
+				val = (val & ~ovfl_mask) | (hw_val & ovfl_mask);
+			else
+				val = hw_val;
+		}
+
+		PFM_DBG("set%u pmd%u=0x%llx sw_thr=%llu lval=0x%llx"
+			" can_access=%d",
+			set->id,
+			cnum,
+			(unsigned long long)val,
+			(unsigned long long)sw_cnt,
+			(unsigned long long)lval, can_access_pmu);
+
+		req->reg_value = val;
+		req->reg_last_reset_val = lval;
+		req->reg_ovfl_switch_cnt = sw_cnt;
+
+		prev_set_id = set_id;
+	}
+	ret = 0;
+error:
+	return ret;
+}
diff -Naur linux-2.6.25-org/perfmon/perfmon_sets.c linux-2.6.25-id/perfmon/perfmon_sets.c
--- linux-2.6.25-org/perfmon/perfmon_sets.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_sets.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,773 @@
+/*
+ * perfmon_sets.c: perfmon2 event sets and multiplexing functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon.h>
+
+static struct kmem_cache	*pfm_set_cachep;
+
+/*
+ * reload reference interrupt switch thresholds for PMD which
+ * can interrupt
+ */
+static void pfm_reload_switch_thresholds(struct pfm_event_set *set)
+{
+	u64 *used_pmds;
+	u16 i, max, first;
+
+	used_pmds = set->used_pmds;
+	first = pfm_pmu_conf->regs.first_intr_pmd;
+	max = pfm_pmu_conf->regs.max_intr_pmd;
+
+	for (i = first; i < max; i++) {
+		if (test_bit(i, cast_ulp(used_pmds))) {
+			set->pmds[i].ovflsw_thres = set->pmds[i].ovflsw_ref_thres;
+
+			PFM_DBG("set%u pmd%u ovflsw_thres=%llu",
+				set->id,
+				i,
+				(unsigned long long)set->pmds[i].ovflsw_thres);
+		}
+	}
+}
+
+/*
+ * connect all sets, reset internal fields
+ */
+int pfm_prepare_sets(struct pfm_context *ctx, struct pfm_event_set *act_set)
+{
+	struct pfm_event_set *set;
+	u16 max;
+
+	max = pfm_pmu_conf->regs.max_intr_pmd;
+
+	list_for_each_entry(set, &ctx->set_list, list) {
+		/*
+		 * cleanup bitvectors
+		 */
+		bitmap_zero(cast_ulp(set->ovfl_pmds), max);
+		bitmap_zero(cast_ulp(set->povfl_pmds), max);
+
+		set->npend_ovfls = 0;
+		/*
+		 * timer is reset when the context is loaded, so any
+		 * remainder timeout is cancelled
+		 */
+		set->hrtimer_rem.tv64 = 0;
+
+		/*
+		 * we cannot just use plain clear because of arch-specific flags
+		 */
+		set->priv_flags &= ~(PFM_SETFL_PRIV_MOD_BOTH|PFM_SETFL_PRIV_SWITCH);
+		/*
+		 * neither duration nor runs are reset because typically loading/unloading
+		 * does not mean counts are reset. To reset, the set must be modified
+		 */
+	}
+
+	if (act_set->flags & PFM_SETFL_OVFL_SWITCH)
+		pfm_reload_switch_thresholds(act_set);
+
+	return 0;
+}
+
+/*
+ * called by run_hrtimer_softirq()
+ */
+enum hrtimer_restart pfm_handle_switch_timeout(struct hrtimer *t)
+{
+	struct pfm_event_set *set;
+	struct pfm_context *ctx;
+	unsigned long flags;
+	enum hrtimer_restart ret = HRTIMER_NORESTART;
+
+	/*
+	 * prevent against race with unload
+	 */
+	ctx  = __get_cpu_var(pmu_ctx);
+	if (!ctx)
+		return HRTIMER_NORESTART;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	set = ctx->active_set;
+
+	/*
+	 * switching occurs only when context is attached
+	 */
+	switch(ctx->state) {
+		case PFM_CTX_LOADED:
+			break;
+		case PFM_CTX_ZOMBIE:
+		case PFM_CTX_UNLOADED:
+			goto done;
+		case PFM_CTX_MASKED:
+			/*
+			 * no switching occurs while masked
+			 * but timeout is re-armed
+			 */
+			ret = HRTIMER_RESTART;
+			goto done;
+	}
+	BUG_ON(ctx->flags.system && ctx->cpu != smp_processor_id());
+
+	/*
+	 * timer does not run while monitoring is inactive (not started)
+	 */
+	if (!pfm_arch_is_active(ctx))
+		goto done;
+
+	pfm_stats_inc(handle_timeout_count);
+
+	ret  = pfm_switch_sets(ctx, NULL, PFM_PMD_RESET_SHORT, 0);
+done:
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	return ret;
+}
+
+/*
+ *
+ * always operating on the current task
+ * interrupts are masked
+ *
+ * input:
+ * 	- new_set: new set to switch to, if NULL follow normal chain
+ */
+enum hrtimer_restart pfm_switch_sets(struct pfm_context *ctx,
+				     struct pfm_event_set *new_set,
+				     int reset_mode,
+				     int no_restart)
+{
+	struct pfm_event_set *set;
+	u64 now, end;
+	u32 new_flags;
+	int is_system, is_active, nn;
+	enum hrtimer_restart ret = HRTIMER_NORESTART;
+
+	now = sched_clock();
+	set = ctx->active_set;
+	is_active = pfm_arch_is_active(ctx);
+
+	/*
+	 * if no set is explicitly requested,
+	 * use the set_switch_next field
+	 */
+	if (!new_set) {
+		/*
+		 * we use round-robin unless the user specified
+		 * a particular set to go to.
+		 */
+		new_set = list_first_entry(&set->list, struct pfm_event_set, list);
+		if (&new_set->list == &ctx->set_list)
+			new_set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+	}
+
+	PFM_DBG_ovfl("state=%d act=%d cur_set=%u cur_runs=%llu cur_npend=%d next_set=%u "
+		  "next_runs=%llu new_npend=%d reset_mode=%d reset_pmds=%llx",
+		  ctx->state,
+		  is_active,
+		  set->id,
+		  (unsigned long long)set->runs,
+		  set->npend_ovfls,
+		  new_set->id,
+		  (unsigned long long)new_set->runs,
+		  new_set->npend_ovfls,
+		  reset_mode,
+		  (unsigned long long)new_set->reset_pmds[0]);
+
+	is_system = ctx->flags.system;
+	new_flags = new_set->flags;
+
+	/*
+	 * nothing more to do
+	 */
+	if (new_set == set)
+		goto skip_same_set;
+
+	if (is_active) {
+		pfm_arch_stop(current, ctx, set);
+		pfm_save_pmds(ctx, set);
+		/*
+		 * compute elapsed ns for active set
+		 */
+		set->duration += now - set->duration_start;
+	}
+
+	pfm_arch_restore_pmds(ctx, new_set);
+	/*
+	 * if masked, we must restore the pmcs such that they
+	 * do not capture anything.
+	 */
+	pfm_arch_restore_pmcs(ctx, new_set);
+
+	if (new_set->npend_ovfls) {
+		pfm_arch_resend_irq();
+		pfm_stats_inc(ovfl_intr_replay_count);
+	}
+
+	new_set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
+
+skip_same_set:
+	new_set->runs++;
+	/*
+	 * reset switch threshold
+	 */
+	if (new_flags & PFM_SETFL_OVFL_SWITCH)
+		pfm_reload_switch_thresholds(new_set);
+
+	/*
+	 * reset overflowed PMD registers
+	 */
+	nn = bitmap_weight(cast_ulp(new_set->reset_pmds),
+			   pfm_pmu_conf->regs.max_pmd);
+	if (nn)
+		pfm_reset_pmds(ctx, new_set, nn, reset_mode);
+	/*
+	 * this is needed when coming from pfm_start()
+	 */
+	if (no_restart)
+		goto skip_restart;
+
+	if (is_active) {
+		pfm_arch_start(current, ctx, new_set);
+		new_set->duration_start = now;
+
+		/*
+		 * install new timeout if necessary
+		 */
+		if (new_flags & PFM_SETFL_TIME_SWITCH) {
+			struct hrtimer *h;
+			h = &__get_cpu_var(pfm_hrtimer);
+			hrtimer_forward(h, h->base->get_time(), new_set->hrtimer_exp);
+			ret = HRTIMER_RESTART;
+		}
+	}
+
+skip_restart:
+	ctx->active_set = new_set;
+
+	end = sched_clock();
+
+	pfm_stats_inc(set_switch_count);
+	pfm_stats_add(set_switch_ns, end - now);
+
+	return ret;
+}
+
+/*
+ * called from __pfm_overflow_handler() to switch event sets.
+ * monitoring is stopped, task is current, interrupts are masked.
+ * compared to pfm_switch_sets(), this version is simplified because
+ * it knows about the call path. There is no need to stop monitoring
+ * because it is already frozen by PMU handler.
+ */
+void pfm_switch_sets_from_intr(struct pfm_context *ctx)
+{
+	struct pfm_event_set *set, *new_set;
+	u64 now, end;
+	u32 new_flags;
+	int is_system, n;
+
+	now = sched_clock();
+	set = ctx->active_set;
+	new_set = list_first_entry(&set->list, struct pfm_event_set, list);
+	if (&new_set->list == &ctx->set_list)
+		new_set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+
+	PFM_DBG_ovfl("state=%d cur_set=%u cur_runs=%llu cur_npend=%d next_set=%u "
+		  "next_runs=%llu new_npend=%d new_r_pmds=%llx",
+		  ctx->state,
+		  set->id,
+		  (unsigned long long)set->runs,
+		  set->npend_ovfls,
+		  new_set->id,
+		  (unsigned long long)new_set->runs,
+		  new_set->npend_ovfls,
+		  (unsigned long long)new_set->reset_pmds[0]);
+
+	is_system = ctx->flags.system;
+	new_flags = new_set->flags;
+
+	/*
+	 * nothing more to do
+	 */
+	if (new_set == set)
+		goto skip_same_set;
+
+	if (set->flags & PFM_SETFL_TIME_SWITCH) {
+		hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG_ovfl("cancelled timer for set%u", set->id);
+	}
+
+	/*
+	 * when called from PMU intr handler, monitoring
+	 * is already stopped
+	 *
+	 * save current PMD registers, we use a special
+	 * form for performance reason. On some architectures,
+	 * such as x86, the pmds are already saved when entering
+	 * the PMU interrupt handler via pfm-arch_intr_freeze()
+	 * so we don't need to save them again. On the contrary,
+	 * on IA-64, they are not saved by freeze, thus we have to
+	 * to it here.
+	 */
+	pfm_arch_save_pmds_from_intr(ctx, set);
+
+	/*
+	 * compute elapsed ns for active set
+	 */
+	set->duration += now - set->duration_start;
+
+	pfm_arch_restore_pmds(ctx, new_set);
+
+	/*
+	 * must not be restored active as we are still executing in the
+	 * PMU interrupt handler. activation is deferred to unfreeze PMU
+	 */
+	pfm_arch_restore_pmcs(ctx, new_set);
+
+	/*
+	 * check for pending interrupt on incoming set.
+	 * interrupts are masked so handler call deferred
+	 */
+	if (new_set->npend_ovfls) {
+		pfm_arch_resend_irq();
+		pfm_stats_inc(ovfl_intr_replay_count);
+	}
+	/*
+	 * no need to restore anything, that is already done
+	 */
+	new_set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
+	/*
+	 * reset duration counter
+	 */
+	new_set->duration_start = now;
+
+skip_same_set:
+	new_set->runs++;
+
+	/*
+	 * reset switch threshold
+	 */
+	if (new_flags & PFM_SETFL_OVFL_SWITCH)
+		pfm_reload_switch_thresholds(new_set);
+
+	/*
+	 * reset overflowed PMD registers
+	 */
+	n = bitmap_weight(cast_ulp(new_set->reset_pmds), pfm_pmu_conf->regs.max_pmd);
+	if (n)
+		pfm_reset_pmds(ctx, new_set, n, PFM_PMD_RESET_SHORT);
+
+	/*
+	 * XXX: isactive?
+	 *
+	 * Came here following a interrupt which triggered a switch, i.e.,
+	 * previous set was using OVFL_SWITCH, thus we just need to arm
+	 * check if the next set is using timeout, and if so arm the timer.
+	 */
+	if (new_flags & PFM_SETFL_TIME_SWITCH) {
+		hrtimer_start(&__get_cpu_var(pfm_hrtimer), set->hrtimer_exp, HRTIMER_MODE_REL);
+		PFM_DBG("armed new timeout for set%u", new_set->id);
+	}
+
+	ctx->active_set = new_set;
+
+	end = sched_clock();
+
+	pfm_stats_inc(set_switch_count);
+	pfm_stats_add(set_switch_ns, end - now);
+}
+
+
+static int pfm_setfl_sane(struct pfm_context *ctx, u32 flags)
+{
+#define PFM_SETFL_BOTH_SWITCH	(PFM_SETFL_OVFL_SWITCH|PFM_SETFL_TIME_SWITCH)
+	int ret;
+
+	ret = pfm_arch_setfl_sane(ctx, flags);
+	if (ret)
+		return ret;
+
+	if ((flags & PFM_SETFL_BOTH_SWITCH) == PFM_SETFL_BOTH_SWITCH) {
+		PFM_DBG("both switch ovfl and switch time are set");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * it is never possible to change the identification of an existing set
+ */
+static int __pfm_change_evtset(struct pfm_context *ctx,
+			       struct pfm_event_set *set,
+			       struct pfarg_setdesc *req)
+{
+	struct timeval tv;
+	struct timespec ts;
+	ktime_t kt;
+	long d, rem, res_ns;
+	u32 flags;
+	int ret;
+	u16 set_id;
+
+	BUG_ON(ctx->state == PFM_CTX_LOADED);
+
+	set_id = req->set_id;
+	flags = req->set_flags;
+
+	ret = pfm_setfl_sane(ctx, flags);
+	if (ret) {
+		PFM_DBG("invalid flags 0x%x set %u", flags, set_id);
+		return -EINVAL;
+	}
+
+	hrtimer_get_res(CLOCK_MONOTONIC, &ts);
+	res_ns = (long)ktime_to_ns(timespec_to_ktime(ts));
+	PFM_DBG("clock_res=%ldns", res_ns);
+
+	/*
+	 * round-up to multiple of clock resolution
+	 * timeout = ((req->set_timeout+res_ns-1)/res_ns)*res_ns;
+	 *
+	 * u64 division missing on 32-bit arch, so use div_long_long
+	 */
+	d = div_long_long_rem_signed(req->set_timeout, res_ns, &rem);
+
+	PFM_DBG("set%u flags=0x%x req_timeout=%lluns "
+		"HZ=%u TICK_NSEC=%lu eff_timeout=%luns",
+		set_id,
+		flags,
+		(unsigned long long)req->set_timeout,
+		HZ, TICK_NSEC,
+		d * res_ns);
+
+	/*
+	 * Only accept timeout, we can actually achieve.
+	 * users can invoke clock_getres(CLOCK_MONOTONIC)
+	 * to figure out resolution and adjust timeout
+	 */
+	if (rem) {
+		PFM_DBG("set%u invalid timeout=%llu",
+			set_id,
+			(unsigned long long)req->set_timeout);
+		return -EINVAL;
+	}
+
+	tv = ns_to_timeval(req->set_timeout);
+	kt = timeval_to_ktime(tv);
+	set->hrtimer_exp = kt;
+
+	/*
+	 * commit changes
+	 */
+	set->id = set_id;
+	set->flags = flags;
+	set->priv_flags = 0;
+
+	/*
+	 * activation and duration counters are reset as
+	 * most likely major things will change in the set
+	 */
+	set->runs = 0;
+	set->duration = 0;
+
+	return 0;
+}
+
+/*
+ * this function does not modify the next field
+ */
+void pfm_init_evtset(struct pfm_event_set *set)
+{
+	u64 *impl_pmcs;
+	u16 i, max_pmc;
+
+	max_pmc = pfm_pmu_conf->regs.max_pmc;
+	impl_pmcs =  pfm_pmu_conf->regs.pmcs;
+
+	/*
+	 * install default values for all PMC  registers
+	 */
+	for (i=0; i < max_pmc;  i++) {
+		if (test_bit(i, cast_ulp(impl_pmcs))) {
+			set->pmcs[i] = pfm_pmu_conf->pmc_desc[i].dfl_val;
+			PFM_DBG("set%u pmc%u=0x%llx",
+				set->id,
+				i,
+				(unsigned long long)set->pmcs[i]);
+		}
+	}
+
+	/*
+	 * PMD registers are set to 0 when the event set is allocated,
+	 * hence we do not need to explicitly initialize them.
+	 *
+	 * For virtual PMD registers (i.e., those tied to a SW resource)
+	 * their value becomes meaningful once the context is attached.
+	 */
+}
+
+/*
+ * look for an event set using its identification. If the set does not
+ * exist:
+ * 	- if alloc == 0 then return error
+ * 	- if alloc == 1  then allocate set
+ *
+ * alloc is one ONLY when coming from pfm_create_evtsets() which can only
+ * be called when the context is detached, i.e. monitoring is stopped.
+ */
+struct pfm_event_set *pfm_find_set(struct pfm_context *ctx, u16 set_id, int alloc)
+{
+	struct pfm_event_set *set = NULL, *prev, *new_set;
+
+	PFM_DBG("looking for set=%u", set_id);
+
+	prev = NULL;
+	list_for_each_entry(set, &ctx->set_list, list) {
+		if (set->id == set_id)
+			return set;
+		if (set->id > set_id)
+			break;
+		prev = set;
+	}
+
+	if (!alloc)
+		return NULL;
+
+	/*
+	 * we are holding the context spinlock and interrupts
+	 * are unmasked. We must use GFP_ATOMIC as we cannot
+	 * sleep while holding a spin lock.
+	 */
+	new_set = kmem_cache_zalloc(pfm_set_cachep, GFP_ATOMIC);
+	if (!new_set)
+		return NULL;
+
+	new_set->id = set_id;
+
+	INIT_LIST_HEAD(&new_set->list);
+
+	if (prev == NULL) {
+		list_add(&(new_set->list), &ctx->set_list);
+	} else {
+		PFM_DBG("add after set=%u", prev->id);
+		list_add(&(new_set->list), &prev->list);
+	}
+	return new_set;
+}
+
+/*
+ * context is unloaded for this command. Interrupts are enabled
+ */
+int __pfm_create_evtsets(struct pfm_context *ctx, struct pfarg_setdesc *req,
+			int count)
+{
+	struct pfm_event_set *set;
+	u16 set_id;
+	int i, ret;
+
+	for (i = 0; i < count; i++, req++) {
+		set_id = req->set_id;
+
+		PFM_DBG("set_id=%u", set_id);
+
+		set = pfm_find_set(ctx, set_id, 1);
+		if (set == NULL)
+			goto error_mem;
+
+		ret = __pfm_change_evtset(ctx, set, req);
+		if (ret)
+			goto error_params;
+
+		pfm_init_evtset(set);
+	}
+	return 0;
+error_mem:
+	PFM_DBG("cannot allocate set %u", set_id);
+	return -ENOMEM;
+error_params:
+	return ret;
+}
+
+int __pfm_getinfo_evtsets(struct pfm_context *ctx, struct pfarg_setinfo *req,
+				 int count)
+{
+	struct pfm_event_set *set;
+	int i, is_system, is_loaded, ret;
+	u16 set_id, max, max_pmc, max_pmd;
+	u64 end;
+
+	end = sched_clock();
+
+	is_system = ctx->flags.system;
+	is_loaded = ctx->state == PFM_CTX_LOADED;
+
+	max = pfm_pmu_conf->regs.max_intr_pmd;
+	max_pmc  = pfm_pmu_conf->regs.max_pmc;
+	max_pmd = pfm_pmu_conf->regs.max_pmd;
+
+	ret = -EINVAL;
+	for (i = 0; i < count; i++, req++) {
+
+		set_id = req->set_id;
+
+		list_for_each_entry(set, &ctx->set_list, list) {
+			if (set->id == set_id)
+				goto found;
+			if (set->id > set_id)
+				goto error;
+		}
+found:
+		/*
+		 * compute leftover timeout
+		 */
+		req->set_flags = set->flags;
+
+		/*
+		 * XXX: fixme
+		 */
+		req->set_timeout = 0;
+
+		req->set_runs = set->runs;
+		req->set_act_duration = set->duration;
+
+		/*
+		 * adjust for active set if needed
+		 */
+		if (is_system && is_loaded && ctx->flags.started
+		    && set == ctx->active_set)
+			req->set_act_duration  += end - set->duration_start;
+
+		/*
+		 * copy the list of pmds which last overflowed for
+		 * the set
+		 */
+		bitmap_copy(cast_ulp(req->set_ovfl_pmds),
+			    cast_ulp(set->ovfl_pmds),
+			    max);
+
+		/*
+		 * copy bitmask of available PMU registers
+		 */
+		bitmap_copy(cast_ulp(req->set_avail_pmcs),
+			    cast_ulp(pfm_pmu_conf->regs.pmcs),
+			    max_pmc);
+
+		bitmap_copy(cast_ulp(req->set_avail_pmds),
+			    cast_ulp(pfm_pmu_conf->regs.pmds),
+			    max_pmd);
+
+		PFM_DBG("set%u flags=0x%x eff_usec=%llu runs=%llu "
+			"a_pmcs=0x%llx a_pmds=0x%llx",
+			set_id,
+			set->flags,
+			(unsigned long long)req->set_timeout,
+			(unsigned long long)set->runs,
+			(unsigned long long)pfm_pmu_conf->regs.pmcs[0],
+			(unsigned long long)pfm_pmu_conf->regs.pmds[0]);
+	}
+	ret = 0;
+error:
+	return ret;
+}
+
+/*
+ * context is unloaded for this command. Interrupts are enabled
+ */
+int __pfm_delete_evtsets(struct pfm_context *ctx, void *arg, int count)
+{
+	struct pfarg_setdesc *req = arg;
+	struct pfm_event_set *set;
+	u16 set_id;
+	int i, ret;
+
+	ret = -EINVAL;
+	for (i = 0; i < count; i++, req++) {
+		set_id = req->set_id;
+
+		list_for_each_entry(set, &ctx->set_list, list) {
+			if (set->id == set_id)
+				goto found;
+			if (set->id > set_id)
+				goto error;
+		}
+		goto error;
+found:
+		/*
+		 * clear active set if necessary.
+		 * will be updated when context is loaded
+		 */
+		if (set == ctx->active_set)
+			ctx->active_set = NULL;
+
+		list_del(&set->list);
+
+		kmem_cache_free(pfm_set_cachep, set);
+
+		PFM_DBG("set%u deleted", set_id);
+	}
+	ret = 0;
+error:
+	return ret;
+}
+
+/*
+ * called from pfm_context_free() to free all sets
+ */
+void pfm_free_sets(struct pfm_context *ctx)
+{
+	struct pfm_event_set *set, *tmp;
+
+	list_for_each_entry_safe(set, tmp, &ctx->set_list, list) {
+		list_del(&set->list);
+		kmem_cache_free(pfm_set_cachep, set);
+	}
+}
+
+int pfm_sets_init(void)
+{
+
+	pfm_set_cachep = kmem_cache_create("pfm_event_set",
+					   sizeof(struct pfm_event_set),
+					   SLAB_HWCACHE_ALIGN, 0, NULL);
+	if (pfm_set_cachep == NULL) {
+		PFM_ERR("cannot initialize event set slab");
+		return -ENOMEM;
+	}
+	return 0;
+}
diff -Naur linux-2.6.25-org/perfmon/perfmon_syscalls.c linux-2.6.25-id/perfmon/perfmon_syscalls.c
--- linux-2.6.25-org/perfmon/perfmon_syscalls.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_syscalls.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,1067 @@
+/*
+ * perfmon_syscalls.c: perfmon2 system call interface
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon.h>
+#include <linux/fs.h>
+#include <linux/ptrace.h>
+#include <asm/uaccess.h>
+
+/*
+ * Context locking rules:
+ * ---------------------
+ * 	- any thread with access to the file descriptor of a context can
+ * 	  potentially issue perfmon calls
+ *
+ * 	- calls must be serialized to guarantee correctness
+ *
+ * 	- as soon as a context is attached to a thread or CPU, it may be
+ * 	  actively monitoring. On some architectures, such as IA-64, this
+ * 	  is true even though the pfm_start() call has not been made. This
+ * 	  comes from the fact that on some architectures, it is possible to
+ * 	  start/stop monitoring from userland.
+ *
+ *	- If monitoring is active, then there can PMU interrupts. Because
+ *	  context accesses must be serialized, the perfmon system calls
+ *	  must mask interrupts as soon as the context is attached.
+ *
+ *	- perfmon system calls that operate with the context unloaded cannot
+ *	  assume it is actually unloaded when they are called. They first need
+ *	  to check and for that they need interrupts masked. Then if the context
+ *	  is actually unloaded, they can unmask interrupts.
+ *
+ *	- interrupt masking holds true for other internal perfmon functions as
+ *	  well. Except for PMU interrupt handler because those interrupts cannot
+ *	  be nested.
+ *
+ * 	- we mask ALL interrupts instead of just the PMU interrupt because we
+ * 	  also need to protect against timer interrupts which could trigger
+ * 	  a set switch.
+ */
+
+/*
+ * cannot attach if :
+ * 	- kernel task
+ * 	- task not owned by caller (checked by ptrace_may_attach())
+ * 	- task is dead or zombie
+ * 	- cannot use blocking notification when self-monitoring
+ */
+static int pfm_task_incompatible(struct pfm_context *ctx, struct task_struct *task)
+{
+	/*
+	 * cannot attach to a kernel thread
+	 */
+	if (!task->mm) {
+		PFM_DBG("cannot attach to kernel thread [%d]", task->pid);
+		return -EPERM;
+	}
+
+	/*
+	 * cannot use block on notification when
+	 * self-monitoring.
+	 */
+	if (ctx->flags.block && task == current) {
+		PFM_DBG("cannot use block on notification when self-monitoring"
+			"[%d]", task->pid);
+		return -EINVAL;
+	}
+	/*
+	 * cannot attach to a zombie task
+	 */
+	if (task->exit_state == EXIT_ZOMBIE || task->exit_state == EXIT_DEAD) {
+		PFM_DBG("cannot attach to zombie/dead task [%d]", task->pid);
+		return -EBUSY;
+	}
+	return 0;
+}
+
+/*
+ * This function  is used in per-thread mode only AND when not
+ * self-monitoring. It finds the task to monitor and checks
+ * that the caller has persmissions to attach. It also checks
+ * that the task is stopped via ptrace so that we can safely
+ * modify its state.
+ *
+ * task refcount is increment when succesful.
+ * This function is not declared static because it is used by the
+ * IA-64 compatiblity module arch/ia64/perfmon/perfmon_comapt.c
+ */
+int pfm_get_task(struct pfm_context *ctx, pid_t pid, struct task_struct **task)
+{
+	struct task_struct *p;
+	int ret = 0, ret1 = 0;
+
+	/*
+	 * When attaching to another thread we must ensure
+	 * that the thread is actually stopped. Just like with
+	 * perfmon system calls, we enforce that the thread
+	 * be ptraced and STOPPED by using ptrace_check_attach().
+	 *
+	 * As a consequence, only the ptracing parent can actually
+	 * attach a context to a thread. Obviously, this constraint
+	 * does not exist for self-monitoring threads.
+	 *
+	 * We use ptrace_may_attach() to check for permission.
+	 * No permission checking is needed for self monitoring.
+	 */
+	read_lock(&tasklist_lock);
+
+	p = find_task_by_pid(pid);
+	if (p)
+		get_task_struct(p);
+
+	read_unlock(&tasklist_lock);
+
+	if (p == NULL)
+		return -ESRCH;
+
+	ret = -EPERM;
+
+	/*
+	 * returns 0 if cannot attach
+	 */
+	ret1 = ptrace_may_attach(p);
+	if (ret1)
+		ret = ptrace_check_attach(p, 0);
+
+	PFM_DBG("may_attach=%d check_attach=%d", ret1, ret);
+
+	if (ret || !ret1)
+		goto error;
+
+	ret = pfm_task_incompatible(ctx, p);
+	if (ret)
+		goto error;
+
+	*task = p;
+
+	return 0;
+error:
+	if (!(ret1 || ret))
+		ret = -EPERM;
+
+	put_task_struct(p);
+
+	return ret;
+}
+
+/*
+ * context must be locked when calling this function
+ */
+int pfm_check_task_state(struct pfm_context *ctx, int check_mask,
+			 unsigned long *flags)
+{
+	struct task_struct *task;
+	unsigned long local_flags, new_flags;
+	int state, ret;
+
+recheck:
+	/*
+	 * task is NULL for system-wide context
+	 */
+	task = ctx->task;
+	state = ctx->state;
+	local_flags = *flags;
+
+	PFM_DBG("state=%d check_mask=0x%x", state, check_mask);
+	/*
+	 * if the context is detached, then we do not touch
+	 * hardware, therefore there is not restriction on when we can
+	 * access it.
+	 */
+	if (state == PFM_CTX_UNLOADED)
+		return 0;
+	/*
+	 * no command can operate on a zombie context.
+	 * A context becomes zombie when the file that identifies
+	 * it is closed while the context is still attached to the
+	 * thread it monitors.
+	 */
+	if (state == PFM_CTX_ZOMBIE)
+		return -EINVAL;
+
+	/*
+	 * at this point, state is PFM_CTX_LOADED or PFM_CTX_MASKED
+	 */
+
+	/*
+	 * some commands require the context to be unloaded to operate
+	 */
+	if (check_mask & PFM_CMD_UNLOADED)  {
+		PFM_DBG("state=%d, cmd needs context unloaded", state);
+		return -EBUSY;
+	}
+
+	/*
+	 * self-monitoring always ok.
+	 */
+	if (task == current)
+		return 0;
+
+	/*
+	 * for syswide, the calling thread must be running on the cpu
+	 * the context is bound to.
+	 */
+	if (ctx->flags.system) {
+		if (ctx->cpu != smp_processor_id())
+			return -EBUSY;
+		return 0;
+	}
+
+	/*
+	 * at this point, monitoring another thread
+	 */
+
+	/*
+	 * the pfm_unload_context() command is allowed on masked context
+	 */
+	if (state == PFM_CTX_MASKED && !(check_mask & PFM_CMD_UNLOAD))
+		return 0;
+
+	/*
+	 * When we operate on another thread, we must wait for it to be
+	 * stopped and completely off any CPU as we need to access the
+	 * PMU state (or machine state).
+	 *
+	 * A thread can be put in the STOPPED state in various ways
+	 * including PTRACE_ATTACH, or when it receives a SIGSTOP signal.
+	 * We enforce that the thread must be ptraced, so it is stopped
+	 * AND it CANNOT wake up while we operate on it because this
+	 * would require an action from the ptracing parent which is the
+	 * thread that is calling this function.
+	 *
+	 * The dependency on ptrace, imposes that only the ptracing
+	 * parent can issue command on a thread. This is unfortunate
+	 * but we do not know of a better way of doing this.
+	 */
+	if (check_mask & PFM_CMD_STOPPED) {
+
+		spin_unlock_irqrestore(&ctx->lock, local_flags);
+
+		/*
+		 * check that the thread is ptraced AND STOPPED
+		 */
+		ret = ptrace_check_attach(task, 0);
+
+		spin_lock_irqsave(&ctx->lock, new_flags);
+
+		/*
+		 * flags may be different than when we released the lock
+		 */
+		*flags = new_flags;
+
+		if (ret)
+			return ret;
+		/*
+		 * we must recheck to verify if state has changed
+		 */
+		if (unlikely(ctx->state != state)) {
+			PFM_DBG("old_state=%d new_state=%d",
+				state,
+				ctx->state);
+			goto recheck;
+		}
+	}
+	return 0;
+}
+
+/*
+ * pfm_get_args - Function used to copy the syscall argument into kernel memory.
+ * @ureq: user argument
+ * @sz: user argument size
+ * @lsz: size of stack buffer
+ * @laddr: stack buffer address
+ * @req: point to start of kernel copy of the argument
+ * @ptr_free: address of kernel copy to free
+ *
+ * There are two options:
+ * 	- use a stack buffer described by laddr (addresses) and lsz (size)
+ * 	- allocate memory
+ *
+ * return:
+ * 	< 0 : in case of error (ptr_free may not be updated)
+ * 	  0 : success
+ *      - req: points to base of kernel copy of arguments
+ *	- ptr_free: address of buffer to free by caller on exit.
+ *		    NULL if using the stack buffer
+ *
+ * when ptr_free is not NULL upon return, the caller must kfree()
+ */
+int pfm_get_args(void __user *ureq, size_t sz, size_t lsz, void *laddr,
+		 void **req, void **ptr_free)
+{
+	void *addr;
+
+	/*
+	 * check syadmin argument limit
+	 */
+	if (unlikely(sz > pfm_controls.arg_mem_max)) {
+		PFM_DBG("argument too big %zu max=%zu",
+			sz,
+			pfm_controls.arg_mem_max);
+		return -E2BIG;
+	}
+
+	/*
+	 * check if vector fits on stack buffer
+	 */
+	if (sz > lsz) {
+		addr = kmalloc(sz, GFP_KERNEL);
+		if (unlikely(addr == NULL))
+			return -ENOMEM;
+		*ptr_free = addr;
+	} else {
+		addr = laddr;
+		*req = laddr;
+		*ptr_free = NULL;
+	}
+
+	/*
+	 * bring the data in
+	 */
+	if (unlikely(copy_from_user(addr, ureq, sz))) {
+		if (addr != laddr)
+			kfree(addr);
+		return -EFAULT;
+	}
+
+	/*
+	 * base address of kernel buffer
+	 */
+	*req = addr;
+
+	return 0;
+}
+
+/*
+ * arg is kmalloc'ed, thus it needs a kfree by caller
+ */
+int pfm_get_smpl_arg(char __user *fmt_uname, void __user *fmt_uarg, size_t usize, void **arg,
+		     struct pfm_smpl_fmt **fmt)
+{
+	struct pfm_smpl_fmt *f;
+	char *fmt_name;
+	void *addr = NULL;
+	size_t sz;
+	int ret;
+
+	fmt_name = getname(fmt_uname);
+	if (!fmt_name) {
+		PFM_DBG("getname failed");
+		return -ENOMEM;
+	}
+
+	/*
+	 * find fmt and increase refcount
+	 */
+	f = pfm_smpl_fmt_get(fmt_name);
+
+	putname(fmt_name);
+
+	if (f == NULL) {
+		PFM_DBG("buffer format not found");
+		return -EINVAL;
+	}
+
+	/*
+	 * expected format argument size
+	 */
+	sz = f->fmt_arg_size;
+
+	/*
+	 * check user size matches expected size
+	 * usize = -1 is for IA-64 backward compatibility
+	 */
+	ret = -EINVAL;
+	if (sz != usize && usize != -1) {
+		PFM_DBG("invalid arg size %zu, format expects %zu",
+			usize, sz);
+		goto error;
+	}
+
+	if (sz) {
+		ret = -ENOMEM;
+		addr = kmalloc(sz, GFP_KERNEL);
+		if (addr == NULL)
+			goto error;
+
+		ret = -EFAULT;
+		if (copy_from_user(addr, fmt_uarg, sz))
+			goto error;
+	}
+	*arg = addr;
+	*fmt = f;
+	return 0;
+
+error:
+	kfree(addr);
+	pfm_smpl_fmt_put(f);
+	return ret;
+}
+
+/*
+ * unlike the other perfmon system calls, this one return a file descriptor
+ * or a value < 0 in case of error, very much like open() or socket()
+ */
+asmlinkage long sys_pfm_create_context(struct pfarg_ctx __user *ureq,
+				       char __user *fmt_name,
+				       void __user *fmt_uarg, size_t fmt_size)
+{
+	struct pfarg_ctx req;
+	struct pfm_context *new_ctx;
+	struct pfm_smpl_fmt *fmt = NULL;
+	void *fmt_arg = NULL;
+	int ret;
+
+	PFM_DBG("req=%p fmt=%p fmt_arg=%p size=%zu",
+		ureq, fmt_name, fmt_uarg, fmt_size);
+
+	if (perfmon_disabled)
+		return -ENOSYS;
+
+	if (copy_from_user(&req, ureq, sizeof(req)))
+		return -EFAULT;
+
+	if (fmt_name) {
+		ret = pfm_get_smpl_arg(fmt_name, fmt_uarg, fmt_size, &fmt_arg, &fmt);
+		if (ret)
+			goto abort;
+	}
+
+	ret = __pfm_create_context(&req, fmt, fmt_arg, PFM_NORMAL, &new_ctx);
+
+	kfree(fmt_arg);
+abort:
+	return ret;
+}
+
+asmlinkage long sys_pfm_write_pmcs(int fd, struct pfarg_pmc __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_pmc pmcs[PFM_PMC_STK_ARG];
+	struct pfarg_pmc *req;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq)) {
+		PFM_DBG("invalid arg count %d", count);
+		return -EINVAL;
+	}
+
+	sz = count*sizeof(*ureq);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, sizeof(pmcs), pmcs, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (!ret)
+		ret = __pfm_write_pmcs(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/*
+	 * This function may be on the critical path.
+	 * We want to avoid the branch if unecessary.
+	 */
+	if (fptr)
+		kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_write_pmds(int fd, struct pfarg_pmd __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_pmd pmds[PFM_PMD_STK_ARG];
+	struct pfarg_pmd *req;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq)) {
+		PFM_DBG("invalid arg count %d", count);
+		return -EINVAL;
+	}
+
+	sz = count*sizeof(*ureq);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, sizeof(pmds), pmds, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (!ret)
+		ret = __pfm_write_pmds(ctx, req, count, 0);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (fptr)
+		kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_read_pmds(int fd, struct pfarg_pmd __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_pmd pmds[PFM_PMD_STK_ARG];
+	struct pfarg_pmd *req;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
+		return -EINVAL;
+
+	sz = count*sizeof(*ureq);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, sizeof(pmds), pmds, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (!ret)
+		ret = __pfm_read_pmds(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	if (fptr)
+		kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_restart(int fd)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	unsigned long flags;
+	int ret, fput_needed, unblock;
+
+	PFM_DBG("fd=%d", fd);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, 0, &flags);
+	if (!ret)
+		ret = __pfm_restart(ctx, &unblock);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	/*
+	 * In per-thread mode with blocking notification, i.e.
+	 * ctx->flags.blocking=1, we need to defer issuing the
+	 * complete to unblock the blocked monitored thread.
+	 * Otherwise we have a potential deadlock due to a lock
+	 * inversion between the context lock and the task_rq_lock()
+	 * which can happen if one thread is in this call and the other
+	 * (the monitored thread) is in the context switch code.
+	 *
+	 * It is safe to access the context outside the critical section
+	 * because:
+	 * 	- we are protected by the fget_light(), thus the context
+	 * 	  cannot disappear
+	 */
+	if (ret == 0 && unblock)
+		complete(&ctx->restart_complete);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_stop(int fd)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	unsigned long flags;
+	int ret, fput_needed;
+	int release_info;
+
+	PFM_DBG("fd=%d", fd);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (!ret)
+		ret = __pfm_stop(ctx, &release_info);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	/*
+	 * defer cancellation of timer to avoid race
+	 * with pfm_handle_switch_timeout()
+	 *
+	 * applies only when self-monitoring
+	 */
+	if (release_info & 0x2)
+		hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *ureq)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_start req;
+	unsigned long flags;
+	int ret, fput_needed;
+
+	PFM_DBG("fd=%d req=%p", fd, ureq);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	/*
+	 * the one argument is actually optional
+	 */
+	if (ureq && copy_from_user(&req, ureq, sizeof(req)))
+		return -EFAULT;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags);
+	if (!ret)
+		ret = __pfm_start(ctx, ureq ? &req : NULL);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ureq)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct file *filp;
+	unsigned long flags;
+	struct pfarg_load req;
+	int ret, fput_needed;
+
+	PFM_DBG("fd=%d req=%p", fd, ureq);
+
+	if (copy_from_user(&req, ureq, sizeof(req)))
+		return -EFAULT;
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	task = NULL;
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	/*
+	 * in per-thread mode (not self-monitoring), get a reference
+	 * on task to monitor. This must be done with interrupts enabled
+	 * Upon succesful return, refcount on task is increased.
+	 *
+	 * fget_light() is protecting the context.
+	 */
+	if (!ctx->flags.system) {
+		if (req.load_pid != current->pid) {
+			ret = pfm_get_task(ctx, req.load_pid, &task);
+			if (ret)
+				goto error;
+		} else
+			task = current;
+	}
+
+	/*
+	 * irqsave is required to avoid race in case context is already
+	 * loaded or with switch timeout in the case of self-monitoring
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags);
+	if (!ret)
+		ret = __pfm_load_context(ctx, &req, task);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+#ifdef CONFIG_PPC
+	if (!ctx->flags.system && ret == 0) {
+		pfm_arch_add_ctxsw_hook(ctx);
+	}
+#endif
+
+	/*
+	 * in per-thread mode (not self-monitoring), we need
+	 * to decrease refcount on task to monitor:
+	 *   - load successful: we have a reference to the task in ctx->task
+	 *   - load failed    : undo the effect of pfm_get_task()
+	 */
+	if (task && task != current)
+		put_task_struct(task);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_unload_context(int fd)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	unsigned long flags;
+	int ret, fput_needed;
+	int is_system, release_info = 0;
+	u32 cpu;
+
+	PFM_DBG("fd=%d", fd);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+	is_system = ctx->flags.system;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	cpu = ctx->cpu;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED|PFM_CMD_UNLOAD, &flags);
+	if (!ret)
+		ret = __pfm_unload_context(ctx, &release_info);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/*
+	 * cancel time now that context is unlocked
+	 * avoid race with pfm_handle_switch_timeout()
+	 */
+	if (release_info & 0x2) {
+		int r;
+		r = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG("timeout cancel=%d", r);
+	}
+
+	if (release_info & 0x1)
+		pfm_release_session(is_system, cpu);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_create_evtsets(int fd, struct pfarg_setdesc __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_setdesc *req;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
+		return -EINVAL;
+
+	sz = count*sizeof(*ureq);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	/*
+	 * must mask interrupts because we do not know the state of context,
+	 * could be attached and we could be getting PMU interrupts. So
+	 * we mask and lock context and we check and possibly relax masking
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags);
+	if (!ret)
+		ret = __pfm_create_evtsets(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	kfree(fptr);
+
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long  sys_pfm_getinfo_evtsets(int fd, struct pfarg_setinfo __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_setinfo *req;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
+		return -EINVAL;
+
+	sz = count*sizeof(*ureq);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	/*
+	 * this command operate even when context is loaded, so we need
+	 * to keep interrupts masked to avoid a race with PMU interrupt
+	 * which may switch active set
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, 0, &flags);
+	if (!ret)
+		ret = __pfm_getinfo_evtsets(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+asmlinkage long sys_pfm_delete_evtsets(int fd, struct pfarg_setinfo __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct file *filp;
+	struct pfarg_setinfo *req;
+	void *fptr;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
+		return -EINVAL;
+
+	sz = count*sizeof(*ureq);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	/*
+	 * must mask interrupts because we do not know the state of context,
+	 * could be attached and we could be getting PMU interrupts
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags);
+	if (!ret)
+		ret = __pfm_delete_evtsets(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	kfree(fptr);
+
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
diff -Naur linux-2.6.25-org/perfmon/perfmon_sysfs.c linux-2.6.25-id/perfmon/perfmon_sysfs.c
--- linux-2.6.25-org/perfmon/perfmon_sysfs.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/perfmon/perfmon_sysfs.c	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,677 @@
+/*
+ * perfmon_sysfs.c: perfmon2 sysfs interface
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon.h>
+#include <linux/module.h> /* for EXPORT_SYMBOL */
+
+
+extern void pfm_reset_stats(int cpu);
+
+struct pfm_attribute {
+	struct attribute attr;
+	ssize_t (*show)(void *, char *);
+	ssize_t (*store)(void *, const char *, size_t);
+};
+#define to_attr(n) container_of(n, struct pfm_attribute, attr);
+
+#define PFM_RO_ATTR(_name) \
+struct pfm_attribute attr_##_name = __ATTR_RO(_name)
+
+#define PFM_RW_ATTR(_name,_mode,_show,_store) 			\
+struct pfm_attribute attr_##_name = __ATTR(_name,_mode,_show,_store);
+
+static int pfm_sysfs_init_done;	/* true when pfm_sysfs_init() completed */
+
+int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu);
+
+struct pfm_controls pfm_controls = {
+	.sys_group = PFM_GROUP_PERM_ANY,
+	.task_group = PFM_GROUP_PERM_ANY,
+	.arg_mem_max = PAGE_SIZE,
+	.smpl_buffer_mem_max = ~0,
+};
+EXPORT_SYMBOL(pfm_controls);
+
+DECLARE_PER_CPU(struct pfm_stats, pfm_stats);
+
+static struct kobject pfm_kernel_kobj, pfm_kernel_fmt_kobj;
+
+static ssize_t pfm_fmt_attr_show(struct kobject *kobj,
+		struct attribute *attr, char *buf)
+{
+	struct pfm_smpl_fmt *fmt = to_smpl_fmt(kobj);
+	struct pfm_attribute *attribute = to_attr(attr);
+	return attribute->show ? attribute->show(fmt, buf) : -EIO;
+}
+
+static ssize_t pfm_pmu_attr_show(struct kobject *kobj,
+		struct attribute *attr, char *buf)
+{
+	struct pfm_pmu_config *pmu= to_pmu(kobj);
+	struct pfm_attribute *attribute = to_attr(attr);
+	return attribute->show ? attribute->show(pmu, buf) : -EIO;
+}
+
+static ssize_t pfm_regs_attr_show(struct kobject *kobj,
+		struct attribute *attr, char *buf)
+{
+	struct pfm_regmap_desc *reg = to_reg(kobj);
+	struct pfm_attribute *attribute = to_attr(attr);
+	return attribute->show ? attribute->show(reg, buf) : -EIO;
+}
+
+static struct sysfs_ops pfm_fmt_sysfs_ops = {
+	.show = pfm_fmt_attr_show
+};
+
+static struct sysfs_ops pfm_pmu_sysfs_ops = {
+	.show = pfm_pmu_attr_show
+};
+
+static struct sysfs_ops pfm_regs_sysfs_ops = {
+	.show  = pfm_regs_attr_show
+};
+
+static struct kobj_type pfm_fmt_ktype = {
+	.sysfs_ops = &pfm_fmt_sysfs_ops,
+};
+
+static struct kobj_type pfm_pmu_ktype = {
+	.sysfs_ops = &pfm_pmu_sysfs_ops,
+};
+
+static struct kobj_type pfm_regs_ktype = {
+	.sysfs_ops = &pfm_regs_sysfs_ops,
+};
+
+static ssize_t version_show(void *info, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u.%u\n",  PFM_VERSION_MAJ, PFM_VERSION_MIN);
+}
+
+static ssize_t task_sessions_count_show(void *info, char *buf)
+{
+	return pfm_sysfs_session_show(buf, PAGE_SIZE, 0);
+}
+
+static ssize_t sys_sessions_count_show(void *info, char *buf)
+{
+	return pfm_sysfs_session_show(buf, PAGE_SIZE, 1);
+}
+
+static ssize_t smpl_buffer_mem_cur_show(void *info, char *buf)
+{
+	return pfm_sysfs_session_show(buf, PAGE_SIZE, 2);
+}
+
+static ssize_t debug_show(void *info, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.debug);
+}
+
+static ssize_t debug_store(void *info, const char *buf, size_t sz)
+{
+	int d, i;
+
+	if (sscanf(buf,"%d", &d) != 1)
+		return -EINVAL;
+
+	pfm_controls.debug = d;
+
+	if (d == 0) {
+		for_each_online_cpu(i) {
+			pfm_reset_stats(i);
+		}
+	}
+	return sz;
+}
+
+static ssize_t debug_ovfl_show(void *info, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.debug_ovfl);
+}
+
+static ssize_t debug_ovfl_store(void *info, const char *buf, size_t sz)
+{
+	int d;
+
+	if (sscanf(buf,"%d", &d) != 1)
+		return -EINVAL;
+
+	pfm_controls.debug_ovfl = d;
+
+	return strnlen(buf, PAGE_SIZE);
+}
+
+static ssize_t reset_stats_show(void *info, char *buf)
+{
+	buf[0]='0';
+	buf[1]='\0';
+	return strnlen(buf, PAGE_SIZE);
+}
+
+static ssize_t reset_stats_store(void *info, const char *buf, size_t count)
+{
+	int i;
+
+	for_each_online_cpu(i) {
+		pfm_reset_stats(i);
+	}
+	return count;
+}
+
+static ssize_t sys_group_show(void *info, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.sys_group);
+}
+
+static ssize_t sys_group_store(void *info, const char *buf, size_t sz)
+{
+	int d;
+
+	if (sscanf(buf,"%d", &d) != 1)
+		return -EINVAL;
+
+	pfm_controls.sys_group = d;
+
+	return strnlen(buf, PAGE_SIZE);
+}
+
+static ssize_t task_group_show(void *info, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.task_group);
+}
+
+static ssize_t task_group_store(void *info, const char *buf, size_t sz)
+{
+	int d;
+
+	if (sscanf(buf,"%d", &d) != 1)
+		return -EINVAL;
+
+	pfm_controls.task_group = d;
+
+	return strnlen(buf, PAGE_SIZE);
+}
+
+static ssize_t buf_size_show(void *info, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%zu\n", pfm_controls.smpl_buffer_mem_max);
+}
+
+static ssize_t buf_size_store(void *info, const char *buf, size_t sz)
+{
+	size_t d;
+
+	if (sscanf(buf,"%zu", &d) != 1)
+		return -EINVAL;
+	/*
+	 * we impose a page as the minimum
+	 */
+	if (d < PAGE_SIZE)
+		return -EINVAL;
+
+	pfm_controls.smpl_buffer_mem_max = d;
+
+	return strnlen(buf, PAGE_SIZE);
+}
+
+static ssize_t arg_size_show(void *info, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%zu\n", pfm_controls.arg_mem_max);
+}
+
+static ssize_t arg_size_store(void *info, const char *buf, size_t sz)
+{
+	size_t d;
+
+	if (sscanf(buf,"%zu", &d) != 1)
+		return -EINVAL;
+
+	/*
+	 * we impose a page as the minimum.
+	 *
+	 * This limit may be smaller than the stack buffer
+	 * available and that is fine.
+	 */
+	if (d < PAGE_SIZE)
+		return -EINVAL;
+
+	pfm_controls.arg_mem_max = d;
+
+	return strnlen(buf, PAGE_SIZE);
+}
+
+/*
+ * /sys/kernel/perfmon attributes
+ */
+static PFM_RO_ATTR(version);
+static PFM_RO_ATTR(task_sessions_count);
+static PFM_RO_ATTR(sys_sessions_count);
+static PFM_RO_ATTR(smpl_buffer_mem_cur);
+
+static PFM_RW_ATTR(debug, 0644, debug_show, debug_store);
+static PFM_RW_ATTR(debug_ovfl, 0644, debug_ovfl_show, debug_ovfl_store);
+static PFM_RW_ATTR(reset_stats, 0644, reset_stats_show, reset_stats_store);
+static PFM_RW_ATTR(sys_group, 0644, sys_group_show, sys_group_store);
+static PFM_RW_ATTR(task_group, 0644, task_group_show, task_group_store);
+static PFM_RW_ATTR(smpl_buffer_mem_max, 0644, buf_size_show, buf_size_store);
+static PFM_RW_ATTR(arg_mem_max, 0644, arg_size_show, arg_size_store);
+
+static struct attribute *pfm_kernel_attrs[] = {
+	&attr_version.attr,
+	&attr_task_sessions_count.attr,
+	&attr_sys_sessions_count.attr,
+	&attr_smpl_buffer_mem_cur.attr,
+	&attr_debug.attr,
+	&attr_debug_ovfl.attr,
+	&attr_reset_stats.attr,
+	&attr_sys_group.attr,
+	&attr_task_group.attr,
+	&attr_smpl_buffer_mem_max.attr,
+	&attr_arg_mem_max.attr,
+	NULL
+};
+
+static struct attribute_group pfm_kernel_attr_group = {
+	.attrs = pfm_kernel_attrs,
+};
+
+int __init pfm_init_sysfs(void)
+{
+	int ret;
+
+	kobject_init(&pfm_kernel_kobj);
+	kobject_init(&pfm_kernel_fmt_kobj);
+
+	pfm_kernel_kobj.parent = &kernel_subsys.kobj;
+	kobject_set_name(&pfm_kernel_kobj, "perfmon");
+
+	pfm_kernel_fmt_kobj.parent = &pfm_kernel_kobj;
+	kobject_set_name(&pfm_kernel_fmt_kobj, "formats");
+
+	ret = kobject_add(&pfm_kernel_kobj);
+	if (ret) {
+		PFM_INFO("cannot add kernel object: %d", ret);
+		goto error;
+	}
+
+	ret = kobject_add(&pfm_kernel_fmt_kobj);
+	if (ret) {
+		PFM_INFO("cannot add fmt object: %d", ret);
+		goto error_fmt;
+	}
+
+	ret = sysfs_create_group(&pfm_kernel_kobj, &pfm_kernel_attr_group);
+	if (ret) {
+		PFM_INFO("cannot create kernel group");
+		goto error_group;
+	}
+
+	if (pfm_pmu_conf)
+		pfm_sysfs_add_pmu(pfm_pmu_conf);
+	/*
+	 * must be set before builtin_fmt and
+	 * add_pmu() calls
+	 */
+	pfm_sysfs_init_done = 1;
+	pfm_sysfs_builtin_fmt_add();
+	return 0;
+
+error_group:
+	kobject_del(&pfm_kernel_fmt_kobj);
+error_fmt:
+	kobject_del(&pfm_kernel_kobj);
+error:
+	pfm_sysfs_init_done = 0;
+	return ret;
+}
+
+/*
+ * per-cpu perfmon stats attributes
+ */
+#define PFM_DECL_STATS_ATTR(name) \
+static ssize_t name##_show(void *info, char *buf) \
+{ \
+	struct pfm_stats *st = info;\
+	return snprintf(buf, PAGE_SIZE, "%llu\n", \
+			(unsigned long long)st->name); \
+} \
+static PFM_RO_ATTR(name)
+
+/*
+ * per-reg attributes
+ */
+static ssize_t name_show(void *info, char *buf)
+{
+	struct pfm_regmap_desc *reg = info;
+	return snprintf(buf, PAGE_SIZE, "%s\n", reg->desc);
+}
+static PFM_RO_ATTR(name);
+
+static ssize_t dfl_val_show(void *info, char *buf)
+{
+	struct pfm_regmap_desc *reg = info;
+	return snprintf(buf, PAGE_SIZE, "0x%llx\n",
+			(unsigned long long)reg->dfl_val);
+}
+static PFM_RO_ATTR(dfl_val);
+
+static ssize_t rsvd_msk_show(void *info, char *buf)
+{
+	struct pfm_regmap_desc *reg = info;
+	return snprintf(buf, PAGE_SIZE, "0x%llx\n",
+			(unsigned long long)reg->rsvd_msk);
+}
+static PFM_RO_ATTR(rsvd_msk);
+
+static ssize_t width_show(void *info, char *buf)
+{
+	struct pfm_regmap_desc *reg = info;
+	int w;
+
+	w = (reg->type & PFM_REG_C64) ? pfm_pmu_conf->counter_width : 64;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", w);
+}
+static PFM_RO_ATTR(width);
+
+
+static ssize_t addr_show(void *info, char *buf)
+{
+	struct pfm_regmap_desc *reg = info;
+	return snprintf(buf, PAGE_SIZE, "0x%lx\n", reg->hw_addr);
+}
+static PFM_RO_ATTR(addr);
+
+static ssize_t fmt_version_show(void *data, char *buf)
+{
+	struct pfm_smpl_fmt *fmt = data;
+
+	return snprintf(buf, PAGE_SIZE, "%u.%u",
+		fmt->fmt_version >>16 & 0xffff,
+		fmt->fmt_version & 0xffff);
+}
+
+/*
+ * do not use predefined macros because of name conflict
+ * with /sys/kernel/perfmon/version
+ */
+struct pfm_attribute attr_fmt_version = {
+	.attr	= { .name = "version", .mode = 0444 },
+	.show	= fmt_version_show,
+};
+
+static struct attribute *pfm_fmt_attrs[] = {
+	&attr_fmt_version.attr,
+	NULL
+};
+
+static struct attribute_group pfm_fmt_attr_group = {
+	.attrs = pfm_fmt_attrs,
+};
+
+
+/*
+ * when a sampling format module is inserted, we populate
+ * sysfs with some information
+ */
+int pfm_sysfs_add_fmt(struct pfm_smpl_fmt *fmt)
+{
+	int ret;
+
+	if (pfm_sysfs_init_done == 0)
+		return 0;
+
+	fmt->kobj.ktype = &pfm_fmt_ktype;
+	kobject_init(&fmt->kobj);
+
+	kobject_set_name(&fmt->kobj, fmt->fmt_name);
+	//kobj_set_kset_s(fmt, pfm_fmt_subsys);
+	fmt->kobj.parent = &pfm_kernel_fmt_kobj;
+
+	ret = kobject_add(&fmt->kobj);
+	if (ret)
+		return ret;
+
+	ret = sysfs_create_group(&fmt->kobj, &pfm_fmt_attr_group);
+	if (ret)
+		kobject_del(&fmt->kobj);
+
+	return ret;
+}
+
+/*
+ * when a sampling format module is removed, its information
+ * must also be removed from sysfs
+ */
+int pfm_sysfs_remove_fmt(struct pfm_smpl_fmt *fmt)
+{
+	if (pfm_sysfs_init_done == 0)
+		return 0;
+
+	sysfs_remove_group(&fmt->kobj, &pfm_fmt_attr_group);
+	kobject_del(&fmt->kobj);
+
+	return 0;
+}
+
+static struct attribute *pfm_reg_attrs[] = {
+	&attr_name.attr,
+	&attr_dfl_val.attr,
+	&attr_rsvd_msk.attr,
+	&attr_width.attr,
+	&attr_addr.attr,
+	NULL
+};
+
+static struct attribute_group pfm_reg_attr_group = {
+	.attrs = pfm_reg_attrs,
+};
+
+static ssize_t model_show(void *info, char *buf)
+{
+	struct pfm_pmu_config *p = info;
+	return snprintf(buf, PAGE_SIZE, "%s\n", p->pmu_name);
+}
+static PFM_RO_ATTR(model);
+
+static struct attribute *pfm_pmu_desc_attrs[] = {
+	&attr_model.attr,
+	NULL
+};
+
+static struct attribute_group pfm_pmu_desc_attr_group = {
+	.attrs = pfm_pmu_desc_attrs,
+};
+
+static int pfm_sysfs_add_pmu_regs(struct pfm_pmu_config *pmu)
+{
+	struct pfm_regmap_desc *reg;
+	unsigned int i, k;
+	int ret;
+	char reg_name[8];
+
+	reg = pmu->pmc_desc;
+	for(i=0; i < pmu->num_pmc_entries; i++, reg++) {
+
+		if (!(reg->type & PFM_REG_I))
+			continue;
+
+		reg->kobj.ktype = &pfm_regs_ktype;
+		kobject_init(&reg->kobj);
+
+		reg->kobj.parent = &pmu->kobj;
+		snprintf(reg_name, sizeof(reg_name), "pmc%u", i);
+		kobject_set_name(&reg->kobj, reg_name);
+		//kobj_set_kset_s(reg, pfm_regs_subsys);
+
+		ret = kobject_add(&reg->kobj);
+		if (ret)
+			goto undo_pmcs;
+
+		ret = sysfs_create_group(&reg->kobj, &pfm_reg_attr_group);
+		if (ret) {
+			kobject_del(&reg->kobj);
+			goto undo_pmcs;
+		}
+	}
+
+	reg = pmu->pmd_desc;
+	for(i=0; i < pmu->num_pmd_entries; i++, reg++) {
+
+		if (!(reg->type & PFM_REG_I))
+			continue;
+
+		reg->kobj.ktype = &pfm_regs_ktype;
+		kobject_init(&reg->kobj);
+
+		reg->kobj.parent = &pmu->kobj;
+		snprintf(reg_name, sizeof(reg_name), "pmd%u", i);
+		kobject_set_name(&reg->kobj, reg_name);
+		//kobj_set_kset_s(reg, pfm_regs_subsys);
+
+		ret = kobject_add(&reg->kobj);
+		if (ret)
+			goto undo_pmds;
+
+		ret = sysfs_create_group(&reg->kobj, &pfm_reg_attr_group);
+		if (ret) {
+			kobject_del(&reg->kobj);
+			goto undo_pmds;
+		}
+	}
+	return 0;
+undo_pmds:
+	reg = pmu->pmd_desc;
+	for(k = 0; k < i; k++, reg++) {
+		if (!(reg->type & PFM_REG_I))
+			continue;
+		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
+		kobject_del(&reg->kobj);
+	}
+	i = pmu->num_pmc_entries;
+	/* fall through */
+undo_pmcs:
+	reg = pmu->pmc_desc;
+	for(k=0; k < i; k++, reg++) {
+		if (!(reg->type & PFM_REG_I))
+			continue;
+		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
+		kobject_del(&reg->kobj);
+	}
+	return ret;
+}
+
+static int pfm_sysfs_del_pmu_regs(struct pfm_pmu_config *pmu)
+{
+	struct pfm_regmap_desc *reg;
+	unsigned int i;
+
+	reg = pmu->pmc_desc;
+	for(i=0; i < pmu->regs.max_pmc; i++, reg++) {
+
+		if (!(reg->type & PFM_REG_I))
+			continue;
+
+		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
+		kobject_del(&reg->kobj);
+	}
+
+	reg = pmu->pmd_desc;
+	for(i=0; i < pmu->regs.max_pmd; i++, reg++) {
+
+		if (!(reg->type & PFM_REG_I))
+			continue;
+
+		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
+		kobject_del(&reg->kobj);
+	}
+	return 0;
+}
+
+/*
+ * when a PMU description module is inserted, we create
+ * a pmu_desc subdir in sysfs and we populate it with
+ * PMU specific information, such as register mappings
+ */
+int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu)
+{
+	int ret;
+
+	if (pfm_sysfs_init_done == 0)
+		return 0;
+
+	pmu->kobj.ktype = &pfm_pmu_ktype;
+	kobject_init(&pmu->kobj);
+	kobject_set_name(&pmu->kobj, "pmu_desc");
+	//kobj_set_kset_s(pmu, pfm_pmu_subsys);
+	pmu->kobj.parent = &pfm_kernel_kobj;
+
+	ret = kobject_add(&pmu->kobj);
+	if (ret)
+		return ret;
+
+	ret = sysfs_create_group(&pmu->kobj, &pfm_pmu_desc_attr_group);
+	if (ret)
+		kobject_del(&pmu->kobj);
+
+	ret = pfm_sysfs_add_pmu_regs(pmu);
+	if (ret) {
+		sysfs_remove_group(&pmu->kobj, &pfm_pmu_desc_attr_group);
+		kobject_del(&pmu->kobj);
+	}
+	return ret;
+}
+
+/*
+ * when a PMU description module is removed, we also remove
+ * all its information from sysfs, i.e., the pmu_desc subdir
+ * disappears
+ */
+int pfm_sysfs_remove_pmu(struct pfm_pmu_config *pmu)
+{
+	if (pfm_sysfs_init_done == 0)
+		return 0;
+
+	pfm_sysfs_del_pmu_regs(pmu);
+	sysfs_remove_group(&pmu->kobj, &pfm_pmu_desc_attr_group);
+	kobject_del(&pmu->kobj);
+
+	return 0;
+}
diff -Naur linux-2.6.25-org/scripts/oprofile-test linux-2.6.25-id/scripts/oprofile-test
--- linux-2.6.25-org/scripts/oprofile-test	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.25-id/scripts/oprofile-test	2008-04-23 11:22:12.000000000 +0200
@@ -0,0 +1,104 @@
+#!/bin/sh -x
+
+name=oprofile
+prefixes="/usr"
+
+for prefix in ${prefixes}; do
+
+	if [ ! -d ${prefix} ]; then
+		echo "$0: bad prefix: '${prefix}'"
+		echo "$0: -- ${name} FAILED --"
+		exit 1
+	fi
+
+	dir=${prefix}/bin
+
+	if [ ! -d ${dir} ]; then
+		echo "$0: can't find root dir: '${dir}'"
+		echo "$0: -- ${name} FAILED --"
+		exit 1
+	fi
+
+	prog=${dir}/opcontrol
+
+	if [ ! -f ${prog} ] ; then
+		echo "$0: can't find progam: '${prog}'"
+		echo "$0: -- ${name} FAILED --"
+		exit 1
+	fi
+
+	prog=${dir}/opreport
+
+	if [ ! -f ${prog} ] ; then
+		echo "$0: can't find progam: '${prog}'"
+		echo "$0: -- ${name} FAILED --"
+		exit 1
+	fi
+
+	time=`date +%y.%m.%d-%H.%M.%S`
+	log=/root/${name}-dump-`hostname`-`uname -r`-${time}.dmp
+
+	echo "$0: root dir = '${dir}'"
+	echo "$0: log = '$log'"
+
+	${dir}/opcontrol --shutdown
+	${dir}/opcontrol --reset
+	${dir}/opcontrol --reset
+	rm -rf /var/lib/oprofile/*
+
+	if [ ! -f /boot/vmlinux ] ; then
+		echo "$0: can't find: '/boot/vmlinux'"
+		echo "$0: -- ${name} FAILED --"
+		exit 1
+	fi
+
+	${dir}/opcontrol --vmlinux=/boot/vmlinux
+
+	if [ $? -ne 0 ]; then
+		echo "$0: opcontrol --vmlinux failed."
+		echo "$0: -- ${name} FAILED --"
+		exit -1
+	fi
+
+	${dir}/opcontrol --event=cache_hit:10000 --event=cache_miss:10000
+
+	if [ $? -ne 0 ]; then
+		echo "$0: opcontrol --event=cache_hit:10000 --event=cache_miss:10000 failed."
+	fi
+
+	${dir}/opcontrol --verbose --start
+
+	if [ $? -ne 0 ]; then
+		echo "$0: opcontrol --start failed."
+		echo "$0: -- ${name} FAILED --"
+		exit -1
+	fi
+
+	echo sleeping 25 sec...
+	sleep 25
+
+	${dir}/opcontrol --dump
+
+	if [ $? -ne 0 ]; then
+		echo "$0: opcontrol --dump failed."
+		echo "$0: -- ${name} FAILED --"
+		exit -1
+	fi
+
+	${dir}/opreport
+
+	if [ $? -ne 0 ]; then
+		echo "$0: opreport failed."
+		echo "$0: -- ${name} FAILED --"
+		exit -1
+	fi
+
+	${dir}/opcontrol --shutdown
+	${dir}/opcontrol --reset
+	${dir}/opcontrol --reset
+	rm -rf /var/lib/oprofile/*
+
+	echo "$0: ++ ${name} OK ++"
+done
+
+exit 0
